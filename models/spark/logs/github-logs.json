{"author": "ChenMichael", "sha": "", "commit_date": "2021/09/17 16:32:01", "commit_message": "TBD: trigger???", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/ExplainUtils.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/ExplainSuite.scala", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [0, 0, 5]}]}
{"author": "f-thiele", "sha": "", "commit_date": "2021/09/16 15:24:44", "commit_message": "[SPARK-36782] Avoid blocking dispatcher-BlockManagerMaster during UpdateBlockInfo\n\nDelegate task to threadpool and register callback for succesful\ncompletion. Reply to caller once future finished succesfully.", "title": "", "body": "", "failed_tests": ["pyspark.mllib.tests.test_algorithms", "pyspark.ml.tests.test_algorithms", "org.apache.spark.graphx.lib.PageRankSuite", "org.apache.spark.ml.tuning.JavaCrossValidatorSuite", "org.apache.spark.mllib.clustering.KMeansSuite", "org.apache.spark.ml.classification.DecisionTreeClassifierSuite", "org.apache.spark.ml.tree.impl.GradientBoostedTreesSuite", "org.apache.spark.ml.classification.LogisticRegressionSuite", "org.apache.spark.ml.tuning.CrossValidatorSuite", "org.apache.spark.mllib.classification.LogisticRegressionSuite", "org.apache.spark.ml.classification.GBTClassifierSuite", "org.apache.spark.ml.regression.LinearRegressionSuite", "org.apache.spark.ml.regression.DecisionTreeRegressorSuite", "org.apache.spark.ml.regression.RandomForestRegressorSuite", "org.apache.spark.ml.regression.GBTRegressorSuite", "org.apache.spark.ml.tuning.JavaCrossValidatorSuite", "org.apache.spark.ml.clustering.PowerIterationClusteringSuite", "org.apache.spark.ml.classification.RandomForestClassifierSuite", "org.apache.spark.ml.classification.FMClassifierSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala", "additions": "11", "deletions": "6", "changes": "17"}, "updated": [0, 1, 5]}, {"file": {"name": "core/src/test/scala/org/apache/spark/storage/BlockManagerDecommissionIntegrationSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "viirya", "sha": "8db8b50e0621b46e6572ec72c1cc9aeca2a66807", "commit_date": "2021/09/22 18:49:34", "commit_message": "For review comment.", "title": "[SPARK-36797][SQL] Union should resolve nested columns as top-level columns", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\nThis patch proposes to generalize the resolving-by-position behavior to nested columns for Union.\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\nUnion, by the API definition, resolves columns by position. Currently we only follow this behavior at top-level columns, but not nested columns.\r\n\r\nAs we are making nested columns as first-class citizen, the nested-column-only limitation and the difference between top-level column and nested column do not make sense. We should also resolve nested columns like top-level columns for Union.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\nYes. After this change, Union also resolves nested columns by position.\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\n\r\nAdded tests.\r\n", "failed_tests": ["org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SQLQuerySuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala", "additions": "19", "deletions": "4", "changes": "23"}, "updated": [0, 0, 7]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/types/DataType.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSetOperationsSuite.scala", "additions": "32", "deletions": "9", "changes": "41"}, "updated": [0, 2, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala", "additions": "3", "deletions": "4", "changes": "7"}, "updated": [0, 0, 3]}]}
{"author": "viirya", "sha": "82ccaf18d64f46ffe7b5452f001a8ea68931c439", "commit_date": "2021/09/20 21:46:52", "commit_message": "Update for non-DPP case.", "title": "[SPARK-36809][SQL] Remove broadcast for InSubqueryExec used in DPP", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\nThis patch proposes to remove broadcast variable in `InSubqueryExec` which is used in DPP.\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\nCurrently we include a broadcast variable in `InSubqueryExec`. We use it to hold filtering side query result of DPP. It looks weird because we don't use the result in executors but only need the result in the driver during query planning. We already hold the original result, so basically we hold two copied of query result at this moment.\r\n\r\nAnother thing related is, in `pruningHasBenefit` we estimate if DPP pruning has benefit when the join type does not support broadcast. Due to the broadcast variable above, we also check the filtering side against the config `autoBroadcastJoinThreshold`. The config is not for the purpose and it is not a broadcast join. As the broadcast variable is unnecessary, we can remove this check and leave benefit estimation to overhead and pruning size.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\nYes. We don't rely on `autoBroadcastJoinThreshold` for non-broadcast join DPP.\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\n\r\nUpdated existing test.\r\n", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/PlanAdaptiveDynamicPruningFilters.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/PlanAdaptiveSubqueries.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/dynamicpruning/PartitionPruning.scala", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 2, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala", "additions": "14", "deletions": "9", "changes": "23"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DynamicPartitionPruningSuite.scala", "additions": "8", "deletions": "7", "changes": "15"}, "updated": [0, 2, 3]}]}
{"author": "viirya", "sha": "ba4172076f3f8030510632978a0e47d5b720617a", "commit_date": "2021/07/04 06:37:05", "commit_message": "Use config.", "title": "[WIP][SPARK-33625][SQL] Subexpression elimination for whole-stage codegen in Filter", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\nThis patch proposes to enable whole-stage subexpression elimination for Filter.\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\nWe made subexpression elimination available for whole-stage codegen in ProjectExec. Another one operator that frequently runs into subexpressions, is Filter. We should also make whole-stage codegen subexpression elimination in FilterExec too.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\nNo\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\n\r\nUnit test", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [1, 2, 5]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [2, 3, 10]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 3]}]}
{"author": "viirya", "sha": "", "commit_date": "2021/04/15 01:29:22", "commit_message": "Fix custom metric initialization.", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/metric/CustomMetrics.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLAppStatusListener.scala", "additions": "28", "deletions": "12", "changes": "40"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SparkPlanGraph.scala", "additions": "2", "deletions": "4", "changes": "6"}, "updated": [0, 0, 0]}]}
{"author": "karenfeng", "sha": "", "commit_date": "2021/06/09 21:22:58", "commit_message": "Fix docs issue\n\nSigned-off-by: Karen Feng <karen.feng@databricks.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/SparkError.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "ulysses-you", "sha": "2aa2ee06a8110a9d2d7efe36aadc83afd75b71c8", "commit_date": "2021/09/23 01:33:46", "commit_message": "broadcast", "title": "[SPARK-36823][SQL] Support broadcast nested loop join hint for equi-join", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nAdd a new hint `BROADCAST_NL`\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nFor the join if one side is small and other side is large, the shuffle overhead is also very big. Due to the\r\nbhj limitation, we can only broadcast right side for left join and left side for right join. So for the other case, we can try to use `BroadcastNestedLoopJoin` as the join strategy.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nyes, new hint\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nAdd new test in `JoinHintSuite`", "failed_tests": [], "files": [{"file": {"name": "docs/sql-ref-syntax-qry-select-hints.md", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/hints.scala", "additions": "10", "deletions": "1", "changes": "11"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [1, 2, 6]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/LogicalQueryStageStrategy.scala", "additions": "12", "deletions": "6", "changes": "18"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/JoinHintSuite.scala", "additions": "38", "deletions": "0", "changes": "38"}, "updated": [0, 0, 2]}]}
{"author": "ulysses-you", "sha": "a846ecd5221bc4b21416c9c52552cdaa0e683d0d", "commit_date": "2021/09/17 02:02:01", "commit_message": "cleanup", "title": "[SPARK-34980][SQL] Support coalesce partition through union in AQE", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n- Split plan into several groups, and every child of union is a new group\r\n- Coalesce paritition for every group\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\n#### First Issue\r\nThe rule `CoalesceShufflePartitions` can only coalesce paritition if\r\n* leaf node is ShuffleQueryStage\r\n* all shuffle have same partition number\r\n\r\nWith `Union`, it might break the assumption. Let's say we have such plan\r\n```\r\nUnion\r\n   HashAggregate\r\n      ShuffleQueryStage\r\n   FileScan\r\n```\r\n`CoalesceShufflePartitions` can not optimize it and the result partition would be `shuffle partition + FileScan partition` which can be quite lagre.\r\n\r\nIt's better to support partial optimize with `Union`.\r\n\r\n#### Second Issue\r\nthe coalesce partition formule used the **sum value** as the total input size and it's not friendly for union, see\r\n```\r\n// ShufflePartitionsUtil.coalescePartitions\r\nval totalPostShuffleInputSize = mapOutputStatistics.flatMap(_.map(_.bytesByPartitionId.sum)).sum\r\n```\r\n\r\nSo for such case:\r\n```\r\nUnion\r\n   HashAggregate\r\n      ShuffleQueryStage\r\n   HashAggregate\r\n      ShuffleQueryStage\r\n```\r\nThe `CoalesceShufflePartitions` rule will return an unexpected partition number.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nProbably yes, the result partition might changed.\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nAdd test.", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "78", "deletions": "52", "changes": "130"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/CoalesceShufflePartitionsSuite.scala", "additions": "1", "deletions": "3", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "86", "deletions": "1", "changes": "87"}, "updated": [0, 1, 7]}]}
{"author": "ulysses-you", "sha": "5beb51810dfec69964e570d90d0634e5a8e0499d", "commit_date": "2021/08/13 14:26:41", "commit_message": "fix", "title": "[SPARK-36321][K8S] Do not fail application in kubernetes if name is too long", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nUse short string as executor pod name prefix if app name is long.\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nIf we have a long spark app name and start with k8s master, we will get the execption.\r\n```\r\njava.lang.IllegalArgumentException: 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa-89fe2f7ae71c3570' in spark.kubernetes.executor.podNamePrefix is invalid. must conform https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#dns-label-names and the value length <= 47\r\n\tat org.apache.spark.internal.config.TypedConfigBuilder.$anonfun$checkValue$1(ConfigBuilder.scala:108)\r\n\tat org.apache.spark.internal.config.TypedConfigBuilder.$anonfun$transform$1(ConfigBuilder.scala:101)\r\n\tat scala.Option.map(Option.scala:230)\r\n\tat org.apache.spark.internal.config.OptionalConfigEntry.readFrom(ConfigEntry.scala:239)\r\n\tat org.apache.spark.internal.config.OptionalConfigEntry.readFrom(ConfigEntry.scala:214)\r\n\tat org.apache.spark.SparkConf.get(SparkConf.scala:261)\r\n\tat org.apache.spark.deploy.k8s.KubernetesConf.get(KubernetesConf.scala:67)\r\n\tat org.apache.spark.deploy.k8s.KubernetesExecutorConf.<init>(KubernetesConf.scala:147)\r\n\tat org.apache.spark.deploy.k8s.KubernetesConf$.createExecutorConf(KubernetesConf.scala:231)\r\n\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$requestNewExecutors$2(ExecutorPodsAllocator.scala:367)\r\n```\r\nUse app name as the executor pod name is the Spark internal behavior and we should not make application failure.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nyes\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nAdd test\r\n\r\nthe new log:\r\n```\r\n21/07/28 09:35:53 INFO SparkEnv: Registering OutputCommitCoordinator\r\n21/07/28 09:35:54 INFO Utils: Successfully started service 'SparkUI' on port 41926.\r\n21/07/28 09:35:54 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://:41926\r\n21/07/28 09:35:54 WARN KubernetesClusterManager: Use spark-c460617aeac0fda9 as the executor pod's name prefix due to spark.app.name is too long. Please set 'spark.kubernetes.executor.podNamePrefix' if you need a custom executor pod's name prefix.\r\n21/07/28 09:35:54 INFO SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file\r\n21/07/28 09:35:55 WARN Utils: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.\r\n```\r\n\r\nverify the config:\r\n![image](https://user-images.githubusercontent.com/12025282/127258223-fbcaaac8-451d-4c55-8c09-e802511a510d.png)\r\n\r\nverify the executor pod name\r\n![image](https://user-images.githubusercontent.com/12025282/127258284-be15b862-b826-4440-9a11-023d69c61fc4.png)\r\n", "failed_tests": [], "files": [{"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPi.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPiWithoutAppName.scala", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "4", "deletions": "14", "changes": "18"}, "updated": [1, 1, 5]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala", "additions": "19", "deletions": "7", "changes": "26"}, "updated": [0, 0, 3]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesUtils.scala", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala", "additions": "10", "deletions": "6", "changes": "16"}, "updated": [0, 0, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterManager.scala", "additions": "10", "deletions": "2", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/KubernetesConfSuite.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 0, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 3]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/BasicTestsSuite.scala", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 1, 1]}]}
{"author": "ulysses-you", "sha": "", "commit_date": "2020/12/15 01:33:39", "commit_message": "Merge remote-tracking branch 'upstream/master'", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": ".github/workflows/build_and_test.yml", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 6, 14]}, {"file": {"name": "LICENSE-binary", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "core/pom.xml", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [1, 3, 5]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/PythonRunner.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala", "additions": "29", "deletions": "25", "changes": "54"}, "updated": [1, 3, 8]}, {"file": {"name": "core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 1, 3]}, {"file": {"name": "dev/deps/spark-deps-hadoop-2.7-hive-2.3", "additions": "14", "deletions": "13", "changes": "27"}, "updated": [3, 8, 17]}, {"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "14", "deletions": "12", "changes": "26"}, "updated": [3, 8, 17]}, {"file": {"name": "dev/tox.ini", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 2, 2]}, {"file": {"name": "docs/running-on-kubernetes.md", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [1, 2, 5]}, {"file": {"name": "docs/sql-migration-guide.md", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [2, 4, 14]}, {"file": {"name": "docs/sql-ref-ansi-compliance.md", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [1, 4, 9]}, {"file": {"name": "external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaOffsetReaderAdmin.scala", "additions": "25", "deletions": "56", "changes": "81"}, "updated": [0, 2, 2]}, {"file": {"name": "external/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaRelationSuite.scala", "additions": "5", "deletions": "18", "changes": "23"}, "updated": [0, 2, 3]}, {"file": {"name": "launcher/src/main/java/org/apache/spark/launcher/SparkSubmitCommandBuilder.java", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "pom.xml", "additions": "5", "deletions": "6", "changes": "11"}, "updated": [3, 10, 24]}, {"file": {"name": "project/SparkBuild.scala", "additions": "26", "deletions": "1", "changes": "27"}, "updated": [1, 2, 5]}, {"file": {"name": "python/pyspark/context.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 2, 6]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "32", "deletions": "4", "changes": "36"}, "updated": [2, 3, 6]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Constants.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 1, 3]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/SparkPod.scala", "additions": "10", "deletions": "1", "changes": "11"}, "updated": [1, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/DriverCommandFeatureStep.scala", "additions": "32", "deletions": "5", "changes": "37"}, "updated": [1, 1, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/KubernetesFeatureConfigStep.scala", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [1, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/submit/KubernetesDriverBuilder.scala", "additions": "7", "deletions": "1", "changes": "8"}, "updated": [1, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/ExecutorPodsSnapshot.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 1, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesExecutorBuilder.scala", "additions": "7", "deletions": "1", "changes": "8"}, "updated": [1, 1, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/PodBuilderSuite.scala", "additions": "76", "deletions": "0", "changes": "76"}, "updated": [1, 1, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/DriverCommandFeatureStepSuite.scala", "additions": "50", "deletions": "7", "changes": "57"}, "updated": [1, 1, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/submit/KubernetesDriverBuilderSuite.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [1, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesExecutorBuilderSuite.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [1, 1, 2]}, {"file": {"name": "resource-managers/kubernetes/docker/src/main/dockerfiles/spark/entrypoint.sh", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [1, 1, 3]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/README.md", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/DepsTestsSuite.scala", "additions": "63", "deletions": "22", "changes": "85"}, "updated": [1, 3, 4]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [1, 3, 5]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesTestComponents.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [1, 1, 2]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/ProcessUtils.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [1, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/Utils.scala", "additions": "7", "deletions": "2", "changes": "9"}, "updated": [1, 2, 5]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/tests/py_container_checks.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/tests/python_executable_check.py", "additions": "40", "deletions": "0", "changes": "40"}, "updated": [1, 1, 1]}, {"file": {"name": "resource-managers/yarn/pom.xml", "additions": "0", "deletions": "7", "changes": "7"}, "updated": [1, 3, 5]}, {"file": {"name": "sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 2, 5]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/SupportsPartitionManagement.java", "additions": "9", "deletions": "5", "changes": "14"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/distributions/ClusteredDistribution.java", "additions": "35", "deletions": "0", "changes": "35"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/distributions/Distribution.java", "additions": "28", "deletions": "0", "changes": "28"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/distributions/Distributions.java", "additions": "56", "deletions": "0", "changes": "56"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/distributions/OrderedDistribution.java", "additions": "35", "deletions": "0", "changes": "35"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/distributions/UnspecifiedDistribution.java", "additions": "28", "deletions": "0", "changes": "28"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/expressions/Expressions.java", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/expressions/NullOrdering.java", "additions": "42", "deletions": "0", "changes": "42"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/expressions/SortDirection.java", "additions": "42", "deletions": "0", "changes": "42"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/expressions/SortOrder.java", "additions": "43", "deletions": "0", "changes": "43"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/RequiresDistributionAndOrdering.java", "additions": "57", "deletions": "0", "changes": "57"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/Write.java", "additions": "65", "deletions": "0", "changes": "65"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/WriteBuilder.java", "additions": "29", "deletions": "12", "changes": "41"}, "updated": [2, 2, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/QueryCompilationErrors.scala", "additions": "167", "deletions": "2", "changes": "169"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala", "additions": "72", "deletions": "75", "changes": "147"}, "updated": [1, 11, 33]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [1, 7, 16]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolvePartitionSpec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 3, 8]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/SessionCatalog.scala", "additions": "59", "deletions": "11", "changes": "70"}, "updated": [1, 6, 11]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/CallMethodViaReflection.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala", "additions": "19", "deletions": "8", "changes": "27"}, "updated": [1, 3, 11]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala", "additions": "61", "deletions": "1", "changes": "62"}, "updated": [0, 2, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala", "additions": "37", "deletions": "8", "changes": "45"}, "updated": [3, 12, 33]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/ParseDriver.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala", "additions": "0", "deletions": "16", "changes": "16"}, "updated": [1, 4, 9]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/statements.scala", "additions": "0", "deletions": "8", "changes": "8"}, "updated": [1, 6, 16]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2Commands.scala", "additions": "29", "deletions": "3", "changes": "32"}, "updated": [2, 8, 21]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala", "additions": "7", "deletions": "4", "changes": "11"}, "updated": [1, 2, 5]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/NumberConverter.scala", "additions": "13", "deletions": "51", "changes": "64"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/distributions/distributions.scala", "additions": "59", "deletions": "0", "changes": "59"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/expressions/expressions.scala", "additions": "96", "deletions": "0", "changes": "96"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "24", "deletions": "1", "changes": "25"}, "updated": [1, 14, 37]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisSuite.scala", "additions": "22", "deletions": "0", "changes": "22"}, "updated": [1, 3, 7]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/catalog/ExternalCatalogSuite.scala", "additions": "7", "deletions": "2", "changes": "9"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CastSuite.scala", "additions": "34", "deletions": "7", "changes": "41"}, "updated": [1, 3, 9]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CodeGenerationSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 2]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/MathExpressionsSuite.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [1, 2, 2]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [0, 2, 4]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala", "additions": "37", "deletions": "6", "changes": "43"}, "updated": [2, 9, 24]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/NumberConverterSuite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/TimestampFormatterSuite.scala", "additions": "8", "deletions": "2", "changes": "10"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/InMemoryPartitionTable.scala", "additions": "0", "deletions": "3", "changes": "3"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/SupportsPartitionManagementSuite.scala", "additions": "21", "deletions": "2", "changes": "23"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [1, 4, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala", "additions": "4", "deletions": "19", "changes": "23"}, "updated": [2, 9, 22]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala", "additions": "5", "deletions": "29", "changes": "34"}, "updated": [1, 3, 13]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/command/cache.scala", "additions": "2", "deletions": "50", "changes": "52"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/command/views.scala", "additions": "4", "deletions": "15", "changes": "19"}, "updated": [2, 4, 7]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/CacheTableExec.scala", "additions": "89", "deletions": "0", "changes": "89"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala", "additions": "20", "deletions": "4", "changes": "24"}, "updated": [2, 9, 22]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/RefreshTableExec.scala", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [1, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ShowPartitionsExec.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [1, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetScanBuilder.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStore.scala", "additions": "11", "deletions": "15", "changes": "26"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreConf.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/functions.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 7]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/internal/CatalogImpl.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/internal/SharedState.scala", "additions": "10", "deletions": "3", "changes": "13"}, "updated": [0, 2, 4]}, {"file": {"name": "sql/core/src/test/resources/hive-site.xml", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/datetime.sql", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [1, 3, 5]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/string-functions.sql", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/datetime.sql.out", "additions": "10", "deletions": "1", "changes": "11"}, "updated": [1, 3, 5]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/string-functions.sql.out", "additions": "67", "deletions": "1", "changes": "68"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/datetime-legacy.sql.out", "additions": "9", "deletions": "1", "changes": "10"}, "updated": [1, 3, 5]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/datetime.sql.out", "additions": "9", "deletions": "1", "changes": "10"}, "updated": [1, 3, 5]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/string-functions.sql.out", "additions": "67", "deletions": "1", "changes": "68"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/CachedTableSuite.scala", "additions": "46", "deletions": "0", "changes": "46"}, "updated": [1, 4, 7]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "38", "deletions": "0", "changes": "38"}, "updated": [1, 3, 4]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/ExplainSuite.scala", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [2, 2, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/MathFunctionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala", "additions": "27", "deletions": "14", "changes": "41"}, "updated": [2, 11, 34]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/SQLViewSuite.scala", "additions": "8", "deletions": "17", "changes": "25"}, "updated": [2, 5, 15]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/SQLViewTestSuite.scala", "additions": "50", "deletions": "1", "changes": "51"}, "updated": [1, 5, 5]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/SparkSqlParserSuite.scala", "additions": "0", "deletions": "29", "changes": "29"}, "updated": [0, 2, 6]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/AlterTableAddPartitionSuiteBase.scala", "additions": "19", "deletions": "1", "changes": "20"}, "updated": [1, 3, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/ShowPartitionsSuiteBase.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/v1/AlterTableAddPartitionSuite.scala", "additions": "1", "deletions": "19", "changes": "20"}, "updated": [0, 2, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/v2/AlterTableAddPartitionSuite.scala", "additions": "1", "deletions": "18", "changes": "19"}, "updated": [0, 2, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/StateStoreSuite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 1, 4]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/internal/SharedStateSuite.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkExecuteStatementOperation.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 2, 5]}, {"file": {"name": "sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkSQLSessionManager.scala", "additions": "5", "deletions": "3", "changes": "8"}, "updated": [1, 1, 3]}, {"file": {"name": "sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/HiveThriftServer2Suites.scala", "additions": "118", "deletions": "81", "changes": "199"}, "updated": [1, 1, 3]}, {"file": {"name": "sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/JdbcConnectionUriSuite.scala", "additions": "0", "deletions": "70", "changes": "70"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/SparkMetadataOperationSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/SparkThriftServerProtocolVersionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/ThriftServerWithSparkContextSuite.scala", "additions": "37", "deletions": "0", "changes": "37"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/UISeleniumSuite.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/hive/compatibility/src/test/scala/org/apache/spark/sql/hive/execution/HiveCompatibilitySuite.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [1, 3, 5]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala", "additions": "13", "deletions": "2", "changes": "15"}, "updated": [0, 3, 6]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveExternalCatalogVersionsSuite.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/StatisticsSuite.scala", "additions": "8", "deletions": "4", "changes": "12"}, "updated": [1, 3, 6]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/client/VersionsSuite.scala", "additions": "22", "deletions": "1", "changes": "23"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/HiveDDLSuite.scala", "additions": "103", "deletions": "31", "changes": "134"}, "updated": [2, 6, 10]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/command/AlterTableAddPartitionSuite.scala", "additions": "0", "deletions": "18", "changes": "18"}, "updated": [0, 2, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/test/TestHive.scala", "additions": "2", "deletions": "3", "changes": "5"}, "updated": [0, 2, 7]}]}
{"author": "HyukjinKwon", "sha": "", "commit_date": "2020/08/12 10:07:02", "commit_message": "Test uploading Junit test report artifact", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": ".github/workflows/master.yml", "additions": "12", "deletions": "139", "changes": "151"}, "updated": [0, 3, 9]}, {"file": {"name": ".github/workflows/test_report.yml", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [0, 0, 0]}, {"file": {"name": "common/unsafe/src/test/scala/org/apache/spark/unsafe/types/UTF8StringPropertyCheckSuite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "dev/run-tests.py", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [0, 4, 11]}, {"file": {"name": "python/pyspark/sql/tests/test_arrow.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 2, 6]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 1, 3]}]}
{"author": "pralabhkumar", "sha": "a659e4cbdbef6f2fc7fd8c1d7749a545ad275f32", "commit_date": "2021/09/17 19:54:17", "commit_message": "Commented test cases which suppose to fail for ArrayType(TimeStamp)", "title": "[WIP][SPARK-32285][PYTHON] Add PySpark support for nested timestamps with arrow", "body": "### What changes were proposed in this pull request?\r\nAdded nested timestamp support for Pyspark with Arrow\r\n\r\n\r\n\r\n### Why are the changes needed?\r\nThis change is required to convert ArrayType(TimeStamp) to pandas via arrow. \r\n\r\n\r\n### Does this PR introduce any user-facing change?\r\nYes user will be able to convert DF which contain Arraytype(Timestamp) to pandas\r\n\r\n### How was this patch tested?\r\nunit tests", "failed_tests": ["pyspark.pandas.tests.data_type_ops.test_complex_ops", "pyspark.pandas.tests.test_dataframe"], "files": [{"file": {"name": "python/pyspark/sql/pandas/conversion.py", "additions": "23", "deletions": "6", "changes": "29"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/sql/pandas/types.py", "additions": "43", "deletions": "6", "changes": "49"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_arrow.py", "additions": "72", "deletions": "53", "changes": "125"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_pandas_cogrouped_map.py", "additions": "11", "deletions": "11", "changes": "22"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/tests/test_pandas_grouped_map.py", "additions": "11", "deletions": "11", "changes": "22"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/tests/test_pandas_udf_grouped_agg.py", "additions": "13", "deletions": "13", "changes": "26"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/tests/test_pandas_udf_scalar.py", "additions": "11", "deletions": "11", "changes": "22"}, "updated": [0, 0, 0]}]}
{"author": "pralabhkumar", "sha": "73565a28a55ee52266f04416ba7b1bbe1dcb94dc", "commit_date": "2021/09/13 06:44:48", "commit_message": "Removed curly braces for single import", "title": "[SPARK-36622][CORE] Making spark.history.kerberos.principal _HOST compliant", "body": "### What changes were proposed in this pull request?\r\nspark.history.kerberos.principal can have _HOST , which will be replaced by host canonical address\r\n\r\n### Why are the changes needed?\r\nThis change is required for user to provide prinicipal _HOST complaint . User don't need to hardcode the History server URL . This is in line with Hiveserver2, livy server and other hadoop components.  \r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nYes, users can now add _HOST in the spark.history.kerberos.principal\r\n\r\n### How was this patch tested?\r\n\r\nunit tests/local testing\r\n\r\n", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/deploy/SparkHadoopUtilSuite.scala", "additions": "13", "deletions": "0", "changes": "13"}, "updated": [0, 0, 0]}]}
{"author": "pralabhkumar", "sha": "", "commit_date": "2021/09/03 18:18:41", "commit_message": "Dummy commit", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}]}
{"author": "q2w", "sha": "beb0ef17e61581d9b0996860f89af7c6267743f6", "commit_date": "2021/06/07 05:25:55", "commit_message": "protect access to executorsPendingDecommission by CoarseGrainedSchedulerBackend", "title": "[SPARK-35627][CORE] Decommission executors in batches to not overload network bandwidth", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error message, please read the guideline first:\r\n     https://spark.apache.org/error-message-guidelines.html\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nThis PR is adding a thread which will run at scheduled interval to ask a batch of executors to start decommissioning themselves.\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nCurrenlty, each executor is asked to starts offloading rdd and shuffle blocks as soon it is decommissioned. This can overload the network bandwidth of the application.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nNo\r\n\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nUT in progress", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala", "additions": "43", "deletions": "6", "changes": "49"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/WorkerDecommissionSuite.scala", "additions": "27", "deletions": "0", "changes": "27"}, "updated": [0, 0, 0]}]}
{"author": "q2w", "sha": "c462df9035aaec6624fc90219a1ab7946e743ca5", "commit_date": "2021/06/28 03:45:20", "commit_message": "fix UT faling in BlockManagerDecommissionUnitSuite because of change in number of arguments in replicateBlocks method in BlockManager", "title": "[SPARK-35754][CORE] Add config to put migrating blocks on disk only", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error message, please read the guideline first:\r\n     https://spark.apache.org/error-message-guidelines.html\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nThis PR adds a config which makes block manager decommissioner to migrate block data on disk only. \r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nWhile migrating block data, if enough memory is not available on peer block managers existing blocks are dropped. After this PR migrating blocks won't drop any existing blocks. \r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nNo\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nUT in BlockManagerSuite", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 1, 4]}, {"file": {"name": "core/src/main/scala/org/apache/spark/storage/BlockManager.scala", "additions": "6", "deletions": "3", "changes": "9"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/storage/BlockManagerDecommissioner.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 1, 1]}, {"file": {"name": "core/src/test/scala/org/apache/spark/storage/BlockManagerDecommissionUnitSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/storage/BlockManagerSuite.scala", "additions": "20", "deletions": "0", "changes": "20"}, "updated": [0, 1, 5]}]}
{"author": "rmcyang", "sha": "", "commit_date": "2021/09/03 22:57:21", "commit_message": "SPARK-33781 Improve caching of MergeStatus on the executor side", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/MapOutputTracker.scala", "additions": "127", "deletions": "38", "changes": "165"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 1, 6]}, {"file": {"name": "core/src/test/scala/org/apache/spark/MapOutputTrackerSuite.scala", "additions": "57", "deletions": "4", "changes": "61"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/test/scala/org/apache/spark/MapStatusesSerDeserBenchmark.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}]}
{"author": "xinrong-databricks", "sha": "87172874344406fac65abb26f072c06855674b7f", "commit_date": "2021/09/01 18:59:43", "commit_message": "refactor", "title": "[SPARK-36628][PYTHON] Implement `__getitem__`  of label-based MultiIndex select", "body": "### What changes were proposed in this pull request?\r\nImplement `__getitem__`  of label-based MultiIndex select\r\n\r\n\r\n### Why are the changes needed?\r\nIncrease pandas API coverage\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes, reading values by label-based MultiIndex select is supported now.\r\n\r\n```py\r\n>>> psdf = ps.DataFrame(\r\n...             {\"a\": [1, 2, 3, 4, 5, 6, 7, 8, 9], \"b\": [4, 5, 6, 3, 2, 1, 0, 0, 0]},\r\n...             index=[0, 1, 3, 5, 6, 8, 9, 9, 9],\r\n...         )\r\n>>> psdf = psdf.set_index(\"b\", append=True)\r\n>>> psdf\r\n     a\r\n  b   \r\n0 4  1\r\n1 5  2\r\n3 6  3\r\n5 3  4\r\n6 2  5\r\n8 1  6\r\n9 0  7\r\n  0  8\r\n  0  9\r\n>>> psdf.loc[6, 2]\r\n     a\r\n  b   \r\n6 2  5\r\n```\r\n\r\n### How was this patch tested?\r\nUnit tests.\r\n", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "74", "deletions": "18", "changes": "92"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}]}
{"author": "xinrong-databricks", "sha": "8b7b395c0faa78d8809489eaf6f88a013fcfe2a0", "commit_date": "2021/08/10 21:37:08", "commit_message": "empty", "title": "[WIP][SPARK-36397][PYTHON] Implement DataFrame.mode", "body": "### What changes were proposed in this pull request?\r\nImplement DataFrame.mode (along index axis).\r\n\r\n\r\n### Why are the changes needed?\r\nGet the mode(s) of each element along the selected axis is a common functionality, which is supported in pandas. We should support that.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes. `DataFrame.mode` can be used now.\r\n\r\n```py\r\n>>> psdf = ps.DataFrame(\r\n...     [(\"bird\", 2, 2), (\"mammal\", 4, np.nan), (\"arthropod\", 8, 0), (\"bird\", 2, np.nan)],\r\n...     index=(\"falcon\", \"horse\", \"spider\", \"ostrich\"),\r\n...     columns=(\"species\", \"legs\", \"wings\"),\r\n... )\r\n>>> psdf\r\n           species  legs  wings                                                 \r\nfalcon        bird     2    2.0\r\nhorse       mammal     4    NaN\r\nspider   arthropod     8    0.0\r\nostrich       bird     2    NaN\r\n\r\n>>> psdf.mode()\r\n  species  legs  wings\r\n0    bird   2.0    0.0\r\n1    None   NaN    2.0\r\n\r\n>>> psdf.mode(dropna=False)\r\n  species  legs  wings\r\n0    bird     2    NaN\r\n\r\n>>> psdf.mode(numeric_only=True)\r\n   legs  wings\r\n0   2.0    0.0\r\n1   NaN    2.0\r\n```\r\n\r\n### How was this patch tested?\r\nUnit tests.\r\n", "failed_tests": ["pyspark.pandas.tests.test_dataframe"], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/frame.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "103", "deletions": "1", "changes": "104"}, "updated": [0, 5, 20]}, {"file": {"name": "python/pyspark/pandas/missing/frame.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [0, 2, 6]}]}
{"author": "xinrong-databricks", "sha": "", "commit_date": "2021/04/13 17:37:14", "commit_message": "Misc unit tests", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "dev/sparktestsupport/modules.py", "additions": "12", "deletions": "0", "changes": "12"}, "updated": [2, 4, 4]}, {"file": {"name": "python/pyspark/pandas/tests/test_categorical.py", "additions": "475", "deletions": "0", "changes": "475"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_csv.py", "additions": "434", "deletions": "0", "changes": "434"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_expanding.py", "additions": "306", "deletions": "0", "changes": "306"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "2815", "deletions": "0", "changes": "2815"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "1334", "deletions": "0", "changes": "1334"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_namespace.py", "additions": "337", "deletions": "0", "changes": "337"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_repr.py", "additions": "189", "deletions": "0", "changes": "189"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_reshape.py", "additions": "313", "deletions": "0", "changes": "313"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_rolling.py", "additions": "184", "deletions": "0", "changes": "184"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_sql.py", "additions": "50", "deletions": "0", "changes": "50"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_stats.py", "additions": "412", "deletions": "0", "changes": "412"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_window.py", "additions": "316", "deletions": "0", "changes": "316"}, "updated": [0, 0, 0]}]}
{"author": "gaoyajun02", "sha": "", "commit_date": "2021/06/29 08:40:52", "commit_message": "[SPARK-35920][FOLLOWUP][BUILD] Fix Kryo Shaded dependency", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/pom.xml", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "pom.xml", "additions": "7", "deletions": "2", "changes": "9"}, "updated": [2, 9, 33]}]}
{"author": "zhengruifeng", "sha": "6b434b0baad47063ba1eca1f8f788e35e0af39b3", "commit_date": "2021/09/16 02:51:06", "commit_message": "move agg stringArgs to subclasses", "title": "[SPARK-36638][SQL] Generalize OptimizeSkewedJoin", "body": "### What changes were proposed in this pull request?\r\nThis PR aims to generalize `OptimizeSkewedJoin` to support all patterns that can be handled by current _split-duplicate_ strategy:\r\n\r\n1, find the _splittable_ shuffle query stages by the semantics of internal nodes;\r\n\r\n2, for each _splittable_ shuffle query stage, check whether skew partitions exists, if true, split them into specs;\r\n\r\n3, handle _Combinatorial Explosion_: for each skew partition, check whether the combination number is too large, if so, re-split the stages to keep a reasonable number of combinations. For example, for partition 0, stage A/B/C are split into 100/100/100 specs, respectively. Then there are 1M combinations, which is too large, and will cause performance regression.\r\n\r\n4, attach new specs to shuffle query stages;\r\n\r\n\r\n### Why are the changes needed?\r\nto Generalize OptimizeSkewedJoin \r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\ntwo additional configs are added\r\n\r\n\r\n### How was this patch tested?\r\nexisting testsuites, added testsuites, some cases on our productive system\r\n", "failed_tests": ["org.apache.spark.sql.TPCDSModifiedPlanStabilitySuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilityWithStatsSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "19", "deletions": "0", "changes": "19"}, "updated": [1, 2, 8]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "316", "deletions": "147", "changes": "463"}, "updated": [1, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/BaseAggregateExec.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/SortAggregateExec.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowExec.scala", "additions": "14", "deletions": "7", "changes": "21"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/WholeStageCodegenSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "227", "deletions": "0", "changes": "227"}, "updated": [1, 1, 7]}]}
{"author": "zhengruifeng", "sha": "a9888ba9458d012ab71644bc0808e8123955a824", "commit_date": "2021/07/12 06:18:19", "commit_message": "init\n\ninit\n\ninit", "title": "[SPARK-36087][SQL][WIP] An Impl of skew key detection and data inflation optimization", "body": "### What changes were proposed in this pull request?\r\n1, introduce `ShuffleExecAccumulator` in `ShuffleExchangeExec` to support arbitrary statistics;\r\n\r\n2, impl a key sampling `ShuffleExecAccumulator` to detect skew keys and show debug info on SparkUI;\r\n\r\n3, in `OptimizeSkewedJoin`, estimate the joined size of each partition based on the sampled keys, and split a partition if it is not split yet and its estimated joined size is too larger.\r\n\r\n\r\n### Why are the changes needed?\r\n1, make it easy to add a new statistics which can be used in AQE rules;\r\n2, showing skew info on sparkUI is usefully;\r\n3, spliting partitions based on joined size can resolve data inflation;\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes, new features are added\r\n\r\n\r\n### How was this patch tested?\r\nadded testsuites\r\n", "failed_tests": ["org.apache.spark.sql.TPCDSModifiedPlanStabilityWithStatsSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "36", "deletions": "0", "changes": "36"}, "updated": [0, 8, 16]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeDataInflation.scala", "additions": "125", "deletions": "0", "changes": "125"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeLocalShuffleReader.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "58", "deletions": "15", "changes": "73"}, "updated": [0, 2, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "7", "deletions": "2", "changes": "9"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala", "additions": "79", "deletions": "1", "changes": "80"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExecAccumulator.scala", "additions": "552", "deletions": "0", "changes": "552"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/ShuffleExecAccumulatorSuite.scala", "additions": "155", "deletions": "0", "changes": "155"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "137", "deletions": "1", "changes": "138"}, "updated": [0, 8, 16]}]}
{"author": "zhengruifeng", "sha": "", "commit_date": "2021/04/15 02:22:09", "commit_message": "init", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "mllib-local/src/main/scala/org/apache/spark/ml/linalg/BLAS.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "mllib-local/src/main/scala/org/apache/spark/ml/linalg/Matrices.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 1, 1]}, {"file": {"name": "mllib/src/main/scala/org/apache/spark/ml/classification/GBTClassifier.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "mllib/src/main/scala/org/apache/spark/ml/classification/LogisticRegression.scala", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [1, 1, 3]}, {"file": {"name": "mllib/src/main/scala/org/apache/spark/ml/recommendation/ALS.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [1, 1, 1]}, {"file": {"name": "mllib/src/main/scala/org/apache/spark/ml/regression/GBTRegressor.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}]}
{"author": "shahidki31", "sha": "", "commit_date": "2021/05/11 15:08:33", "commit_message": "Don't allow to set spark.driver.cores=0", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/deploy/SparkSubmitArguments.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}]}
{"author": "warrenzhu25", "sha": "f9057749f7d7d25d396c03a8041a0a55e97148ab", "commit_date": "2021/09/20 17:27:07", "commit_message": "update version in doc", "title": "[SPARK-36793][K8S] Support write container stdout/stderr to file", "body": "### What changes were proposed in this pull request?\r\nSupport write container stdout/stderr to file\r\n\r\n### Why are the changes needed?\r\nIf users want to sidecar logging agent to send stdout/stderr to external log storage,  only way is to change entrypoint.sh, which might break compatibility with community version.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes. User can enable this feature by spark config.\r\n\r\n### How was this patch tested?\r\nAdded UT in BasicDriverFeatureStepSuite and BasicExecutorFeatureStepSuite\r\n", "failed_tests": [], "files": [{"file": {"name": "docs/running-on-kubernetes.md", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [0, 0, 4]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Constants.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicDriverFeatureStep.scala", "additions": "8", "deletions": "1", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala", "additions": "8", "deletions": "1", "changes": "9"}, "updated": [0, 0, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicDriverFeatureStepSuite.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala", "additions": "13", "deletions": "0", "changes": "13"}, "updated": [0, 0, 1]}, {"file": {"name": "resource-managers/kubernetes/docker/src/main/dockerfiles/spark/entrypoint.sh", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [0, 0, 1]}]}
{"author": "warrenzhu25", "sha": "", "commit_date": "2021/06/02 21:33:52", "commit_message": "[SPARK-34777][UI] StagePage input/output size records not show when records greater than zero", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/stagepage.js", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala", "additions": "6", "deletions": "2", "changes": "8"}, "updated": [0, 0, 1]}]}
{"author": "fsamuel-bs", "sha": "279c063d88fb12071f6142386cc3ab66bf369a87", "commit_date": "2021/08/31 17:25:55", "commit_message": "docs", "title": "[SPARK-36627][CORE] Fix java deserialization of proxy classes", "body": "## Upstream SPARK-XXXXX ticket and PR link (if not applicable, explain)\r\nhttps://issues.apache.org/jira/browse/SPARK-36627\r\n\r\n## What changes were proposed in this pull request?\r\nSee issue above for issue description.\r\n\r\n### Why are the changes needed?\r\nSpark deserialization fails with no recourse for the user.\r\n\r\n### Does this PR introduce any user-facing change?\r\nNo.\r\n\r\n### How was this patch tested?\r\nUnit tests.", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/serializer/JavaSerializer.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/java/org/apache/spark/serializer/ProxySerializerTest.java", "additions": "33", "deletions": "0", "changes": "33"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/serializer/JavaSerializerSuite.scala", "additions": "24", "deletions": "2", "changes": "26"}, "updated": [0, 0, 0]}]}
{"author": "senthh", "sha": "2db3bf31572ebd15ca7009fa366f56a37af3f74f", "commit_date": "2021/09/19 18:26:59", "commit_message": "[SPARK-36801][DOCUMENTATION] ADD CHANGES IN SPARK SQL JDBC DOCUMENT", "title": "[SPARK-36801][DOCS] ADD \"All columns are automatically converted to be nullable for compatibility reasons.\" IN SPARK SQL JDBC DOCUMENT", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nAdded a line \"All columns are automatically converted to be nullable for compatibility reasons.\" in Documentation[1].\r\n\r\n Ref:\r\n\r\n[1 ]https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#jdbc-to-other-databases\r\n\r\n \r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n<p>Reading using Spark SQL jdbc DataSource\u00a0 does not maintain nullable type and changes \"non nullable\" columns to \"nullable\".</p>\r\n\r\n<p>\u00a0</p>\r\n\r\n<p>For example:</p>\r\n\r\n<p>mysql&gt; CREATE TABLE Persons(Id int NOT NULL, FirstName varchar(255), LastName varchar(255), Age int);<br>\r\nQuery OK, 0 rows affected (0.04 sec)</p>\r\n\r\n\u00a0Spark-shell:\r\n\r\nval df = spark.read.format(\"jdbc\")\u2028.option(\"database\",\"Test_DB\")\u2028.option(\"user\", \"root\")\u2028.option(\"password\", \"\")\u2028.option(\"driver\", \"com.mysql.cj.jdbc.Driver\")\u2028.option(\"url\", \"jdbc:mysql://localhost:3306/Test_DB\")\u2028.option(\"query\", \"(select * from Persons)\")\u2028.load()\u2028df.printSchema()\r\n\u00a0\r\noutput:\r\n\u00a0\r\nroot\r\n\u2013 Id: integer (nullable = true)\r\n\u2013 FirstName: string (nullable = true)\r\n\u2013 LastName: string (nullable = true)\r\n\u2013 Age: integer (nullable = true)\r\n\r\n<p>So we need to add a note, in Documentation<span class=\"error\">[1]</span>, \"All columns are automatically converted to be nullable for compatibility reasons.\"</p>\r\n\r\n<p>\u00a0Ref:</p>\r\n\r\n<p><span class=\"error\">[1 ]</span><a href=\"https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#jdbc-to-other-databases\" class=\"external-link\" rel=\"nofollow\" title=\"Follow link\">https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#jdbc-to-other-databases</a></p>\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nYes, Document changes\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nTested using \"SKIP_API=1 bundle exec jekyll build\"", "failed_tests": [], "files": [{"file": {"name": "docs/sql-data-sources-jdbc.md", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 1]}]}
{"author": "senthh", "sha": "", "commit_date": "2021/07/29 10:46:49", "commit_message": "SPARK-36327 -  Spark sql creates staging dir inside database directory rather than creating inside table directory", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/SaveAsHiveFile.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "cxzl25", "sha": "bec1e6d7ba85b3486ca7b2e16ecd453c1b98dd6e", "commit_date": "2021/09/18 17:24:39", "commit_message": "Pass queryExecution name in CLI when only select query.", "title": "[SPARK-36799][SQL] Pass queryExecution name in CLI when only select query", "body": "### What changes were proposed in this pull request?\r\nWhen sql is only a select query, call `SQLExecution.withNewExecutionId` and specify `collect` as `executionName` so that `QueryExecutionListener` can get the query.\r\n### Why are the changes needed?\r\nNow when in spark-sql CLI, `QueryExecutionListener` can receive command , but not select query.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\nmanual test.", "failed_tests": [], "files": [{"file": {"name": "sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkSQLDriver.scala", "additions": "6", "deletions": "2", "changes": "8"}, "updated": [0, 0, 0]}]}
{"author": "cxzl25", "sha": "7ec3ccce0e3b7ac445b7ce8224c2eac9bab420bd", "commit_date": "2021/09/08 09:26:29", "commit_message": "trigger test", "title": "[SPARK-35437][SQL] Use expressions to filter Hive partitions at client side", "body": "### What changes were proposed in this pull request?\r\nImprove partition filtering speed and reduce metastore pressure.\r\nWe can first pull all the partition names, filter by expressions, and then obtain detailed information about the corresponding partitions from the MetaStore Server.\r\n\r\n### Why are the changes needed?\r\nWhen `convertFilters` cannot take effect, cannot filter the queried partitions in advance on the hive MetaStore Server. At this time, `getAllPartitionsOf` will get all partition details.\r\n\r\nWhen the Hive client cannot use the server filter, it will first obtain the values of all partitions, and then filter.\r\n\r\nWhen we have a table with a lot of partitions and there is no way to filter it on the MetaStore Server, we will get all the partition details and filter it on the client side. This is slow and puts a lot of pressure on the MetaStore Server.\r\n\r\n\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n\r\n### How was this patch tested?\r\nAdd UT\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/ExternalCatalogUtils.scala", "additions": "27", "deletions": "17", "changes": "44"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "13", "deletions": "0", "changes": "13"}, "updated": [0, 1, 12]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveShim.scala", "additions": "68", "deletions": "11", "changes": "79"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/client/HivePartitionFilteringSuite.scala", "additions": "58", "deletions": "1", "changes": "59"}, "updated": [0, 0, 1]}]}
{"author": "cxzl25", "sha": "d3086a535794dcd4984d30ea66063f8eab5ad95a", "commit_date": "2021/08/09 12:31:16", "commit_message": "Merge remote-tracking branch 'origin' into SPARK-36390", "title": "[SPARK-36390][SQL] Replace SessionState.close with SessionState.detachSession", "body": "### What changes were proposed in this pull request?\r\nSPARK-35286 replace `SessionState.start` with `SessionState.setCurrentSessionState`, but `SessionState.close` will create a `HiveMetaStoreClient` , connect to the Hive Meta Store Server, and then load all functions.\r\n\r\nSPARK-35556 (Remove close HiveClient's SessionState) When the Hive version used is greater than or equal to 2.1, `SessionState.close` is not called and the resource dir of HiveClient is not cleaned up.\r\n\r\n### Why are the changes needed?\r\nClean up the hive resources dir temporary directory.\r\nAvoid wasting resources and accelerate the exit speed.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\nadd UT\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/hive-thriftserver/src/main/java/org/apache/hive/service/cli/session/HiveSessionImpl.java", "additions": "3", "deletions": "15", "changes": "18"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkSQLCLIDriver.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/CliSuite.scala", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveUtils.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 1, 2]}]}
{"author": "cxzl25", "sha": "41d4c4f72f65c8bf2c388d2980240b445522313f", "commit_date": "2021/08/20 07:13:45", "commit_message": "Propagation cause when UDF reflection fails", "title": "[SPARK-36550][SQL] Propagation cause when UDF reflection fails", "body": "### What changes were proposed in this pull request?\r\nWhen the exception is InvocationTargetException, get cause and stack trace.\r\n\r\n### Why are the changes needed?\r\nNow when UDF reflection fails, InvocationTargetException is thrown, but it is not a specific exception.\r\n```\r\nError in query: No handler for Hive UDF 'XXX': java.lang.reflect.InvocationTargetException\r\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n```\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo.\r\n\r\n\r\n### How was this patch tested?\r\nmanual test\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveSessionCatalog.scala", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 0, 0]}]}
{"author": "cxzl25", "sha": "c85ab7aa98873176def007b6e2655dfcdafef1a8", "commit_date": "2021/06/30 08:33:25", "commit_message": "renameFunction use the original name", "title": "[SPARK-35913][SQL] Create hive permanent function with owner name", "body": "### What changes were proposed in this pull request?\r\nCreate hive permanent function with owner name\r\n\r\n\r\n### Why are the changes needed?\r\nThe hive permanent function created by spark does not have an owner name, while the permanent function created by hive has an owner name\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n\r\n### How was this patch tested?\r\nmanual test\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 7]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveShim.scala", "additions": "17", "deletions": "11", "changes": "28"}, "updated": [0, 1, 5]}]}
{"author": "cxzl25", "sha": "", "commit_date": "2020/10/14 12:46:06", "commit_message": "use distinct", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/deploy/DependencyUtils.scala", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 0, 0]}]}
{"author": "baibaichen", "sha": "", "commit_date": "2021/04/05 23:52:48", "commit_message": "[SPARK-34935][SQL] CREATE TABLE LIKE should respect the reserved table properties\n\n### What changes were proposed in this pull request?\n\nCREATE TABLE LIKE should respect the reserved properties of tables and fail if specified, using `spark.sql.legacy.notReserveProperties` to restore.\n\n### Why are the changes needed?\n\nMake DDLs consistently treat reserved properties\n\n### Does this PR introduce _any_ user-facing change?\n\nYES, this is a breaking change as using `create table like` w/ reserved properties will fail.\n\n### How was this patch tested?\n\nnew test\n\nCloses #32025 from yaooqinn/SPARK-34935.\n\nAuthored-by: Kent Yao <yao@apache.org>\nSigned-off-by: Takeshi Yamamuro <yamamuro@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "docs/sql-migration-guide.md", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 4, 11]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/SparkSqlParserSuite.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [1, 1, 1]}]}
{"author": "liangz1", "sha": "", "commit_date": "2021/08/26 07:11:28", "commit_message": "add withMetadata and make withColumn public", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala", "additions": "22", "deletions": "1", "changes": "23"}, "updated": [1, 1, 8]}]}
{"author": "kyoty", "sha": "2bcbcf4db872041b7c7370c96b2513aff1e8c21c", "commit_date": "2021/07/13 17:44:07", "commit_message": "Bump addressable from 2.7.0 to 2.8.0 in /docs\n\nBumps [addressable](https://github.com/sporkmonger/addressable) from 2.7.0 to 2.8.0.\n- [Release notes](https://github.com/sporkmonger/addressable/releases)\n- [Changelog](https://github.com/sporkmonger/addressable/blob/main/CHANGELOG.md)\n- [Commits](https://github.com/sporkmonger/addressable/compare/addressable-2.7.0...addressable-2.8.0)\n\n---\nupdated-dependencies:\n- dependency-name: addressable\n  dependency-type: indirect\n...\n\nSigned-off-by: dependabot[bot] <support@github.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "docs/Gemfile.lock", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "kyoty", "sha": "", "commit_date": "2021/04/17 16:40:55", "commit_message": "Update stagepage.js", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/stagepage.js", "additions": "8", "deletions": "12", "changes": "20"}, "updated": [0, 0, 0]}]}
{"author": "cfmcgrady", "sha": "05a11c880da09799bccfbd3f5dd48c3448315c27", "commit_date": "2021/06/21 05:59:07", "commit_message": "add test", "title": "[SPARK-35688][SQL]Subexpressions should be lazy evaluation in GeneratePredicate", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error message, please read the guideline first:\r\n     https://spark.apache.org/error-message-guidelines.html\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nCurrently the subexpression elimination in GeneraterPredicate is eager execution, add lazy evaluation mode for subexpression elimination. Fix error when the subexression elimination is inside a condition branch and the subexression depends on the other conditions.\r\nFor instance:\r\n```sql\r\n-- c1 as type of array\r\n-- +-------------------------------+\r\n-- |c1                             |\r\n-- +-------------------------------+\r\n-- |[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]|\r\n-- |[1, 2, 3, 4, 5]                |\r\n-- +-------------------------------+\r\nset spark.sql.ansi.enabled = true;\r\nset spark.sql.codegen.wholeStage = false;\r\n\r\nselect * from table_name where size(c1) > 5 and (element_at(c1, 7) = 8 or element_at(c1, 7) = 7);\r\n```\r\nIn this case, `element_at` depends on condition `size(c1) > 5`, before this pr, an exception will throw when we disable codegen.\r\n\r\n\r\nAs the lazy evaluation is expensive (we need to extract a variable and check the subexpr is initialized or not before we use the subexpr), we keep subexpression eager execution in `GenerateUnsafeProjection/GenerateMutableProjection`\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\nFix bug when subexpression elimination enabled.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\nNo.\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\n\r\nExsiting test and new test.", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "115", "deletions": "33", "changes": "148"}, "updated": [0, 3, 7]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/GenerateMutableProjection.scala", "additions": "11", "deletions": "6", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/GeneratePredicate.scala", "additions": "6", "deletions": "4", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/GenerateUnsafeProjection.scala", "additions": "10", "deletions": "7", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CodeGenerationSuite.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 3]}]}
{"author": "cfmcgrady", "sha": "", "commit_date": "2021/05/10 03:18:26", "commit_message": "[SPARK-35316][SQL] UnwrapCastInBinaryComparison support In/InSet predicate", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/UnwrapCastInBinaryComparison.scala", "additions": "15", "deletions": "3", "changes": "18"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/UnwrapCastInBinaryComparisonSuite.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 0, 0]}]}
{"author": "HeartSaVioR", "sha": "", "commit_date": "2021/03/31 08:21:44", "commit_message": "Add test suite", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StreamingSessionWindowStateManager.scala", "additions": "12", "deletions": "0", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/StreamingSessionWindowStateManagerSuite.scala", "additions": "201", "deletions": "0", "changes": "201"}, "updated": [0, 0, 0]}]}
{"author": "BelodengKlaus", "sha": "", "commit_date": "2021/09/16 03:29:03", "commit_message": "[SPARK-36773] Fixed uts to check the compression for parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetCompressionCodecPrecedenceSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "JoshRosen", "sha": "", "commit_date": "2021/09/16 05:15:38", "commit_message": "Fix import ordering", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.execution.SQLMetricsSuite"], "files": [{"file": {"name": "sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/CliSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/HiveThriftServer2Suites.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}]}
{"author": "exmy", "sha": "", "commit_date": "2021/06/01 15:27:58", "commit_message": "SPARK-35596: HighlyCompressedMapStatus should record accurately the size of skewed shuffle blocks", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 1, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/MapStatus.scala", "additions": "12", "deletions": "3", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/MapStatusSuite.scala", "additions": "26", "deletions": "0", "changes": "26"}, "updated": [0, 0, 0]}]}
{"author": "yaooqinn", "sha": "a3b7d08983115c84225ce52f8db3dd16efd5471e", "commit_date": "2021/09/06 05:56:10", "commit_message": "add doc", "title": "[SPARK-36662][SQL] Special timestamps support for path filters -  modifiedBefore/modifiedAfter", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\nSpecial timestamps support for path filters -  modifiedBefore/modifiedAfter\r\n\r\n- epoch\r\n- now\r\n- today\r\n- tomorrow\r\n- yesterday\r\n#### examples\r\n\r\n```scala\r\n    val beforeTodayDF = spark.read.format(\"parquet\")\r\n      // Files modified after the midnight of today are allowed\r\n      .option(\"modifiedBefore\", \"today\")\r\n      .load(\"examples/src/main/resources/dir1\");\r\n    beforeTodayDF.show();\r\n    // +-------------+\r\n    // |         file|\r\n    // +-------------+\r\n    // |file1.parquet|\r\n    // +-------------+\r\n    val afterYesterdayDF = spark.read.format(\"parquet\")\r\n      // Files modified after the midnight of yesterday are allowed\r\n      .option(\"modifiedAfter\", \"yesterday\")\r\n      .load(\"examples/src/main/resources/dir1\");\r\n    afterYesterdayDF.show();\r\n    // +-------------+\r\n    // |         file|\r\n    // +-------------+\r\n    // +-------------+\r\n    // $example off:load_with_modified_time_filter$\r\n```\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\ntheses special values can be useful to be supported in path filters -  modifiedBefore/modifiedAfter. e.g. for daily scheduled jobs\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\nyes, special timestamps are supported by modifiedBefore/modifiedAfter\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\n\r\nnewly added tests", "failed_tests": [], "files": [{"file": {"name": "docs/sql-data-sources-generic-options.md", "additions": "11", "deletions": "4", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 3, 13]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/pathFilters.scala", "additions": "6", "deletions": "3", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/PathFilterSuite.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [0, 0, 0]}]}
{"author": "yaooqinn", "sha": "b989aa3821dbaf6cacccde886ed1051068d43cf5", "commit_date": "2021/09/03 17:21:28", "commit_message": "address comments", "title": "[SPARK-36634][SQL] Support access and read parquet file by column ordinal", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\nAdd a config `spark.sql.parquet.accessByColumnOrdinal` \r\n\r\nWhen true, we access the parquet files by column original instead of catalyst schema mapping at the executor side. \r\n\r\nThis is useful when the parquet file meta is inconsistent with those in Metastore, e.g. creating a table with existing parquet files with column names renamed, the column names are changed by external systems.\r\n\r\n- What if the number of columns/inner fields does not match?\r\n  - if the number of requests columns (M)is greater than the one(N) in the parquet file, the [M, N-1] of the outputs will be filled with nulls, which is also the same as the current name-based mapping\r\n  - if the number of requests columns (M)is less than or equal to the one(N) in the parquet file, the [0, M-1] of fields will be token in order.\r\n- What if the types are not compatible?\r\n  - In this case, it throws unsupportedSchemaColumnConvertError \r\n\r\nBy default, it is off\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\n- What's the concrete use case that requires this feature?\r\n\r\nParquet's schema evolution is implementation-dependent and may causes inconsistency from file to file or file to metastore. Sometimes, 1) the table Spark's processing might be produced by other systems, e.g. renamed by hive https://issues.apache.org/jira/browse/HIVE-6938, 2)some operations that do not introduce a force schema checking, e.g. `set location, `add partition`. With this feature, the users as data consumers can still read those data produced by different providers.\r\n\r\nSee also, https://issues.apache.org/jira/browse/IMPALA-2835\r\n\r\n\r\nbetter data accessibility \r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\nNO\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\n\r\nnewly added test", "failed_tests": ["org.apache.spark.sql.execution.datasources.parquet.ParquetV2SchemaPruningSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV1SchemaPruningSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [1, 1, 12]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport.scala", "additions": "54", "deletions": "21", "changes": "75"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRecordMaterializer.scala", "additions": "6", "deletions": "2", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "27", "deletions": "13", "changes": "40"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "85", "deletions": "0", "changes": "85"}, "updated": [0, 0, 0]}]}
{"author": "yaooqinn", "sha": "4078349ac282f44673918d2c2a40e2a8799ce07a", "commit_date": "2021/06/08 16:23:45", "commit_message": "fix test", "title": "[WIP][SPARK-35677][Core][SQL] Support dynamic range of executor numbers for dynamic allocation", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error message, please read the guideline first:\r\n     https://spark.apache.org/error-message-guidelines.html\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n\r\nCurrently, Spark allows users to set scalability within a Spark application using dynamic allocation. `spark.dynamicAllocation.minExecutors` & `spark.dynamicAllocation.maxExecutors` are used for scaling up and down. Within an application\uff0cSpark tactfully use them to request executors from cluster manager according to the real-time workload. Once set, the range is fixed through the whole application lifecycle. This is not very convenient for long-running application when the range should be changeable for some cases, such as:\r\n1. the cluster manager itself or the queue will scale up and down, which looks very likely to happen in modern cloud platforms\r\n2. the application is long-running, but the timeliness, priority, e.t.c are not only determined by the workload of the application, but also by the traffic across the cluster manager or just different moments\r\n3. e.t.c.\r\n\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\nmake the dynamic allocation for long term Spark applications\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\nConfigs below are changeable:\r\n\r\nspark.dynamicAllocation.maxExecutors \r\nspark.dynamicAllocation.minExecutors \r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nnew tests", "failed_tests": [], "files": [{"file": {"name": "core/src/main/java/org/apache/spark/SparkFirehoseListener.java", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala", "additions": "22", "deletions": "21", "changes": "43"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [1, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/SparkListenerBus.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/ExecutorAllocationManagerSuite.scala", "additions": "31", "deletions": "2", "changes": "33"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/command/SetCommand.scala", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 1, 3]}]}
{"author": "yaooqinn", "sha": "9e8b227e91070c35b98247acdbf4caf022cfaf72", "commit_date": "2021/08/20 13:52:21", "commit_message": "address comments", "title": "[SPARK-36477][SQL] Inferring schema from JSON file shall handle CharConversionException/MalformedInputException", "body": "\r\n<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\nWhen set `ignoreCorruptFiles=true`, reading JSON still fails with corrupt files during inferring schema.\r\n\r\n```scala\r\njava.io.CharConversionException: Unsupported UCS-4 endianness (2143) detected\r\n\tat com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.reportWeirdUCS4(ByteSourceJsonBootstrapper.java:504)\r\n\tat com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.checkUTF32(ByteSourceJsonBootstrapper.java:471)\r\n\tat com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.detectEncoding(ByteSourceJsonBootstrapper.java:144)\r\n\tat com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.constructParser(ByteSourceJsonBootstrapper.java:247)\r\n\tat com.fasterxml.jackson.core.JsonFactory._createParser(JsonFactory.java:1528)\r\n\tat com.fasterxml.jackson.core.JsonFactory.createParser(JsonFactory.java:1034)\r\n\tat org.apache.spark.sql.catalyst.json.CreateJacksonParser$.internalRow(CreateJacksonParser.scala:86)\r\n\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.$anonfun$inferFromDataset$4(JsonDataSource.scala:107)\r\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$3(JsonInferSchema.scala:66)\r\n\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2621)\r\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$2(JsonInferSchema.scala:66)\r\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\r\n\tat scala.collection.TraversableOnce.reduceLeft(TraversableOnce.scala:190)\r\n\tat scala.collection.TraversableOnce.reduceLeft$(TraversableOnce.scala:183)\r\n\tat scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1429)\r\n\tat scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:208)\r\n\tat scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:207)\r\n\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1429)\r\n\tat scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:215)\r\n\tat scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:215)\r\n\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1429)\r\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:81)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n```\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\nbugfix\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nyes, ignoreCorruptFiles will ignore JSON files corrupted\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\n\r\nnew test", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JsonInferSchema.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/json/JsonSuite.scala", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [1, 1, 4]}]}
{"author": "yaooqinn", "sha": "623dc4659e016505d1245bf7637c12d499aa947d", "commit_date": "2021/08/11 04:22:48", "commit_message": "Update sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala\n\nCo-authored-by: Hyukjin Kwon <gurwls223@gmail.com>", "title": "[SPARK-36180][SQL] Store TIMESTAMP_NTZ into hive catalog as TIMESTAMP", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\nThis PR fix a issue that HMS can not recognize timestamp_ntz by mapping timestamp_ntz to `timestamp` of hive\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\nThe hive 2.3.9 does not have 2 timestamp or a type named timestamp_ntz.\r\nFYI, In hive 3.0, the will be a timestamp with local timezone added.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\nno, timestamp_ntz is new and not public yet\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\n\r\nnew test", "failed_tests": [], "files": [{"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala", "additions": "15", "deletions": "14", "changes": "29"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/SQLQuerySuite.scala", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [0, 1, 2]}]}
{"author": "yaooqinn", "sha": "d28f8690f9d0d7c5250d0f8cd49d0f3822df37d3", "commit_date": "2021/06/19 18:50:11", "commit_message": "[SPARK-35828][K8S] Skip retrieving the non-exist driver pod for client mode", "title": "[SPARK-35828][K8S] Skip retrieving the non-exist driver pod for client mode", "body": "\r\n<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error message, please read the guideline first:\r\n     https://spark.apache.org/error-message-guidelines.html\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\nFor a case like,\r\n\r\n```scala\r\nbin/spark-submit  \\\r\n--conf spark.kubernetes.file.upload.path=./ \\\r\n--deploy-mode client \\\r\n--master k8s://https://kubernetes.docker.internal:6443 \\\r\n--conf spark.kubernetes.container.image=yaooqinn/spark:v20210619 \\\r\n-c spark.kubernetes.context=docker-for-desktop_1 \\\r\n--conf spark.kubernetes.executor.podNamePrefix=sparksql \\\r\n--conf spark.dynamicAllocation.shuffleTracking.enabled=true \\\r\n--conf spark.dynamicAllocation.enabled=true \\\r\n--conf spark.kubernetes.driver.pod.name=abc \\\r\n--class org.apache.spark.examples.SparkPi examples/jars/spark-examples_2.12-3.2.0-SNAPSHOT.jar\r\n```\r\n\r\nWhen `spark.kubernetes.driver.pod.name` is specific, we now get the driver pod for whatever the deploy mode is, while the driver pod only exists in cluster mode. So we should skip retrieving it instead of getting the following error:\r\n\r\n```logtalk\r\n21/06/19 16:18:49 ERROR SparkContext: Error initializing SparkContext.\r\norg.apache.spark.SparkException: No pod was found named abc in the cluster in the namespace default (this was supposed to be the driver pod.).\r\n\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$driverPod$2(ExecutorPodsAllocator.scala:81)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$driverPod$1(ExecutorPodsAllocator.scala:79)\r\n\tat scala.Option.map(Option.scala:230)\r\n\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.<init>(ExecutorPodsAllocator.scala:76)\r\n\tat org.apache.spark.scheduler.cluster.k8s.KubernetesClusterManager.createSchedulerBackend(KubernetesClusterManager.scala:118)\r\n\tat org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2969)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:559)\r\n\tat org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2686)\r\n\tat org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:948)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:942)\r\n\tat org.apache.spark.examples.SparkPi$.main(SparkPi.scala:30)\r\n\tat org.apache.spark.examples.SparkPi.main(SparkPi.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\n```\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\nan unused config should stay inoperative instead of failing an application at runtime.\r\n\r\nwhen we switch deploy modes, we do need to justify irrelevant configs.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\nyes, spark.kubernetes.driver.pod.name will cause the client mode app to fail\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\n\r\nnew test added\r\n", "failed_tests": [], "files": [{"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/ExecutorPodsAllocator.scala", "additions": "5", "deletions": "2", "changes": "7"}, "updated": [0, 3, 6]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/scheduler/cluster/k8s/ExecutorPodsAllocatorSuite.scala", "additions": "15", "deletions": "1", "changes": "16"}, "updated": [0, 2, 4]}]}
{"author": "yaooqinn", "sha": "", "commit_date": "2021/04/16 03:26:02", "commit_message": "[SPARK-35102][SQL] Make spark.sql.hive.version read-only", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkSQLEnv.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkSQLSessionManager.scala", "additions": "0", "deletions": "2", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/CliSuite.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/HiveThriftServer2Suites.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveUtils.scala", "additions": "21", "deletions": "12", "changes": "33"}, "updated": [0, 0, 1]}]}
{"author": "sigmod", "sha": "", "commit_date": "2021/04/20 06:38:37", "commit_message": "update", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 13, 47]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/predicates.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/subquery.scala", "additions": "6", "deletions": "5", "changes": "11"}, "updated": [1, 3, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/InsertAdaptiveSparkPlan.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/PlanAdaptiveSubqueries.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "lrytz", "sha": "", "commit_date": "2021/09/09 14:37:09", "commit_message": "Use sed instead of profile to enable scala-parallel-collections on 2.13\n\nWorkaround for the POM issue explained in\nhttps://lists.apache.org/thread.html/rc812979bf41bac070d7d0437bb226e7a778fbe46f18727636daaac5e%40%3Cdev.spark.apache.org%3E", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/pom.xml", "additions": "6", "deletions": "9", "changes": "15"}, "updated": [0, 0, 2]}, {"file": {"name": "dev/change-scala-version.sh", "additions": "8", "deletions": "4", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "pom.xml", "additions": "7", "deletions": "9", "changes": "16"}, "updated": [1, 4, 22]}]}
{"author": "pingsutw", "sha": "", "commit_date": "2021/06/01 09:57:46", "commit_message": "Enable disallow_untyped_defs mypy check for pyspark.pandas.indexing.", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/mypy.ini", "additions": "0", "deletions": "3", "changes": "3"}, "updated": [0, 2, 4]}, {"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [0, 2, 9]}]}
{"author": "luxu1-ms", "sha": "ae35e8cdf757f29abad224d38511541783212e65", "commit_date": "2021/06/03 17:13:44", "commit_message": "update test case for datetime2 mapping", "title": "[SPARK-33743]change TimestampType match to datetime2 instead of datetime for MsSQLServerDialect", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error message, please read the guideline first:\r\n     https://spark.apache.org/error-message-guidelines.html\r\n-->\r\n\r\nSPARK-33743 is to change datetime datatype mapping in JDBC mssqldialect.\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\noverride def getJDBCType(dt: DataType): Option[JdbcType] = dt match {\r\ncase TimestampType => Some(JdbcType(\"DATETIME2\", java.sql.Types.TIMESTAMP))\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nSpark datetime type is timestamp type. This supports a microsecond resolution.\r\nSql supports 2 date time types:\r\n\r\ndatetime can support only milli seconds resolution (0 to 999).\r\ndatetime2 is extension of datetime , is compatible with datetime and supports 0 to 9999999 sub second resolution.\r\ndatetime2 (Transact-SQL) - SQL Server | Microsoft Docs\r\ndatetime (Transact-SQL) - SQL Server | Microsoft Docs\r\n\r\nCurrently MsSQLServerDialect maps timestamp type to datetime. Datetime only allows 3 digits of microseconds. This implies results in errors when writing timestamp with more than 3 digits of microseconds to sql server table. We want to map timestamp to datetime2, which is compatible with datetime but allows 7 digits of microseconds.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nNo.\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nUnit tests were updated and passed in JDBCSuit.scala.\r\nE2E test done with SQL Server.", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/jdbc/MsSqlServerDialect.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "planga82", "sha": "02a265d4eb38a1b562db6ce02e56f739a16d9ce7", "commit_date": "2021/09/09 00:57:53", "commit_message": "Apply only to unresolved regex", "title": "[SPARK-36698][SQL] Allow expand 'quotedRegexColumnNames' in all expressions when it\u2019s expanded to one named expression", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nWith `spark.sql.parser.quotedRegexColumnNames=true` regular expressions are not allowed in expressions like  \r\n``` SELECT `col_.*`/exp FROM (SELECT 3 AS col_a, 1 as exp) ``` \r\n\r\nThis PR propose to improve this behavior and allow this regular expression when it expands to only one named expression. It\u2019s the same behavior in Hive.\r\n\r\nExample 1:\r\n```\r\nSELECT `col_.*`/exp FROM (SELECT 3 AS col_a, 1 as exp) \r\n```\r\ncol_.* expands to col_a:  OK\r\n\r\nExample 2:\r\n```\r\nSELECT `col_.*`/col_b FROM (SELECT 3 AS col_a, 1 as col_b) \r\n```\r\ncol_.* expands to (col_a, col_b) : Fail like now\r\n\r\nExample 3:\r\n```\r\nSELECT `col_a`/exp FROM (SELECT 3 AS col_a, 1 as exp) \r\n```\r\ncol_a expands to col_a:  OK\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nImprove this feature and approaching hive behavior\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nNo\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nUnit testing", "failed_tests": ["org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite", "org.apache.spark.sql.DataFrameSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 2, 18]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [1, 1, 8]}]}
{"author": "planga82", "sha": "533cb7ca056e746ce0d4047662e0ae5574d47c36", "commit_date": "2021/08/15 22:47:03", "commit_message": "Fix style", "title": "[SPARK-36453][SQL] Improve consistency processing floating point special literals", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nSpecial literals in floating point are not consistent between cast and json expressions\r\n```\r\nscala> spark.sql(\"SELECT CAST('+Inf' as Double)\").show\r\n+--------------------+                                                        \r\n|CAST(+Inf AS DOUBLE)|\r\n+--------------------+\r\n|            Infinity|\r\n+--------------------+\r\n```\r\n```\r\nscala> val schema =  StructType(StructField(\"a\", DoubleType) :: Nil)\r\n\r\nscala> Seq(\"\"\"{\"a\" : \"+Inf\"}\"\"\").toDF(\"col1\").select(from_json(col(\"col1\"),schema)).show\r\n+---------------+\r\n|from_json(col1)|\r\n+---------------+\r\n|         {null}|\r\n+---------------+\r\n\r\nscala> Seq(\"\"\"{\"a\" : \"+Inf\"}\"\"\").toDF(\"col\").withColumn(\"col\", from_json(col(\"col\"), StructType.fromDDL(\"a DOUBLE\"))).write.json(\"/tmp/jsontests12345\")\r\nscala> spark.read.schema(StructType(Seq(StructField(\"col\",schema)))).json(\"/tmp/jsontests12345\").show\r\n+------+\r\n|   col|\r\n+------+\r\n|{null}|\r\n+------+\r\n```\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nImprove consistency between operations\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nYes, we are going to support the same special literal in Cast and Json expressions\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nUnit testing and manual testing", "failed_tests": ["pyspark.pandas.tests.data_type_ops.test_string_ops", "pyspark.pandas.tests.test_dataframe", "org.apache.spark.sql.catalyst.expressions.AnsiCastSuite", "org.apache.spark.sql.catalyst.expressions.AnsiCastSuite", "org.apache.spark.sql.catalyst.expressions.CastSuite", "org.apache.spark.sql.catalyst.expressions.TryCastSuite", "org.apache.spark.sql.catalyst.expressions.CastSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SQLQuerySuite", "org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite", "org.apache.spark.sql.DynamicPartitionPruningV1Suite", "org.apache.spark.sql.DynamicPartitionPruningV2Suite", "org.apache.spark.sql.DynamicPartitionPruningV1Suite", "org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite", "org.apache.spark.sql.DataFrameWindowFunctionsSuite", "org.apache.spark.sql.SubquerySuite", "org.apache.spark.sql.DynamicPartitionPruningV2Suite", "org.apache.spark.sql.DataFrameSuite", "org.apache.spark.sql.ColumnExpressionSuite", "org.apache.spark.sql.execution.datasources.json.JsonV1Suite", "org.apache.spark.sql.execution.datasources.json.JsonV2Suite", "org.apache.spark.sql.execution.datasources.json.JsonLegacyTimeParserSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala", "additions": "5", "deletions": "20", "changes": "25"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ExprUtils.scala", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JacksonParser.scala", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [0, 4, 7]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/json/JacksonParserSuite.scala", "additions": "27", "deletions": "12", "changes": "39"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "28", "deletions": "0", "changes": "28"}, "updated": [1, 2, 11]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/json/JsonSuite.scala", "additions": "0", "deletions": "17", "changes": "17"}, "updated": [0, 1, 3]}]}
{"author": "planga82", "sha": "", "commit_date": "2020/09/21 09:15:12", "commit_message": "[SPARK-32867][SQL] When explain, HiveTableRelation show limited message\n\n### What changes were proposed in this pull request?\nIn current mode, when explain a SQL plan with HiveTableRelation, it will show so many info about HiveTableRelation's prunedPartition,  this make plan hard to read, this pr make this information simpler.\n\nBefore:\n![image](https://user-images.githubusercontent.com/46485123/93012078-aeeca080-f5cf-11ea-9286-f5c15eadbee3.png)\n\nFor UT\n```\n test(\"Make HiveTableScanExec message simple\") {\n  withSQLConf(\"hive.exec.dynamic.partition.mode\" -> \"nonstrict\") {\n      withTable(\"df\") {\n        spark.range(30)\n          .select(col(\"id\"), col(\"id\").as(\"k\"))\n          .write\n          .partitionBy(\"k\")\n          .format(\"hive\")\n          .mode(\"overwrite\")\n          .saveAsTable(\"df\")\n\n        val df = sql(\"SELECT df.id, df.k FROM df WHERE df.k < 2\")\n        df.explain(true)\n      }\n    }\n  }\n```\n\nAfter this pr will show\n```\n== Parsed Logical Plan ==\n'Project ['df.id, 'df.k]\n+- 'Filter ('df.k < 2)\n   +- 'UnresolvedRelation [df], []\n\n== Analyzed Logical Plan ==\nid: bigint, k: bigint\nProject [id#11L, k#12L]\n+- Filter (k#12L < cast(2 as bigint))\n   +- SubqueryAlias spark_catalog.default.df\n      +- HiveTableRelation [`default`.`df`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#11L], Partition Cols: [k#12L]]\n\n== Optimized Logical Plan ==\nFilter (isnotnull(k#12L) AND (k#12L < 2))\n+- HiveTableRelation [`default`.`df`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#11L], Partition Cols: [k#12L], Pruned Partitions: [(k=0), (k=1)]]\n\n== Physical Plan ==\nScan hive default.df [id#11L, k#12L], HiveTableRelation [`default`.`df`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#11L], Partition Cols: [k#12L], Pruned Partitions: [(k=0), (k=1)]], [isnotnull(k#12L), (k#12L < 2)]\n\n```\n\nIn my pr, I will construct `HiveTableRelation`'s `simpleString` method to avoid show too much unnecessary info in explain plan. compared to what we had before\uff0cI decrease the detail metadata of each partition and only retain the partSpec to show each partition was pruned. Since for detail information, we always don't see this in Plan but to use DESC EXTENDED statement.\n\n### Why are the changes needed?\nMake plan about HiveTableRelation more readable\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nNo\n\nCloses #29739 from AngersZhuuuu/HiveTableScan-meta-location-info.\n\nAuthored-by: angerszhu <angers.zhu@gmail.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/interface.scala", "additions": "39", "deletions": "2", "changes": "41"}, "updated": [1, 2, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/HiveTableScanSuite.scala", "additions": "61", "deletions": "0", "changes": "61"}, "updated": [1, 1, 1]}]}
{"author": "FatalLin", "sha": "", "commit_date": "2021/04/19 15:31:34", "commit_message": "add new config to enable the function", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [1, 3, 14]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMetastoreCatalog.scala", "additions": "1", "deletions": "4", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveMetastoreCatalogSuite.scala", "additions": "15", "deletions": "18", "changes": "33"}, "updated": [0, 0, 0]}]}
{"author": "ekoifman", "sha": "af5a71ab3d61056f0fa309e89868d57d584d0994", "commit_date": "2021/08/04 18:18:29", "commit_message": "[SPARK-36416][SQL] Add SQL metrics to AdaptiveSparkPlanExec for BHJs and Skew joins\n[SPARK-36416][SQL] fix typo", "title": "[SPARK-36416][SQL] Add SQL metrics to AdaptiveSparkPlanExec for BHJs and Skew joins", "body": "### What changes were proposed in this pull request?\r\n\r\nAdd \"num broadcast joins conversions\" and \"num skew join conversions\"\r\nmetrics to AdaptiveSparkPlanExec to report how many joins were changed to BHJ or had skew mitigated due to AQE.\r\n\r\n### Why are the changes needed?\r\n\r\nTo make it easy to get a sense of how much impact AQE had on an a complex query.\r\n\r\nIt's also useful for systems that collect metrics for later analysis of AQE effectiveness in large production deployment.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\n2 new SQL metrics will now be emitted\r\n\r\n### How was this patch tested?\r\n\r\nExisting and new Unit tests.", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "32", "deletions": "0", "changes": "32"}, "updated": [1, 3, 9]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "56", "deletions": "1", "changes": "57"}, "updated": [1, 4, 16]}]}
{"author": "ekoifman", "sha": "", "commit_date": "2021/01/04 16:14:33", "commit_message": "[SPARK-33875][SQL] Implement DESCRIBE COLUMN for v2 tables\n\n### What changes were proposed in this pull request?\n\nThis PR proposes to implement `DESCRIBE COLUMN` for v2 tables.\n\nNote that `isExnteded` option is not implemented in this PR.\n\n### Why are the changes needed?\n\nParity with v1 tables.\n\n### Does this PR introduce _any_ user-facing change?\n\nYes, now, `DESCRIBE COLUMN` works for v2 tables.\n```scala\nsql(\"CREATE TABLE testcat.tbl (id bigint, data string COMMENT 'hello') USING foo\")\nsql(\"DESCRIBE testcat.tbl data\").show\n```\n```\n+---------+----------+\n|info_name|info_value|\n+---------+----------+\n| col_name|      data|\n|data_type|    string|\n|  comment|     hello|\n+---------+----------+\n```\n\nBefore this PR, the command would fail with: `Describing columns is not supported for v2 tables.`\n\n### How was this patch tested?\n\nAdded new test.\n\nCloses #30881 from imback82/describe_col_v2.\n\nAuthored-by: Terry Kim <yuminkim@gmail.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/QueryCompilationErrors.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [2, 3, 7]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [2, 5, 31]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [1, 3, 20]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolvePartitionSpec.scala", "additions": "5", "deletions": "4", "changes": "9"}, "updated": [1, 2, 11]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/v2ResolutionPlans.scala", "additions": "20", "deletions": "2", "changes": "22"}, "updated": [1, 2, 9]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 6, 41]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2Commands.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 3, 24]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala", "additions": "21", "deletions": "7", "changes": "28"}, "updated": [1, 2, 30]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala", "additions": "28", "deletions": "4", "changes": "32"}, "updated": [1, 4, 27]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [1, 3, 11]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala", "additions": "20", "deletions": "9", "changes": "29"}, "updated": [1, 4, 29]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DescribeColumnExec.scala", "additions": "56", "deletions": "0", "changes": "56"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/describe-table-column.sql", "additions": "7", "deletions": "1", "changes": "8"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/describe-table-column.sql.out", "additions": "21", "deletions": "3", "changes": "24"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/describe.sql.out", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 5]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala", "additions": "47", "deletions": "6", "changes": "53"}, "updated": [1, 3, 38]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/PlanResolutionSuite.scala", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [1, 2, 13]}]}
{"author": "ivoson", "sha": "", "commit_date": "2021/05/17 05:53:52", "commit_message": "modify annotation", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreeNode.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 9]}]}
{"author": "sungpeo", "sha": "a35a6ea8a7c346e43a7e065eceed25aabcfd5a24", "commit_date": "2021/08/25 07:22:15", "commit_message": "SPARK-36582 Spark HistoryPage show 'NotFound' in not logged multiple attempts\n\nattemptId should be always, so deleted hasMultipleAttempts", "title": "[SPARK-36582][UI] Spark HistoryPage show 'NotFound' in not logged multiple attempts", "body": "### What changes were proposed in this pull request?\r\n\r\n`attemptId` should be always, so deleted `hasMultipleAttempts`\r\n\r\n### Why are the changes needed?\r\n\r\nDescribed in https://issues.apache.org/jira/browse/SPARK-36582\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\n#### before change\r\n\r\nWhen all applications in spark history had an attempt only, it doesn't show attemptId\r\n\r\n![Screen Shot 2021-08-26 at 10 47 23 AM](https://user-images.githubusercontent.com/13159599/130886943-36666846-4dca-4687-9e3f-9c6d339207bd.png)\r\n\r\nNow it shows a attemptId column regardless of hasMultipleAttempts.\r\n\r\n![Screen Shot 2021-08-26 at 10 04 43 AM](https://user-images.githubusercontent.com/13159599/130883769-b10916ef-ccaa-4811-8e70-fa27e8a8fceb.png)\r\n\r\n\r\n### How was this patch tested?\r\n\r\nI checked chrome developer tool's console in changed web ui. (History Server's home page)\r\n", "failed_tests": ["org.apache.spark.sql.streaming.FileStreamSinkV2Suite"], "files": [{"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/historypage-template.html", "additions": "0", "deletions": "2", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/historypage.js", "additions": "6", "deletions": "17", "changes": "23"}, "updated": [0, 0, 0]}]}
{"author": "hddong", "sha": "", "commit_date": "2021/04/20 06:13:17", "commit_message": "[SPARK-35143][SQL][SHELL]Add default log level config for spark-sql", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "conf/log4j.properties.template", "additions": "5", "deletions": "3", "changes": "8"}, "updated": [0, 0, 1]}]}
{"author": "fhygh", "sha": "e604f1ba111d208526ec3d49799fed063dc5c990", "commit_date": "2021/09/01 07:22:47", "commit_message": "[SPARK-36604][SQL] timestamp type column stats result consistent with\nthe time zone", "title": "[SPARK-36604][SQL] timestamp type column stats result consistent with the time zone", "body": "\r\n### What changes were proposed in this pull request?\r\ntimestamp type column stats result should consistent with time zone\r\n\r\n\r\n### Why are the changes needed?\r\nfor now timestamp type column stats result is based on UTC TimeZone\uff0cindependent from the time zone. Thus the stats result may not correct\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n\r\n### How was this patch tested?\r\nAdd new test\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/interface.scala", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/StatisticsCollectionSuite.scala", "additions": "39", "deletions": "12", "changes": "51"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/StatisticsSuite.scala", "additions": "29", "deletions": "19", "changes": "48"}, "updated": [0, 0, 0]}]}
{"author": "fhygh", "sha": "691bb5c0add0420ca51eacfb41b384eefb9637cc", "commit_date": "2021/08/17 06:39:37", "commit_message": "[SPARK-36518][Deploy] Spark should support distribute directory to\ncluster", "title": "[SPARK-36518][Deploy] Spark should support distribute directory to cluster", "body": "\r\n### What changes were proposed in this pull request?\r\nsupport distribute directory to cluster via --files\r\nbefore:\r\n[root@kwephispra41893 spark]# ll /opt/ygh/testdir/\r\ntotal 8\r\ndrwxr-xr-x 2 root root 4096 Aug 17 16:07 dd1\r\ndrwxr-xr-x 2 root root 4096 Aug 17 16:07 dd2\r\n-rw-r--r-- 1 root root    0 Aug 17 16:07 t1.txt\r\n-rw-r--r-- 1 root root    0 Aug 17 16:07 t2.txt\r\n-rw-r--r-- 1 root root    0 Aug 17 16:07 t3.conf\r\n\r\nspark-shell --master yarn --files file:///opt/ygh/testdir\r\n![image](https://user-images.githubusercontent.com/25889738/129689226-d63cc7f6-c529-4c6f-a94d-d48c062dbf29.png)\r\n\r\nafter:\r\nspark-shell --master yarn --files file:///opt/ygh/testdir\r\n![image](https://user-images.githubusercontent.com/25889738/129689406-432c796c-52e5-49c1-8016-9eec151257fb.png)\r\n\r\n\r\n### Why are the changes needed?\r\nwhen we submit spark application we can't directly distribute a directory to cluster\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo, user do not need change any code\r\n\r\n\r\n### How was this patch tested?\r\ntested by existing UT\r\n", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/util/DependencyUtils.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "fhygh", "sha": "0abed27967ad76b9b30d70d47d907388aa417b98", "commit_date": "2021/06/26 23:47:00", "commit_message": "[SPARK-35902][Core]spark.driver.log.dfsDir with hdfs scheme failed", "title": "[SPARK-35902][Core] spark.driver.log.dfsDir with hdfs scheme failed", "body": "### What changes were proposed in this pull request?\r\nwhen persist driver logs in client mode to dfs, log dir should support scheme path\r\n\r\n\r\n### Why are the changes needed?\r\nwhen spark.driver.log.dfsDir start with scheme like hdfs\uff0cit failed  \r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nno, user do not need change any code\r\n\r\n\r\n### How was this patch tested?\r\ntested by existing UT\r\n", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/util/logging/DriverLogger.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}]}
{"author": "Kimahriman", "sha": "8b53f7b2193836265437201d27409ae7b0619475", "commit_date": "2021/07/22 13:02:02", "commit_message": "Track conditionally evaluated expressions to resolve as subexpressions for cases they are already being evaluated", "title": "[SPARK-35564][SQL] Support subexpression elimination for conditionally evaluated expressions", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error message, please read the guideline first:\r\n     https://spark.apache.org/error-message-guidelines.html\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nI am proposing to add support for conditionally evaluated expressions during subexpression elimination. Currently, only expressions that will definitely be always at least twice are candidates for subexpression elimination. This PR updates that logic so that expressions that are always evaluated at least once and conditionally evaluated at least once are also candidates for subexpression elimination. This helps optimize a common case during data normalization and cleaning and want to null out values that don't match a certain pattern, where you have something like:\r\n\r\n```\r\ntransformed = F.regexp_replace(F.lower(F.trim('my_column')))\r\ndf.withColumn('normalized_value', F.when(F.length(transformed) > 0, transformed))\r\n```\r\nor\r\n```\r\ndf.withColumn('normalized_value', F.when(transformed.rlike(<some regex>), transformed))\r\n```\r\n\r\nIn these cases, `transformed` will always be fully calculated twice, because it might only be needed once. I am proposing creating a subexpression for `transformed` in this case.\r\n\r\nIn practice I've seen a decrease in runtime and codegen size of 10-30% in our production pipelines that heavily make use of this type of logic.\r\n\r\nThe only potential downside is creating extra subexpressions, and therefore function calls, more than necessary. This should only be an issue for certain edge cases where your conditional overwhelming evaluates to false. And then the only overhead is running your conditional logic potentially in a separate function rather than inlined in the codegen. I added a config to control this behavior if that is actually a real concern to anyone, but I'd be happy to just remove the config.\r\n\r\nI also updated some of the existing logic for common expressions in coalesce and when that are actually better handled by the new logic, since you are only guaranteed to have the first value of a Coalesce evaluated, as well as the first conditional of a CaseWhen expression.\r\n\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nTo increase the performance of conditional expressions.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nNo, just performance improvements.\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nNew and updated UT.\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala", "additions": "113", "deletions": "76", "changes": "189"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [2, 6, 18]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/SubexpressionEliminationSuite.scala", "additions": "55", "deletions": "13", "changes": "68"}, "updated": [0, 0, 4]}]}
{"author": "Kimahriman", "sha": "", "commit_date": "2021/05/06 00:41:00", "commit_message": "Use StructType merging for unionByName with null filling", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveUnion.scala", "additions": "34", "deletions": "111", "changes": "145"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/SchemaPruning.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 4, 20]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/types/StructType.scala", "additions": "10", "deletions": "14", "changes": "24"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SchemaMergeUtils.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSetOperationsSuite.scala", "additions": "110", "deletions": "33", "changes": "143"}, "updated": [0, 0, 0]}]}
{"author": "noslowerdna", "sha": "", "commit_date": "2021/08/24 14:29:26", "commit_message": "[SPARK-36419][CORE] Optionally move final aggregation in RDD.treeAggregate to executor\n\n## What changes were proposed in this pull request?\n\nMove final iteration of aggregation of RDD.treeAggregate to an executor with one partition and fetch that result to the driver\n\n## Why are the changes needed?\n1. RDD.fold pulls all shuffle partitions to the driver to merge the result\n        a. Driver becomes a single point of failure in the case that there are a lot of partitions to do the final aggregation on\n2. Shuffle machinery at executors is much more robust/fault tolerant compared to fetching results to driver.\n\n## Does this PR introduce any user-facing change?\nThe previous behavior always did the final aggregation in the driver. The user can now (optionally) provide a boolean config (default = false) ENABLE_EXECUTOR_TREE_AGGREGATE to do that final aggregation in a single partition executor before fetching the results to the driver. The only additional cost is that the user will see an extra stage in their job.\n\n## How was this patch tested?\nThis patch was tested via unit tests, and also tested on a cluster.\nThe screenshots showing the extra stage on a cluster are attached below (before vs after).\n![before](https://user-images.githubusercontent.com/24758726/128249830-eefc4bda-f737-4d68-960e-1d1907762538.png)\n![after](https://user-images.githubusercontent.com/24758726/128249838-be70bc95-9f39-489c-be17-c9c80c4846a4.png)\n\nCloses #33644 from akpatnam25/SPARK-36419.\n\nAuthored-by: Aravind Patnam <apatnam@linkedin.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala", "additions": "13", "deletions": "0", "changes": "13"}, "updated": [1, 1, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/rdd/RDD.scala", "additions": "31", "deletions": "1", "changes": "32"}, "updated": [1, 1, 2]}, {"file": {"name": "core/src/test/java/test/org/apache/spark/JavaAPISuite.java", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [1, 1, 1]}, {"file": {"name": "core/src/test/scala/org/apache/spark/rdd/RDDSuite.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [1, 1, 1]}]}
{"author": "maropu", "sha": "3be38824fec054d1556c7e347def0b54413e563d", "commit_date": "2021/07/16 02:22:14", "commit_message": "Update the golden file", "title": "[SPARK-34819][SQL] MapType supports comparable semantics", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error message, please read the guideline first:\r\n     https://spark.apache.org/error-message-guidelines.html\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nThis PR proposes to support comparable semantics for map types.\r\n\r\nNOTE: This PR is the rework of #31967(@WangGuangxin)/#15970(@hvanhovell).\r\n\r\nThe approach of the PR is similar to `NormalizeFloatingNumbers` and it has the same restriction; in the plan optimizing phase, a new rule named `NormalizeMaps` inserts an expression `SortMapKeys` to make sure two maps having the same key value pairs but with different key ordering are equal (e.g., Map('a' -> 1, 'b' -> 2) should equal to Map('b' -> 2, 'a' -> 1). As for aggregates, this rule is applied in the physical planning phase because all the grouping exprs are not extracted during the logical phase (This is the same restriction with `NormalizeFloatingNumbers`).\r\n\r\nThe major differences from `NormalizeFloatingNumbers` are as follows;\r\n - The rule covers all the binary comparisons (`EqualTo`, `GreaterThan`, ...) and `In`/`InSet` in a plan (`NormalizeFloatingNumbers` is applied only into the `EqualTo` comparison in a join plan, an equi-join).\r\n - This rule does not apply `normalize` recursively and just adds a `SortMapKeys` expr just on each top-level expr (e.g., top-level grouping expr and left/right side expr of binary comparisons).\r\n - This rule additionally handles `SortOrder`s in sort-related plans.\r\n\r\nFor sorting map entries, I reused the array ordering logic (See: `MapType.compare` and `CodegenContext.genComp`) because keys and values in map entries follow the array format; it checks if key arrays in two maps are the same first, an then check if value arrays are the same. \r\n\r\nNOTE: Adding duplicate `SortMapKeys` exprs in a binary comparison tree is a known issue; for example, in a query below, `MapType`'s column, `a`, is sorted twice;\r\n```\r\nscala> Seq((Map(1->1), Map(1->2), Map(1->1))).toDF(\"a\", \"b\", \"c\").write.saveAsTable(\"t\")\r\nscala> sql(\"select * from t where a = b and a = c\").explain()\r\n== Physical Plan ==\r\n*(1) Filter ((sortmapkeys(a#35) = sortmapkeys(b#36)) AND (sortmapkeys(a#35) = sortmapkeys(c#37)))\r\n+- FileScan parquet default.t[a#35,b#36,c#37] Batched: false, DataFilters: [(sortmapkeys(a#35) = sortmapkeys(b#36)), (sortmapkeys(a#35) = sortmapkeys(c#37))], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/maropu/Repositories/spark/spark-master/spark-warehouse/t], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<a:map<int,int>,b:map<int,int>,c:map<int,int>>\r\n```\r\nBut, I don't have a smart idea to avoid it in this PR for now. Probably, I think common subexpression elimination in filter plans can solve it, but Spark does not have the optimization now. (Fro more details, see the previous @viirya PR: https://github.com/apache/spark/pull/30565).\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nTo improve map usability.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nYes, a user can use map-typed data in GROUP BY, ORDER BY, and PARTITION BY in WINDOW clauses.\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nAdd unit tests.", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeMapData.java", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala", "additions": "0", "deletions": "16", "changes": "16"}, "updated": [0, 3, 15]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "27", "deletions": "0", "changes": "27"}, "updated": [1, 3, 10]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ordering.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeFloatingNumbers.scala", "additions": "22", "deletions": "4", "changes": "26"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeMaps.scala", "additions": "192", "deletions": "0", "changes": "192"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "8", "deletions": "2", "changes": "10"}, "updated": [0, 2, 13]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/ArrayBasedMapData.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/MapData.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/TypeUtils.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/types/ArrayType.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/types/MapType.scala", "additions": "28", "deletions": "0", "changes": "28"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala", "additions": "15", "deletions": "25", "changes": "40"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/ExpressionTypeCheckingSuite.scala", "additions": "2", "deletions": "17", "changes": "19"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/PredicateSuite.scala", "additions": "0", "deletions": "8", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeFloatingPointNumbersSuite.scala", "additions": "63", "deletions": "1", "changes": "64"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeMapsSuite.scala", "additions": "108", "deletions": "0", "changes": "108"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/ComplexDataSuite.scala", "additions": "22", "deletions": "16", "changes": "38"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "7", "deletions": "32", "changes": "39"}, "updated": [0, 1, 6]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala", "additions": "33", "deletions": "0", "changes": "33"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/comparable-map-explain.sql", "additions": "34", "deletions": "0", "changes": "34"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/comparable-map.sql", "additions": "182", "deletions": "0", "changes": "182"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/comparable-map-explain.sql.out", "additions": "130", "deletions": "0", "changes": "130"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/comparable-map.sql.out", "additions": "986", "deletions": "0", "changes": "986"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/pivot.sql.out", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/udf/udf-pivot.sql.out", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameAggregateSuite.scala", "additions": "8", "deletions": "10", "changes": "18"}, "updated": [1, 2, 6]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameFunctionsSuite.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSetOperationsSuite.scala", "additions": "0", "deletions": "19", "changes": "19"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 2, 7]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/sources/BucketedWriteSuite.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 0, 0]}]}
{"author": "maropu", "sha": "", "commit_date": "2020/08/20 13:03:03", "commit_message": "Invokes GitHub Actions", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": ".github/workflows/build_and_test.yml", "additions": "1", "deletions": "4", "changes": "5"}, "updated": [2, 2, 2]}, {"file": {"name": "dev/run-tests.py", "additions": "2", "deletions": "16", "changes": "18"}, "updated": [0, 1, 11]}]}
{"author": "eddyxu", "sha": "", "commit_date": "2021/03/02 15:14:19", "commit_message": "[SPARK-34558][SQL] warehouse path should be qualified ahead of populating and use\n\n### What changes were proposed in this pull request?\n\nCurrently, the warehouse path gets fully qualified in the caller side for creating a database, table, partition, etc. An unqualified path is populated into Spark and Hadoop confs, which leads to inconsistent API behaviors.  We should make it qualified ahead.\n\nWhen the value is a relative path `spark.sql.warehouse.dir=lakehouse`, some behaviors become inconsistent, for example.\n\nIf the default database is absent at runtime, the app fails with\n\n```java\nCaused by: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: file:./lakehouse\n\tat org.apache.hadoop.fs.Path.initialize(Path.java:263)\n\tat org.apache.hadoop.fs.Path.<init>(Path.java:254)\n\tat org.apache.hadoop.hive.metastore.Warehouse.getDnsPath(Warehouse.java:133)\n\tat org.apache.hadoop.hive.metastore.Warehouse.getDnsPath(Warehouse.java:137)\n\tat org.apache.hadoop.hive.metastore.Warehouse.getWhRoot(Warehouse.java:150)\n\tat org.apache.hadoop.hive.metastore.Warehouse.getDefaultDatabasePath(Warehouse.java:163)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB_core(HiveMetaStore.java:636)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:655)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:431)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:79)\n\t... 73 more\n```\n\nIf the default database is present at runtime, the app can work with it, and if we create a database, it gets fully qualified, for example\n\n```sql\nspark-sql> create database test;\nTime taken: 0.052 seconds\nspark-sql> desc database test;\nDatabase Name\ttest\nComment\nLocation\tfile:/Users/kentyao/Downloads/spark/spark-3.2.0-SNAPSHOT-bin-20210226/lakehouse/test.db\nOwner\tkentyao\nTime taken: 0.023 seconds, Fetched 4 row(s)\n```\n\nAnother thing is that the log becomes nubilous, for example.\n\n```logtalk\n21/02/27 13:54:17 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('datalake').\n21/02/27 13:54:17 INFO SharedState: Warehouse path is 'lakehouse'.\n```\n\n### Why are the changes needed?\n\nfix bug and ambiguity\n### Does this PR introduce _any_ user-facing change?\n\nyes, the path now resolved with proper order - `warehouse->database->table->partition`\n\n### How was this patch tested?\n\nw/ ut added\n\nCloses #31671 from yaooqinn/SPARK-34558.\n\nAuthored-by: Kent Yao <yao@apache.org>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/internal/SharedState.scala", "additions": "9", "deletions": "7", "changes": "16"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SparkSessionBuilderSuite.scala", "additions": "34", "deletions": "7", "changes": "41"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveSharedStateSuite.scala", "additions": "12", "deletions": "8", "changes": "20"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveSparkSubmitSuite.scala", "additions": "7", "deletions": "5", "changes": "12"}, "updated": [1, 1, 2]}]}
{"author": "holdenk", "sha": "", "commit_date": "2020/11/18 20:39:00", "commit_message": "[SPARK-32381][CORE][SQL][FOLLOWUP] More cleanup on HadoopFSUtils\n\n### What changes were proposed in this pull request?\n\nThis PR is a follow-up of #29471 and does the following improvements for `HadoopFSUtils`:\n1. Removes the extra `filterFun` from the listing API and combines it with the `filter`.\n2. Removes `SerializableBlockLocation` and `SerializableFileStatus` given that `BlockLocation` and `FileStatus` are already serializable.\n3. Hides the `isRootLevel` flag from the top-level API.\n\n### Why are the changes needed?\n\nMain purpose is to simplify the logic within `HadoopFSUtils` as well as cleanup the API.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting unit tests (e.g., `FileIndexSuite`)\n\nCloses #29959 from sunchao/hadoop-fs-utils-followup.\n\nAuthored-by: Chao Sun <sunchao@apple.com>\nSigned-off-by: Holden Karau <hkarau@apple.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/util/HadoopFSUtils.scala", "additions": "19", "deletions": "85", "changes": "104"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/command/CommandUtils.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InMemoryFileIndex.scala", "additions": "11", "deletions": "8", "changes": "19"}, "updated": [1, 1, 2]}]}
{"author": "aokolnychyi", "sha": "ad57d05af8108f60a676e71b53bb8e4514cdb972", "commit_date": "2021/06/17 20:26:36", "commit_message": "[SPARK-35801][SQL] Support DELETE operations that require rewriting data", "title": "[WIP][SPARK-35801][SQL] Support DELETE operations that require rewriting data", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error message, please read the guideline first:\r\n     https://spark.apache.org/error-message-guidelines.html\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\nThis WIP PR shows how we can use the proposed API in SPARK-35801 (per [design doc](https://docs.google.com/document/d/12Ywmc47j3l2WF4anG5vL4qlrhT2OKigb7_EbIKhxg60)) to support DELETE statements that require rewriting data.\r\n\r\n**Note**: This PR must be split into a number of smaller PRs if we decide to adopt this approach. All changes are grouped here only to simplify the review process and support the design doc.\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\nThese changes are required so that Spark can provide support for DELETE, UPDATE, MERGE statements.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\nYes, this PR introduces a set of new APIs for Data Source V2.\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\n\r\nThis PR comes with a trivial test. More tests to come.\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/SupportsRowLevelOperations.java", "additions": "43", "deletions": "0", "changes": "43"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/DeltaBatchWrite.java", "additions": "30", "deletions": "0", "changes": "30"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/DeltaWrite.java", "additions": "30", "deletions": "0", "changes": "30"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/DeltaWriter.java", "additions": "38", "deletions": "0", "changes": "38"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/DeltaWriterFactory.java", "additions": "33", "deletions": "0", "changes": "33"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/RowLevelOperation.java", "additions": "90", "deletions": "0", "changes": "90"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/RowLevelOperationBuilder.java", "additions": "34", "deletions": "0", "changes": "34"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/SupportsDelta.java", "additions": "35", "deletions": "0", "changes": "35"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/InternalRowProjection.scala", "additions": "91", "deletions": "0", "changes": "91"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [2, 7, 20]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/RewriteDeleteFromTable.scala", "additions": "186", "deletions": "0", "changes": "186"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/ReplaceNullWithFalseInPredicate.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/SimplifyConditionalsInPredicate.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/planning/patterns.scala", "additions": "56", "deletions": "0", "changes": "56"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2Commands.scala", "additions": "72", "deletions": "5", "changes": "77"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/write/RowLevelOperationTable.scala", "additions": "47", "deletions": "0", "changes": "47"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Implicits.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/PullupCorrelatedPredicatesSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryRowLevelOperationTable.scala", "additions": "155", "deletions": "0", "changes": "155"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryRowLevelOperationTableCatalog.scala", "additions": "46", "deletions": "0", "changes": "46"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryTable.scala", "additions": "29", "deletions": "7", "changes": "36"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [2, 3, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala", "additions": "7", "deletions": "2", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala", "additions": "10", "deletions": "1", "changes": "11"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/OptimizeMetadataOnlyDeleteFromTable.scala", "additions": "63", "deletions": "0", "changes": "63"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ReplaceRowLevelOperations.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala", "additions": "30", "deletions": "1", "changes": "31"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2Writes.scala", "additions": "21", "deletions": "2", "changes": "23"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2Exec.scala", "additions": "77", "deletions": "6", "changes": "83"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/DeleteFromTableSuite.scala", "additions": "89", "deletions": "0", "changes": "89"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/PlanResolutionSuite.scala", "additions": "9", "deletions": "6", "changes": "15"}, "updated": [0, 0, 3]}]}
{"author": "aokolnychyi", "sha": "", "commit_date": "2021/06/15 07:15:47", "commit_message": "[SPARK-35779][SQL] Dynamic filtering for Data Source V2", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "external/avro/src/test/scala/org/apache/spark/sql/avro/AvroRowReaderSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "external/avro/src/test/scala/org/apache/spark/sql/avro/AvroSuite.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/read/SupportsRuntimeFiltering.java", "additions": "61", "deletions": "0", "changes": "61"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryTable.scala", "additions": "61", "deletions": "5", "changes": "66"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala", "additions": "14", "deletions": "1", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/BatchScanExec.scala", "additions": "60", "deletions": "7", "changes": "67"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala", "additions": "7", "deletions": "3", "changes": "10"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/dynamicpruning/CleanupDynamicPruningFilters.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/dynamicpruning/PartitionPruning.scala", "additions": "30", "deletions": "2", "changes": "32"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DynamicPartitionPruningSuite.scala", "additions": "120", "deletions": "85", "changes": "205"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/FileBasedDataSourceSuite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcV2SchemaPruningSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/ui/SQLAppStatusListenerSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 2]}]}
{"author": "WangGuangxin", "sha": "", "commit_date": "2020/09/15 05:38:01", "commit_message": "[SPARK-32884][TESTS] Mark TPCDSQuery*Suite as ExtendedSQLTest\n\n### What changes were proposed in this pull request?\n\nThis PR aims to mark the following suite as `ExtendedSQLTest` to reduce GitHub Action test time.\n- TPCDSQuerySuite\n- TPCDSQueryANSISuite\n- TPCDSQueryWithStatsSuite\n\n### Why are the changes needed?\n\nCurrently, the longest GitHub Action task is `Build and test / Build modules: sql - other tests` with `1h 57m 10s` while `Build and test / Build modules: sql - slow tests` takes `42m 20s`. With this PR, we can move the workload from `other tests` to `slow tests` task and reduce the total waiting time about 7 ~ 8 minutes.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo. This is a test-only change.\n\n### How was this patch tested?\n\nPass the GitHub Action with the reduced running time.\n\nCloses #29755 from dongjoon-hyun/SPARK-SLOWTEST.\n\nAuthored-by: Dongjoon Hyun <dongjoon@apache.org>\nSigned-off-by: HyukjinKwon <gurwls223@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/TPCDSQuerySuite.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [1, 1, 5]}]}
{"author": "dnskr", "sha": "7d4d4e39a31a815f97b55228f1123c7f0edb2afa", "commit_date": "2021/08/14 10:33:16", "commit_message": "Merge branch 'apache:master' into docs/SPARK-36510", "title": "[SPARK-36510][DOCS] Add spark.redaction.string.regex property to the docs", "body": "### What changes were proposed in this pull request?\r\nThe PR fixes [SPARK-36510](https://issues.apache.org/jira/browse/SPARK-36510) by adding missing `spark.redaction.string.regex` property to the docs\r\n\r\n### Why are the changes needed?\r\nThe property referred by `spark.sql.redaction.string.regex` description as its default value\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\nNot needed for docs\r\n", "failed_tests": [], "files": [{"file": {"name": "docs/configuration.md", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [1, 2, 3]}]}
{"author": "dnskr", "sha": "", "commit_date": "2021/08/14 05:31:21", "commit_message": "[SPARK-34952][SQL][FOLLOWUP] Normalize pushed down aggregate col name and group by col name\n\n### What changes were proposed in this pull request?\nNormalize pushed down aggregate col names and group by col names ...\n\n### Why are the changes needed?\nto handle case sensitive col names\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nModify existing test\n\nCloses #33739 from huaxingao/normalize.\n\nAuthored-by: Huaxin Gao <huaxin_gao@apple.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala", "additions": "7", "deletions": "3", "changes": "10"}, "updated": [1, 2, 5]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 1, 4]}]}
{"author": "wankunde", "sha": "7f823ae04db1cbc70577e04a827699191c7b3bf9", "commit_date": "2021/06/10 14:07:06", "commit_message": "[SPAKR-35713]Bug fix for thread leak in JobCancellationSuite", "title": "[SPARK-35713]Bug fix for thread leak in JobCancellationSuite", "body": "### What changes were proposed in this pull request?\r\n\r\nBug fix for thread leak in JobCancellationSuite UT\r\n\r\n### Why are the changes needed?\r\n\r\nWhen we call Thread.interrupt() method, that thread's interrupt status will be set but it may not really interrupt.\r\nSo when spark task runs in an infinite loop, spark context may fail to interrupt the task thread, and resulting in thread leak. \r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nNo\r\n\r\n### How was this patch tested?\r\n\r\nTest case \"task reaper kills JVM if killed tasks keep running for too long\" in JobCancellationSuite\r\n", "failed_tests": [], "files": [{"file": {"name": "core/src/test/scala/org/apache/spark/JobCancellationSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "wankunde", "sha": "", "commit_date": "2021/03/10 06:55:27", "commit_message": "[SPARK-34681][SQL] Fix bug for full outer shuffled hash join when building left side with non-equal condition\n\n### What changes were proposed in this pull request?\n\nFor full outer shuffled hash join with building hash map on left side, and having non-equal condition, the join can produce wrong result.\n\nThe root cause is `boundCondition` in `HashJoin.scala` always assumes the left side row is `streamedPlan` and right side row is `buildPlan` ([streamedPlan.output ++ buildPlan.output](https://github.com/apache/spark/blob/branch-3.1/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashJoin.scala#L141)). This is valid assumption, except for full outer + build left case.\n\nThe fix is to correct `boundCondition` in `HashJoin.scala` to handle full outer + build left case properly. See reproduce in https://issues.apache.org/jira/browse/SPARK-32399?focusedCommentId=17298414&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17298414 .\n\n### Why are the changes needed?\n\nFix data correctness bug.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nChanged the test in `OuterJoinSuite.scala` to cover full outer shuffled hash join.\nBefore this change, the unit test `basic full outer join using ShuffledHashJoin` in `OuterJoinSuite.scala` is failed.\n\nCloses #31792 from c21/join-bugfix.\n\nAuthored-by: Cheng Su <chengsu@fb.com>\nSigned-off-by: Dongjoon Hyun <dhyun@apple.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashJoin.scala", "additions": "7", "deletions": "1", "changes": "8"}, "updated": [2, 2, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/joins/OuterJoinSuite.scala", "additions": "10", "deletions": "12", "changes": "22"}, "updated": [1, 1, 1]}]}
{"author": "sumeetgajjar", "sha": "", "commit_date": "2021/05/13 23:30:17", "commit_message": "Intial fix v2", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala", "additions": "1", "deletions": "3", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala", "additions": "64", "deletions": "28", "changes": "92"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/Utils.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/scala/org/apache/spark/storage/BlockManagerSuite.scala", "additions": "16", "deletions": "4", "changes": "20"}, "updated": [0, 0, 1]}]}
{"author": "yeshengm", "sha": "", "commit_date": "2021/08/06 20:47:26", "commit_message": "[SPARK-36448] Exceptions in NoSuchItemException.scala have to be case classes to preserve specific exceptions", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.kafka010.KafkaMicroBatchV2SourceWithAdminSuite", "org.apache.spark.sql.hive.execution.command.ShowTablesSuite", "org.apache.spark.sql.execution.command.v1.ShowTablesSuite", "org.apache.spark.sql.execution.datasources.v2.V2SessionCatalogNamespaceSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/NoSuchItemException.scala", "additions": "23", "deletions": "11", "changes": "34"}, "updated": [0, 0, 0]}]}
{"author": "zsxwing", "sha": "", "commit_date": "2021/04/23 22:33:11", "commit_message": "fix", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/streaming/continuous/ContinuousSuite.scala", "additions": "13", "deletions": "9", "changes": "22"}, "updated": [0, 0, 0]}]}
{"author": "tooptoop4", "sha": "", "commit_date": "2021/07/14 02:36:50", "commit_message": "[SQL] Warn if less files visible after stats write\n\n[SQL] Warn if less files visible after stats write", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/BasicWriteStatsTracker.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "Cedric-Magnan", "sha": "", "commit_date": "2021/08/09 09:18:06", "commit_message": "[SPARK-36271][SQL] Unify V1 insert check field name before prepare writter\n\n### What changes were proposed in this pull request?\nUnify DataSource V1 insert schema check field name before prepare writer.\nAnd in this PR we add check for avro V1 insert too.\n\n### Why are the changes needed?\nUnify code and add check for avro V1 insert too.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nAdded UT\n\nCloses #33566 from AngersZhuuuu/SPARK-36271.\n\nAuthored-by: Angerszhuuuu <angers.zhu@gmail.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "external/avro/src/test/scala/org/apache/spark/sql/avro/AvroSuite.scala", "additions": "22", "deletions": "0", "changes": "22"}, "updated": [1, 3, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceUtils.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [1, 2, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [1, 2, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/FileWrite.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetWrite.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/FileBasedDataSourceSuite.scala", "additions": "22", "deletions": "0", "changes": "22"}, "updated": [1, 2, 4]}]}
{"author": "linar-jether", "sha": "b324a5de0fa4842f6af340967d8397ffe27d2903", "commit_date": "2021/06/07 14:04:46", "commit_message": "Merge branch 'master' into pandas-rdd-to-spark-df-SPARK-3284", "title": "[SPARK-32846][SQL][PYTHON] Support createDataFrame from an RDD of pd.DataFrames", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\nAdded support to `createDataFrame` to receive an RDD of `pd.DataFrame` objects, and convert them using arrow into an RDD of record batches which is then directly converted to a spark DF.\r\n\r\nAdded a `pandasRDD` flag to `createDataFrame` to distinguish between `RDD[pd.DataFrame]` and other RDDs without peeking into their content.\r\n\r\n```python\r\nfrom pyspark.sql import SparkSession\r\nimport pyspark\r\nimport pyarrow as pa\r\nimport numpy as np\r\nimport pandas as pd\r\nimport re\r\n\r\nspark = SparkSession \\\r\n    .builder \\\r\n    .master(\"local\") \\\r\n    .appName(\"Python RDD[pd.DataFrame] to spark DF example\") \\\r\n    .getOrCreate()\r\n\r\n# Enable Arrow-based columnar data transfers\r\nspark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\r\nsc = spark.sparkContext\r\n\r\n# Create a spark DF from an RDD of pandas DFs\r\nprdd = sc.range(0, 4).map(lambda x: pd.DataFrame([[x,]*4], columns=list('ABCD')))\r\n\r\nprdd_large = sc.range(0, 32, numSlices=32). \\\r\n    map(lambda x: pd.DataFrame(np.random.randint(0, 100, size=(40 << 15, 4)), columns=list('ABCD')))\r\n\r\ndf = spark.createDataFrame(prdd, schema=None, pandasRDD=True)\r\ndf.toPandas()\r\n```\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\n\r\n### How was this patch tested?\r\nAdded a new test using for creating a spark DF from an RDD of pandas dataframes. \r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\n", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/sql/pandas/conversion.py", "additions": "108", "deletions": "25", "changes": "133"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/pandas/types.py", "additions": "36", "deletions": "0", "changes": "36"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/session.py", "additions": "20", "deletions": "7", "changes": "27"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_arrow.py", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [0, 1, 1]}]}
{"author": "brandondahler", "sha": "2314d1d41f26973a27f6576aeec2f7c6de6dff4f", "commit_date": "2021/06/14 15:09:40", "commit_message": "[SPARK-35739][SQL] Add Java-compatible Dataset.join overloads", "title": "[SPARK-35739][SQL] Add Java-compatible Dataset.join overloads", "body": "### What changes were proposed in this pull request?\r\nAdds 3 new syntactic sugar overloads to Dataset's join method as proposed in [SPARK-35739](https://issues.apache.org/jira/browse/SPARK-35739).\r\n\r\n### Why are the changes needed?\r\nImproved development experience for developers using Spark SQL, specifically when coding in Java.  \r\n\r\nPrior to changes the Seq overloads required developers to use less-known Java-to-Scala converter methods that made code less readable.  The overloads internalize those converter calls for two of the new methods and the third method adds a single-item overload that is useful for both Java and Scala.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes, the three new overloads technically constitute an API change to the Dataset class.  These overloads are net-new and have been commented appropriately in line with the existing methods.\r\n\r\n### How was this patch tested?\r\nTest cases were not added because it is unclear to me where/how syntactic sugar overloads fit into the testing suites (if at all).  Happy to add them if I can be pointed in the correct direction.\r\n\r\n* Changes were tested in Scala via spark-shell.\r\n* Changes were tested in Java by modifying an example:\r\n  ```\r\n  diff --git a/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java b/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\r\n  index 86a9045d8a..342810c1e6 100644\r\n  --- a/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\r\n  +++ b/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\r\n  @@ -124,6 +124,10 @@ public class JavaSparkSQLExample {\r\n       // |-- age: long (nullable = true)\r\n       // |-- name: string (nullable = true)\r\n\r\n  +    df.join(df, new String[] {\"age\"}).show();\r\n  +    df.join(df, \"age\", \"left\").show();\r\n  +    df.join(df, new String[] {\"age\"}, \"left\").show();\r\n  +\r\n       // Select only the \"name\" column\r\n       df.select(\"name\").show();\r\n       // +-------+\r\n  ```", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala", "additions": "79", "deletions": "2", "changes": "81"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameJoinSuite.scala", "additions": "55", "deletions": "0", "changes": "55"}, "updated": [0, 0, 0]}]}
{"author": "brandondahler", "sha": "", "commit_date": "2021/08/14 05:31:21", "commit_message": "[SPARK-34952][SQL][FOLLOWUP] Normalize pushed down aggregate col name and group by col name\n\n### What changes were proposed in this pull request?\nNormalize pushed down aggregate col names and group by col names ...\n\n### Why are the changes needed?\nto handle case sensitive col names\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nModify existing test\n\nCloses #33739 from huaxingao/normalize.\n\nAuthored-by: Huaxin Gao <huaxin_gao@apple.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala", "additions": "7", "deletions": "3", "changes": "10"}, "updated": [1, 2, 5]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 1, 4]}]}
{"author": "Yikun", "sha": "3f5102d5be8240053b7092b329ba71f67220770c", "commit_date": "2021/05/07 08:33:51", "commit_message": "address nit", "title": "[SPARK-35173][SQL][PYTHON] Add multiple columns adding support", "body": "### What changes were proposed in this pull request?\r\nThis PR added the multiple columns adding support for Spark scala/java/python API.\r\n- Expose `withColumns` with Map input as public API in Scala/Java\r\n- Add `withColumns` in PySpark\r\n\r\nThere was also some discussion about adding multiple columns in past JIRA([SPARK-1225](https://issues.apache.org/jira/browse/SPARK-12225), [SPARK-26224](https://issues.apache.org/jira/browse/SPARK-26224)) and [ML](http://apache-spark-developers-list.1001551.n3.nabble.com/DISCUSS-Multiple-columns-adding-replacing-support-in-PySpark-DataFrame-API-td31164.html).\r\n\r\n### Why are the changes needed?\r\nThere were a private method `withColumns` can add columns at one pass [1]:\r\nhttps://github.com/apache/spark/blob/b5241c97b17a1139a4ff719bfce7f68aef094d95/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L2402\r\n\r\nHowever, it was not exposed as public API in Scala/Java, and also PySpark user can only use `withColumn` to add\u00a0one column or replacing the existing one column\u00a0that\u00a0has the same name. \r\n\r\nFor example, if the PySpark user want to add multiple columns, they should call `withColumn` again and again like:\r\n```Python\r\ndf.withColumn(\"key1\", col(\"key1\")).withColumn(\"key2\", col(\"key2\")).withColumn(\"key3\", col(\"key3\"))\r\n```\r\nAfter this patch, the user can use the `withColumn` with columns list args complete columns adding at one pass:\r\n```Python\r\ndf.withColumn({\"key1\":  col(\"key1\"), \"key2\":col(\"key2\"), \"key3\": col(\"key3\")})\r\n```\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes, this PR exposes `withColumns` as public API, and also adds `withColumns` API in PySpark .\r\n\r\n\r\n### How was this patch tested?\r\n- Add new multiple columns adding test, passed\r\n- Existing test, passed", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/sql/dataframe.py", "additions": "32", "deletions": "0", "changes": "32"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/sql/dataframe.pyi", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_dataframe.py", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/java/test/org/apache/spark/sql/JavaDataFrameSuite.java", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "15", "deletions": "3", "changes": "18"}, "updated": [0, 0, 4]}]}
{"author": "Yikun", "sha": "c6d4f21ca368bbc7ba4236dcd9d09904e7b82e5b", "commit_date": "2021/07/08 01:32:50", "commit_message": "Address doctest and use unittest countTestCases", "title": "[SPARK-35721][PYTHON] Path level discover for python unittests", "body": "### What changes were proposed in this pull request?\r\nAdd path level discover for python unittests.\r\n![image](https://user-images.githubusercontent.com/1736354/124094503-6bdeb980-da8b-11eb-9bbe-b086024f6902.png)\r\n\r\nChange list:\r\n- Introduce a **python_discover_paths** in modules.\r\n- Add **_discover_python_unittests** function: it would be called in pthon/run-tests.py to load test module.\r\n- Add **_append_discovred_goals function**: call _discover_python_unittests to refresh m.python_test_goals\r\n- if modules have python_test_goals or **python_discover_paths** would also be considered as python tests.\r\n- Fix: Move logging.basicConfig to head to make sure logging config before any possible logging print.\r\n- Fix: Change python/pyspark/testing/utils.py SPARK_HOME use _find_spark_home to get value.\r\n- Fix: export py4j PYTHONPATH before run test.\r\n\r\nNote:\r\n- **Why use walk_packages but not unittest.defaultTestLoader.discover?** we use `pkgutil.walk_packages` and `unittest.defaultTestLoader.loadTestsFromModule` to load test modules, consider we will add doctest discover in future, we can add something like blow as the impletations of doctest discover: \r\n```python\r\nimport doctest\r\n\r\ndef _contain_doctests_class(module):\r\n    suite = doctest.DocTestSuite(module)\r\n    if suite.countTestCases():\r\n        return True\r\n    else:\r\n        return False\r\n```\r\n- **Why we doesn't add doctests in here**? Currently, not all modules doctests are added to `python_test_goals`, that means these doctests doesn't be excuted, so better add discover doctests in a separate PR.\r\n\r\n- **What's the deps of discover?** the test discover will do real import for every modules, so we need install **all deps of PySpark test modules** before run-tests otherwise the ImportError would be raised.\r\n\r\n\r\n\r\n### Why are the changes needed?\r\nNow we need to specify the python test cases by manually when we add a new testcase. Sometime, we forgot to add the testcase to module list, the testcase would not be executed.\r\n\r\nSuch as:\r\n\r\npyspark-core pyspark.tests.test_pin_thread\r\n\r\nThus we need some auto-discover way to find all testcase rather than specified every case by manually.\r\n\r\nrelated: https://github.com/apache/spark/pull/32867\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n\r\n### How was this patch tested?\r\n1. Add doc tests for _discover_python_unittests.\r\n2. Compare the CI results (this patch and before), see diff in:\r\nBuild modules: pyspark-sql, pyspark-mllib, pyspark-resource: https://www.diffchecker.com/4RAQydBB\r\nBuild modules: pyspark-core, pyspark-streaming, pyspark-ml: https://www.diffchecker.com/F1ccZDKG\r\nBuild modules: pyspark-pandas\uff1ahttps://www.diffchecker.com/eBDne4uA\r\nBuild modules: pyspark-pandas-slow\uff1ahttps://www.diffchecker.com/lySQGrhA\r\n3. local test for python modules:\r\n./dev/run-tests --parallelism 2 --modules \"pyspark-sql\"", "failed_tests": [], "files": [{"file": {"name": ".github/workflows/build_and_test.yml", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 2, 22]}, {"file": {"name": "dev/run-tests.py", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 1, 6]}, {"file": {"name": "dev/sparktestsupport/modules.py", "additions": "39", "deletions": "129", "changes": "168"}, "updated": [0, 4, 16]}, {"file": {"name": "python/pyspark/pandas/tests/indexes/test_base.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 2, 7]}, {"file": {"name": "python/pyspark/pandas/tests/indexes/test_datetime.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 2, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 2, 10]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 2, 5]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 2, 3]}, {"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 2, 4]}, {"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames_groupby.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 2, 4]}, {"file": {"name": "python/pyspark/pandas/tests/test_series.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 4, 8]}, {"file": {"name": "python/pyspark/pandas/tests/test_stats.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 3, 6]}, {"file": {"name": "python/pyspark/testing/utils.py", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 1]}, {"file": {"name": "python/run-tests.py", "additions": "119", "deletions": "2", "changes": "121"}, "updated": [0, 0, 1]}]}
{"author": "Yikun", "sha": "b2dbe9b6c9225267b7a3160e59be13530c7b7f52", "commit_date": "2021/04/21 09:34:25", "commit_message": "Add columns batch adding support for PySpark.", "title": "[WIP][SPARK-35173][PYTHON][SQL] Add multiple columns adding support for PySpark", "body": "### What changes were proposed in this pull request?\r\nThis PR added the multiple columns adding support for PySpark.dataframe.withColumn.\r\n\r\n### Why are the changes needed?\r\nNow, the spark private method `withColumns` can add columns at one pass [1]:\r\nhttps://github.com/apache/spark/blob/b5241c97b17a1139a4ff719bfce7f68aef094d95/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L2402\r\nbut the PySpark user can only use `withColumn` to add\u00a0one column or replacing the existing one column\u00a0that\u00a0has the same name. \r\n\r\nFor example, if the PySpark user want to add multiple columns, they should call `withColumn` again and again like:\r\n```Python\r\nself.df.withColumn(\"key1\", col(\"key1\")).withColumn(\"key2\", col(\"key2\")).withColumn(\"key3\", col(\"key3\"))\r\n```\r\nAfter this patch, the user can use the `withColumn` with columns list args complete columns adding at one pass:\r\n```Python\r\nself.df.withColumn([\"key1\", \"key2\", \"key3\"], [col(\"key1\"), col(\"key2\"), col(\"key3\")])\r\n```\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes, the input types of withColumn are changed, the PySpark can use withColumn to add multiple columns directly.\r\n\r\n\r\n### How was this patch tested?\r\n- Add new multiple columns adding test, passed\r\n- Existing test, passed", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/sql/dataframe.py", "additions": "26", "deletions": "7", "changes": "33"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/dataframe.pyi", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_dataframe.py", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}]}
{"author": "Yikun", "sha": "63a31fee864b9462ca435b11a449c3205282a3d5", "commit_date": "2021/04/29 05:29:31", "commit_message": "Add pyspark_3.1_to_3.2 into index", "title": "[SPARK-35176][PYTHON] Standardize input validation error type", "body": "### What changes were proposed in this pull request?\r\nThis PR corrects some exception type when the function input params are failed to validate due to TypeError.\r\nIn order to convenient to review, there are 3 commits in this PR:\r\n- Standardize input validation error type on sql\r\n- Standardize input validation error type on ml\r\n- Standardize input validation error type on pandas\r\n\r\n### Why are the changes needed?\r\nAs suggestion from Python exception doc [1]: \"Raised when an operation or function is applied to an object of inappropriate type.\", but there are many Value error are raised in some pyspark code, this patch fix them.\r\n\r\n[1] https://docs.python.org/3/library/exceptions.html#TypeError\r\n\r\nNote that: this patch only addresses the exsiting some wrong raise type for input validation, the input validation decorator/framework which mentioned in [SPARK-35176](https://issues.apache.org/jira/browse/SPARK-35176), would be submited in a speparated patch.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes, code can raise the right TypeError instead of ValueError.\r\n\r\n### How was this patch tested?\r\nExisting test case and UT", "failed_tests": [], "files": [{"file": {"name": "python/docs/source/migration_guide/index.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/docs/source/migration_guide/pyspark_3.1_to_3.2.rst", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/base.py", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/classification.py", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 2, 2]}, {"file": {"name": "python/pyspark/ml/evaluation.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/param/__init__.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/regression.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/ml/tests/test_base.py", "additions": "18", "deletions": "5", "changes": "23"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/tests/test_evaluation.py", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/tests/test_param.py", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/mllib/linalg/distributed.py", "additions": "2", "deletions": "4", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/mllib/tests/test_linalg.py", "additions": "12", "deletions": "1", "changes": "13"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/base.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 4]}, {"file": {"name": "python/pyspark/pandas/config.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 4]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "19", "deletions": "19", "changes": "38"}, "updated": [0, 0, 5]}, {"file": {"name": "python/pyspark/pandas/generic.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 3, 8]}, {"file": {"name": "python/pyspark/pandas/groupby.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 6]}, {"file": {"name": "python/pyspark/pandas/indexes/base.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 5]}, {"file": {"name": "python/pyspark/pandas/indexes/multi.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 5]}, {"file": {"name": "python/pyspark/pandas/namespace.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 5]}, {"file": {"name": "python/pyspark/pandas/plot/core.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 3]}, {"file": {"name": "python/pyspark/pandas/series.py", "additions": "11", "deletions": "11", "changes": "22"}, "updated": [0, 0, 5]}, {"file": {"name": "python/pyspark/pandas/strings.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 4]}, {"file": {"name": "python/pyspark/pandas/tests/indexes/test_base.py", "additions": "11", "deletions": "11", "changes": "22"}, "updated": [0, 3, 3]}, {"file": {"name": "python/pyspark/pandas/tests/test_config.py", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "26", "deletions": "26", "changes": "52"}, "updated": [0, 2, 4]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_namespace.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 5, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_series.py", "additions": "16", "deletions": "16", "changes": "32"}, "updated": [0, 2, 3]}, {"file": {"name": "python/pyspark/pandas/tests/test_series_string.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_utils.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/pandas/utils.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 5]}, {"file": {"name": "python/pyspark/sql/dataframe.py", "additions": "31", "deletions": "28", "changes": "59"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_dataframe.py", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/tests/test_functions.py", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/taskcontext.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "Yikun", "sha": "", "commit_date": "2021/04/14 15:07:05", "commit_message": "[SPARK-35061][BUILD] Upgrade pycodestyle from 2.6.0 to 2.7.0\n\n### What changes were proposed in this pull request?\n\nThis PR bumps up the version of pycodestyle from 2.6.0 to 2.7.0 released a month ago.\n\n### Why are the changes needed?\n\n2.7.0 includes three major fixes below (see https://readthedocs.org/projects/pycodestyle/downloads/pdf/latest/):\n\n- Fix physical checks (such as W191) at end of file. PR #961.\n- Add --indent-size option (defaulting to 4). PR #970.\n- W605: fix escaped crlf false positive on windows. PR #976\n\nThe first and third ones could be useful for dev to detect the styles.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo, dev-only.\n\n### How was this patch tested?\n\nManually tested locally.\n\nCloses #32160 from HyukjinKwon/SPARK-35061.\n\nAuthored-by: HyukjinKwon <gurwls223@apache.org>\nSigned-off-by: Dongjoon Hyun <dhyun@apple.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "dev/lint-python", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 2, 3]}]}
{"author": "Yikun", "sha": "e46233af0bbd8f125a0b31f045628e239d0c8382", "commit_date": "2021/06/30 15:03:00", "commit_message": "Path level discover in python/run-test.py", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": ".github/workflows/build_and_test.yml", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 1, 26]}, {"file": {"name": "dev/run-tests.py", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [1, 1, 6]}, {"file": {"name": "dev/sparktestsupport/modules.py", "additions": "39", "deletions": "129", "changes": "168"}, "updated": [4, 6, 16]}, {"file": {"name": "python/pyspark/pandas/tests/indexes/test_base.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 7]}, {"file": {"name": "python/pyspark/pandas/tests/indexes/test_datetime.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 10]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 5]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 3]}, {"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 4]}, {"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames_groupby.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 4]}, {"file": {"name": "python/pyspark/pandas/tests/test_series.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [3, 3, 7]}, {"file": {"name": "python/pyspark/pandas/tests/test_stats.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 5]}, {"file": {"name": "python/pyspark/testing/utils.py", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 1]}, {"file": {"name": "python/run-tests.py", "additions": "119", "deletions": "2", "changes": "121"}, "updated": [0, 0, 1]}]}
{"author": "Yikun", "sha": "0915bf6bb053a7fe53c5eb7ba52a81fc26957c8b", "commit_date": "2021/06/30 15:03:00", "commit_message": "Path level discover in python/run-test.py", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": ".github/workflows/build_and_test.yml", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 1, 26]}, {"file": {"name": "dev/run-tests.py", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [1, 1, 6]}, {"file": {"name": "dev/sparktestsupport/modules.py", "additions": "40", "deletions": "130", "changes": "170"}, "updated": [4, 6, 16]}, {"file": {"name": "python/pyspark/pandas/tests/indexes/test_base.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 7]}, {"file": {"name": "python/pyspark/pandas/tests/indexes/test_datetime.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 10]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 5]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 3]}, {"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 4]}, {"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames_groupby.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 4]}, {"file": {"name": "python/pyspark/pandas/tests/test_series.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [3, 3, 7]}, {"file": {"name": "python/pyspark/pandas/tests/test_stats.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 5]}, {"file": {"name": "python/pyspark/testing/utils.py", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 1]}, {"file": {"name": "python/run-tests.py", "additions": "118", "deletions": "2", "changes": "120"}, "updated": [0, 0, 1]}]}
{"author": "Yikun", "sha": "05ff042611c27270e9b059e4fd0483c7194672c1", "commit_date": "2021/06/29 08:56:13", "commit_message": "[SPARK-35721][PYTHON] Path level discover for python unittests\n\n### What changes were proposed in this pull request?\nAdd path level discover for python unittests.\n\n### Why are the changes needed?\nNow we need to specify the python test cases by manually when we add a new testcase. Sometime, we forgot to add the testcase to module list, the testcase would not be executed.\n\nSuch as:\n- pyspark-core pyspark.tests.test_pin_thread\n\nThus we need some auto-discover way to find all testcase rather than specified every case by manually.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nAdd below code in end of `dev/sparktestsupport/modules.py`\n```python\nfor m in sorted(all_modules):\n    for g in sorted(m.python_test_goals):\n        print(m.name, g)\n```\nCompare the result before and after:\nhttps://www.diffchecker.com/iO3FvhKL\n\nCloses #32867 from Yikun/SPARK_DISCOVER_TEST.\n\nAuthored-by: Yikun Jiang <yikunkero@gmail.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": ".github/workflows/build_and_test.yml", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 25]}, {"file": {"name": "dev/run-tests.py", "additions": "9", "deletions": "1", "changes": "10"}, "updated": [1, 1, 6]}, {"file": {"name": "dev/sparktestsupport/modules.py", "additions": "86", "deletions": "140", "changes": "226"}, "updated": [3, 5, 15]}, {"file": {"name": "python/pyspark/pandas/tests/indexes/test_base.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 1, 6]}, {"file": {"name": "python/pyspark/pandas/tests/indexes/test_datetime.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 1, 5]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 1, 9]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 1, 4]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 1, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 1, 3]}, {"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames_groupby.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 1, 3]}, {"file": {"name": "python/pyspark/pandas/tests/test_series.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_stats.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 1, 4]}, {"file": {"name": "python/pyspark/testing/utils.py", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 1]}]}
{"author": "Yikun", "sha": "8e0065acce17a938482bb102afb6d99ef8d65a8a", "commit_date": "2021/06/29 11:15:11", "commit_message": "Add missing test modules check", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "dev/sparktestsupport/modules.py", "additions": "17", "deletions": "1", "changes": "18"}, "updated": [3, 5, 15]}]}
{"author": "Yikun", "sha": "ce80943c94b6e220467f153b4ef981b5b0da2e99", "commit_date": "2021/06/29 11:15:11", "commit_message": "Add missing test modules check", "title": "", "body": "", "failed_tests": ["org.apache.spark.scheduler.BasicSchedulerIntegrationSuite"], "files": [{"file": {"name": "dev/sparktestsupport/modules.py", "additions": "13", "deletions": "0", "changes": "13"}, "updated": [3, 5, 15]}]}
{"author": "Yikun", "sha": "33a9eaf0fd7f01f56a1f795a283a782053d4ac63", "commit_date": "2021/06/29 05:27:00", "commit_message": "discover-assert", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "dev/sparktestsupport/modules.py", "additions": "16", "deletions": "1", "changes": "17"}, "updated": [1, 3, 13]}]}
{"author": "Yikun", "sha": "32bfa94dbc780ffc575b3c83408902277c8959f3", "commit_date": "2021/04/15 02:30:07", "commit_message": "remove --ff-only and add --squash", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": ".github/workflows/build_and_test.yml", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [2, 7, 14]}]}
{"author": "zwangsheng", "sha": "4517c2cffc461a8a4196a92ec3fe893c851329c2", "commit_date": "2021/06/02 06:10:03", "commit_message": "do some optimizations", "title": "[WIP] [SPARK-35572] [K8S] add hostNetwork feature to executor", "body": "### What changes were proposed in this pull request?\r\nadd hostNetwork feature to executor\r\nmodify BasicExecutorFeatureStep add hostNetwork in executorpod \r\n\r\n### Why are the changes needed?\r\nIn the process of the company's business promotion, the function of k8s hostNetwork is used. But it is found that spark does not support enable hostNetwork in the configuration.\r\nFound that this function will still be commonly used, so I decided to add a perceptible parameter to enable hostNetwork in spark way.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nyes, add new config spark.kubernetes.executor.hostNetwork.enable\r\n\r\n### How was this patch tested?\r\nadd unit test", "failed_tests": [], "files": [{"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 3]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 1, 1]}]}
{"author": "zwangsheng", "sha": "", "commit_date": "2021/08/12 03:50:27", "commit_message": "modify exit executor log logic", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 3]}]}
{"author": "timarmstrong", "sha": "", "commit_date": "2021/07/02 00:45:57", "commit_message": "remove tab", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/test/scala/org/apache/spark/ThreadAudit.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "IonutBoicuAms", "sha": "", "commit_date": "2021/08/11 16:10:15", "commit_message": "fix bug in in disable unnecessary bucketed scan", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/bucketing/DisableUnnecessaryBucketedScan.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/sources/DisableUnnecessaryBucketedScanSuite.scala", "additions": "26", "deletions": "0", "changes": "26"}, "updated": [0, 0, 0]}]}
{"author": "pan3793", "sha": "", "commit_date": "2021/04/19 15:38:50", "commit_message": "[MINOR][SQL] Remove Antlr4 workaround", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/ParseDriver.scala", "additions": "1", "deletions": "12", "changes": "13"}, "updated": [0, 0, 0]}]}
{"author": "mickjermsurawong-stripe", "sha": "", "commit_date": "2020/01/14 07:08:25", "commit_message": "add filter tests for explicitness in sql schema", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "28", "deletions": "1", "changes": "29"}, "updated": [1, 1, 5]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/test/SQLTestData.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 1]}]}
{"author": "jerqi", "sha": "8e2f4fb0b48cb96cc7b6a774269022f2bd2a3882", "commit_date": "2021/08/09 15:20:22", "commit_message": "modify", "title": "[SPARK-36223][SQL][TEST] Cover 3 kinds of join in the TPCDSQueryTestSuite", "body": "### What changes were proposed in this pull request?\r\nIn current github actions we run TPCDSQueryTestSuite for tpcds benchmark. But it's only tested under default configurations. Since we have added the `spark.sql.join.forceApplyShuffledHashJoin` config. Now we can test all 3 join strategies in TPCDS to improve the coverage.\r\n\r\n### Why are the changes needed?\r\nImprove the coverage of join strategies in the TPCS.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo, only for testing.\r\n\r\n### How was this patch tested?\r\nNo need.", "failed_tests": ["org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/TPCDSQueryTestSuite.scala", "additions": "40", "deletions": "4", "changes": "44"}, "updated": [0, 0, 0]}]}
{"author": "jerqi", "sha": "", "commit_date": "2021/05/03 18:37:07", "commit_message": "[SPARK-35297][CORE][DOC][MINOR] Modify the comment about the executor", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/executor/Executor.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}]}
{"author": "WeichenXu123", "sha": "", "commit_date": "2021/05/11 01:34:15", "commit_message": "update", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "mllib/src/test/scala/org/apache/spark/ml/tuning/CrossValidatorSuite.scala", "additions": "0", "deletions": "2", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "mllib/src/test/scala/org/apache/spark/ml/tuning/TrainValidationSplitSuite.scala", "additions": "0", "deletions": "2", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/tuning.py", "additions": "24", "deletions": "28", "changes": "52"}, "updated": [0, 0, 0]}]}
{"author": "tiehexue", "sha": "", "commit_date": "2021/04/25 12:20:32", "commit_message": "revert import statement to stay with scalastyle.", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "external/kafka-0-10/src/main/scala/org/apache/spark/streaming/kafka010/DirectKafkaInputDStream.scala", "additions": "10", "deletions": "8", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "external/kafka-0-10/src/main/scala/org/apache/spark/streaming/kafka010/KafkaUtils.scala", "additions": "5", "deletions": "4", "changes": "9"}, "updated": [0, 0, 0]}]}
{"author": "robert3005", "sha": "e3754487a8fc49622059a47e403945f2850731f2", "commit_date": "2021/08/06 18:25:53", "commit_message": "boun", "title": "[SPARK-21195][CORE] MetricSystem should pick up dynamically registered metrics in sources", "body": "### What changes were proposed in this pull request?\r\nMetricSystem picks up new metrics from sources that are added throughout execution. If you do measurements via dynamic proxies you might not want to redeclare all metrics that the proxies will create and you'd prefer them to get populated as they're being produced. Right now all sources are processed only onceat startup and metrics are picked up only if they have been registered statically at compile time. Behaviour I am proposing lets you not have to declare metrics in two places.\r\n\r\nThis had been previously suggested in https://github.com/apache/spark/pull/18406 and https://github.com/apache/spark/pull/29980. I have reduced the scope of the change to just dynamic metric registration.\r\n\r\n### Why are the changes needed?\r\nCurrently there's no way to access MetricRegistry that MetricsSystem uses to hold its state and as such it's not possible to reprocess a source. MetricsSystem throws if any metric had already been registered previously.\r\n\r\nn.b. the MetricRegistry is added as a constructor argument to make testing easier but could as well be accessed via reflection as a private variable.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\nAdded tests", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala", "additions": "66", "deletions": "19", "changes": "85"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/metrics/MetricsSystemSuite.scala", "additions": "30", "deletions": "9", "changes": "39"}, "updated": [0, 0, 0]}, {"file": {"name": "streaming/src/test/scala/org/apache/spark/streaming/StreamingContextSuite.scala", "additions": "6", "deletions": "3", "changes": "9"}, "updated": [0, 0, 0]}]}
{"author": "zheniantoushipashi", "sha": "6b75f43fee3b83d1e14e09ed3a5e588336602913", "commit_date": "2021/07/03 07:23:45", "commit_message": "[SPARK-36005][SQL] The canCast method of type of char/varchar is modified to be consistent with StringType", "title": "[SPARK-36005][SQL] The canCast method of type of char/varchar is modified to be consistent with StringType", "body": "\r\n### What changes were proposed in this pull request?\r\n\r\n\r\nThe canCast method of type of char/varchar is modified to be consistent with StringType\r\n\r\n\r\nthe method cast will change the type char/varchar to StringType \r\n\r\n  def cast(to: DataType): Column = withExpr {\r\n    val cast = Cast(expr, CharVarcharUtils.replaceCharVarcharWithStringForCast(to))\r\n    cast.setTagValue(Cast.USER_SPECIFIED_CAST, true)\r\n    cast\r\n  }\r\n\r\n\r\nThe canCast method of type of char/varchar  must   be consistent with StringType\r\n\r\n\r\n\r\n\r\n### Why are the changes needed?\r\n\r\nBefore I used stringType instead of char/varchar, my application code has the logic to judge using canCast. There was no problem before, but now it\u2019s changed to char/varchar, and the judgment of canCast fails. If it doesn\u2019t pass, I Need to change a lot of application code\r\n\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nno\r\n\r\n\r\n### How was this patch tested?\r\n\r\ni add UT\u3002\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [1, 5, 16]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CastSuite.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 5, 16]}]}
{"author": "zheniantoushipashi", "sha": "", "commit_date": "2021/07/03 07:23:45", "commit_message": "[SPARK-36005][SQL] The canCast method of type of char/varchar is modified to be consistent with StringType", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala", "additions": "9", "deletions": "9", "changes": "18"}, "updated": [1, 5, 16]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CastSuite.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 5, 16]}]}
{"author": "Run-Lin", "sha": "", "commit_date": "2021/04/25 10:22:47", "commit_message": "In YARN mode, for better user experience, when Spark is started, not only the AppID is printed, but the Tracking URL is also printed to allow users to better track Spark Job", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala", "additions": "8", "deletions": "1", "changes": "9"}, "updated": [0, 1, 1]}]}
{"author": "ReachInfi", "sha": "05dd8b760a994284f2f83ade49869d43835e3e51", "commit_date": "2021/07/15 20:49:19", "commit_message": "Update build_and_test.yml", "title": "[SPARK-36118][SQL] Add bitmap functions for Spark SQL", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nadd functions of bitmap building and computing cardinality for Spark SQL, If this is ok, I will update function.scala and FunctionRegistry.scala.\r\n\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nBitmaps are used more and more widely, and many frameworks have native support, such as Clickhouse\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nNo\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nCI, it performs well on billions of rows based on our real demand", "failed_tests": ["pyspark.mllib.tests.test_streaming_algorithms"], "files": [{"file": {"name": ".github/workflows/build_and_test.yml", "additions": "4", "deletions": "20", "changes": "24"}, "updated": [3, 4, 21]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/bitmap.scala", "additions": "176", "deletions": "0", "changes": "176"}, "updated": [0, 0, 0]}]}
{"author": "rashtao", "sha": "", "commit_date": "2021/04/23 08:01:55", "commit_message": "reverted default JsonFactory creation", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JacksonGenerator.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}]}
{"author": "turboFei", "sha": "", "commit_date": "2021/04/23 02:37:11", "commit_message": "[SPARK-21499][FOLLOWUP] Update doc for registering Spark UDAFs in Spark SQL queries", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "docs/sql-ref-functions-udf-aggregate.md", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "FridaPolished", "sha": "", "commit_date": "2021/04/09 18:52:55", "commit_message": "[SPARK-34963][SQL] Fix nested column pruning for extracting case-insensitive struct field from array of struct\n\n### What changes were proposed in this pull request?\n\nThis patch proposes a fix of nested column pruning for extracting case-insensitive struct field from array of struct.\n\n### Why are the changes needed?\n\nUnder case-insensitive mode, nested column pruning rule cannot correctly push down extractor of a struct field of an array of struct, e.g.,\n\n```scala\nval query = spark.table(\"contacts\").select(\"friends.First\", \"friends.MiDDle\")\n```\n\nError stack:\n```\n[info]   java.lang.IllegalArgumentException: Field \"First\" does not exist.\n[info] Available fields:\n[info]   at org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)\n[info]   at org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)\n[info]   at scala.collection.MapLike$class.getOrElse(MapLike.scala:128)\n[info]   at scala.collection.AbstractMap.getOrElse(Map.scala:59)\n[info]   at org.apache.spark.sql.types.StructType.apply(StructType.scala:273)\n[info]   at org.apache.spark.sql.execution.ProjectionOverSchema$$anonfun$getProjection$3.apply(ProjectionOverSchema.scala:44)\n[info]   at org.apache.spark.sql.execution.ProjectionOverSchema$$anonfun$getProjection$3.apply(ProjectionOverSchema.scala:41)\n```\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nUnit test\n\nCloses #32059 from viirya/fix-array-nested-pruning.\n\nAuthored-by: Liang-Chi Hsieh <viirya@gmail.com>\nSigned-off-by: Liang-Chi Hsieh <viirya@gmail.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ProjectionOverSchema.scala", "additions": "7", "deletions": "2", "changes": "9"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/SelectedField.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/SchemaPruningSuite.scala", "additions": "42", "deletions": "0", "changes": "42"}, "updated": [1, 1, 1]}]}
{"author": "eejbyfeldt", "sha": "", "commit_date": "2021/07/10 10:04:47", "commit_message": "Add and adopt DataFrameSuite tests cases\n\nThe test cases are from:\nhttps://github.com/apache/spark/pull/33205", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.HiveExternalCatalogVersionsSuite", "org.apache.spark.sql.DataFrameSuite"], "files": [{"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "51", "deletions": "1", "changes": "52"}, "updated": [0, 3, 7]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/test/SQLTestData.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 1]}]}
{"author": "sammyjmoseley", "sha": "", "commit_date": "2021/07/15 14:02:15", "commit_message": "fix diff", "title": "", "body": "", "failed_tests": ["pyspark.pandas.indexes.base", "pyspark.pandas.tests.indexes.test_base"], "files": [{"file": {"name": "build/zinc-0.3.15/bin/nailgun", "additions": "0", "deletions": "50", "changes": "50"}, "updated": [0, 0, 0]}, {"file": {"name": "build/zinc-0.3.15/bin/ng/darwin32/ng", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "build/zinc-0.3.15/bin/ng/darwin64/ng", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "build/zinc-0.3.15/bin/ng/linux32/ng", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "build/zinc-0.3.15/bin/ng/linux64/ng", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "build/zinc-0.3.15/bin/ng/linuxppc64le/ng", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "build/zinc-0.3.15/bin/ng/win32/ng.exe", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "build/zinc-0.3.15/bin/zinc", "additions": "0", "deletions": "257", "changes": "257"}, "updated": [0, 0, 0]}, {"file": {"name": "build/zinc-0.3.15/lib/compiler-interface-sources.jar", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "build/zinc-0.3.15/lib/incremental-compiler.jar", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "build/zinc-0.3.15/lib/nailgun-server.jar", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "build/zinc-0.3.15/lib/sbt-interface.jar", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "build/zinc-0.3.15/lib/scala-compiler.jar", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "build/zinc-0.3.15/lib/scala-library.jar", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "build/zinc-0.3.15/lib/scala-reflect.jar", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "build/zinc-0.3.15/lib/zinc.jar", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}]}
{"author": "vkorukanti", "sha": "", "commit_date": "2021/06/15 06:26:31", "commit_message": "[SPARK-35763][SS] Add a new copy method to StateStoreCustomMetric", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStore.scala", "additions": "10", "deletions": "3", "changes": "13"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/SymmetricHashJoinStateManager.scala", "additions": "1", "deletions": "9", "changes": "10"}, "updated": [0, 2, 2]}]}
{"author": "shardulm94", "sha": "f39e7c997562c67b4fa9a374e993744512ad3b84", "commit_date": "2021/07/22 21:07:10", "commit_message": "Fix compile error after revert", "title": "[SPARK-36215][SHUFFLE] Add logging for slow fetches to diagnose external shuffle service issues", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\nAdd logging to `ShuffleBlockFetcherIterator` to log \"slow\" fetches, where slow is defined by two confs: `spark.reducer.shuffleFetchSlowLogThreshold.time` and `spark.reducer.shuffleFetchSlowLogThreshold.bytesPerSec`\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\n\r\n### Why are the changes needed?\r\nCurrently we can see from the metrics that a task or stage has slow fetches, and the logs indicate *all* of the shuffle servers those tasks were fetching from, but often this is a big set (dozens or even hundreds) and narrowing down which one caused issues can be very difficult. This change makes it easier to understand which fetch is \"slow\".\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nAdds two configs `spark.reducer.shuffleFetchSlowLogThreshold.time` and `spark.reducer.shuffleFetchSlowLogThreshold.bytesPerSec`\r\n\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nAdded unit test", "failed_tests": ["pyspark.mllib.tests.test_streaming_algorithms"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/TestUtils.scala", "additions": "32", "deletions": "1", "changes": "33"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [1, 2, 5]}, {"file": {"name": "core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala", "additions": "43", "deletions": "5", "changes": "48"}, "updated": [0, 1, 4]}, {"file": {"name": "core/src/test/scala/org/apache/spark/storage/ShuffleBlockFetcherIteratorSuite.scala", "additions": "47", "deletions": "3", "changes": "50"}, "updated": [0, 0, 2]}, {"file": {"name": "docs/configuration.md", "additions": "22", "deletions": "0", "changes": "22"}, "updated": [0, 0, 1]}]}
{"author": "shardulm94", "sha": "", "commit_date": "2021/02/23 14:25:50", "commit_message": "Fix tests [Take 2]", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/HiveQuerySuite.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 2]}]}
{"author": "wForget", "sha": "d37c84358b6fe1574941feb6943f16bd0277dd3d", "commit_date": "2021/04/29 06:48:02", "commit_message": "[SPARK-35270][SQL][CORE] Remove the use of guava in order to upgrade guava version to 27.", "title": "[SPARK-35270][SQL][CORE] Remove the use of guava in order to upgrade guava version to 27", "body": "### What changes were proposed in this pull request?\r\n\r\nRemove the use of guava in order to upgrade guava version to 27.\r\n\r\n\r\n### Why are the changes needed?\r\n\r\nHadoop 3.2.2 uses Guava 27, the change is for the guava version upgrade.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nno\r\n\r\n\r\n### How was this patch tested?\r\n\r\nModify the guava version to 27.0-jre, and then compile.", "failed_tests": [], "files": [{"file": {"name": "common/kvstore/src/main/java/org/apache/spark/util/kvstore/InMemoryStore.java", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "common/network-common/src/main/java/org/apache/spark/network/protocol/AbstractMessage.java", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/RemoteBlockPushResolver.java", "additions": "8", "deletions": "6", "changes": "14"}, "updated": [0, 0, 1]}, {"file": {"name": "common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/protocol/FinalizeShuffleMerge.java", "additions": "9", "deletions": "6", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/protocol/MergeStatuses.java", "additions": "8", "deletions": "6", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/protocol/PushBlockStream.java", "additions": "12", "deletions": "9", "changes": "21"}, "updated": [0, 0, 0]}, {"file": {"name": "common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/protocol/mesos/RegisterDriver.java", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/rdd/RDDOperationScope.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "mllib/src/main/scala/org/apache/spark/mllib/linalg/Matrices.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "mllib/src/main/scala/org/apache/spark/mllib/linalg/distributed/BlockMatrix.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "mllib/src/main/scala/org/apache/spark/mllib/tree/model/InformationGainStats.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "mllib/src/main/scala/org/apache/spark/mllib/tree/model/Predict.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala", "additions": "2", "deletions": "3", "changes": "5"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/java/test/org/apache/spark/sql/JavaDatasetSuite.java", "additions": "12", "deletions": "13", "changes": "25"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveShim.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}]}
{"author": "hanover-fiste", "sha": "", "commit_date": "2021/08/12 13:35:28", "commit_message": "[SPARK-35881][SQL][FOLLOWUP] Remove the AQE post stage creation extension\n\n### What changes were proposed in this pull request?\n\nThis is a followup of #33140\n\nIt turns out that we may be able to complete the AQE and columnar execution integration without the AQE post stage creation extension. The rule `ApplyColumnarRulesAndInsertTransitions` can add to-columnar transition if the shuffle/broadcast supports columnar.\n\n### Why are the changes needed?\n\nremove APIs that are not needed.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo, the APIs are not released yet.\n\n### How was this patch tested?\n\nexisting and manual tests\n\nCloses #33701 from cloud-fan/aqe.\n\nAuthored-by: Wenchen Fan <wenchen@databricks.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/SparkSessionExtensions.scala", "additions": "0", "deletions": "20", "changes": "20"}, "updated": [1, 2, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "10", "deletions": "6", "changes": "16"}, "updated": [1, 4, 11]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala", "additions": "1", "deletions": "6", "changes": "7"}, "updated": [1, 2, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [1, 2, 2]}]}
{"author": "rahulsmahadev", "sha": "", "commit_date": "2021/06/29 22:19:52", "commit_message": "Fix build", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.streaming.FlatMapGroupsWithStateSuite"], "files": [{"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/UnsupportedOperationsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/KeyValueGroupedDataset.scala", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 3, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/FlatMapGroupsWithStateExecHelper.scala", "additions": "0", "deletions": "2", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/streaming/FlatMapGroupsWithStateSuite.scala", "additions": "3", "deletions": "4", "changes": "7"}, "updated": [0, 1, 1]}]}
{"author": "kotlovs", "sha": "814e391a0f7df53c7a1ad96c895c80f5745e591e", "commit_date": "2021/04/15 13:48:35", "commit_message": "Close SparkContext after the Main method has finished, to allow SparkApplication on K8S to complete", "title": "[SPARK-34674][CORE][K8S] Close SparkContext after the Main method has finished", "body": "### What changes were proposed in this pull request?\r\nClose SparkContext after the Main method has finished, to allow SparkApplication on K8S to complete.\r\nThis is fixed version of [merged and reverted PR](https://github.com/apache/spark/pull/32081).\r\n\r\n### Why are the changes needed?\r\nif I don't call the method sparkContext.stop() explicitly, then a Spark driver process doesn't terminate even after its Main method has been completed. This behaviour is different from spark on yarn, where the manual sparkContext stopping is not required. It looks like, the problem is in using non-daemon threads, which prevent the driver jvm process from terminating.\r\nSo I have inserted code that closes sparkContext automatically.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\nManually on the production AWS EKS environment in my company.\r\n", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [1, 3, 7]}]}
{"author": "kotlovs", "sha": "a7efe992484c9012fb8c5d7aae07ddb4de67459b", "commit_date": "2021/04/14 23:36:43", "commit_message": "Add logging", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 2, 6]}]}
{"author": "kotlovs", "sha": "7fbd41b73feceb108522d8771edd04cb2471da89", "commit_date": "2021/04/07 12:57:29", "commit_message": "Close SparkContext after the Main method has finished, to allow SparkApplication on K8S to complete", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 0, 4]}]}
{"author": "JkSelf", "sha": "", "commit_date": "2021/06/04 08:06:21", "commit_message": "fix the failed uts", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "5", "deletions": "2", "changes": "7"}, "updated": [0, 2, 4]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DynamicPartitionPruningSuite.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 1, 2]}]}
{"author": "steven-aerts", "sha": "", "commit_date": "2021/07/02 11:57:52", "commit_message": "[SPARK-35985][SQL] push partitionFilters for empty readDataSchema\n\nthis commit makes sure that for File Source V2 partition filters are\nalso taken into account when the readDataSchema is empty.\nThis is the case for queries like:\n\n  SELECT count(*) FROM tbl WHERE partition=foo\n  SELECT input_file_name() FROM tbl WHERE partition=foo", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/PruneFileSourcePartitionsSuite.scala", "additions": "25", "deletions": "1", "changes": "26"}, "updated": [0, 0, 0]}]}
{"author": "geekyouth", "sha": "", "commit_date": "2021/07/01 03:40:00", "commit_message": "[SPARK-35714][FOLLOW-UP][CORE] Use a shared stopping flag for WorkerWatcher to avoid the duplicate System.exit\n\n### What changes were proposed in this pull request?\n\nThis PR proposes to let `WorkerWatcher` reuse the `stopping` flag in `CoarseGrainedExecutorBackend` to avoid the duplicate call of `System.exit`.\n\n### Why are the changes needed?\n\nAs a followup of https://github.com/apache/spark/pull/32868, this PR tries to give a more robust fix.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nPass existing tests.\n\nCloses #33028 from Ngone51/spark-35714-followup.\n\nLead-authored-by: yi.wu <yi.wu@databricks.com>\nCo-authored-by: wuyi <yi.wu@databricks.com>\nSigned-off-by: yi.wu <yi.wu@databricks.com>\n(cherry picked from commit 868a59470650cc12272de0d0b04c6d98b1fe076d)\nSigned-off-by: yi.wu <yi.wu@databricks.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/deploy/worker/WorkerWatcher.scala", "additions": "8", "deletions": "9", "changes": "17"}, "updated": [1, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala", "additions": "19", "deletions": "14", "changes": "33"}, "updated": [1, 3, 4]}]}
{"author": "leoluan2009", "sha": "", "commit_date": "2020/09/16 06:05:35", "commit_message": "[SPARK-32861][SQL] GenerateExec should require column ordering\n\n### What changes were proposed in this pull request?\nThis PR updates the `RemoveRedundantProjects` rule to make `GenerateExec` require column ordering.\n\n### Why are the changes needed?\n`GenerateExec` was originally considered as a node that does not require column ordering. However, `GenerateExec` binds its input rows directly with its `requiredChildOutput` without using the child's output schema.\nIn `doExecute()`:\n```scala\nval proj = UnsafeProjection.create(output, output)\n```\nIn `doConsume()`:\n```scala\nval values = if (requiredChildOutput.nonEmpty) {\n  input\n} else {\n  Seq.empty\n}\n```\nIn this case, changing input column ordering will result in `GenerateExec` binding the wrong schema to the input columns. For example, if we do not require child columns to be ordered, the `requiredChildOutput` [a, b, c] will directly bind to the schema of the input columns [c, b, a], which is incorrect:\n```\nGenerateExec explode(array(a, b, c)), [a, b, c], false, [d]\n  HashAggregate(keys=[a, b, c], functions=[], output=[c, b, a])\n    ...\n```\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nUnit test\n\nCloses #29734 from allisonwang-db/generator.\n\nAuthored-by: allisonwang-db <66282705+allisonwang-db@users.noreply.github.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/RemoveRedundantProjects.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/RemoveRedundantProjectsSuite.scala", "additions": "48", "deletions": "6", "changes": "54"}, "updated": [1, 1, 2]}]}
{"author": "weixiuli", "sha": "", "commit_date": "2021/09/01 08:23:46", "commit_message": " [SPARK-36635][SQL] Fix a mismatch issue that select name expressions as string type", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.parser.ExpressionParserSuite", "org.apache.spark.sql.catalyst.parser.DDLParserSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.DateFunctionsSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala", "additions": "26", "deletions": "0", "changes": "26"}, "updated": [0, 2, 7]}]}
{"author": "dtarima", "sha": "", "commit_date": "2021/07/08 02:46:55", "commit_message": "[SPARK-36036] [CORE] Add JIRA number to test description.", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/test/scala/org/apache/spark/storage/BlockManagerSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 5]}]}
{"author": "tomvanbussel", "sha": "", "commit_date": "2021/06/27 14:56:29", "commit_message": "Fix", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/RowToColumnConverterSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "matthewrj", "sha": "", "commit_date": "2021/06/19 20:49:49", "commit_message": "Fix compile error", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "graphx/src/main/scala/org/apache/spark/graphx/GraphXUtils.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "shipra-a", "sha": "a7e97ddf3c46641d9c8c298d09c4c4e4ac6024ae", "commit_date": "2021/07/01 00:13:09", "commit_message": "remove unintended changes", "title": "[WIP][SPARK-31973][SQL] Skip partial aggregates in run-time if reduction ratio is low", "body": "### What changes were proposed in this pull request?\r\n\r\nThis PR builds on top of https://github.com/apache/spark/pull/28804. In addition to the other PR, one other change is that the partial aggregation hashmap is freed as soon as partial aggregation is disabled to prevent off-heap OOMs.\r\n\r\n\r\n### Why are the changes needed?\r\n\r\nThis change can help improve query performance.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nNo\r\n\r\n### How was this patch tested?\r\n\r\nExisting and additional unit tests.\r\n\r\n", "failed_tests": ["org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "42", "deletions": "0", "changes": "42"}, "updated": [2, 3, 13]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap.java", "additions": "13", "deletions": "0", "changes": "13"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala", "additions": "206", "deletions": "49", "changes": "255"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashMapGenerator.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/WholeStageCodegenSuite.scala", "additions": "37", "deletions": "0", "changes": "37"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/metric/SQLMetricsSuite.scala", "additions": "45", "deletions": "41", "changes": "86"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/AggregationQuerySuite.scala", "additions": "34", "deletions": "27", "changes": "61"}, "updated": [0, 0, 1]}]}
{"author": "clarkead", "sha": "", "commit_date": "2020/08/26 17:34:49", "commit_message": "[MINOR][PYTHON] Fix typo in a docsting of RDD.toDF\n\n### What changes were proposed in this pull request?\n\nFixes typo in docsting of `toDF`\n\n### Why are the changes needed?\n\nThe third argument of `toDF` is actually `sampleRatio`.\nrelated discussion: https://github.com/apache/spark/pull/12746#discussion-diff-62704834\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nThis patch doesn't affect any logic, so existing tests should cover it.\n\nCloses #29551 from unirt/minor_fix_docs.\n\nAuthored-by: unirt <lunirtc@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/sql/session.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [2, 2, 5]}]}
{"author": "PavithraRamachandran", "sha": "", "commit_date": "2021/06/24 12:44:59", "commit_message": "address cve CVE-2015-523 in protypebuf-java jar", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-2.7-hive-2.3", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 19]}, {"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 21]}, {"file": {"name": "pom.xml", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 7, 32]}]}
{"author": "opensky142857", "sha": "3e27a7e119b135fe0c050a840edd4f57a16c7e02", "commit_date": "2021/05/28 07:04:45", "commit_message": "refine code", "title": "[SPARK-35531][SQL] Can not insert into hive bucket table if create table with upper case schema", "body": "### What changes were proposed in this pull request?\r\n\r\nwhen convert to HiveTable, respect table schema cases.\r\n\r\n### Why are the changes needed?\r\n\r\nWhen user create a hive bucket table with upper case schema, the table schema will be stored as lower cases while bucket column info will stay the same with user input.\r\n\r\nif we try to insert into this table, an HiveException reports bucket column is not in table schema. \r\n\r\nhere is a simple repro\r\n\r\n```scala\r\nspark.sql(\"\"\"\r\n  CREATE TABLE TEST1(\r\n    V1 BIGINT,\r\n    S1 INT)\r\n  PARTITIONED BY (PK BIGINT)\r\n  CLUSTERED BY (V1)\r\n  SORTED BY (S1)\r\n  INTO 200 BUCKETS\r\n  STORED AS PARQUET \"\"\").show\r\n\r\nspark.sql(\"INSERT INTO TEST1 SELECT * FROM VALUES(1,1,1)\").show\r\n```\r\n\r\nError message:\r\n```\r\nscala> spark.sql(\"INSERT INTO TEST1 SELECT * FROM VALUES(1,1,1)\").show\r\norg.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: Bucket columns V1 is not part of the table columns ([FieldSchema(name:v1, type:bigint, comment:null), FieldSchema(name:s1, type:int, comment:null)]\r\n  at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:112)\r\n  at org.apache.spark.sql.hive.HiveExternalCatalog.listPartitions(HiveExternalCatalog.scala:1242)\r\n  at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.listPartitions(ExternalCatalogWithListener.scala:254)\r\n  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.listPartitions(SessionCatalog.scala:1166)\r\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:103)\r\n  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\r\n  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\r\n  at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:120)\r\n  at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:228)\r\n  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r\n  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\r\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r\n  at org.apache.spark.sql.Dataset.<init>(Dataset.scala:228)\r\n  at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\r\n  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\r\n  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\r\n  at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)\r\n  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\r\n  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)\r\n  ... 47 elided\r\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Bucket columns V1 is not part of the table columns ([FieldSchema(name:v1, type:bigint, comment:null), FieldSchema(name:s1, type:int, comment:null)]\r\n  at org.apache.hadoop.hive.ql.metadata.Table.setBucketCols(Table.java:552)\r\n  at org.apache.spark.sql.hive.client.HiveClientImpl$.toHiveTable(HiveClientImpl.scala:1082)\r\n  at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$getPartitions$1(HiveClientImpl.scala:732)\r\n  at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:291)\r\n  at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:224)\r\n  at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:223)\r\n  at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:273)\r\n  at org.apache.spark.sql.hive.client.HiveClientImpl.getPartitions(HiveClientImpl.scala:731)\r\n  at org.apache.spark.sql.hive.client.HiveClient.getPartitions(HiveClient.scala:222)\r\n  at org.apache.spark.sql.hive.client.HiveClient.getPartitions$(HiveClient.scala:218)\r\n  at org.apache.spark.sql.hive.client.HiveClientImpl.getPartitions(HiveClientImpl.scala:91)\r\n  at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$listPartitions$1(HiveExternalCatalog.scala:1245)\r\n  at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)\r\n  ... 69 more\r\n```\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nNo\r\n\r\n### How was this patch tested?\r\n\r\nUT\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala", "additions": "11", "deletions": "2", "changes": "13"}, "updated": [0, 1, 6]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/InsertSuite.scala", "additions": "65", "deletions": "0", "changes": "65"}, "updated": [0, 0, 0]}]}
{"author": "kbendick", "sha": "", "commit_date": "2020/11/03 22:20:58", "commit_message": "Remove outdated comment", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": ".github/labeler.yml", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}]}
{"author": "kbendick", "sha": "9f9ccf7c8fb5e2569f7832b3147c89b404196dcf", "commit_date": "2021/06/23 04:39:14", "commit_message": "Increase stack size in build with maven", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": ".github/workflows/build_and_test.yml", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 3, 26]}, {"file": {"name": "pom.xml", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 8, 32]}]}
{"author": "kbendick", "sha": "f276fcf5d7db71bba424935cf320eac53440d16c", "commit_date": "2020/10/29 21:37:59", "commit_message": "Set up new PR labeler workflow and begin porting the probot labeler config to the new format", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": ".github/labeler.yml", "additions": "153", "deletions": "0", "changes": "153"}, "updated": [0, 0, 0]}, {"file": {"name": ".github/workflows/build_and_test.yml", "additions": "422", "deletions": "414", "changes": "836"}, "updated": [1, 6, 11]}, {"file": {"name": ".github/workflows/labeler.yml", "additions": "30", "deletions": "0", "changes": "30"}, "updated": [0, 0, 0]}, {"file": {"name": ".github/workflows/stale.yml", "additions": "21", "deletions": "21", "changes": "42"}, "updated": [0, 0, 0]}, {"file": {"name": ".github/workflows/test_report.yml", "additions": "23", "deletions": "23", "changes": "46"}, "updated": [0, 1, 2]}, {"file": {"name": "R/README.md", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "yos1p", "sha": "", "commit_date": "2021/06/07 05:03:54", "commit_message": "Remove ps_10mins.ipynb", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/docs/source/getting_started/ps_10mins.ipynb", "additions": "0", "deletions": "14471", "changes": "14471"}, "updated": [0, 1, 1]}, {"file": {"name": "python/docs/source/getting_started/ps_install.rst", "additions": "0", "deletions": "145", "changes": "145"}, "updated": [0, 1, 1]}, {"file": {"name": "python/docs/source/getting_started/ps_videos_blogs.rst", "additions": "0", "deletions": "130", "changes": "130"}, "updated": [0, 1, 1]}]}
{"author": "lizhangdatabricks", "sha": "", "commit_date": "2021/06/22 06:30:05", "commit_message": "Improve ScalaDoc and error msgs; Add Java unit tests", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/GroupStateImpl.scala", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/streaming/TestGroupState.scala", "additions": "27", "deletions": "24", "changes": "51"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/java/test/org/apache/spark/sql/JavaDatasetSuite.java", "additions": "92", "deletions": "0", "changes": "92"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/streaming/FlatMapGroupsWithStateSuite.scala", "additions": "5", "deletions": "2", "changes": "7"}, "updated": [0, 0, 0]}]}
{"author": "VasilyKolpakov", "sha": "", "commit_date": "2021/05/12 20:34:21", "commit_message": "Fix memory leak in ExecutorAllocationListener", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala", "additions": "26", "deletions": "15", "changes": "41"}, "updated": [0, 0, 1]}]}
{"author": "copperybean", "sha": "", "commit_date": "2021/05/20 05:35:18", "commit_message": "Create scala.yml", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": ".github/workflows/scala.yml", "additions": "22", "deletions": "0", "changes": "22"}, "updated": [0, 0, 0]}]}
{"author": "haiyangsun-db", "sha": "", "commit_date": "2021/06/08 15:53:43", "commit_message": "Use copy-on-write semantics for SQLConf registered configurations.", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "65", "deletions": "33", "changes": "98"}, "updated": [0, 3, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/plans/SQLHelper.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/RuntimeConfig.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/api/python/PythonSQLUtils.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/internal/SharedState.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}]}
{"author": "xuechendi", "sha": "", "commit_date": "2021/05/17 09:00:47", "commit_message": "Add AutoCloseable close to MemoryStore\n\nThis commit is aimed to close some object may not be close by JVM GC\nFixes upon @srowen reviews:\n1. Add try catch to skip NonFatal exception during close\n2. Add an async caller to avoid close may hold BlockManager lock for too long\n3. Add UT to clarify this PR\n\nSigned-off-by: Chendi Xue <chendi.xue@intel.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/pom.xml", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala", "additions": "40", "deletions": "1", "changes": "41"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/storage/MemoryStoreSuite.scala", "additions": "105", "deletions": "0", "changes": "105"}, "updated": [0, 0, 0]}]}
{"author": "nolanliou", "sha": "", "commit_date": "2021/05/26 06:49:57", "commit_message": "fix(pyspark): fix OverflowError in partitionBy function", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/rdd.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}]}
{"author": "satishgopalani", "sha": "", "commit_date": "2021/05/04 17:36:54", "commit_message": "Fixing load of maxTriggerDelayMs", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaSource.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}]}
{"author": "satishgopalani", "sha": "090688510e82d925fe2fb2688c40f94de73b2b0c", "commit_date": "2021/05/03 17:20:12", "commit_message": "Adding Smart Trigger which skips batch if the number of available records in Kafka are less than minOffsetsPerTrigger", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "docs/structured-streaming-kafka-integration.md", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaMicroBatchStream.scala", "additions": "56", "deletions": "3", "changes": "59"}, "updated": [0, 0, 0]}, {"file": {"name": "external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaSource.scala", "additions": "65", "deletions": "7", "changes": "72"}, "updated": [0, 0, 0]}, {"file": {"name": "external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaSourceProvider.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 2, 2]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/read/streaming/CompositeReadLimit.java", "additions": "61", "deletions": "0", "changes": "61"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/read/streaming/ReadLimit.java", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/read/streaming/ReadMinRows.java", "additions": "55", "deletions": "0", "changes": "55"}, "updated": [0, 0, 0]}]}
{"author": "alkispoly-db", "sha": "", "commit_date": "2021/05/29 01:44:17", "commit_message": "Fixed tests for Null values", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/stat/StatFunctions.scala", "additions": "46", "deletions": "69", "changes": "115"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameStatSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "mariosmeim-db", "sha": "", "commit_date": "2021/05/20 10:01:00", "commit_message": "revert import changes", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/jdbc/MySQLDialect.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCSuite.scala", "additions": "9", "deletions": "1", "changes": "10"}, "updated": [0, 0, 1]}]}
{"author": "keerthanvasist", "sha": "", "commit_date": "2021/06/03 17:03:56", "commit_message": "[Python][Bug] Fix ambiguous reference in functions.py column()\n\nFixes TypeError: 'str' object is not callable in 3.1.2", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/sql/functions.py", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [2, 3, 5]}]}
{"author": "mggger", "sha": "", "commit_date": "2021/06/04 08:47:05", "commit_message": "Merge branch 'apache:master' into master", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/SessionCatalog.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/catalog/SessionCatalogSuite.scala", "additions": "2", "deletions": "11", "changes": "13"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/DDLSuite.scala", "additions": "1", "deletions": "6", "changes": "7"}, "updated": [1, 1, 1]}]}
{"author": "huskysun", "sha": "", "commit_date": "2021/05/31 03:54:17", "commit_message": "Revert change on k8s example, fix yarn example\n\nThis is because, in bash, '#' can't come after '\\'.", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "docs/submitting-applications.md", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 1]}]}
{"author": "ankurdave", "sha": "", "commit_date": "2021/05/21 19:11:06", "commit_message": "[SPARK-35486][CORE] TaskMemoryManager: retry if other task takes memory freed by partial self-spill", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/java/org/apache/spark/memory/TaskMemoryManager.java", "additions": "10", "deletions": "2", "changes": "12"}, "updated": [0, 0, 0]}]}
{"author": "felixcheung", "sha": "", "commit_date": "2021/05/24 00:19:27", "commit_message": "change SparkR maintainer", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "R/pkg/DESCRIPTION", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}
{"author": "jacobhjkim", "sha": "", "commit_date": "2020/09/03 11:56:03", "commit_message": "[SPARK-32755][SQL] Maintain the order of expressions in AttributeSet and ExpressionSet\n\n### What changes were proposed in this pull request?\nThis PR changes `AttributeSet` and `ExpressionSet` to maintain the insertion order of the elements. More specifically, we:\n- change the underlying data structure of `AttributeSet` from `HashSet` to `LinkedHashSet` to maintain the insertion order.\n- `ExpressionSet` already uses a list to keep track of the expressions, however, since it is extending Scala's immutable.Set class, operations such as map and flatMap are delegated to the immutable.Set itself. This means that the result of these operations is not an instance of ExpressionSet anymore, rather it's a implementation picked up by the parent class. We also remove this inheritance from `immutable.Set `and implement the needed methods directly. ExpressionSet has a very specific semantics and it does not make sense to extend `immutable.Set` anyway.\n- change the `PlanStabilitySuite` to not sort the attributes, to be able to catch changes in the order of expressions in different runs.\n\n### Why are the changes needed?\nExpressions identity is based on the `ExprId` which is an auto-incremented number. This means that the same query can yield a query plan with different expression ids in different runs. `AttributeSet` and `ExpressionSet` internally use a `HashSet` as the underlying data structure, and therefore cannot guarantee the a fixed order of operations in different runs. This can be problematic in cases we like to check for plan changes in different runs.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nPasses `PlanStabilitySuite` after regenerating the golden files.\n\nCloses #29598 from dbaliafroozeh/FixOrderOfExpressions.\n\nAuthored-by: Ali Afroozeh <ali.afroozeh@databricks.com>\nSigned-off-by: herman <herman@databricks.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala-2.12/org/apache/spark/sql/catalyst/expressions/ExpressionSet.scala", "additions": "78", "deletions": "18", "changes": "96"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/AttributeSet.scala", "additions": "7", "deletions": "3", "changes": "10"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/CostBasedJoinReorder.scala", "additions": "11", "deletions": "11", "changes": "22"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [1, 1, 13]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/LogicalPlan.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/QueryPlanConstraints.scala", "additions": "16", "deletions": "18", "changes": "34"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala", "additions": "16", "deletions": "16", "changes": "32"}, "updated": [1, 1, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q10.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q10.sf100/simplified.txt", "additions": "20", "deletions": "20", "changes": "40"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q10/simplified.txt", "additions": "20", "deletions": "20", "changes": "40"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q19.sf100/simplified.txt", "additions": "22", "deletions": "22", "changes": "44"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q19/simplified.txt", "additions": "22", "deletions": "22", "changes": "44"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q27.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q27.sf100/simplified.txt", "additions": "41", "deletions": "41", "changes": "82"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q27/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q27/simplified.txt", "additions": "44", "deletions": "44", "changes": "88"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q3.sf100/simplified.txt", "additions": "13", "deletions": "13", "changes": "26"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q3/simplified.txt", "additions": "11", "deletions": "11", "changes": "22"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q34.sf100/simplified.txt", "additions": "18", "deletions": "18", "changes": "36"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q34/simplified.txt", "additions": "18", "deletions": "18", "changes": "36"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q42.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q42.sf100/simplified.txt", "additions": "14", "deletions": "14", "changes": "28"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q42/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q42/simplified.txt", "additions": "14", "deletions": "14", "changes": "28"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q43.sf100/simplified.txt", "additions": "12", "deletions": "12", "changes": "24"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q43/simplified.txt", "additions": "12", "deletions": "12", "changes": "24"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q46.sf100/simplified.txt", "additions": "23", "deletions": "23", "changes": "46"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q46/simplified.txt", "additions": "24", "deletions": "24", "changes": "48"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q52.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q52.sf100/simplified.txt", "additions": "12", "deletions": "12", "changes": "24"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q52/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q52/simplified.txt", "additions": "12", "deletions": "12", "changes": "24"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q53.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q53.sf100/simplified.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q53/simplified.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q55.sf100/simplified.txt", "additions": "12", "deletions": "12", "changes": "24"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q55/simplified.txt", "additions": "12", "deletions": "12", "changes": "24"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q59.sf100/simplified.txt", "additions": "25", "deletions": "25", "changes": "50"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q59/simplified.txt", "additions": "25", "deletions": "25", "changes": "50"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q63.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q63.sf100/simplified.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q63/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q63/simplified.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q65.sf100/simplified.txt", "additions": "25", "deletions": "25", "changes": "50"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q65/simplified.txt", "additions": "24", "deletions": "24", "changes": "48"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q68.sf100/simplified.txt", "additions": "24", "deletions": "24", "changes": "48"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q68/simplified.txt", "additions": "24", "deletions": "24", "changes": "48"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q7.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q7.sf100/simplified.txt", "additions": "17", "deletions": "17", "changes": "34"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q7/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q7/simplified.txt", "additions": "18", "deletions": "18", "changes": "36"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q73.sf100/simplified.txt", "additions": "16", "deletions": "16", "changes": "32"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q73/simplified.txt", "additions": "16", "deletions": "16", "changes": "32"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q79.sf100/simplified.txt", "additions": "19", "deletions": "19", "changes": "38"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q79/simplified.txt", "additions": "19", "deletions": "19", "changes": "38"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q89.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q89.sf100/simplified.txt", "additions": "20", "deletions": "20", "changes": "40"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q89/simplified.txt", "additions": "19", "deletions": "19", "changes": "38"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q98.sf100/simplified.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q98/simplified.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/ss_max.sf100/simplified.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/ss_max/simplified.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q1.sf100/simplified.txt", "additions": "18", "deletions": "18", "changes": "36"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q1/simplified.txt", "additions": "18", "deletions": "18", "changes": "36"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q10.sf100/simplified.txt", "additions": "16", "deletions": "16", "changes": "32"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q10/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q10/simplified.txt", "additions": "16", "deletions": "16", "changes": "32"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q11.sf100/simplified.txt", "additions": "41", "deletions": "41", "changes": "82"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q11/simplified.txt", "additions": "40", "deletions": "40", "changes": "80"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q12.sf100/simplified.txt", "additions": "12", "deletions": "12", "changes": "24"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q12/simplified.txt", "additions": "12", "deletions": "12", "changes": "24"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q13.sf100/simplified.txt", "additions": "19", "deletions": "19", "changes": "38"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q13/simplified.txt", "additions": "19", "deletions": "19", "changes": "38"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a.sf100/simplified.txt", "additions": "78", "deletions": "78", "changes": "156"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a/simplified.txt", "additions": "75", "deletions": "75", "changes": "150"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b.sf100/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b.sf100/simplified.txt", "additions": "71", "deletions": "71", "changes": "142"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b/explain.txt", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b/simplified.txt", "additions": "68", "deletions": "68", "changes": "136"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q15.sf100/simplified.txt", "additions": "8", "deletions": "8", "changes": "16"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q15/simplified.txt", "additions": "11", "deletions": "11", "changes": "22"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q16.sf100/simplified.txt", "additions": "19", "deletions": "19", "changes": "38"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q16/simplified.txt", "additions": "19", "deletions": "19", "changes": "38"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q17.sf100/explain.txt", "additions": "8", "deletions": "8", "changes": "16"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q17.sf100/simplified.txt", "additions": "24", "deletions": "24", "changes": "48"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q17/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q17/simplified.txt", "additions": "26", "deletions": "26", "changes": "52"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q18.sf100/simplified.txt", "additions": "24", "deletions": "24", "changes": "48"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q18/simplified.txt", "additions": "24", "deletions": "24", "changes": "48"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q19.sf100/simplified.txt", "additions": "23", "deletions": "23", "changes": "46"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q19/simplified.txt", "additions": "22", "deletions": "22", "changes": "44"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q2.sf100/simplified.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q2/simplified.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q20.sf100/simplified.txt", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q20/simplified.txt", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q21.sf100/simplified.txt", "additions": "16", "deletions": "16", "changes": "32"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q21/simplified.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q22.sf100/simplified.txt", "additions": "11", "deletions": "11", "changes": "22"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q22/simplified.txt", "additions": "13", "deletions": "13", "changes": "26"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23a.sf100/simplified.txt", "additions": "44", "deletions": "44", "changes": "88"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23a/simplified.txt", "additions": "44", "deletions": "44", "changes": "88"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23b.sf100/simplified.txt", "additions": "59", "deletions": "59", "changes": "118"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23b/simplified.txt", "additions": "55", "deletions": "55", "changes": "110"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24a.sf100/explain.txt", "additions": "11", "deletions": "11", "changes": "22"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24a.sf100/simplified.txt", "additions": "57", "deletions": "57", "changes": "114"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24a/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24a/simplified.txt", "additions": "50", "deletions": "50", "changes": "100"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24b.sf100/explain.txt", "additions": "11", "deletions": "11", "changes": "22"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24b.sf100/simplified.txt", "additions": "57", "deletions": "57", "changes": "114"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24b/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24b/simplified.txt", "additions": "50", "deletions": "50", "changes": "100"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q25.sf100/explain.txt", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q25.sf100/simplified.txt", "additions": "26", "deletions": "26", "changes": "52"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q25/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q25/simplified.txt", "additions": "28", "deletions": "28", "changes": "56"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q26.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q26.sf100/simplified.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q26/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q26/simplified.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q27.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q27.sf100/simplified.txt", "additions": "19", "deletions": "19", "changes": "38"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q27/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q27/simplified.txt", "additions": "19", "deletions": "19", "changes": "38"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q28.sf100/simplified.txt", "additions": "36", "deletions": "36", "changes": "72"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q28/simplified.txt", "additions": "36", "deletions": "36", "changes": "72"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q29.sf100/explain.txt", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q29.sf100/simplified.txt", "additions": "27", "deletions": "27", "changes": "54"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q29/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q29/simplified.txt", "additions": "29", "deletions": "29", "changes": "58"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q3.sf100/simplified.txt", "additions": "13", "deletions": "13", "changes": "26"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q3/simplified.txt", "additions": "12", "deletions": "12", "changes": "24"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q30.sf100/simplified.txt", "additions": "27", "deletions": "27", "changes": "54"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q30/simplified.txt", "additions": "27", "deletions": "27", "changes": "54"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q31.sf100/simplified.txt", "additions": "57", "deletions": "57", "changes": "114"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q31/simplified.txt", "additions": "55", "deletions": "55", "changes": "110"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q32.sf100/simplified.txt", "additions": "12", "deletions": "12", "changes": "24"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q32/simplified.txt", "additions": "11", "deletions": "11", "changes": "22"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q33.sf100/simplified.txt", "additions": "30", "deletions": "30", "changes": "60"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q33/simplified.txt", "additions": "31", "deletions": "31", "changes": "62"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q34.sf100/simplified.txt", "additions": "18", "deletions": "18", "changes": "36"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q34/simplified.txt", "additions": "18", "deletions": "18", "changes": "36"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q35.sf100/simplified.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q35/simplified.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q36.sf100/simplified.txt", "additions": "16", "deletions": "16", "changes": "32"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q36/simplified.txt", "additions": "16", "deletions": "16", "changes": "32"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q37.sf100/simplified.txt", "additions": "14", "deletions": "14", "changes": "28"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q37/simplified.txt", "additions": "14", "deletions": "14", "changes": "28"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q38.sf100/simplified.txt", "additions": "43", "deletions": "43", "changes": "86"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q38/simplified.txt", "additions": "37", "deletions": "37", "changes": "74"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q39a.sf100/simplified.txt", "additions": "31", "deletions": "31", "changes": "62"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q39a/simplified.txt", "additions": "31", "deletions": "31", "changes": "62"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q39b.sf100/simplified.txt", "additions": "31", "deletions": "31", "changes": "62"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q39b/simplified.txt", "additions": "31", "deletions": "31", "changes": "62"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q4.sf100/simplified.txt", "additions": "60", "deletions": "60", "changes": "120"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q4/simplified.txt", "additions": "59", "deletions": "59", "changes": "118"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q40.sf100/simplified.txt", "additions": "20", "deletions": "20", "changes": "40"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q40/simplified.txt", "additions": "16", "deletions": "16", "changes": "32"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q41.sf100/simplified.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q41/simplified.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q42.sf100/simplified.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q42/simplified.txt", "additions": "14", "deletions": "14", "changes": "28"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q43.sf100/simplified.txt", "additions": "12", "deletions": "12", "changes": "24"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q43/simplified.txt", "additions": "12", "deletions": "12", "changes": "24"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q44.sf100/simplified.txt", "additions": "18", "deletions": "18", "changes": "36"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q44/simplified.txt", "additions": "18", "deletions": "18", "changes": "36"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q45.sf100/simplified.txt", "additions": "21", "deletions": "21", "changes": "42"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q45/simplified.txt", "additions": "20", "deletions": "20", "changes": "40"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q46.sf100/simplified.txt", "additions": "21", "deletions": "21", "changes": "42"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q46/simplified.txt", "additions": "24", "deletions": "24", "changes": "48"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q47.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q47.sf100/simplified.txt", "additions": "45", "deletions": "45", "changes": "90"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q47/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q47/simplified.txt", "additions": "38", "deletions": "38", "changes": "76"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q48.sf100/simplified.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q48/simplified.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q49.sf100/explain.txt", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q49.sf100/simplified.txt", "additions": "56", "deletions": "56", "changes": "112"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q49/explain.txt", "additions": "8", "deletions": "8", "changes": "16"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q49/simplified.txt", "additions": "44", "deletions": "44", "changes": "88"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q5.sf100/simplified.txt", "additions": "41", "deletions": "41", "changes": "82"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q5/simplified.txt", "additions": "41", "deletions": "41", "changes": "82"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q50.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q50.sf100/simplified.txt", "additions": "22", "deletions": "22", "changes": "44"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q50/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q50/simplified.txt", "additions": "19", "deletions": "19", "changes": "38"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q51.sf100/simplified.txt", "additions": "32", "deletions": "32", "changes": "64"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q51/simplified.txt", "additions": "32", "deletions": "32", "changes": "64"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q52.sf100/simplified.txt", "additions": "13", "deletions": "13", "changes": "26"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q52/simplified.txt", "additions": "12", "deletions": "12", "changes": "24"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q53.sf100/simplified.txt", "additions": "14", "deletions": "14", "changes": "28"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q53/simplified.txt", "additions": "14", "deletions": "14", "changes": "28"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q54.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q54.sf100/simplified.txt", "additions": "31", "deletions": "31", "changes": "62"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q54/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q54/simplified.txt", "additions": "32", "deletions": "32", "changes": "64"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q55.sf100/simplified.txt", "additions": "13", "deletions": "13", "changes": "26"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q55/simplified.txt", "additions": "12", "deletions": "12", "changes": "24"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q56.sf100/simplified.txt", "additions": "35", "deletions": "35", "changes": "70"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q56/simplified.txt", "additions": "35", "deletions": "35", "changes": "70"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q57.sf100/simplified.txt", "additions": "42", "deletions": "42", "changes": "84"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q57/simplified.txt", "additions": "37", "deletions": "37", "changes": "74"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q58.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q58.sf100/simplified.txt", "additions": "28", "deletions": "28", "changes": "56"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q58/simplified.txt", "additions": "28", "deletions": "28", "changes": "56"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q59.sf100/simplified.txt", "additions": "20", "deletions": "20", "changes": "40"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q59/simplified.txt", "additions": "20", "deletions": "20", "changes": "40"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q6.sf100/simplified.txt", "additions": "19", "deletions": "19", "changes": "38"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q6/simplified.txt", "additions": "18", "deletions": "18", "changes": "36"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q60.sf100/simplified.txt", "additions": "34", "deletions": "34", "changes": "68"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q60/simplified.txt", "additions": "34", "deletions": "34", "changes": "68"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q61.sf100/simplified.txt", "additions": "31", "deletions": "31", "changes": "62"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q61/simplified.txt", "additions": "34", "deletions": "34", "changes": "68"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q62.sf100/simplified.txt", "additions": "16", "deletions": "16", "changes": "32"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q62/simplified.txt", "additions": "17", "deletions": "17", "changes": "34"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q63.sf100/simplified.txt", "additions": "14", "deletions": "14", "changes": "28"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q63/simplified.txt", "additions": "14", "deletions": "14", "changes": "28"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q64.sf100/simplified.txt", "additions": "87", "deletions": "87", "changes": "174"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q64/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q64/simplified.txt", "additions": "88", "deletions": "88", "changes": "176"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q65.sf100/simplified.txt", "additions": "25", "deletions": "25", "changes": "50"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q65/simplified.txt", "additions": "24", "deletions": "24", "changes": "48"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q66.sf100/simplified.txt", "additions": "33", "deletions": "33", "changes": "66"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q66/simplified.txt", "additions": "33", "deletions": "33", "changes": "66"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q67.sf100/simplified.txt", "additions": "19", "deletions": "19", "changes": "38"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q67/simplified.txt", "additions": "19", "deletions": "19", "changes": "38"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q68.sf100/simplified.txt", "additions": "21", "deletions": "21", "changes": "42"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q68/simplified.txt", "additions": "24", "deletions": "24", "changes": "48"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q69.sf100/simplified.txt", "additions": "16", "deletions": "16", "changes": "32"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q69/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q69/simplified.txt", "additions": "16", "deletions": "16", "changes": "32"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q7.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q7.sf100/simplified.txt", "additions": "18", "deletions": "18", "changes": "36"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q7/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q7/simplified.txt", "additions": "18", "deletions": "18", "changes": "36"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q70.sf100/simplified.txt", "additions": "23", "deletions": "23", "changes": "46"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q70/simplified.txt", "additions": "23", "deletions": "23", "changes": "46"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q71.sf100/simplified.txt", "additions": "22", "deletions": "22", "changes": "44"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q71/simplified.txt", "additions": "22", "deletions": "22", "changes": "44"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q72.sf100/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q72.sf100/simplified.txt", "additions": "35", "deletions": "35", "changes": "70"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q72/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q72/simplified.txt", "additions": "32", "deletions": "32", "changes": "64"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q73.sf100/simplified.txt", "additions": "16", "deletions": "16", "changes": "32"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q73/simplified.txt", "additions": "16", "deletions": "16", "changes": "32"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q74.sf100/simplified.txt", "additions": "34", "deletions": "34", "changes": "68"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q74/simplified.txt", "additions": "33", "deletions": "33", "changes": "66"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q75.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q75.sf100/simplified.txt", "additions": "103", "deletions": "103", "changes": "206"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q75/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q75/simplified.txt", "additions": "78", "deletions": "78", "changes": "156"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q76.sf100/simplified.txt", "additions": "26", "deletions": "26", "changes": "52"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q76/simplified.txt", "additions": "26", "deletions": "26", "changes": "52"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q77.sf100/simplified.txt", "additions": "40", "deletions": "40", "changes": "80"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q77/simplified.txt", "additions": "40", "deletions": "40", "changes": "80"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q78.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q78.sf100/simplified.txt", "additions": "50", "deletions": "50", "changes": "100"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q78/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q78/simplified.txt", "additions": "35", "deletions": "35", "changes": "70"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q79.sf100/simplified.txt", "additions": "19", "deletions": "19", "changes": "38"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q79/simplified.txt", "additions": "19", "deletions": "19", "changes": "38"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q8.sf100/simplified.txt", "additions": "14", "deletions": "14", "changes": "28"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q8/simplified.txt", "additions": "14", "deletions": "14", "changes": "28"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q80.sf100/simplified.txt", "additions": "52", "deletions": "52", "changes": "104"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q80/simplified.txt", "additions": "52", "deletions": "52", "changes": "104"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q81.sf100/simplified.txt", "additions": "26", "deletions": "26", "changes": "52"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q81/simplified.txt", "additions": "26", "deletions": "26", "changes": "52"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q82.sf100/simplified.txt", "additions": "13", "deletions": "13", "changes": "26"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q82/simplified.txt", "additions": "13", "deletions": "13", "changes": "26"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q83.sf100/simplified.txt", "additions": "21", "deletions": "21", "changes": "42"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q83/simplified.txt", "additions": "24", "deletions": "24", "changes": "48"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q84.sf100/simplified.txt", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q84/simplified.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q85.sf100/explain.txt", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q85.sf100/simplified.txt", "additions": "28", "deletions": "28", "changes": "56"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q85/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q85/simplified.txt", "additions": "29", "deletions": "29", "changes": "58"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q86.sf100/simplified.txt", "additions": "12", "deletions": "12", "changes": "24"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q86/simplified.txt", "additions": "12", "deletions": "12", "changes": "24"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q87.sf100/simplified.txt", "additions": "43", "deletions": "43", "changes": "86"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q87/simplified.txt", "additions": "37", "deletions": "37", "changes": "74"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q88.sf100/simplified.txt", "additions": "42", "deletions": "42", "changes": "84"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q88/simplified.txt", "additions": "42", "deletions": "42", "changes": "84"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q89.sf100/simplified.txt", "additions": "18", "deletions": "18", "changes": "36"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q89/simplified.txt", "additions": "18", "deletions": "18", "changes": "36"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9.sf100/simplified.txt", "additions": "35", "deletions": "35", "changes": "70"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9/simplified.txt", "additions": "35", "deletions": "35", "changes": "70"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q90.sf100/simplified.txt", "additions": "16", "deletions": "16", "changes": "32"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q90/simplified.txt", "additions": "14", "deletions": "14", "changes": "28"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q91.sf100/simplified.txt", "additions": "22", "deletions": "22", "changes": "44"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q91/simplified.txt", "additions": "20", "deletions": "20", "changes": "40"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q92.sf100/simplified.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q92/simplified.txt", "additions": "16", "deletions": "16", "changes": "32"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q93.sf100/simplified.txt", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q93/simplified.txt", "additions": "11", "deletions": "11", "changes": "22"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q94.sf100/simplified.txt", "additions": "20", "deletions": "20", "changes": "40"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q94/simplified.txt", "additions": "20", "deletions": "20", "changes": "40"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q95.sf100/simplified.txt", "additions": "22", "deletions": "22", "changes": "44"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q95/simplified.txt", "additions": "21", "deletions": "21", "changes": "42"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q96.sf100/simplified.txt", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q96/simplified.txt", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q97.sf100/simplified.txt", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q97/simplified.txt", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q98.sf100/simplified.txt", "additions": "14", "deletions": "14", "changes": "28"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q98/simplified.txt", "additions": "14", "deletions": "14", "changes": "28"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q99.sf100/simplified.txt", "additions": "14", "deletions": "14", "changes": "28"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q99/simplified.txt", "additions": "13", "deletions": "13", "changes": "26"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q10a.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q10a.sf100/simplified.txt", "additions": "16", "deletions": "16", "changes": "32"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q10a/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q10a/simplified.txt", "additions": "16", "deletions": "16", "changes": "32"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q11.sf100/simplified.txt", "additions": "42", "deletions": "42", "changes": "84"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q11/simplified.txt", "additions": "41", "deletions": "41", "changes": "82"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q12.sf100/simplified.txt", "additions": "12", "deletions": "12", "changes": "24"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q12/simplified.txt", "additions": "12", "deletions": "12", "changes": "24"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14.sf100/explain.txt", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14.sf100/simplified.txt", "additions": "71", "deletions": "71", "changes": "142"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14/explain.txt", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14/simplified.txt", "additions": "68", "deletions": "68", "changes": "136"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a.sf100/simplified.txt", "additions": "146", "deletions": "146", "changes": "292"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a/simplified.txt", "additions": "143", "deletions": "143", "changes": "286"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q18a.sf100/simplified.txt", "additions": "77", "deletions": "77", "changes": "154"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q18a/simplified.txt", "additions": "85", "deletions": "85", "changes": "170"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q20.sf100/simplified.txt", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q20/simplified.txt", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q22.sf100/simplified.txt", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q22/simplified.txt", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q22a.sf100/simplified.txt", "additions": "30", "deletions": "30", "changes": "60"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q22a/simplified.txt", "additions": "32", "deletions": "32", "changes": "64"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q24.sf100/explain.txt", "additions": "16", "deletions": "16", "changes": "32"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q24.sf100/simplified.txt", "additions": "60", "deletions": "60", "changes": "120"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q24/explain.txt", "additions": "8", "deletions": "8", "changes": "16"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q24/simplified.txt", "additions": "52", "deletions": "52", "changes": "104"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q27a.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q27a.sf100/simplified.txt", "additions": "44", "deletions": "44", "changes": "88"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q27a/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q27a/simplified.txt", "additions": "44", "deletions": "44", "changes": "88"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q34.sf100/simplified.txt", "additions": "18", "deletions": "18", "changes": "36"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q34/simplified.txt", "additions": "18", "deletions": "18", "changes": "36"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35.sf100/simplified.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35/simplified.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35a.sf100/simplified.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35a/simplified.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q36a.sf100/simplified.txt", "additions": "29", "deletions": "29", "changes": "58"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q36a/simplified.txt", "additions": "29", "deletions": "29", "changes": "58"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q47.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q47.sf100/simplified.txt", "additions": "45", "deletions": "45", "changes": "90"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q47/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q47/simplified.txt", "additions": "38", "deletions": "38", "changes": "76"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q49.sf100/explain.txt", "additions": "8", "deletions": "8", "changes": "16"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q49.sf100/simplified.txt", "additions": "56", "deletions": "56", "changes": "112"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q49/explain.txt", "additions": "12", "deletions": "12", "changes": "24"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q49/simplified.txt", "additions": "44", "deletions": "44", "changes": "88"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q51a.sf100/simplified.txt", "additions": "53", "deletions": "53", "changes": "106"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q51a/simplified.txt", "additions": "55", "deletions": "55", "changes": "110"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q57.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q57.sf100/simplified.txt", "additions": "42", "deletions": "42", "changes": "84"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q57/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q57/simplified.txt", "additions": "37", "deletions": "37", "changes": "74"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q5a.sf100/simplified.txt", "additions": "54", "deletions": "54", "changes": "108"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q5a/simplified.txt", "additions": "54", "deletions": "54", "changes": "108"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q6.sf100/simplified.txt", "additions": "20", "deletions": "20", "changes": "40"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q6/simplified.txt", "additions": "19", "deletions": "19", "changes": "38"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q64.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q64.sf100/simplified.txt", "additions": "87", "deletions": "87", "changes": "174"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q64/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q64/simplified.txt", "additions": "88", "deletions": "88", "changes": "176"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q67a.sf100/simplified.txt", "additions": "55", "deletions": "55", "changes": "110"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q67a/simplified.txt", "additions": "55", "deletions": "55", "changes": "110"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q70a.sf100/simplified.txt", "additions": "38", "deletions": "38", "changes": "76"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q70a/simplified.txt", "additions": "38", "deletions": "38", "changes": "76"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q72.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q72.sf100/simplified.txt", "additions": "35", "deletions": "35", "changes": "70"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q72/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q72/simplified.txt", "additions": "32", "deletions": "32", "changes": "64"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q74.sf100/simplified.txt", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q74/simplified.txt", "additions": "32", "deletions": "32", "changes": "64"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q75.sf100/explain.txt", "additions": "8", "deletions": "8", "changes": "16"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q75.sf100/simplified.txt", "additions": "103", "deletions": "103", "changes": "206"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q75/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q75/simplified.txt", "additions": "78", "deletions": "78", "changes": "156"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q77a.sf100/simplified.txt", "additions": "53", "deletions": "53", "changes": "106"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q77a/simplified.txt", "additions": "53", "deletions": "53", "changes": "106"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q78.sf100/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q78.sf100/simplified.txt", "additions": "50", "deletions": "50", "changes": "100"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q78/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q78/simplified.txt", "additions": "35", "deletions": "35", "changes": "70"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q80a.sf100/simplified.txt", "additions": "65", "deletions": "65", "changes": "130"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q80a/simplified.txt", "additions": "65", "deletions": "65", "changes": "130"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q86a.sf100/simplified.txt", "additions": "25", "deletions": "25", "changes": "50"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q86a/simplified.txt", "additions": "25", "deletions": "25", "changes": "50"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q98.sf100/simplified.txt", "additions": "13", "deletions": "13", "changes": "26"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q98/simplified.txt", "additions": "13", "deletions": "13", "changes": "26"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/PlanStabilitySuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 2, 3]}]}
{"author": "ashrayjain", "sha": "", "commit_date": "2021/05/19 11:08:45", "commit_message": "Mark K8s Secrets and ConfigMaps created by Spark as immutable\n\nKubernetes supports marking secrets and config maps as immutable to gain performance.\n\nhttps://kubernetes.io/docs/concepts/configuration/configmap/#configmap-immutable\nhttps://kubernetes.io/docs/concepts/configuration/secret/#secret-immutable\n\nFor K8s clusters that run many thousands of Spark applications, this can yield significant reduction in load on the kube-apiserver.\nFrom the K8s docs:\n\nFor clusters that extensively use Secrets (at least tens of thousands of unique Secret to Pod mounts), preventing changes to their data has the following advantages:\n- protects you from accidental (or unwanted) updates that could cause applications outages\n- improves performance of your cluster by significantly reducing load on kube-apiserver, by closing watches for secrets marked as immutable.\n\nFor any secrets and config maps we create in Spark that are immutable, we can mark them as immutable by including the following when building\nthe secret/config map: \".withImmutable(true)\"\n\nThis feature has been supported in K8s as beta since K8s 1.19 and as GA since K8s 1.21", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/DriverKubernetesCredentialsFeatureStep.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/HadoopConfDriverFeatureStep.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/KerberosConfDriverFeatureStep.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/PodTemplateConfigMapStep.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/submit/KubernetesClientUtils.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/PodTemplateConfigMapStepSuite.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/submit/ClientSuite.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 1]}]}
{"author": "YuzhouSun", "sha": "", "commit_date": "2021/05/12 05:31:01", "commit_message": "fix bug", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveExternalCatalog.scala", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}]}
{"author": "lipzhu", "sha": "", "commit_date": "2021/04/28 13:02:39", "commit_message": "Automated formatting for Scala Code for Blank Lines", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "dev/.scalafmt.conf", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "pom.xml", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 4, 15]}]}
{"author": "chrisheaththomas", "sha": "", "commit_date": "2021/05/15 13:42:57", "commit_message": "Update job-scheduling.md\n\nTrigger Github build and test action.", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "docs/job-scheduling.md", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}
{"author": "gerashegalov", "sha": "", "commit_date": "2021/05/15 05:52:59", "commit_message": "[SPARK-35408][PYSPARK] Improve parameter validation in DataFrame.show", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/sql/dataframe.py", "additions": "13", "deletions": "1", "changes": "14"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/sql/tests/test_dataframe.py", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 1, 1]}]}
{"author": "luhenry", "sha": "", "commit_date": "2021/01/04 14:20:18", "commit_message": "Correct typo in command to run benchmark", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "mllib-local/src/test/scala/org/apache/spark/ml/linalg/BLASBenchmark.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "LittleCuteBug", "sha": "", "commit_date": "2021/05/14 01:54:08", "commit_message": "[SPARK-32484][SQL] Fix log info BroadcastExchangeExec.scala", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/BroadcastExchangeExec.scala", "additions": "6", "deletions": "5", "changes": "11"}, "updated": [0, 0, 1]}]}
{"author": "gaborgsomogyi", "sha": "", "commit_date": "2020/09/01 07:23:20", "commit_message": "[SPARK-32579][SQL] Implement JDBCScan/ScanBuilder/WriteBuilder\n\n### What changes were proposed in this pull request?\nAdd JDBCScan, JDBCScanBuilder, JDBCWriteBuilder in Datasource V2 JDBC\n\n### Why are the changes needed?\nComplete Datasource V2 JDBC implementation\n\n### Does this PR introduce _any_ user-facing change?\nYes\n\n### How was this patch tested?\nnew tests\n\nCloses #29396 from huaxingao/v2jdbc.\n\nAuthored-by: Huaxin Gao <huaxing@us.ibm.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCScan.scala", "additions": "50", "deletions": "0", "changes": "50"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCScanBuilder.scala", "additions": "70", "deletions": "0", "changes": "70"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCTable.scala", "additions": "25", "deletions": "10", "changes": "35"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCWriteBuilder.scala", "additions": "46", "deletions": "0", "changes": "46"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala", "additions": "224", "deletions": "0", "changes": "224"}, "updated": [1, 1, 1]}]}
{"author": "bonnal-enzo", "sha": "", "commit_date": "2021/05/09 11:52:42", "commit_message": "overload PageRank.runWithOptions and runWithOptionsWithPreviousPageRank with a 'normalized' parameter to trigger or not the normalization", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "graphx/src/main/scala/org/apache/spark/graphx/lib/PageRank.scala", "additions": "65", "deletions": "6", "changes": "71"}, "updated": [0, 0, 0]}]}
{"author": "seayoun", "sha": "", "commit_date": "2021/01/26 08:13:11", "commit_message": "[SPARK-34235][SS] Make spark.sql.hive as a private package\n\n### What changes were proposed in this pull request?\nFollow the comment https://github.com/apache/spark/pull/31271#discussion_r562598983:\n\n- Remove the API tag `Unstable` for `HiveSessionStateBuilder`\n- Add document for spark.sql.hive package to emphasize it's a private package\n\n### Why are the changes needed?\nFollow the rule for a private package.\n\n### Does this PR introduce _any_ user-facing change?\nNo.\n\n### How was this patch tested?\nDoc change only.\n\nCloses #31321 from xuanyuanking/SPARK-34185-follow.\n\nAuthored-by: Yuanjian Li <yuanjian.li@databricks.com>\nSigned-off-by: HyukjinKwon <gurwls223@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 3, 10]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveSessionStateBuilder.scala", "additions": "0", "deletions": "2", "changes": "2"}, "updated": [2, 2, 4]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/package.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [1, 1, 1]}]}
{"author": "yijiacui-db", "sha": "", "commit_date": "2021/04/23 07:49:35", "commit_message": "update test", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "external/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaMicroBatchSourceSuite.scala", "additions": "0", "deletions": "17", "changes": "17"}, "updated": [0, 0, 0]}]}
{"author": "byungsoo-oh", "sha": "", "commit_date": "2021/05/03 06:33:47", "commit_message": "Add message when creating directory", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/test/scala/org/apache/spark/benchmark/BenchmarkBase.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 2]}]}
{"author": "jose-torres", "sha": "", "commit_date": "2021/04/27 16:55:43", "commit_message": "fix check", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/UnsupportedOperationChecker.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/UnsupportedOperationsSuite.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 3]}]}
{"author": "adrian-wang", "sha": "", "commit_date": "2021/04/27 07:00:36", "commit_message": "[DOC] Add JindoFS SDK in cloud integration documents", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "docs/cloud-integration.md", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}]}
{"author": "harupy", "sha": "", "commit_date": "2021/04/20 02:30:08", "commit_message": "specify return type for rawPredictionUDF", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/ml/classification.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}]}
{"author": "andersonm-ibm", "sha": "", "commit_date": "2021/04/20 06:02:19", "commit_message": "Change test to use TestHiveSingleton like other tests in Hive module instead of SharedSparkSession", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/ParquetEncryptionSuite.scala", "additions": "3", "deletions": "4", "changes": "7"}, "updated": [0, 0, 0]}]}
{"author": "sfcoy", "sha": "", "commit_date": "2021/08/26 04:59:18", "commit_message": "[SPARK-36457][DOCS] Review and fix issues in Scala/Java API docs\n\n### What changes were proposed in this pull request?\n\nCompare the 3.2.0 API doc with the latest release version 3.1.2. Fix the following issues:\n\n- Add missing `Since` annotation for new APIs\n- Remove the leaking class/object in API doc\n\n### Why are the changes needed?\n\nImprove API docs\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting UT\n\nCloses #33824 from gengliangwang/auditDoc.\n\nAuthored-by: Gengliang Wang <gengliang@apache.org>\nSigned-off-by: Gengliang Wang <gengliang@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/SparkException.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [2, 2, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/errors/SparkCoreErrors.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 2, 4]}, {"file": {"name": "core/src/main/scala/org/apache/spark/io/MutableCheckedOutputStream.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/MiscellaneousProcessDetails.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 1, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [1, 1, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/DependencyUtils.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [1, 1, 6]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/FunctionCatalog.java", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/TruncatableTable.java", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/functions/AggregateFunction.java", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/functions/BoundFunction.java", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/functions/Function.java", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/functions/ScalarFunction.java", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/functions/UnboundFunction.java", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/read/streaming/ReportsSourceMetrics.java", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [2, 4, 15]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/types/UDTRegistration.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/types/UserDefinedType.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/connector/write/V1Write.java", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 1, 1]}]}
{"author": "ChenDou2021", "sha": "", "commit_date": "2021/04/21 09:59:25", "commit_message": "Add whether the path is dir", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/ml/util.py", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/tests/test_readwrite.py", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 0, 0]}]}
{"author": "sander-goos", "sha": "", "commit_date": "2021/04/16 15:15:35", "commit_message": "Remove initial null value of LiveStage.info", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusListener.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/LiveEntity.scala", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 0, 1]}]}
{"author": "nchammas", "sha": "", "commit_date": "2020/08/21 12:23:41", "commit_message": "[SPARK-32682][INFRA] Use workflow_dispatch to enable manual test triggers\n\n### What changes were proposed in this pull request?\n\nThis PR proposes to add a `workflow_dispatch` entry in the GitHub Action script (`build_and_test.yml`). This update can enable developers to run the Spark tests for a specific branch on their own local repository, so I think it might help to check if al the tests can pass before opening a new PR.\n\n<img width=\"944\" alt=\"Screen Shot 2020-08-21 at 16 28 41\" src=\"https://user-images.githubusercontent.com/692303/90866249-96250c80-e3ce-11ea-8496-3dd6683e92ea.png\">\n\n### Why are the changes needed?\n\nTo reduce the pressure of GitHub Actions on the Spark repository.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nManually checked.\n\nCloses #29504 from maropu/DispatchTest.\n\nAuthored-by: Takeshi Yamamuro <yamamuro@apache.org>\nSigned-off-by: Takeshi Yamamuro <yamamuro@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": ".github/workflows/build_and_test.yml", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [2, 3, 3]}, {"file": {"name": "dev/run-tests.py", "additions": "7", "deletions": "1", "changes": "8"}, "updated": [1, 2, 12]}]}
{"author": "gemelen", "sha": "", "commit_date": "2020/09/12 22:02:50", "commit_message": "Increase stack size for sbt", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": ".github/workflows/build_and_test.yml", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 5]}]}
{"author": "izchen", "sha": "", "commit_date": "2020/10/14 03:13:54", "commit_message": "[SPARK-33134][SQL] Return partial results only for root JSON objects\n\n### What changes were proposed in this pull request?\nIn the PR, I propose to restrict the partial result feature only by root JSON objects. JSON datasource as well as `from_json()` will return `null` for malformed nested JSON objects.\n\n### Why are the changes needed?\n1. To not raise exception to users in the PERMISSIVE mode\n2. To fix a regression and to have the same behavior as Spark 2.4.x has\n3. Current implementation of partial result is supposed to work only for root (top-level) JSON objects, and not tested for bad nested complex JSON fields.\n\n### Does this PR introduce _any_ user-facing change?\nYes. Before the changes, the code below:\n```scala\n    val pokerhand_raw = Seq(\"\"\"[{\"cards\": [19], \"playerId\": 123456}]\"\"\").toDF(\"events\")\n    val event = new StructType().add(\"playerId\", LongType).add(\"cards\", ArrayType(new StructType().add(\"id\", LongType).add(\"rank\", StringType)))\n    val pokerhand_events = pokerhand_raw.select(from_json($\"events\", ArrayType(event)).as(\"event\"))\n    pokerhand_events.show\n```\nthrows the exception even in the default **PERMISSIVE** mode:\n```java\njava.lang.ClassCastException: java.lang.Long cannot be cast to org.apache.spark.sql.catalyst.util.ArrayData\n  at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow.getArray(rows.scala:48)\n  at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow.getArray$(rows.scala:48)\n  at org.apache.spark.sql.catalyst.expressions.GenericInternalRow.getArray(rows.scala:195)\n```\n\nAfter the changes:\n```\n+-----+\n|event|\n+-----+\n| null|\n+-----+\n```\n\n### How was this patch tested?\nAdded a test to `JsonFunctionsSuite`.\n\nCloses #30031 from MaxGekk/json-skip-row-wrong-schema.\n\nAuthored-by: Max Gekk <max.gekk@gmail.com>\nSigned-off-by: HyukjinKwon <gurwls223@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JacksonParser.scala", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/JsonFunctionsSuite.scala", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [1, 1, 1]}]}
{"author": "dbaliafroozeh", "sha": "", "commit_date": "2020/08/31 08:39:12", "commit_message": "[SPARK-32747][R][TESTS] Deduplicate configuration set/unset in test_sparkSQL_arrow.R\n\n### What changes were proposed in this pull request?\n\nThis PR proposes to deduplicate configuration set/unset in `test_sparkSQL_arrow.R`.\nSetting `spark.sql.execution.arrow.sparkr.enabled` can be globally done instead of doing it in each test case.\n\n### Why are the changes needed?\n\nTo duduplicate the codes.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo, dev-only\n\n### How was this patch tested?\n\nManually ran the tests.\n\nCloses #29592 from HyukjinKwon/SPARK-32747.\n\nAuthored-by: HyukjinKwon <gurwls223@apache.org>\nSigned-off-by: HyukjinKwon <gurwls223@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "R/pkg/tests/fulltests/test_sparkSQL_arrow.R", "additions": "60", "deletions": "141", "changes": "201"}, "updated": [1, 1, 2]}]}
{"author": "waitinfuture", "sha": "", "commit_date": "2020/11/26 01:19:38", "commit_message": "[SPARK-33562][UI] Improve the style of the checkbox in executor page\n\n### What changes were proposed in this pull request?\n\n1. Remove the fixed width style of class `container-fluid-div`. So that the UI looks clean when the text is long.\n2. Add one space between a checkbox and the text on the right side, which is consistent with the stage page.\n\n### Why are the changes needed?\n\nThe width of class `container-fluid-div` is set as 200px after https://github.com/apache/spark/pull/21688 . This makes the checkbox in the executor page messy.\n![image](https://user-images.githubusercontent.com/1097932/100242069-3bc5ab80-2ee9-11eb-8c7d-96c221398fee.png)\n\nWe should remove the width limit.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n### How was this patch tested?\n\nManual test.\nAfter the changes:\n![image](https://user-images.githubusercontent.com/1097932/100257802-2f4a4e80-2efb-11eb-9eb0-92d6988ad14b.png)\n\nCloses #30500 from gengliangwang/reviseStyle.\n\nAuthored-by: Gengliang Wang <gengliang.wang@databricks.com>\nSigned-off-by: HyukjinKwon <gurwls223@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/executorspage.js", "additions": "9", "deletions": "9", "changes": "18"}, "updated": [1, 1, 3]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/webui.css", "additions": "0", "deletions": "4", "changes": "4"}, "updated": [1, 1, 1]}]}
{"author": "luluorta", "sha": "", "commit_date": "2020/09/22 04:43:17", "commit_message": "[SPARK-32951][SQL] Foldable propagation from Aggregate\n\n### What changes were proposed in this pull request?\nThis PR adds foldable propagation from `Aggregate` as per: https://github.com/apache/spark/pull/29771#discussion_r490412031\n\n### Why are the changes needed?\nThis is an improvement as `Aggregate`'s `aggregateExpressions` can contain foldables that can be propagated up.\n\n### Does this PR introduce _any_ user-facing change?\nNo.\n\n### How was this patch tested?\nNew UT.\n\nCloses #29816 from peter-toth/SPARK-32951-foldable-propagation-from-aggregate.\n\nAuthored-by: Peter Toth <peter.toth@gmail.com>\nSigned-off-by: Dongjoon Hyun <dhyun@apple.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/expressions.scala", "additions": "15", "deletions": "4", "changes": "19"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/FoldablePropagationSuite.scala", "additions": "12", "deletions": "0", "changes": "12"}, "updated": [1, 2, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a.sf100/explain.txt", "additions": "25", "deletions": "25", "changes": "50"}, "updated": [1, 1, 4]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a.sf100/simplified.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [1, 1, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a/explain.txt", "additions": "25", "deletions": "25", "changes": "50"}, "updated": [1, 1, 4]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a/simplified.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [1, 1, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b.sf100/explain.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 4]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b.sf100/simplified.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [1, 1, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b/explain.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 4]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b/simplified.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [1, 1, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q41.sf100/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [1, 1, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q41.sf100/simplified.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 1, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q41/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [1, 1, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q41/simplified.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 1, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14.sf100/explain.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 4]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14.sf100/simplified.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [1, 1, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14/explain.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 4]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14/simplified.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [1, 1, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a.sf100/explain.txt", "additions": "265", "deletions": "265", "changes": "530"}, "updated": [1, 1, 4]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a.sf100/simplified.txt", "additions": "30", "deletions": "30", "changes": "60"}, "updated": [1, 1, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a/explain.txt", "additions": "265", "deletions": "265", "changes": "530"}, "updated": [1, 1, 4]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a/simplified.txt", "additions": "30", "deletions": "30", "changes": "60"}, "updated": [1, 1, 3]}]}
{"author": "Karl-WangSK", "sha": "", "commit_date": "2020/10/29 03:29:43", "commit_message": "Merge remote-tracking branch 'upstream/master'", "title": "", "body": "", "failed_tests": [], "files": []}
{"author": "davidrabinowitz", "sha": "", "commit_date": "2020/11/13 21:26:07", "commit_message": "Adding support for UserDefinedType for Spark SQL Code generator", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "9", "deletions": "7", "changes": "16"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodegenGetValueFromVectorSuite.scala", "additions": "94", "deletions": "0", "changes": "94"}, "updated": [0, 0, 0]}]}
{"author": "MrPowers", "sha": "", "commit_date": "2021/01/20 02:40:37", "commit_message": "[SPARK-33940][BUILD] Upgrade univocity to 2.9.1\n\n### What changes were proposed in this pull request?\n\nupgrade univocity\n\n### Why are the changes needed?\n\ncsv writer actually has an implicit limit on column name length due to univocity-parser 2.9.0,\n\nwhen we initialize a writer https://github.com/uniVocity/univocity-parsers/blob/e09114c6879fa6c2c15e7365abc02cda3e193ff7/src/main/java/com/univocity/parsers/common/AbstractWriter.java#L211, it calls toIdentifierGroupArray which calls valueOf in NormalizedString.java eventually (https://github.com/uniVocity/univocity-parsers/blob/e09114c6879fa6c2c15e7365abc02cda3e193ff7/src/main/java/com/univocity/parsers/common/NormalizedString.java#L205-L209)\n\nin that stringCache.get, it has a maxStringLength cap https://github.com/uniVocity/univocity-parsers/blob/e09114c6879fa6c2c15e7365abc02cda3e193ff7/src/main/java/com/univocity/parsers/common/StringCache.java#L104 which is 1024 by default\n\nmore details at https://github.com/apache/spark/pull/30972 and https://github.com/uniVocity/univocity-parsers/issues/438\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\nexisting UT\n\nCloses #31246 from CodingCat/upgrade_univocity.\n\nAuthored-by: CodingCat <zhunansjtu@gmail.com>\nSigned-off-by: HyukjinKwon <gurwls223@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-2.7-hive-2.3", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [3, 6, 18]}, {"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [3, 6, 18]}, {"file": {"name": "pom.xml", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [3, 6, 26]}]}
{"author": "kuwii", "sha": "", "commit_date": "2020/10/26 00:06:06", "commit_message": "[SPARK-33234][INFRA] Generates SHA-512 using shasum\n\n### What changes were proposed in this pull request?\n\nI am generating the SHA-512 using the standard shasum which also has a better output compared to GPG.\n\n### Why are the changes needed?\n\nWhich makes the hash much easier to verify for users that don't have GPG.\n\nBecause an user having GPG can check the keys but an user without GPG will have a hard time validating the SHA-512 based on the 'pretty printed' format.\n\nApache Spark is the only project where I've seen this format. Most other Apache projects have a one-line hash file.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nThis patch assumes the build system has shasum (it should, but I can't test this).\n\nCloses #30123 from emilianbold/master.\n\nAuthored-by: Emi <emilian.bold@gmail.com>\nSigned-off-by: Dongjoon Hyun <dhyun@apple.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "dev/create-release/release-build.sh", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [1, 1, 5]}]}
{"author": "CodingCat", "sha": "", "commit_date": "2020/08/31 06:43:13", "commit_message": "[SPARK-32740][SQL] Refactor common partitioning/distribution logic to BaseAggregateExec\n\n### What changes were proposed in this pull request?\n\nFor all three different aggregate physical operator: `HashAggregateExec`, `ObjectHashAggregateExec` and `SortAggregateExec`, they have same `outputPartitioning` and `requiredChildDistribution` logic. Refactor these same logic into their super class `BaseAggregateExec` to avoid code duplication and future bugs (similar to `HashJoin` and `ShuffledJoin`).\n\n### Why are the changes needed?\n\nReduce duplicated code across classes and prevent future bugs if we only update one class but forget another. We already did similar refactoring for join (`HashJoin` and `ShuffledJoin`).\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nExisting unit tests as this is pure refactoring and no new logic added.\n\nCloses #29583 from c21/aggregate-refactor.\n\nAuthored-by: Cheng Su <chengsu@fb.com>\nSigned-off-by: Takeshi Yamamuro <yamamuro@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/BaseAggregateExec.scala", "additions": "17", "deletions": "3", "changes": "20"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala", "additions": "1", "deletions": "15", "changes": "16"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "14", "changes": "15"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/SortAggregateExec.scala", "additions": "2", "deletions": "16", "changes": "18"}, "updated": [1, 1, 1]}]}
{"author": "coderbond007", "sha": "", "commit_date": "2021/01/03 09:31:38", "commit_message": "[SPARK-33955][SS] Add latest offsets to source progress\n\n### What changes were proposed in this pull request?\n\nThis patch proposes to add latest offset to source progress for streaming queries.\n\n### Why are the changes needed?\n\nCurrently we record start and end offsets per source in streaming process. Latest offset is an important information for streaming process but the progress lacks of this info. We can use it to track the process lag and adjust streaming queries. We should add latest offset to source progress.\n\n### Does this PR introduce _any_ user-facing change?\n\nYes, for new metric about latest source offset in source progress.\n\n### How was this patch tested?\n\nUnit test. Manually test in Spark cluster:\n\n```\n    \"description\" : \"KafkaV2[Subscribe[page_view_events]]\",\n    \"startOffset\" : {\n      \"page_view_events\" : {\n        \"2\" : 582370921,\n        \"4\" : 391910836,\n        \"1\" : 631009201,\n        \"3\" : 406601346,\n        \"0\" : 195799112\n      }\n    },\n    \"endOffset\" : {\n      \"page_view_events\" : {\n        \"2\" : 583764414,\n        \"4\" : 392338002,\n        \"1\" : 632183480,\n        \"3\" : 407101489,\n        \"0\" : 197304028\n      }\n    },\n    \"latestOffset\" : {\n      \"page_view_events\" : {\n        \"2\" : 589852545,\n        \"4\" : 394204277,\n        \"1\" : 637313869,\n        \"3\" : 409286602,\n        \"0\" : 203878962\n      }\n    },\n    \"numInputRows\" : 4999997,\n    \"inputRowsPerSecond\" : 29287.70501405811,\n```\n\nCloses #30988 from viirya/latest-offset.\n\nAuthored-by: Liang-Chi Hsieh <viirya@gmail.com>\nSigned-off-by: Dongjoon Hyun <dhyun@apple.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaMicroBatchStream.scala", "additions": "7", "deletions": "1", "changes": "8"}, "updated": [1, 2, 3]}, {"file": {"name": "external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaSource.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [1, 2, 3]}, {"file": {"name": "project/MimaExcludes.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [1, 2, 7]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/read/streaming/SupportsAdmissionControl.java", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala", "additions": "18", "deletions": "7", "changes": "25"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ProgressReporter.scala", "additions": "11", "deletions": "2", "changes": "13"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [1, 1, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/streaming/progress.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingQueryStatusAndProgressSuite.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [1, 1, 1]}]}
{"author": "Fokko", "sha": "150b903ecb7008908d1063694fcd8db8fda604ff", "commit_date": "2021/08/03 04:43:54", "commit_message": "Bump checkstyle from 8.29 to 8.45\n\nBumps [checkstyle](https://github.com/checkstyle/checkstyle) from 8.29 to 8.45.\n- [Release notes](https://github.com/checkstyle/checkstyle/releases)\n- [Commits](https://github.com/checkstyle/checkstyle/compare/checkstyle-8.29...checkstyle-8.45)\n\nSigned-off-by: dependabot-preview[bot] <support@dependabot.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "pom.xml", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 7, 30]}]}
{"author": "Fokko", "sha": "a64e740764e7773c3b623eabebd953abfac08200", "commit_date": "2021/07/08 04:48:11", "commit_message": "[Security] Bump jetty.version from 9.4.28.v20200408 to 9.4.43.v20210629\n\nBumps `jetty.version` from 9.4.28.v20200408 to 9.4.43.v20210629.\n\nUpdates `jetty-http` from 9.4.28.v20200408 to 9.4.43.v20210629\n- [Release notes](https://github.com/eclipse/jetty.project/releases)\n- [Commits](https://github.com/eclipse/jetty.project/compare/jetty-9.4.28.v20200408...jetty-9.4.43.v20210629)\n\nUpdates `jetty-continuation` from 9.4.28.v20200408 to 9.4.43.v20210629\n\nUpdates `jetty-servlet` from 9.4.28.v20200408 to 9.4.43.v20210629\n- [Release notes](https://github.com/eclipse/jetty.project/releases)\n- [Commits](https://github.com/eclipse/jetty.project/compare/jetty-9.4.28.v20200408...jetty-9.4.43.v20210629)\n\nUpdates `jetty-servlets` from 9.4.28.v20200408 to 9.4.43.v20210629\n- [Release notes](https://github.com/eclipse/jetty.project/releases)\n- [Commits](https://github.com/eclipse/jetty.project/compare/jetty-9.4.28.v20200408...jetty-9.4.43.v20210629)\n\nUpdates `jetty-proxy` from 9.4.28.v20200408 to 9.4.43.v20210629\n- [Release notes](https://github.com/eclipse/jetty.project/releases)\n- [Commits](https://github.com/eclipse/jetty.project/compare/jetty-9.4.28.v20200408...jetty-9.4.43.v20210629)\n\nUpdates `jetty-client` from 9.4.28.v20200408 to 9.4.43.v20210629\n- [Release notes](https://github.com/eclipse/jetty.project/releases)\n- [Commits](https://github.com/eclipse/jetty.project/compare/jetty-9.4.28.v20200408...jetty-9.4.43.v20210629)\n\nUpdates `jetty-util` from 9.4.28.v20200408 to 9.4.43.v20210629\n- [Release notes](https://github.com/eclipse/jetty.project/releases)\n- [Commits](https://github.com/eclipse/jetty.project/compare/jetty-9.4.28.v20200408...jetty-9.4.43.v20210629)\n\nUpdates `jetty-security` from 9.4.28.v20200408 to 9.4.43.v20210629\n- [Release notes](https://github.com/eclipse/jetty.project/releases)\n- [Commits](https://github.com/eclipse/jetty.project/compare/jetty-9.4.28.v20200408...jetty-9.4.43.v20210629)\n\nUpdates `jetty-plus` from 9.4.28.v20200408 to 9.4.43.v20210629\n- [Release notes](https://github.com/eclipse/jetty.project/releases)\n- [Commits](https://github.com/eclipse/jetty.project/compare/jetty-9.4.28.v20200408...jetty-9.4.43.v20210629)\n\nUpdates `jetty-server` from 9.4.28.v20200408 to 9.4.43.v20210629\n- [Release notes](https://github.com/eclipse/jetty.project/releases)\n- [Commits](https://github.com/eclipse/jetty.project/compare/jetty-9.4.28.v20200408...jetty-9.4.43.v20210629)\n\nUpdates `jetty-webapp` from 9.4.28.v20200408 to 9.4.43.v20210629\n- [Release notes](https://github.com/eclipse/jetty.project/releases)\n- [Commits](https://github.com/eclipse/jetty.project/compare/jetty-9.4.28.v20200408...jetty-9.4.43.v20210629)\n\nUpdates `jetty-util-ajax` from 9.4.28.v20200408 to 9.4.43.v20210629\n- [Release notes](https://github.com/eclipse/jetty.project/releases)\n- [Commits](https://github.com/eclipse/jetty.project/compare/jetty-9.4.28.v20200408...jetty-9.4.43.v20210629)\n\nSigned-off-by: dependabot-preview[bot] <support@dependabot.com>", "title": "", "body": "", "failed_tests": ["org.apache.spark.ui.UISuite"], "files": [{"file": {"name": "pom.xml", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [2, 11, 36]}]}
{"author": "Fokko", "sha": "78b9602332d4ef9401f5f031de0782ea3910b774", "commit_date": "2021/04/27 21:24:13", "commit_message": "[Security] Bump commons-io from 2.5 to 2.7\n\nBumps commons-io from 2.5 to 2.7. **This update includes a security fix.**\n\nSigned-off-by: dependabot-preview[bot] <support@dependabot.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "pom.xml", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [2, 4, 15]}]}
{"author": "Fokko", "sha": "", "commit_date": "2020/11/03 04:29:37", "commit_message": "Bump checkstyle from 8.29 to 8.37\n\nBumps [checkstyle](https://github.com/checkstyle/checkstyle) from 8.29 to 8.37.\n- [Release notes](https://github.com/checkstyle/checkstyle/releases)\n- [Commits](https://github.com/checkstyle/checkstyle/compare/checkstyle-8.29...checkstyle-8.37)\n\nSigned-off-by: dependabot-preview[bot] <support@dependabot.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "pom.xml", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 3, 10]}]}
{"author": "hotienvu", "sha": "", "commit_date": "2020/09/11 19:30:02", "commit_message": "refactored and add config option", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/expressions.scala", "additions": "33", "deletions": "34", "changes": "67"}, "updated": [0, 2, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [1, 2, 22]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/OptimizeInSuite.scala", "additions": "46", "deletions": "17", "changes": "63"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ReplaceOperatorSuite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/columnar/PartitionBatchPruningSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 0, 1]}]}
{"author": "guiyanakuang", "sha": "", "commit_date": "2021/09/01 06:27:30", "commit_message": "[SPARK-36607][SQL] Support BooleanType in UnwrapCastInBinaryComparison\n\n### What changes were proposed in this pull request?\nThis PR proposes to add `BooleanType` support to the `UnwrapCastInBinaryComparison` optimizer that is currently supports `NumericType` only.\n\nThe main idea is to treat `BooleanType` as 1 bit integer so that we can utilize all optimizations already defined in `UnwrapCastInBinaryComparison`.\n\nThis work is an extension of SPARK-24994 and SPARK-32858\n\n### Why are the changes needed?\nCurrent implementation of Spark without this PR cannot properly optimize the filter for the following case\n```\nSELECT * FROM t WHERE boolean_field = 2\n```\nThe above query creates a filter of `cast(boolean_field, int) = 2`. The casting prevents from pushing down the filter. In contrast, this PR creates a `false` filter and returns early as there cannot be such a matching rows anyway (empty results.)\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nPassed existing tests\n```\nbuild/sbt \"catalyst/test\"\nbuild/sbt \"sql/test\"\n```\nAdded unit tests\n```\nbuild/sbt \"catalyst/testOnly *UnwrapCastInBinaryComparisonSuite   -- -z SPARK-36607\"\nbuild/sbt \"sql/testOnly *UnwrapCastInComparisonEndToEndSuite  -- -z SPARK-36607\"\n```\n\nCloses #33865 from kazuyukitanimura/SPARK-36607.\n\nAuthored-by: Kazuyuki Tanimura <ktanimura@apple.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/UnwrapCastInBinaryComparison.scala", "additions": "5", "deletions": "4", "changes": "9"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/UnwrapCastInBinaryComparisonSuite.scala", "additions": "47", "deletions": "10", "changes": "57"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/UnwrapCastInComparisonEndToEndSuite.scala", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [1, 1, 1]}]}
{"author": "mayurdb", "sha": "", "commit_date": "2020/07/09 11:47:19", "commit_message": "Revert \"Additional checks on deciding the pruning side\"\n\nThis reverts commit 89664b4f657811ef3c30eb77450e8d636742faa0.", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/dynamicpruning/PartitionPruning.scala", "additions": "3", "deletions": "33", "changes": "36"}, "updated": [0, 0, 0]}]}
{"author": "pgillet", "sha": "", "commit_date": "2020/11/12 08:50:32", "commit_message": "[SPARK-33386][SQL] Accessing array elements in ElementAt/Elt/GetArrayItem should failed if index is out of bound\n\n### What changes were proposed in this pull request?\n\nInstead of returning NULL, throws runtime ArrayIndexOutOfBoundsException when ansiMode is enable for `element_at`\uff0c`elt`, `GetArrayItem` functions.\n\n### Why are the changes needed?\n\nFor ansiMode.\n\n### Does this PR introduce any user-facing change?\n\nWhen `spark.sql.ansi.enabled` = true, Spark will throw `ArrayIndexOutOfBoundsException` if out-of-range index when accessing array elements\n\n### How was this patch tested?\n\nAdded UT and existing UT.\n\nCloses #30297 from leanken/leanken-SPARK-33386.\n\nAuthored-by: xuewei.linxuewei <xuewei.linxuewei@alibaba-inc.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "docs/sql-ref-ansi-compliance.md", "additions": "8", "deletions": "1", "changes": "9"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/TypeCoercion.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ProjectionOverSchema.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/SelectedField.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala", "additions": "30", "deletions": "23", "changes": "53"}, "updated": [2, 2, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/complexTypeExtractors.scala", "additions": "53", "deletions": "14", "changes": "67"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala", "additions": "30", "deletions": "3", "changes": "33"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/ComplexTypes.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [1, 4, 18]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala", "additions": "84", "deletions": "52", "changes": "136"}, "updated": [2, 2, 2]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ComplexTypeSuite.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [1, 2, 2]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala", "additions": "31", "deletions": "1", "changes": "32"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/ansi/array.sql", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/array.sql", "additions": "12", "deletions": "0", "changes": "12"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/array.sql.out", "additions": "234", "deletions": "0", "changes": "234"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/array.sql.out", "additions": "66", "deletions": "1", "changes": "67"}, "updated": [1, 1, 1]}]}
{"author": "cchighman", "sha": "", "commit_date": "2020/08/18 03:22:59", "commit_message": "Update Files", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/pathFilters.scala", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/PathFilterSuite.scala", "additions": "22", "deletions": "67", "changes": "89"}, "updated": [0, 0, 0]}]}
{"author": "artiship", "sha": "", "commit_date": "2020/11/05 09:21:17", "commit_message": "[SPARK-30294][SS] Explicitly defines read-only StateStore and optimize for HDFSBackedStateStore\n\n### What changes were proposed in this pull request?\n\nThere's a concept of 'read-only' and 'read+write' state store in Spark which is defined \"implicitly\". Spark doesn't prevent write for 'read-only' state store; Spark just assumes read-only stateful operator will not modify the state store. Given it's not defined explicitly, the instance of state store has to be implemented as 'read+write' even it's being used as 'read-only', which sometimes brings confusion.\n\nFor example, abort() in HDFSBackedStateStore - https://github.com/apache/spark/blob/d38f8167483d4d79e8360f24a8c0bffd51460659/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider.scala#L143-L155\n\nThe comment sounds as if statement works differently between 'read-only' and 'read+write', but that's not true as both state store has state initialized as UPDATING (no difference). So 'read-only' state also creates the temporary file, initializes output streams to write to temporary file, closes output streams, and finally deletes the temporary file. This unnecessary operations are being done per batch/partition.\n\nThis patch explicitly defines 'read-only' StateStore, and enables state store provider to create 'read-only' StateStore instance if requested. Relevant code paths are modified, as well as 'read-only' StateStore implementation for HDFSBackedStateStore is introduced. The new implementation gets rid of unnecessary operations explained above.\n\nIn point of backward-compatibility view, the only thing being changed in public API side is `StateStoreProvider`. The trait `StateStoreProvider` has to be changed to allow requesting 'read-only' StateStore; this patch adds default implementation which leverages 'read+write' StateStore but wrapping with 'write-protected' StateStore instance, so that custom providers don't need to change their code to reflect the change. But if the providers can optimize for read-only workload, they'll be happy to make a change.\n\nPlease note that this patch makes ReadOnlyStateStore extend StateStore and being referred as StateStore, as StateStore is being used in so many places and it's not easy to support both traits if we differentiate them. So unfortunately these write methods are still exposed for read-only state; it just throws UnsupportedOperationException.\n\n### Why are the changes needed?\n\nThe new API opens the chance to optimize read-only state store instance compared with read+write state store instance. HDFSBackedStateStoreProvider is modified to provide read-only version of state store which doesn't deal with temporary file as well as state machine.\n\n### Does this PR introduce any user-facing change?\n\nClearly \"no\" for most end users, and also \"no\" for custom state store providers as it doesn't touch trait `StateStore` as well as provides default implementation for added method in trait `StateStoreProvider`.\n\n### How was this patch tested?\n\nModified UT. Existing UTs ensure the change doesn't break anything.\n\nCloses #26935 from HeartSaVioR/SPARK-30294.\n\nAuthored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>\nSigned-off-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider.scala", "additions": "38", "deletions": "8", "changes": "46"}, "updated": [1, 2, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStore.scala", "additions": "93", "deletions": "18", "changes": "111"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreRDD.scala", "additions": "81", "deletions": "23", "changes": "104"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StreamingAggregationStateManager.scala", "additions": "11", "deletions": "11", "changes": "22"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/package.scala", "additions": "35", "deletions": "0", "changes": "35"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/statefulOperators.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/StateStoreSuite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}]}
{"author": "dongkelun", "sha": "", "commit_date": "2021/08/03 00:12:54", "commit_message": "[SPARK-36373][SQL] DecimalPrecision only add necessary cast\n\n### What changes were proposed in this pull request?\n\nThis pr makes `DecimalPrecision` only add necessary cast similar to [`ImplicitTypeCasts`](https://github.com/apache/spark/blob/96c2919988ddf78d104103876d8d8221e8145baa/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/TypeCoercion.scala#L675-L678). For example:\n```\nEqualTo(AttributeReference(\"d1\", DecimalType(5, 2))(), AttributeReference(\"d2\", DecimalType(2, 1))())\n```\nIt will add a useless cast to _d1_:\n```\n(cast(d1#6 as decimal(5,2)) = cast(d2#7 as decimal(5,2)))\n```\n\n### Why are the changes needed?\n\n1. Avoid adding unnecessary cast. Although it will be removed by  `SimplifyCasts` later.\n2. I'm trying to add an extended rule similar to `PullOutGroupingExpressions`. The current behavior will introduce additional alias. For example: `cast(d1 as decimal(5,2)) as cast_d1`.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nUnit test.\n\nCloses #33602 from wangyum/SPARK-36373.\n\nAuthored-by: Yuming Wang <yumwang@ebay.com>\nSigned-off-by: Yuming Wang <yumwang@ebay.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/DecimalPrecision.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/DecimalPrecisionSuite.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [1, 1, 1]}]}
{"author": "rahij", "sha": "", "commit_date": "2020/10/02 13:16:19", "commit_message": "[SPARK-32741][SQL][FOLLOWUP] Run plan integrity check only for effective plan changes\n\n### What changes were proposed in this pull request?\n\n(This is a followup PR of #29585) The PR modified `RuleExecutor#isPlanIntegral` code for checking if a plan has globally-unique attribute IDs, but this check made Jenkins maven test jobs much longer (See [the Dongjoon comment](https://github.com/apache/spark/pull/29585#issuecomment-702461314) and thanks, dongjoon-hyun !). To recover running time for the Jenkins tests, this PR intends to update the code to run plan integrity check only for effective plans.\n\n### Why are the changes needed?\n\nTo recover running time for Jenkins tests.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nExisting tests.\n\nCloses #29928 from maropu/PR29585-FOLLOWUP.\n\nAuthored-by: Takeshi Yamamuro <yamamuro@apache.org>\nSigned-off-by: Takeshi Yamamuro <yamamuro@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/rules/RuleExecutor.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 2]}]}
{"author": "TJX2014", "sha": "", "commit_date": "2020/08/22 12:32:23", "commit_message": "[SPARK-31792][SS][DOC][FOLLOW-UP] Rephrase the description for some operations\n\n### What changes were proposed in this pull request?\nRephrase the description for some operations to make it clearer.\n\n### Why are the changes needed?\nAdd more detail in the document.\n\n### Does this PR introduce _any_ user-facing change?\nNo, document only.\n\n### How was this patch tested?\nDocument only.\n\nCloses #29269 from xuanyuanking/SPARK-31792-follow.\n\nAuthored-by: Yuanjian Li <yuanjian.li@databricks.com>\nSigned-off-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "docs/web-ui.md", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [1, 1, 1]}]}
{"author": "fqaiser94", "sha": "", "commit_date": "2020/08/23 15:26:15", "commit_message": "Merge branch 'master' of https://github.com/apache/spark", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": ".github/workflows/build_and_test.yml", "additions": "32", "deletions": "11", "changes": "43"}, "updated": [1, 3, 3]}, {"file": {"name": ".github/workflows/test_report.yml", "additions": "24", "deletions": "0", "changes": "24"}, "updated": [0, 2, 2]}, {"file": {"name": "R/pkg/R/functions.R", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "R/pkg/tests/run-all.R", "additions": "6", "deletions": "3", "changes": "9"}, "updated": [0, 1, 2]}, {"file": {"name": "appveyor.yml", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 2]}, {"file": {"name": "common/network-common/src/main/java/org/apache/spark/network/util/TransportConf.java", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ExternalBlockStoreClient.java", "additions": "0", "deletions": "6", "changes": "6"}, "updated": [1, 1, 1]}, {"file": {"name": "core/src/main/java/org/apache/spark/api/plugin/DriverPlugin.java", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 1, 1]}, {"file": {"name": "core/src/main/java/org/apache/spark/shuffle/sort/io/LocalDiskShuffleMapOutputWriter.java", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 1, 1]}, {"file": {"name": "core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java", "additions": "70", "deletions": "0", "changes": "70"}, "updated": [0, 1, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [1, 3, 6]}, {"file": {"name": "core/src/main/scala/org/apache/spark/SparkContext.scala", "additions": "21", "deletions": "6", "changes": "27"}, "updated": [0, 1, 6]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala", "additions": "8", "deletions": "2", "changes": "10"}, "updated": [0, 1, 4]}, {"file": {"name": "core/src/main/scala/org/apache/spark/executor/Executor.scala", "additions": "29", "deletions": "16", "changes": "45"}, "updated": [0, 2, 4]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/Worker.scala", "additions": "0", "deletions": "6", "changes": "6"}, "updated": [0, 1, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 2, 7]}, {"file": {"name": "core/src/main/scala/org/apache/spark/resource/ResourceDiscoveryScriptPlugin.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala", "additions": "29", "deletions": "12", "changes": "41"}, "updated": [0, 1, 6]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/ShuffleMapStage.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/TaskDescription.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala", "additions": "42", "deletions": "8", "changes": "50"}, "updated": [0, 2, 9]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala", "additions": "36", "deletions": "30", "changes": "66"}, "updated": [0, 3, 5]}, {"file": {"name": "core/src/main/scala/org/apache/spark/shuffle/IndexShuffleBlockResolver.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/test/java/org/apache/spark/unsafe/map/AbstractBytesToBytesMapSuite.java", "additions": "24", "deletions": "3", "changes": "27"}, "updated": [0, 1, 1]}, {"file": {"name": "core/src/test/java/test/org/apache/spark/JavaSparkContextSuite.java", "additions": "5", "deletions": "2", "changes": "7"}, "updated": [0, 1, 1]}, {"file": {"name": "core/src/test/scala/org/apache/spark/ExecutorAllocationManagerSuite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 2, 4]}, {"file": {"name": "core/src/test/scala/org/apache/spark/deploy/DecommissionWorkerSuite.scala", "additions": "54", "deletions": "12", "changes": "66"}, "updated": [0, 2, 3]}, {"file": {"name": "core/src/test/scala/org/apache/spark/deploy/SparkSubmitSuite.scala", "additions": "82", "deletions": "1", "changes": "83"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/test/scala/org/apache/spark/deploy/client/AppClientSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/TaskSchedulerImplSuite.scala", "additions": "55", "deletions": "8", "changes": "63"}, "updated": [0, 2, 5]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/TaskSetManagerSuite.scala", "additions": "58", "deletions": "6", "changes": "64"}, "updated": [0, 2, 6]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/WorkerDecommissionExtendedSuite.scala", "additions": "2", "deletions": "3", "changes": "5"}, "updated": [0, 2, 4]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/WorkerDecommissionSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 5]}, {"file": {"name": "core/src/test/scala/org/apache/spark/storage/BlockManagerDecommissionIntegrationSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 3, 7]}, {"file": {"name": "dev/deps/spark-deps-hadoop-2.7-hive-1.2", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 1, 6]}, {"file": {"name": "dev/deps/spark-deps-hadoop-2.7-hive-2.3", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 1, 6]}, {"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 1, 6]}, {"file": {"name": "dev/run-tests.py", "additions": "7", "deletions": "1", "changes": "8"}, "updated": [1, 1, 11]}, {"file": {"name": "docs/job-scheduling.md", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 3]}, {"file": {"name": "docs/monitoring.md", "additions": "4", "deletions": "10", "changes": "14"}, "updated": [0, 3, 6]}, {"file": {"name": "docs/sql-migration-guide.md", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 1, 8]}, {"file": {"name": "docs/sql-performance-tuning.md", "additions": "22", "deletions": "0", "changes": "22"}, "updated": [1, 1, 1]}, {"file": {"name": "docs/sql-ref-syntax-qry-select-groupby.md", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 2]}, {"file": {"name": "docs/sql-ref-syntax-qry-select-hints.md", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "docs/sql-ref.md", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "docs/tuning.md", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [1, 1, 1]}, {"file": {"name": "docs/web-ui.md", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [1, 1, 1]}, {"file": {"name": "external/avro/src/main/java/org/apache/spark/sql/avro/SparkAvroKeyOutputFormat.java", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "external/avro/src/main/scala/org/apache/spark/sql/avro/AvroDataToCatalyst.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 2]}, {"file": {"name": "external/avro/src/main/scala/org/apache/spark/sql/avro/AvroDeserializer.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 2]}, {"file": {"name": "external/avro/src/main/scala/org/apache/spark/sql/avro/AvroOptions.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "external/avro/src/main/scala/org/apache/spark/sql/avro/AvroSerializer.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "external/avro/src/main/scala/org/apache/spark/sql/avro/AvroUtils.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 2]}, {"file": {"name": "external/avro/src/main/scala/org/apache/spark/sql/avro/CatalystDataToAvro.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "external/avro/src/main/scala/org/apache/spark/sql/avro/SchemaConverters.scala", "additions": "18", "deletions": "3", "changes": "21"}, "updated": [1, 1, 2]}, {"file": {"name": "external/avro/src/main/scala/org/apache/spark/sql/v2/avro/AvroDataSourceV2.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 1, 2]}, {"file": {"name": "launcher/src/main/java/org/apache/spark/launcher/LauncherServer.java", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "mllib/src/main/scala/org/apache/spark/ml/feature/CountVectorizer.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [1, 1, 1]}, {"file": {"name": "mllib/src/test/scala/org/apache/spark/ml/feature/CountVectorizerSuite.scala", "additions": "59", "deletions": "15", "changes": "74"}, "updated": [1, 1, 1]}, {"file": {"name": "pom.xml", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [1, 2, 12]}, {"file": {"name": "project/SparkBuild.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [1, 2, 10]}, {"file": {"name": "python/docs/source/reference/pyspark.ml.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 3]}, {"file": {"name": "python/docs/source/reference/pyspark.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 2, 3]}, {"file": {"name": "python/pyspark/ml/clustering.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 3]}, {"file": {"name": "python/pyspark/ml/tests/test_tuning.py", "additions": "120", "deletions": "11", "changes": "131"}, "updated": [1, 1, 2]}, {"file": {"name": "python/pyspark/ml/tuning.py", "additions": "52", "deletions": "15", "changes": "67"}, "updated": [1, 1, 3]}, {"file": {"name": "python/pyspark/sql/functions.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 9]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Constants.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/DecommissionSuite.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 1, 2]}, {"file": {"name": "sbin/decommission-worker.sh", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/TableCatalog.java", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/Row.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/CatalystTypeConverters.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/QueryPlanningTracker.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflection.scala", "additions": "3", "deletions": "4", "changes": "7"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala", "additions": "11", "deletions": "3", "changes": "14"}, "updated": [0, 2, 13]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/encoders/RowEncoder.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/CodeGeneratorWithInterpretedFallback.scala", "additions": "2", "deletions": "3", "changes": "5"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/SpecificInternalRow.scala", "additions": "34", "deletions": "15", "changes": "49"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/ApproxCountDistinctForIntervals.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/HyperLogLogPlusPlus.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "6", "deletions": "8", "changes": "14"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/complexTypeCreator.scala", "additions": "35", "deletions": "75", "changes": "110"}, "updated": [0, 2, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/objects/objects.scala", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [1, 1, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/ComplexTypes.scala", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [0, 4, 13]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/WithFields.scala", "additions": "8", "deletions": "8", "changes": "16"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala", "additions": "21", "deletions": "6", "changes": "27"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/objects.scala", "additions": "9", "deletions": "1", "changes": "10"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/ParserUtils.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/ArrayBasedMapData.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "6", "deletions": "3", "changes": "9"}, "updated": [0, 3, 18]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CodeGenerationSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ComplexTypeSuite.scala", "additions": "18", "deletions": "13", "changes": "31"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ExpressionEvalHelper.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/CombineWithFieldsSuite.scala", "additions": "22", "deletions": "19", "changes": "41"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ObjectSerializerPruningSuite.scala", "additions": "32", "deletions": "1", "changes": "33"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/StarJoinCostBasedReorderSuite.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/complexTypesSuite.scala", "additions": "30", "deletions": "51", "changes": "81"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/Column.scala", "additions": "18", "deletions": "68", "changes": "86"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/AlreadyOptimized.scala", "additions": "37", "deletions": "0", "changes": "37"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/BaseScriptTransformationExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala", "additions": "4", "deletions": "6", "changes": "10"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala", "additions": "9", "deletions": "9", "changes": "18"}, "updated": [0, 2, 7]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 3, 10]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/EliminateNullAwareAntiJoin.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [1, 2, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/compressionSchemes.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/command/functions.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala", "additions": "6", "deletions": "2", "changes": "8"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVDataSource.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonDataSource.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala", "additions": "1", "deletions": "3", "changes": "4"}, "updated": [0, 3, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/FileDataSourceV2.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ShowTablePropertiesExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V1FallbackWriters.scala", "additions": "3", "deletions": "5", "changes": "8"}, "updated": [0, 2, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/csv/CSVDataSourceV2.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/json/JsonDataSourceV2.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/orc/OrcDataSourceV2.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetDataSourceV2.scala", "additions": "5", "deletions": "2", "changes": "7"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/text/TextDataSourceV2.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastHashJoinExec.scala", "additions": "3", "deletions": "15", "changes": "18"}, "updated": [1, 2, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashJoin.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 2, 7]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashedRelation.scala", "additions": "173", "deletions": "12", "changes": "185"}, "updated": [1, 3, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/joins/ShuffledHashJoinExec.scala", "additions": "233", "deletions": "6", "changes": "239"}, "updated": [0, 2, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/joins/ShuffledJoin.scala", "additions": "22", "deletions": "1", "changes": "23"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/joins/SortMergeJoinExec.scala", "additions": "0", "deletions": "20", "changes": "20"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog.scala", "additions": "67", "deletions": "30", "changes": "97"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSinkLog.scala", "additions": "1", "deletions": "9", "changes": "10"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceLog.scala", "additions": "1", "deletions": "5", "changes": "6"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/HDFSMetadataLog.scala", "additions": "53", "deletions": "27", "changes": "80"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLAppStatusListener.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/functions.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/core/src/test/resources/structured-streaming/file-sink-log-version-2.1.0/8", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q10.sf100/explain.txt", "additions": "286", "deletions": "0", "changes": "286"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q10.sf100/simplified.txt", "additions": "81", "deletions": "0", "changes": "81"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q10/explain.txt", "additions": "266", "deletions": "0", "changes": "266"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q10/simplified.txt", "additions": "71", "deletions": "0", "changes": "71"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q19.sf100/explain.txt", "additions": "221", "deletions": "0", "changes": "221"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q19.sf100/simplified.txt", "additions": "58", "deletions": "0", "changes": "58"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q19/explain.txt", "additions": "221", "deletions": "0", "changes": "221"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q19/simplified.txt", "additions": "58", "deletions": "0", "changes": "58"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q27.sf100/explain.txt", "additions": "428", "deletions": "0", "changes": "428"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q27.sf100/simplified.txt", "additions": "113", "deletions": "0", "changes": "113"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q27/explain.txt", "additions": "428", "deletions": "0", "changes": "428"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q27/simplified.txt", "additions": "113", "deletions": "0", "changes": "113"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q3.sf100/explain.txt", "additions": "122", "deletions": "0", "changes": "122"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q3.sf100/simplified.txt", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q3/explain.txt", "additions": "122", "deletions": "0", "changes": "122"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q3/simplified.txt", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q34.sf100/explain.txt", "additions": "218", "deletions": "0", "changes": "218"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q34.sf100/simplified.txt", "additions": "63", "deletions": "0", "changes": "63"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q34/explain.txt", "additions": "203", "deletions": "0", "changes": "203"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q34/simplified.txt", "additions": "54", "deletions": "0", "changes": "54"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q42.sf100/explain.txt", "additions": "122", "deletions": "0", "changes": "122"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q42.sf100/simplified.txt", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q42/explain.txt", "additions": "122", "deletions": "0", "changes": "122"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q42/simplified.txt", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q43.sf100/explain.txt", "additions": "122", "deletions": "0", "changes": "122"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q43.sf100/simplified.txt", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q43/explain.txt", "additions": "122", "deletions": "0", "changes": "122"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q43/simplified.txt", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q46.sf100/explain.txt", "additions": "281", "deletions": "0", "changes": "281"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q46.sf100/simplified.txt", "additions": "87", "deletions": "0", "changes": "87"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q46/explain.txt", "additions": "241", "deletions": "0", "changes": "241"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q46/simplified.txt", "additions": "63", "deletions": "0", "changes": "63"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q52.sf100/explain.txt", "additions": "122", "deletions": "0", "changes": "122"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q52.sf100/simplified.txt", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q52/explain.txt", "additions": "122", "deletions": "0", "changes": "122"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q52/simplified.txt", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q53.sf100/explain.txt", "additions": "180", "deletions": "0", "changes": "180"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q53.sf100/simplified.txt", "additions": "49", "deletions": "0", "changes": "49"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q53/explain.txt", "additions": "180", "deletions": "0", "changes": "180"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q53/simplified.txt", "additions": "49", "deletions": "0", "changes": "49"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q55.sf100/explain.txt", "additions": "122", "deletions": "0", "changes": "122"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q55.sf100/simplified.txt", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q55/explain.txt", "additions": "122", "deletions": "0", "changes": "122"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q55/simplified.txt", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q59.sf100/explain.txt", "additions": "290", "deletions": "0", "changes": "290"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q59.sf100/simplified.txt", "additions": "76", "deletions": "0", "changes": "76"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q59/explain.txt", "additions": "290", "deletions": "0", "changes": "290"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q59/simplified.txt", "additions": "76", "deletions": "0", "changes": "76"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q63.sf100/explain.txt", "additions": "180", "deletions": "0", "changes": "180"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q63.sf100/simplified.txt", "additions": "49", "deletions": "0", "changes": "49"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q63/explain.txt", "additions": "180", "deletions": "0", "changes": "180"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q63/simplified.txt", "additions": "49", "deletions": "0", "changes": "49"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q65.sf100/explain.txt", "additions": "245", "deletions": "0", "changes": "245"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q65.sf100/simplified.txt", "additions": "63", "deletions": "0", "changes": "63"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q65/explain.txt", "additions": "245", "deletions": "0", "changes": "245"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q65/simplified.txt", "additions": "63", "deletions": "0", "changes": "63"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q68.sf100/explain.txt", "additions": "289", "deletions": "0", "changes": "289"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q68.sf100/simplified.txt", "additions": "86", "deletions": "0", "changes": "86"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q68/explain.txt", "additions": "241", "deletions": "0", "changes": "241"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q68/simplified.txt", "additions": "63", "deletions": "0", "changes": "63"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q7.sf100/explain.txt", "additions": "193", "deletions": "0", "changes": "193"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q7.sf100/simplified.txt", "additions": "50", "deletions": "0", "changes": "50"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q7/explain.txt", "additions": "193", "deletions": "0", "changes": "193"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q7/simplified.txt", "additions": "50", "deletions": "0", "changes": "50"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q73.sf100/explain.txt", "additions": "203", "deletions": "0", "changes": "203"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q73.sf100/simplified.txt", "additions": "54", "deletions": "0", "changes": "54"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q73/explain.txt", "additions": "203", "deletions": "0", "changes": "203"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q73/simplified.txt", "additions": "54", "deletions": "0", "changes": "54"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q79.sf100/explain.txt", "additions": "208", "deletions": "0", "changes": "208"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q79.sf100/simplified.txt", "additions": "59", "deletions": "0", "changes": "59"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q79/explain.txt", "additions": "193", "deletions": "0", "changes": "193"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q79/simplified.txt", "additions": "50", "deletions": "0", "changes": "50"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q89.sf100/explain.txt", "additions": "175", "deletions": "0", "changes": "175"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q89.sf100/simplified.txt", "additions": "48", "deletions": "0", "changes": "48"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q89/explain.txt", "additions": "175", "deletions": "0", "changes": "175"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q89/simplified.txt", "additions": "48", "deletions": "0", "changes": "48"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q98.sf100/explain.txt", "additions": "162", "deletions": "0", "changes": "162"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q98.sf100/simplified.txt", "additions": "51", "deletions": "0", "changes": "51"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q98/explain.txt", "additions": "147", "deletions": "0", "changes": "147"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q98/simplified.txt", "additions": "42", "deletions": "0", "changes": "42"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/ss_max.sf100/explain.txt", "additions": "56", "deletions": "0", "changes": "56"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/ss_max.sf100/simplified.txt", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/ss_max/explain.txt", "additions": "56", "deletions": "0", "changes": "56"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/ss_max/simplified.txt", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q1.sf100/explain.txt", "additions": "270", "deletions": "0", "changes": "270"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q1.sf100/simplified.txt", "additions": "74", "deletions": "0", "changes": "74"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q1/explain.txt", "additions": "255", "deletions": "0", "changes": "255"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q1/simplified.txt", "additions": "65", "deletions": "0", "changes": "65"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q10.sf100/explain.txt", "additions": "319", "deletions": "0", "changes": "319"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q10.sf100/simplified.txt", "additions": "95", "deletions": "0", "changes": "95"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q10/explain.txt", "additions": "279", "deletions": "0", "changes": "279"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q10/simplified.txt", "additions": "74", "deletions": "0", "changes": "74"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q11.sf100/explain.txt", "additions": "482", "deletions": "0", "changes": "482"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q11.sf100/simplified.txt", "additions": "158", "deletions": "0", "changes": "158"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q11/explain.txt", "additions": "415", "deletions": "0", "changes": "415"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q11/simplified.txt", "additions": "108", "deletions": "0", "changes": "108"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q12.sf100/explain.txt", "additions": "152", "deletions": "0", "changes": "152"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q12.sf100/simplified.txt", "additions": "47", "deletions": "0", "changes": "47"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q12/explain.txt", "additions": "137", "deletions": "0", "changes": "137"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q12/simplified.txt", "additions": "38", "deletions": "0", "changes": "38"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q13.sf100/explain.txt", "additions": "216", "deletions": "0", "changes": "216"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q13.sf100/simplified.txt", "additions": "57", "deletions": "0", "changes": "57"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q13/explain.txt", "additions": "216", "deletions": "0", "changes": "216"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q13/simplified.txt", "additions": "57", "deletions": "0", "changes": "57"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a.sf100/explain.txt", "additions": "878", "deletions": "0", "changes": "878"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a.sf100/simplified.txt", "additions": "254", "deletions": "0", "changes": "254"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a/explain.txt", "additions": "798", "deletions": "0", "changes": "798"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a/simplified.txt", "additions": "214", "deletions": "0", "changes": "214"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b.sf100/explain.txt", "additions": "810", "deletions": "0", "changes": "810"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b.sf100/simplified.txt", "additions": "231", "deletions": "0", "changes": "231"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b/explain.txt", "additions": "763", "deletions": "0", "changes": "763"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b/simplified.txt", "additions": "204", "deletions": "0", "changes": "204"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q15.sf100/explain.txt", "additions": "180", "deletions": "0", "changes": "180"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q15.sf100/simplified.txt", "additions": "57", "deletions": "0", "changes": "57"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q15/explain.txt", "additions": "150", "deletions": "0", "changes": "150"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q15/simplified.txt", "additions": "39", "deletions": "0", "changes": "39"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q16.sf100/explain.txt", "additions": "250", "deletions": "0", "changes": "250"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q16.sf100/simplified.txt", "additions": "71", "deletions": "0", "changes": "71"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q16/explain.txt", "additions": "235", "deletions": "0", "changes": "235"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q16/simplified.txt", "additions": "62", "deletions": "0", "changes": "62"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q17.sf100/explain.txt", "additions": "314", "deletions": "0", "changes": "314"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q17.sf100/simplified.txt", "additions": "98", "deletions": "0", "changes": "98"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q17/explain.txt", "additions": "269", "deletions": "0", "changes": "269"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q17/simplified.txt", "additions": "71", "deletions": "0", "changes": "71"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q18.sf100/explain.txt", "additions": "294", "deletions": "0", "changes": "294"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q18.sf100/simplified.txt", "additions": "87", "deletions": "0", "changes": "87"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q18/explain.txt", "additions": "264", "deletions": "0", "changes": "264"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q18/simplified.txt", "additions": "69", "deletions": "0", "changes": "69"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q19.sf100/explain.txt", "additions": "251", "deletions": "0", "changes": "251"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q19.sf100/simplified.txt", "additions": "76", "deletions": "0", "changes": "76"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q19/explain.txt", "additions": "221", "deletions": "0", "changes": "221"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q19/simplified.txt", "additions": "58", "deletions": "0", "changes": "58"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q2.sf100/explain.txt", "additions": "233", "deletions": "0", "changes": "233"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q2.sf100/simplified.txt", "additions": "70", "deletions": "0", "changes": "70"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q2/explain.txt", "additions": "218", "deletions": "0", "changes": "218"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q2/simplified.txt", "additions": "61", "deletions": "0", "changes": "61"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q20.sf100/explain.txt", "additions": "152", "deletions": "0", "changes": "152"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q20.sf100/simplified.txt", "additions": "47", "deletions": "0", "changes": "47"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q20/explain.txt", "additions": "137", "deletions": "0", "changes": "137"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q20/simplified.txt", "additions": "38", "deletions": "0", "changes": "38"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q21.sf100/explain.txt", "additions": "155", "deletions": "0", "changes": "155"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q21.sf100/simplified.txt", "additions": "40", "deletions": "0", "changes": "40"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q21/explain.txt", "additions": "155", "deletions": "0", "changes": "155"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q21/simplified.txt", "additions": "40", "deletions": "0", "changes": "40"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q22.sf100/explain.txt", "additions": "170", "deletions": "0", "changes": "170"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q22.sf100/simplified.txt", "additions": "49", "deletions": "0", "changes": "49"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q22/explain.txt", "additions": "155", "deletions": "0", "changes": "155"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q22/simplified.txt", "additions": "40", "deletions": "0", "changes": "40"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23a.sf100/explain.txt", "additions": "655", "deletions": "0", "changes": "655"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23a.sf100/simplified.txt", "additions": "198", "deletions": "0", "changes": "198"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23a/explain.txt", "additions": "542", "deletions": "0", "changes": "542"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23a/simplified.txt", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23b.sf100/explain.txt", "additions": "870", "deletions": "0", "changes": "870"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23b.sf100/simplified.txt", "additions": "268", "deletions": "0", "changes": "268"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23b/explain.txt", "additions": "689", "deletions": "0", "changes": "689"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23b/simplified.txt", "additions": "182", "deletions": "0", "changes": "182"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24a.sf100/explain.txt", "additions": "567", "deletions": "0", "changes": "567"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24a.sf100/simplified.txt", "additions": "179", "deletions": "0", "changes": "179"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24a/explain.txt", "additions": "477", "deletions": "0", "changes": "477"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24a/simplified.txt", "additions": "125", "deletions": "0", "changes": "125"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24b.sf100/explain.txt", "additions": "567", "deletions": "0", "changes": "567"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24b.sf100/simplified.txt", "additions": "179", "deletions": "0", "changes": "179"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24b/explain.txt", "additions": "477", "deletions": "0", "changes": "477"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24b/simplified.txt", "additions": "125", "deletions": "0", "changes": "125"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q25.sf100/explain.txt", "additions": "314", "deletions": "0", "changes": "314"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q25.sf100/simplified.txt", "additions": "98", "deletions": "0", "changes": "98"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q25/explain.txt", "additions": "269", "deletions": "0", "changes": "269"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q25/simplified.txt", "additions": "71", "deletions": "0", "changes": "71"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q26.sf100/explain.txt", "additions": "193", "deletions": "0", "changes": "193"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q26.sf100/simplified.txt", "additions": "50", "deletions": "0", "changes": "50"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q26/explain.txt", "additions": "193", "deletions": "0", "changes": "193"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q26/simplified.txt", "additions": "50", "deletions": "0", "changes": "50"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q27.sf100/explain.txt", "additions": "193", "deletions": "0", "changes": "193"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q27.sf100/simplified.txt", "additions": "50", "deletions": "0", "changes": "50"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q27/explain.txt", "additions": "193", "deletions": "0", "changes": "193"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q27/simplified.txt", "additions": "50", "deletions": "0", "changes": "50"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q28.sf100/explain.txt", "additions": "437", "deletions": "0", "changes": "437"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q28.sf100/simplified.txt", "additions": "107", "deletions": "0", "changes": "107"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q28/explain.txt", "additions": "437", "deletions": "0", "changes": "437"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q28/simplified.txt", "additions": "107", "deletions": "0", "changes": "107"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q29.sf100/explain.txt", "additions": "337", "deletions": "0", "changes": "337"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q29.sf100/simplified.txt", "additions": "104", "deletions": "0", "changes": "104"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q29/explain.txt", "additions": "292", "deletions": "0", "changes": "292"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q29/simplified.txt", "additions": "77", "deletions": "0", "changes": "77"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q3.sf100/explain.txt", "additions": "122", "deletions": "0", "changes": "122"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q3.sf100/simplified.txt", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q3/explain.txt", "additions": "122", "deletions": "0", "changes": "122"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q3/simplified.txt", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q30.sf100/explain.txt", "additions": "333", "deletions": "0", "changes": "333"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q30.sf100/simplified.txt", "additions": "96", "deletions": "0", "changes": "96"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q30/explain.txt", "additions": "303", "deletions": "0", "changes": "303"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q30/simplified.txt", "additions": "78", "deletions": "0", "changes": "78"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q31.sf100/explain.txt", "additions": "663", "deletions": "0", "changes": "663"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q31.sf100/simplified.txt", "additions": "206", "deletions": "0", "changes": "206"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q31/explain.txt", "additions": "563", "deletions": "0", "changes": "563"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q31/simplified.txt", "additions": "150", "deletions": "0", "changes": "150"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q32.sf100/explain.txt", "additions": "175", "deletions": "0", "changes": "175"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q32.sf100/simplified.txt", "additions": "45", "deletions": "0", "changes": "45"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q32/explain.txt", "additions": "175", "deletions": "0", "changes": "175"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q32/simplified.txt", "additions": "45", "deletions": "0", "changes": "45"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q33.sf100/explain.txt", "additions": "378", "deletions": "0", "changes": "378"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q33.sf100/simplified.txt", "additions": "101", "deletions": "0", "changes": "101"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q33/explain.txt", "additions": "378", "deletions": "0", "changes": "378"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q33/simplified.txt", "additions": "101", "deletions": "0", "changes": "101"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q34.sf100/explain.txt", "additions": "218", "deletions": "0", "changes": "218"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q34.sf100/simplified.txt", "additions": "63", "deletions": "0", "changes": "63"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q34/explain.txt", "additions": "203", "deletions": "0", "changes": "203"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q34/simplified.txt", "additions": "54", "deletions": "0", "changes": "54"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q35.sf100/explain.txt", "additions": "329", "deletions": "0", "changes": "329"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q35.sf100/simplified.txt", "additions": "103", "deletions": "0", "changes": "103"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q35/explain.txt", "additions": "274", "deletions": "0", "changes": "274"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q35/simplified.txt", "additions": "73", "deletions": "0", "changes": "73"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q36.sf100/explain.txt", "additions": "180", "deletions": "0", "changes": "180"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q36.sf100/simplified.txt", "additions": "49", "deletions": "0", "changes": "49"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q36/explain.txt", "additions": "180", "deletions": "0", "changes": "180"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q36/simplified.txt", "additions": "49", "deletions": "0", "changes": "49"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q37.sf100/explain.txt", "additions": "175", "deletions": "0", "changes": "175"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q37.sf100/simplified.txt", "additions": "50", "deletions": "0", "changes": "50"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q37/explain.txt", "additions": "160", "deletions": "0", "changes": "160"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q37/simplified.txt", "additions": "41", "deletions": "0", "changes": "41"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q38.sf100/explain.txt", "additions": "393", "deletions": "0", "changes": "393"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q38.sf100/simplified.txt", "additions": "118", "deletions": "0", "changes": "118"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q38/explain.txt", "additions": "328", "deletions": "0", "changes": "328"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q38/simplified.txt", "additions": "81", "deletions": "0", "changes": "81"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q39a.sf100/explain.txt", "additions": "307", "deletions": "0", "changes": "307"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q39a.sf100/simplified.txt", "additions": "86", "deletions": "0", "changes": "86"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q39a/explain.txt", "additions": "292", "deletions": "0", "changes": "292"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q39a/simplified.txt", "additions": "77", "deletions": "0", "changes": "77"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q39b.sf100/explain.txt", "additions": "307", "deletions": "0", "changes": "307"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q39b.sf100/simplified.txt", "additions": "86", "deletions": "0", "changes": "86"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q39b/explain.txt", "additions": "292", "deletions": "0", "changes": "292"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q39b/simplified.txt", "additions": "77", "deletions": "0", "changes": "77"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q4.sf100/explain.txt", "additions": "695", "deletions": "0", "changes": "695"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q4.sf100/simplified.txt", "additions": "231", "deletions": "0", "changes": "231"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q4/explain.txt", "additions": "606", "deletions": "0", "changes": "606"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q4/simplified.txt", "additions": "158", "deletions": "0", "changes": "158"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q40.sf100/explain.txt", "additions": "198", "deletions": "0", "changes": "198"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q40.sf100/simplified.txt", "additions": "56", "deletions": "0", "changes": "56"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q40/explain.txt", "additions": "183", "deletions": "0", "changes": "183"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q40/simplified.txt", "additions": "48", "deletions": "0", "changes": "48"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q41.sf100/explain.txt", "additions": "120", "deletions": "0", "changes": "120"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q41.sf100/simplified.txt", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q41/explain.txt", "additions": "120", "deletions": "0", "changes": "120"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q41/simplified.txt", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q42.sf100/explain.txt", "additions": "122", "deletions": "0", "changes": "122"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q42.sf100/simplified.txt", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q42/explain.txt", "additions": "122", "deletions": "0", "changes": "122"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q42/simplified.txt", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q43.sf100/explain.txt", "additions": "122", "deletions": "0", "changes": "122"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q43.sf100/simplified.txt", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q43/explain.txt", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q43/simplified.txt", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q44.sf100/explain.txt", "additions": "248", "deletions": "0", "changes": "248"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q44.sf100/simplified.txt", "additions": "68", "deletions": "0", "changes": "68"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q44/explain.txt", "additions": "248", "deletions": "0", "changes": "248"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q44/simplified.txt", "additions": "68", "deletions": "0", "changes": "68"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q45.sf100/explain.txt", "additions": "256", "deletions": "0", "changes": "256"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q45.sf100/simplified.txt", "additions": "77", "deletions": "0", "changes": "77"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q45/explain.txt", "additions": "226", "deletions": "0", "changes": "226"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q45/simplified.txt", "additions": "59", "deletions": "0", "changes": "59"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q46.sf100/explain.txt", "additions": "281", "deletions": "0", "changes": "281"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q46.sf100/simplified.txt", "additions": "87", "deletions": "0", "changes": "87"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q46/explain.txt", "additions": "241", "deletions": "0", "changes": "241"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q46/simplified.txt", "additions": "63", "deletions": "0", "changes": "63"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q47.sf100/explain.txt", "additions": "313", "deletions": "0", "changes": "313"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q47.sf100/simplified.txt", "additions": "107", "deletions": "0", "changes": "107"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q47/explain.txt", "additions": "278", "deletions": "0", "changes": "278"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q47/simplified.txt", "additions": "84", "deletions": "0", "changes": "84"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q48.sf100/explain.txt", "additions": "183", "deletions": "0", "changes": "183"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q48.sf100/simplified.txt", "additions": "48", "deletions": "0", "changes": "48"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q48/explain.txt", "additions": "183", "deletions": "0", "changes": "183"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q48/simplified.txt", "additions": "48", "deletions": "0", "changes": "48"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q49.sf100/explain.txt", "additions": "478", "deletions": "0", "changes": "478"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q49.sf100/simplified.txt", "additions": "153", "deletions": "0", "changes": "153"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q49/explain.txt", "additions": "433", "deletions": "0", "changes": "433"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q49/simplified.txt", "additions": "126", "deletions": "0", "changes": "126"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q5.sf100/explain.txt", "additions": "450", "deletions": "0", "changes": "450"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q5.sf100/simplified.txt", "additions": "132", "deletions": "0", "changes": "132"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q5/explain.txt", "additions": "435", "deletions": "0", "changes": "435"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q5/simplified.txt", "additions": "123", "deletions": "0", "changes": "123"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q50.sf100/explain.txt", "additions": "198", "deletions": "0", "changes": "198"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q50.sf100/simplified.txt", "additions": "57", "deletions": "0", "changes": "57"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q50/explain.txt", "additions": "183", "deletions": "0", "changes": "183"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q50/simplified.txt", "additions": "48", "deletions": "0", "changes": "48"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q51.sf100/explain.txt", "additions": "228", "deletions": "0", "changes": "228"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q51.sf100/simplified.txt", "additions": "71", "deletions": "0", "changes": "71"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q51/explain.txt", "additions": "228", "deletions": "0", "changes": "228"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q51/simplified.txt", "additions": "71", "deletions": "0", "changes": "71"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q52.sf100/explain.txt", "additions": "122", "deletions": "0", "changes": "122"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q52.sf100/simplified.txt", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q52/explain.txt", "additions": "122", "deletions": "0", "changes": "122"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q52/simplified.txt", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q53.sf100/explain.txt", "additions": "180", "deletions": "0", "changes": "180"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q53.sf100/simplified.txt", "additions": "49", "deletions": "0", "changes": "49"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q53/explain.txt", "additions": "180", "deletions": "0", "changes": "180"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q53/simplified.txt", "additions": "49", "deletions": "0", "changes": "49"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q54.sf100/explain.txt", "additions": "494", "deletions": "0", "changes": "494"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q54.sf100/simplified.txt", "additions": "142", "deletions": "0", "changes": "142"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q54/explain.txt", "additions": "459", "deletions": "0", "changes": "459"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q54/simplified.txt", "additions": "121", "deletions": "0", "changes": "121"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q55.sf100/explain.txt", "additions": "122", "deletions": "0", "changes": "122"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q55.sf100/simplified.txt", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q55/explain.txt", "additions": "122", "deletions": "0", "changes": "122"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q55/simplified.txt", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q56.sf100/explain.txt", "additions": "378", "deletions": "0", "changes": "378"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q56.sf100/simplified.txt", "additions": "101", "deletions": "0", "changes": "101"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q56/explain.txt", "additions": "378", "deletions": "0", "changes": "378"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q56/simplified.txt", "additions": "101", "deletions": "0", "changes": "101"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q57.sf100/explain.txt", "additions": "313", "deletions": "0", "changes": "313"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q57.sf100/simplified.txt", "additions": "107", "deletions": "0", "changes": "107"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q57/explain.txt", "additions": "278", "deletions": "0", "changes": "278"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q57/simplified.txt", "additions": "84", "deletions": "0", "changes": "84"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q58.sf100/explain.txt", "additions": "477", "deletions": "0", "changes": "477"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q58.sf100/simplified.txt", "additions": "125", "deletions": "0", "changes": "125"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q58/explain.txt", "additions": "477", "deletions": "0", "changes": "477"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q58/simplified.txt", "additions": "125", "deletions": "0", "changes": "125"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q59.sf100/explain.txt", "additions": "249", "deletions": "0", "changes": "249"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q59.sf100/simplified.txt", "additions": "66", "deletions": "0", "changes": "66"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q59/explain.txt", "additions": "249", "deletions": "0", "changes": "249"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q59/simplified.txt", "additions": "66", "deletions": "0", "changes": "66"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q6.sf100/explain.txt", "additions": "331", "deletions": "0", "changes": "331"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q6.sf100/simplified.txt", "additions": "95", "deletions": "0", "changes": "95"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q6/explain.txt", "additions": "301", "deletions": "0", "changes": "301"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q6/simplified.txt", "additions": "77", "deletions": "0", "changes": "77"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q60.sf100/explain.txt", "additions": "378", "deletions": "0", "changes": "378"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q60.sf100/simplified.txt", "additions": "101", "deletions": "0", "changes": "101"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q60/explain.txt", "additions": "378", "deletions": "0", "changes": "378"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q60/simplified.txt", "additions": "101", "deletions": "0", "changes": "101"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q61.sf100/explain.txt", "additions": "414", "deletions": "0", "changes": "414"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q61.sf100/simplified.txt", "additions": "110", "deletions": "0", "changes": "110"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q61/explain.txt", "additions": "396", "deletions": "0", "changes": "396"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q61/simplified.txt", "additions": "105", "deletions": "0", "changes": "105"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q62.sf100/explain.txt", "additions": "183", "deletions": "0", "changes": "183"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q62.sf100/simplified.txt", "additions": "48", "deletions": "0", "changes": "48"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q62/explain.txt", "additions": "183", "deletions": "0", "changes": "183"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q62/simplified.txt", "additions": "48", "deletions": "0", "changes": "48"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q63.sf100/explain.txt", "additions": "180", "deletions": "0", "changes": "180"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q63.sf100/simplified.txt", "additions": "49", "deletions": "0", "changes": "49"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q63/explain.txt", "additions": "180", "deletions": "0", "changes": "180"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q63/simplified.txt", "additions": "49", "deletions": "0", "changes": "49"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q64.sf100/explain.txt", "additions": "1110", "deletions": "0", "changes": "1110"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q64.sf100/simplified.txt", "additions": "367", "deletions": "0", "changes": "367"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q64/explain.txt", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q64/simplified.txt", "additions": "246", "deletions": "0", "changes": "246"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q65.sf100/explain.txt", "additions": "260", "deletions": "0", "changes": "260"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q65.sf100/simplified.txt", "additions": "72", "deletions": "0", "changes": "72"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q65/explain.txt", "additions": "245", "deletions": "0", "changes": "245"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q65/simplified.txt", "additions": "63", "deletions": "0", "changes": "63"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q66.sf100/explain.txt", "additions": "310", "deletions": "0", "changes": "310"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q66.sf100/simplified.txt", "additions": "83", "deletions": "0", "changes": "83"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q66/explain.txt", "additions": "310", "deletions": "0", "changes": "310"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q66/simplified.txt", "additions": "83", "deletions": "0", "changes": "83"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q67.sf100/explain.txt", "additions": "190", "deletions": "0", "changes": "190"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q67.sf100/simplified.txt", "additions": "57", "deletions": "0", "changes": "57"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q67/explain.txt", "additions": "175", "deletions": "0", "changes": "175"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q67/simplified.txt", "additions": "48", "deletions": "0", "changes": "48"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q68.sf100/explain.txt", "additions": "281", "deletions": "0", "changes": "281"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q68.sf100/simplified.txt", "additions": "87", "deletions": "0", "changes": "87"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q68/explain.txt", "additions": "241", "deletions": "0", "changes": "241"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q68/simplified.txt", "additions": "63", "deletions": "0", "changes": "63"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q69.sf100/explain.txt", "additions": "299", "deletions": "0", "changes": "299"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q69.sf100/simplified.txt", "additions": "85", "deletions": "0", "changes": "85"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q69/explain.txt", "additions": "274", "deletions": "0", "changes": "274"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q69/simplified.txt", "additions": "73", "deletions": "0", "changes": "73"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q7.sf100/explain.txt", "additions": "193", "deletions": "0", "changes": "193"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q7.sf100/simplified.txt", "additions": "50", "deletions": "0", "changes": "50"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q7/explain.txt", "additions": "193", "deletions": "0", "changes": "193"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q7/simplified.txt", "additions": "50", "deletions": "0", "changes": "50"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q70.sf100/explain.txt", "additions": "264", "deletions": "0", "changes": "264"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q70.sf100/simplified.txt", "additions": "74", "deletions": "0", "changes": "74"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q70/explain.txt", "additions": "264", "deletions": "0", "changes": "264"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q70/simplified.txt", "additions": "74", "deletions": "0", "changes": "74"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q71.sf100/explain.txt", "additions": "232", "deletions": "0", "changes": "232"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q71.sf100/simplified.txt", "additions": "65", "deletions": "0", "changes": "65"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q71/explain.txt", "additions": "232", "deletions": "0", "changes": "232"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q71/simplified.txt", "additions": "65", "deletions": "0", "changes": "65"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q72.sf100/explain.txt", "additions": "436", "deletions": "0", "changes": "436"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q72.sf100/simplified.txt", "additions": "130", "deletions": "0", "changes": "130"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q72/explain.txt", "additions": "391", "deletions": "0", "changes": "391"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q72/simplified.txt", "additions": "104", "deletions": "0", "changes": "104"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q73.sf100/explain.txt", "additions": "218", "deletions": "0", "changes": "218"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q73.sf100/simplified.txt", "additions": "63", "deletions": "0", "changes": "63"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q73/explain.txt", "additions": "203", "deletions": "0", "changes": "203"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q73/simplified.txt", "additions": "54", "deletions": "0", "changes": "54"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q74.sf100/explain.txt", "additions": "477", "deletions": "0", "changes": "477"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q74.sf100/simplified.txt", "additions": "157", "deletions": "0", "changes": "157"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q74/explain.txt", "additions": "410", "deletions": "0", "changes": "410"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q74/simplified.txt", "additions": "107", "deletions": "0", "changes": "107"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q75.sf100/explain.txt", "additions": "752", "deletions": "0", "changes": "752"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q75.sf100/simplified.txt", "additions": "237", "deletions": "0", "changes": "237"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q75/explain.txt", "additions": "647", "deletions": "0", "changes": "647"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q75/simplified.txt", "additions": "180", "deletions": "0", "changes": "180"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q76.sf100/explain.txt", "additions": "245", "deletions": "0", "changes": "245"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q76.sf100/simplified.txt", "additions": "68", "deletions": "0", "changes": "68"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q76/explain.txt", "additions": "209", "deletions": "0", "changes": "209"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q76/simplified.txt", "additions": "58", "deletions": "0", "changes": "58"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q77.sf100/explain.txt", "additions": "520", "deletions": "0", "changes": "520"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q77.sf100/simplified.txt", "additions": "139", "deletions": "0", "changes": "139"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q77/explain.txt", "additions": "520", "deletions": "0", "changes": "520"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q77/simplified.txt", "additions": "139", "deletions": "0", "changes": "139"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q78.sf100/explain.txt", "additions": "391", "deletions": "0", "changes": "391"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q78.sf100/simplified.txt", "additions": "117", "deletions": "0", "changes": "117"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q78/explain.txt", "additions": "341", "deletions": "0", "changes": "341"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q78/simplified.txt", "additions": "88", "deletions": "0", "changes": "88"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q79.sf100/explain.txt", "additions": "208", "deletions": "0", "changes": "208"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q79.sf100/simplified.txt", "additions": "59", "deletions": "0", "changes": "59"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q79/explain.txt", "additions": "193", "deletions": "0", "changes": "193"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q79/simplified.txt", "additions": "50", "deletions": "0", "changes": "50"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q8.sf100/explain.txt", "additions": "302", "deletions": "0", "changes": "302"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q8.sf100/simplified.txt", "additions": "88", "deletions": "0", "changes": "88"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q8/explain.txt", "additions": "272", "deletions": "0", "changes": "272"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q8/simplified.txt", "additions": "70", "deletions": "0", "changes": "70"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q80.sf100/explain.txt", "additions": "598", "deletions": "0", "changes": "598"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q80.sf100/simplified.txt", "additions": "172", "deletions": "0", "changes": "172"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q80/explain.txt", "additions": "553", "deletions": "0", "changes": "553"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q80/simplified.txt", "additions": "148", "deletions": "0", "changes": "148"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q81.sf100/explain.txt", "additions": "343", "deletions": "0", "changes": "343"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q81.sf100/simplified.txt", "additions": "104", "deletions": "0", "changes": "104"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q81/explain.txt", "additions": "298", "deletions": "0", "changes": "298"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q81/simplified.txt", "additions": "77", "deletions": "0", "changes": "77"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q82.sf100/explain.txt", "additions": "175", "deletions": "0", "changes": "175"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q82.sf100/simplified.txt", "additions": "50", "deletions": "0", "changes": "50"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q82/explain.txt", "additions": "160", "deletions": "0", "changes": "160"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q82/simplified.txt", "additions": "41", "deletions": "0", "changes": "41"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q83.sf100/explain.txt", "additions": "344", "deletions": "0", "changes": "344"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q83.sf100/simplified.txt", "additions": "91", "deletions": "0", "changes": "91"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q83/explain.txt", "additions": "344", "deletions": "0", "changes": "344"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q83/simplified.txt", "additions": "91", "deletions": "0", "changes": "91"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q84.sf100/explain.txt", "additions": "200", "deletions": "0", "changes": "200"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q84.sf100/simplified.txt", "additions": "53", "deletions": "0", "changes": "53"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q84/explain.txt", "additions": "200", "deletions": "0", "changes": "200"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q84/simplified.txt", "additions": "53", "deletions": "0", "changes": "53"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q85.sf100/explain.txt", "additions": "317", "deletions": "0", "changes": "317"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q85.sf100/simplified.txt", "additions": "94", "deletions": "0", "changes": "94"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q85/explain.txt", "additions": "287", "deletions": "0", "changes": "287"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q85/simplified.txt", "additions": "76", "deletions": "0", "changes": "76"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q86.sf100/explain.txt", "additions": "142", "deletions": "0", "changes": "142"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q86.sf100/simplified.txt", "additions": "39", "deletions": "0", "changes": "39"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q86/explain.txt", "additions": "142", "deletions": "0", "changes": "142"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q86/simplified.txt", "additions": "39", "deletions": "0", "changes": "39"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q87.sf100/explain.txt", "additions": "388", "deletions": "0", "changes": "388"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q87.sf100/simplified.txt", "additions": "117", "deletions": "0", "changes": "117"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q87/explain.txt", "additions": "323", "deletions": "0", "changes": "323"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q87/simplified.txt", "additions": "80", "deletions": "0", "changes": "80"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q88.sf100/explain.txt", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q88.sf100/simplified.txt", "additions": "250", "deletions": "0", "changes": "250"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q88/explain.txt", "additions": "960", "deletions": "0", "changes": "960"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q88/simplified.txt", "additions": "250", "deletions": "0", "changes": "250"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q89.sf100/explain.txt", "additions": "175", "deletions": "0", "changes": "175"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q89.sf100/simplified.txt", "additions": "48", "deletions": "0", "changes": "48"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q89/explain.txt", "additions": "175", "deletions": "0", "changes": "175"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q89/simplified.txt", "additions": "48", "deletions": "0", "changes": "48"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9.sf100/explain.txt", "additions": "718", "deletions": "0", "changes": "718"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9.sf100/simplified.txt", "additions": "186", "deletions": "0", "changes": "186"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9/explain.txt", "additions": "718", "deletions": "0", "changes": "718"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9/simplified.txt", "additions": "186", "deletions": "0", "changes": "186"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q90.sf100/explain.txt", "additions": "280", "deletions": "0", "changes": "280"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q90.sf100/simplified.txt", "additions": "74", "deletions": "0", "changes": "74"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q90/explain.txt", "additions": "280", "deletions": "0", "changes": "280"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q90/simplified.txt", "additions": "74", "deletions": "0", "changes": "74"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q91.sf100/explain.txt", "additions": "264", "deletions": "0", "changes": "264"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q91.sf100/simplified.txt", "additions": "71", "deletions": "0", "changes": "71"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q91/explain.txt", "additions": "264", "deletions": "0", "changes": "264"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q91/simplified.txt", "additions": "71", "deletions": "0", "changes": "71"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q92.sf100/explain.txt", "additions": "196", "deletions": "0", "changes": "196"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q92.sf100/simplified.txt", "additions": "50", "deletions": "0", "changes": "50"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q92/explain.txt", "additions": "196", "deletions": "0", "changes": "196"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q92/simplified.txt", "additions": "50", "deletions": "0", "changes": "50"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q93.sf100/explain.txt", "additions": "126", "deletions": "0", "changes": "126"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q93.sf100/simplified.txt", "additions": "38", "deletions": "0", "changes": "38"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q93/explain.txt", "additions": "111", "deletions": "0", "changes": "111"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q93/simplified.txt", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q94.sf100/explain.txt", "additions": "265", "deletions": "0", "changes": "265"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q94.sf100/simplified.txt", "additions": "78", "deletions": "0", "changes": "78"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q94/explain.txt", "additions": "235", "deletions": "0", "changes": "235"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q94/simplified.txt", "additions": "62", "deletions": "0", "changes": "62"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q95.sf100/explain.txt", "additions": "347", "deletions": "0", "changes": "347"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q95.sf100/simplified.txt", "additions": "111", "deletions": "0", "changes": "111"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q95/explain.txt", "additions": "318", "deletions": "0", "changes": "318"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q95/simplified.txt", "additions": "84", "deletions": "0", "changes": "84"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q96.sf100/explain.txt", "additions": "160", "deletions": "0", "changes": "160"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q96.sf100/simplified.txt", "additions": "41", "deletions": "0", "changes": "41"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q96/explain.txt", "additions": "160", "deletions": "0", "changes": "160"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q96/simplified.txt", "additions": "41", "deletions": "0", "changes": "41"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q97.sf100/explain.txt", "additions": "179", "deletions": "0", "changes": "179"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q97.sf100/simplified.txt", "additions": "46", "deletions": "0", "changes": "46"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q97/explain.txt", "additions": "179", "deletions": "0", "changes": "179"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q97/simplified.txt", "additions": "46", "deletions": "0", "changes": "46"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q98.sf100/explain.txt", "additions": "162", "deletions": "0", "changes": "162"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q98.sf100/simplified.txt", "additions": "51", "deletions": "0", "changes": "51"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q98/explain.txt", "additions": "147", "deletions": "0", "changes": "147"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q98/simplified.txt", "additions": "42", "deletions": "0", "changes": "42"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q99.sf100/explain.txt", "additions": "183", "deletions": "0", "changes": "183"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q99.sf100/simplified.txt", "additions": "48", "deletions": "0", "changes": "48"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q99/explain.txt", "additions": "183", "deletions": "0", "changes": "183"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q99/simplified.txt", "additions": "48", "deletions": "0", "changes": "48"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q10a.sf100/explain.txt", "additions": "286", "deletions": "0", "changes": "286"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q10a.sf100/simplified.txt", "additions": "81", "deletions": "0", "changes": "81"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q10a/explain.txt", "additions": "266", "deletions": "0", "changes": "266"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q10a/simplified.txt", "additions": "71", "deletions": "0", "changes": "71"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q11.sf100/explain.txt", "additions": "477", "deletions": "0", "changes": "477"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q11.sf100/simplified.txt", "additions": "157", "deletions": "0", "changes": "157"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q11/explain.txt", "additions": "410", "deletions": "0", "changes": "410"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q11/simplified.txt", "additions": "107", "deletions": "0", "changes": "107"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q12.sf100/explain.txt", "additions": "152", "deletions": "0", "changes": "152"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q12.sf100/simplified.txt", "additions": "47", "deletions": "0", "changes": "47"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q12/explain.txt", "additions": "137", "deletions": "0", "changes": "137"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q12/simplified.txt", "additions": "38", "deletions": "0", "changes": "38"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14.sf100/explain.txt", "additions": "810", "deletions": "0", "changes": "810"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14.sf100/simplified.txt", "additions": "231", "deletions": "0", "changes": "231"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14/explain.txt", "additions": "763", "deletions": "0", "changes": "763"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14/simplified.txt", "additions": "204", "deletions": "0", "changes": "204"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a.sf100/explain.txt", "additions": "1460", "deletions": "0", "changes": "1460"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a.sf100/simplified.txt", "additions": "427", "deletions": "0", "changes": "427"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a/explain.txt", "additions": "1380", "deletions": "0", "changes": "1380"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a/simplified.txt", "additions": "387", "deletions": "0", "changes": "387"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q18a.sf100/explain.txt", "additions": "877", "deletions": "0", "changes": "877"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q18a.sf100/simplified.txt", "additions": "262", "deletions": "0", "changes": "262"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q18a/explain.txt", "additions": "856", "deletions": "0", "changes": "856"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q18a/simplified.txt", "additions": "227", "deletions": "0", "changes": "227"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q20.sf100/explain.txt", "additions": "152", "deletions": "0", "changes": "152"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q20.sf100/simplified.txt", "additions": "47", "deletions": "0", "changes": "47"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q20/explain.txt", "additions": "137", "deletions": "0", "changes": "137"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q20/simplified.txt", "additions": "38", "deletions": "0", "changes": "38"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q22.sf100/explain.txt", "additions": "157", "deletions": "0", "changes": "157"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q22.sf100/simplified.txt", "additions": "48", "deletions": "0", "changes": "48"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q22/explain.txt", "additions": "142", "deletions": "0", "changes": "142"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q22/simplified.txt", "additions": "39", "deletions": "0", "changes": "39"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q22a.sf100/explain.txt", "additions": "316", "deletions": "0", "changes": "316"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q22a.sf100/simplified.txt", "additions": "87", "deletions": "0", "changes": "87"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q22a/explain.txt", "additions": "301", "deletions": "0", "changes": "301"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q22a/simplified.txt", "additions": "78", "deletions": "0", "changes": "78"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q24.sf100/explain.txt", "additions": "532", "deletions": "0", "changes": "532"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q24.sf100/simplified.txt", "additions": "156", "deletions": "0", "changes": "156"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q24/explain.txt", "additions": "487", "deletions": "0", "changes": "487"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q24/simplified.txt", "additions": "129", "deletions": "0", "changes": "129"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q27a.sf100/explain.txt", "additions": "428", "deletions": "0", "changes": "428"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q27a.sf100/simplified.txt", "additions": "113", "deletions": "0", "changes": "113"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q27a/explain.txt", "additions": "428", "deletions": "0", "changes": "428"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q27a/simplified.txt", "additions": "113", "deletions": "0", "changes": "113"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q34.sf100/explain.txt", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q34.sf100/simplified.txt", "additions": "63", "deletions": "0", "changes": "63"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q34/explain.txt", "additions": "203", "deletions": "0", "changes": "203"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q34/simplified.txt", "additions": "54", "deletions": "0", "changes": "54"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35.sf100/explain.txt", "additions": "329", "deletions": "0", "changes": "329"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35.sf100/simplified.txt", "additions": "103", "deletions": "0", "changes": "103"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35/explain.txt", "additions": "274", "deletions": "0", "changes": "274"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35/simplified.txt", "additions": "73", "deletions": "0", "changes": "73"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35a.sf100/explain.txt", "additions": "311", "deletions": "0", "changes": "311"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35a.sf100/simplified.txt", "additions": "98", "deletions": "0", "changes": "98"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35a/explain.txt", "additions": "261", "deletions": "0", "changes": "261"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35a/simplified.txt", "additions": "70", "deletions": "0", "changes": "70"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q36a.sf100/explain.txt", "additions": "289", "deletions": "0", "changes": "289"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q36a.sf100/simplified.txt", "additions": "82", "deletions": "0", "changes": "82"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q36a/explain.txt", "additions": "289", "deletions": "0", "changes": "289"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q36a/simplified.txt", "additions": "82", "deletions": "0", "changes": "82"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q47.sf100/explain.txt", "additions": "313", "deletions": "0", "changes": "313"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q47.sf100/simplified.txt", "additions": "107", "deletions": "0", "changes": "107"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q47/explain.txt", "additions": "278", "deletions": "0", "changes": "278"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q47/simplified.txt", "additions": "84", "deletions": "0", "changes": "84"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q49.sf100/explain.txt", "additions": "478", "deletions": "0", "changes": "478"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q49.sf100/simplified.txt", "additions": "153", "deletions": "0", "changes": "153"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q49/explain.txt", "additions": "433", "deletions": "0", "changes": "433"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q49/simplified.txt", "additions": "126", "deletions": "0", "changes": "126"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q51a.sf100/explain.txt", "additions": "441", "deletions": "0", "changes": "441"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q51a.sf100/simplified.txt", "additions": "139", "deletions": "0", "changes": "139"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q51a/explain.txt", "additions": "426", "deletions": "0", "changes": "426"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q51a/simplified.txt", "additions": "126", "deletions": "0", "changes": "126"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q57.sf100/explain.txt", "additions": "313", "deletions": "0", "changes": "313"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q57.sf100/simplified.txt", "additions": "107", "deletions": "0", "changes": "107"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q57/explain.txt", "additions": "278", "deletions": "0", "changes": "278"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q57/simplified.txt", "additions": "84", "deletions": "0", "changes": "84"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q5a.sf100/explain.txt", "additions": "559", "deletions": "0", "changes": "559"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q5a.sf100/simplified.txt", "additions": "165", "deletions": "0", "changes": "165"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q5a/explain.txt", "additions": "544", "deletions": "0", "changes": "544"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q5a/simplified.txt", "additions": "156", "deletions": "0", "changes": "156"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q6.sf100/explain.txt", "additions": "331", "deletions": "0", "changes": "331"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q6.sf100/simplified.txt", "additions": "95", "deletions": "0", "changes": "95"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q6/explain.txt", "additions": "301", "deletions": "0", "changes": "301"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q6/simplified.txt", "additions": "77", "deletions": "0", "changes": "77"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q64.sf100/explain.txt", "additions": "1110", "deletions": "0", "changes": "1110"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q64.sf100/simplified.txt", "additions": "367", "deletions": "0", "changes": "367"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q64/explain.txt", "additions": "918", "deletions": "0", "changes": "918"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q64/simplified.txt", "additions": "246", "deletions": "0", "changes": "246"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q67a.sf100/explain.txt", "additions": "452", "deletions": "0", "changes": "452"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q67a.sf100/simplified.txt", "additions": "129", "deletions": "0", "changes": "129"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q67a/explain.txt", "additions": "437", "deletions": "0", "changes": "437"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q67a/simplified.txt", "additions": "120", "deletions": "0", "changes": "120"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q70a.sf100/explain.txt", "additions": "373", "deletions": "0", "changes": "373"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q70a.sf100/simplified.txt", "additions": "107", "deletions": "0", "changes": "107"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q70a/explain.txt", "additions": "373", "deletions": "0", "changes": "373"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q70a/simplified.txt", "additions": "107", "deletions": "0", "changes": "107"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q72.sf100/explain.txt", "additions": "436", "deletions": "0", "changes": "436"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q72.sf100/simplified.txt", "additions": "130", "deletions": "0", "changes": "130"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q72/explain.txt", "additions": "391", "deletions": "0", "changes": "391"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q72/simplified.txt", "additions": "104", "deletions": "0", "changes": "104"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q74.sf100/explain.txt", "additions": "477", "deletions": "0", "changes": "477"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q74.sf100/simplified.txt", "additions": "157", "deletions": "0", "changes": "157"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q74/explain.txt", "additions": "410", "deletions": "0", "changes": "410"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q74/simplified.txt", "additions": "107", "deletions": "0", "changes": "107"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q75.sf100/explain.txt", "additions": "752", "deletions": "0", "changes": "752"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q75.sf100/simplified.txt", "additions": "237", "deletions": "0", "changes": "237"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q75/explain.txt", "additions": "647", "deletions": "0", "changes": "647"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q75/simplified.txt", "additions": "180", "deletions": "0", "changes": "180"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q77a.sf100/explain.txt", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q77a.sf100/simplified.txt", "additions": "172", "deletions": "0", "changes": "172"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q77a/explain.txt", "additions": "629", "deletions": "0", "changes": "629"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q77a/simplified.txt", "additions": "172", "deletions": "0", "changes": "172"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q78.sf100/explain.txt", "additions": "391", "deletions": "0", "changes": "391"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q78.sf100/simplified.txt", "additions": "117", "deletions": "0", "changes": "117"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q78/explain.txt", "additions": "341", "deletions": "0", "changes": "341"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q78/simplified.txt", "additions": "88", "deletions": "0", "changes": "88"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q80a.sf100/explain.txt", "additions": "707", "deletions": "0", "changes": "707"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q80a.sf100/simplified.txt", "additions": "205", "deletions": "0", "changes": "205"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q80a/explain.txt", "additions": "662", "deletions": "0", "changes": "662"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q80a/simplified.txt", "additions": "181", "deletions": "0", "changes": "181"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q86a.sf100/explain.txt", "additions": "251", "deletions": "0", "changes": "251"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q86a.sf100/simplified.txt", "additions": "72", "deletions": "0", "changes": "72"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q86a/explain.txt", "additions": "251", "deletions": "0", "changes": "251"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q86a/simplified.txt", "additions": "72", "deletions": "0", "changes": "72"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q98.sf100/explain.txt", "additions": "157", "deletions": "0", "changes": "157"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q98.sf100/simplified.txt", "additions": "50", "deletions": "0", "changes": "50"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q98/explain.txt", "additions": "142", "deletions": "0", "changes": "142"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q98/simplified.txt", "additions": "41", "deletions": "0", "changes": "41"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/ColumnExpressionSuite.scala", "additions": "1", "deletions": "350", "changes": "351"}, "updated": [0, 2, 5]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 1, 4]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DatasetOptimizationSuite.scala", "additions": "11", "deletions": "2", "changes": "13"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/JoinSuite.scala", "additions": "66", "deletions": "0", "changes": "66"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/PlanStabilitySuite.scala", "additions": "327", "deletions": "0", "changes": "327"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SQLQueryTestSuite.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/TPCDSBase.scala", "additions": "65", "deletions": "1", "changes": "66"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/TPCDSQuerySuite.scala", "additions": "8", "deletions": "64", "changes": "72"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/V1WriteFallbackSuite.scala", "additions": "42", "deletions": "0", "changes": "42"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/AlreadyOptimizedSuite.scala", "additions": "85", "deletions": "0", "changes": "85"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/BaseScriptTransformationSuite.scala", "additions": "97", "deletions": "7", "changes": "104"}, "updated": [2, 4, 4]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/SQLViewSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/SparkSqlParserSuite.scala", "additions": "81", "deletions": "2", "changes": "83"}, "updated": [0, 2, 4]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 3, 8]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/columnar/compression/BooleanBitSetSuite.scala", "additions": "26", "deletions": "0", "changes": "26"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/DDLSuite.scala", "additions": "30", "deletions": "2", "changes": "32"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/csv/CSVSuite.scala", "additions": "14", "deletions": "2", "changes": "16"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/json/JsonSuite.scala", "additions": "21", "deletions": "3", "changes": "24"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/joins/HashedRelationSuite.scala", "additions": "79", "deletions": "0", "changes": "79"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/CompactibleFileStreamLogSuite.scala", "additions": "0", "deletions": "2", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/FileStreamSinkLogSuite.scala", "additions": "7", "deletions": "13", "changes": "20"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/ui/SQLAppStatusListenerSuite.scala", "additions": "132", "deletions": "0", "changes": "132"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/v1.2/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcFilterSuite.scala", "additions": "95", "deletions": "2", "changes": "97"}, "updated": [2, 3, 4]}, {"file": {"name": "sql/core/v2.3/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcFilterSuite.scala", "additions": "95", "deletions": "2", "changes": "97"}, "updated": [2, 3, 4]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala", "additions": "23", "deletions": "2", "changes": "25"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/HiveSQLViewSuite.scala", "additions": "21", "deletions": "1", "changes": "22"}, "updated": [0, 1, 1]}, {"file": {"name": "streaming/src/main/scala/org/apache/spark/streaming/scheduler/ExecutorAllocationManager.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 2, 2]}, {"file": {"name": "streaming/src/test/scala/org/apache/spark/streaming/scheduler/ExecutorAllocationManagerSuite.scala", "additions": "2", "deletions": "3", "changes": "5"}, "updated": [0, 2, 2]}]}
{"author": "GuoPhilipse", "sha": "", "commit_date": "2020/08/14 14:18:22", "commit_message": "Merge pull request #23 from apache/master\n\nsync", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": ".github/workflows/master.yml", "additions": "17", "deletions": "11", "changes": "28"}, "updated": [1, 1, 10]}, {"file": {"name": ".github/workflows/test_report.yml", "additions": "35", "deletions": "0", "changes": "35"}, "updated": [1, 1, 1]}, {"file": {"name": "R/pkg/R/functions.R", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "R/pkg/R/utils.R", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 1, 2]}, {"file": {"name": "common/unsafe/src/main/java/org/apache/spark/unsafe/types/UTF8String.java", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/main/java/org/apache/spark/api/java/StorageLevels.java", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/utils.js", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/ExecutorAllocationClient.scala", "additions": "40", "deletions": "0", "changes": "40"}, "updated": [1, 1, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala", "additions": "27", "deletions": "7", "changes": "34"}, "updated": [1, 2, 4]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala", "additions": "10", "deletions": "13", "changes": "23"}, "updated": [1, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 2, 4]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala", "additions": "6", "deletions": "4", "changes": "10"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala", "additions": "110", "deletions": "80", "changes": "190"}, "updated": [1, 3, 7]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 1, 4]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/dynalloc/ExecutorMonitor.scala", "additions": "55", "deletions": "6", "changes": "61"}, "updated": [1, 1, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/storage/BlockManager.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 2, 4]}, {"file": {"name": "core/src/main/scala/org/apache/spark/storage/BlockManagerMaster.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [1, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/storage/StorageLevel.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "core/src/test/scala/org/apache/spark/DistributedSuite.scala", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/test/scala/org/apache/spark/ExecutorAllocationManagerSuite.scala", "additions": "70", "deletions": "1", "changes": "71"}, "updated": [1, 1, 3]}, {"file": {"name": "core/src/test/scala/org/apache/spark/SparkFunSuite.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/test/scala/org/apache/spark/deploy/SparkSubmitUtilsSuite.scala", "additions": "8", "deletions": "8", "changes": "16"}, "updated": [1, 1, 1]}, {"file": {"name": "core/src/test/scala/org/apache/spark/deploy/master/MasterSuite.scala", "additions": "11", "deletions": "3", "changes": "14"}, "updated": [1, 1, 4]}, {"file": {"name": "core/src/test/scala/org/apache/spark/rdd/LocalCheckpointSuite.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/TaskSetManagerSuite.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 1, 5]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/WorkerDecommissionExtendedSuite.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 1, 4]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/WorkerDecommissionSuite.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [1, 2, 4]}, {"file": {"name": "core/src/test/scala/org/apache/spark/storage/BlockManagerDecommissionIntegrationSuite.scala", "additions": "12", "deletions": "7", "changes": "19"}, "updated": [1, 3, 6]}, {"file": {"name": "core/src/test/scala/org/apache/spark/ui/ChromeUISeleniumSuite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 1]}, {"file": {"name": "core/src/test/scala/org/apache/spark/ui/RealBrowserUISeleniumSuite.scala", "additions": "54", "deletions": "2", "changes": "56"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/test/scala/org/apache/spark/ui/UISeleniumSuite.scala", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/test/scala/org/apache/spark/util/JsonProtocolSuite.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 4]}, {"file": {"name": "dev/create-release/release-tag.sh", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 1, 1]}, {"file": {"name": "dev/create-release/release-util.sh", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "dev/create-release/releaseutils.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 3]}, {"file": {"name": "dev/create-release/spark-rm/Dockerfile", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 1, 3]}, {"file": {"name": "dev/lint-python", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 1, 4]}, {"file": {"name": "dev/pip-sanity-check.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "dev/run-tests.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 1, 11]}, {"file": {"name": "dev/tox.ini", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 1, 4]}, {"file": {"name": "docs/_config.yml", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "docs/monitoring.md", "additions": "6", "deletions": "4", "changes": "10"}, "updated": [0, 2, 4]}, {"file": {"name": "docs/pyspark-migration-guide.md", "additions": "1", "deletions": "62", "changes": "63"}, "updated": [0, 1, 3]}, {"file": {"name": "docs/rdd-programming-guide.md", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "docs/running-on-kubernetes.md", "additions": "0", "deletions": "4", "changes": "4"}, "updated": [0, 1, 1]}, {"file": {"name": "docs/sql-ref-syntax-aux-cache-cache-table.md", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 3]}, {"file": {"name": "examples/src/main/python/sql/hive.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "examples/src/main/python/status_api_demo.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 1, 2]}, {"file": {"name": "external/docker-integration-tests/src/test/scala/org/apache/spark/sql/jdbc/PostgresIntegrationSuite.scala", "additions": "38", "deletions": "2", "changes": "40"}, "updated": [0, 2, 2]}, {"file": {"name": "external/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaTestUtils.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 2]}, {"file": {"name": "pom.xml", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 1, 9]}, {"file": {"name": "python/docs/source/migration_guide/index.rst", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 1, 2]}, {"file": {"name": "python/docs/source/migration_guide/pyspark_1.0_1.2_to_1.3.rst", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 1, 1]}, {"file": {"name": "python/docs/source/migration_guide/pyspark_1.4_to_1.5.rst", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [0, 1, 1]}, {"file": {"name": "python/docs/source/migration_guide/pyspark_2.2_to_2.3.rst", "additions": "30", "deletions": "0", "changes": "30"}, "updated": [0, 1, 1]}, {"file": {"name": "python/docs/source/migration_guide/pyspark_2.3.0_to_2.3.1_above.rst", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 1, 1]}, {"file": {"name": "python/docs/source/migration_guide/pyspark_2.3_to_2.4.rst", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 1, 1]}, {"file": {"name": "python/docs/source/migration_guide/pyspark_2.4_to_3.0.rst", "additions": "44", "deletions": "0", "changes": "44"}, "updated": [0, 1, 1]}, {"file": {"name": "python/docs/source/reference/pyspark.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/__init__.py", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/ml/__init__.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/ml/param/__init__.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/ml/pipeline.py", "additions": "0", "deletions": "2", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/ml/regression.py", "additions": "3", "deletions": "5", "changes": "8"}, "updated": [0, 2, 4]}, {"file": {"name": "python/pyspark/ml/stat.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 2, 2]}, {"file": {"name": "python/pyspark/ml/tests/test_algorithms.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/ml/tests/test_base.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/ml/tests/test_evaluation.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/ml/tests/test_feature.py", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/ml/tests/test_image.py", "additions": "2", "deletions": "4", "changes": "6"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/ml/tests/test_linalg.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/ml/tests/test_param.py", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 2, 3]}, {"file": {"name": "python/pyspark/ml/tests/test_persistence.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/ml/tests/test_pipeline.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/ml/tests/test_stat.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/ml/tests/test_training_summary.py", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 1, 6]}, {"file": {"name": "python/pyspark/ml/tests/test_tuning.py", "additions": "1", "deletions": "3", "changes": "4"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/ml/tests/test_wrapper.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/ml/util.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/mllib/classification.py", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/mllib/clustering.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/mllib/feature.py", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/mllib/regression.py", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/mllib/tests/test_algorithms.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/mllib/tests/test_feature.py", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/mllib/tests/test_linalg.py", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/mllib/tests/test_stat.py", "additions": "2", "deletions": "3", "changes": "5"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/mllib/tests/test_streaming_algorithms.py", "additions": "3", "deletions": "4", "changes": "7"}, "updated": [0, 2, 2]}, {"file": {"name": "python/pyspark/mllib/tests/test_util.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/rdd.py", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 1, 3]}, {"file": {"name": "python/pyspark/resource/tests/test_resources.py", "additions": "2", "deletions": "3", "changes": "5"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/sql/avro/functions.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/sql/context.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/sql/functions.py", "additions": "5", "deletions": "4", "changes": "9"}, "updated": [1, 3, 10]}, {"file": {"name": "python/pyspark/sql/pandas/functions.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/sql/readwriter.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 5]}, {"file": {"name": "python/pyspark/sql/session.py", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 1, 3]}, {"file": {"name": "python/pyspark/sql/streaming.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 4]}, {"file": {"name": "python/pyspark/sql/tests/test_arrow.py", "additions": "7", "deletions": "1", "changes": "8"}, "updated": [0, 2, 6]}, {"file": {"name": "python/pyspark/sql/tests/test_catalog.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_column.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/sql/tests/test_conf.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_context.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/sql/tests/test_dataframe.py", "additions": "0", "deletions": "2", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_datasources.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_functions.py", "additions": "2", "deletions": "3", "changes": "5"}, "updated": [0, 1, 3]}, {"file": {"name": "python/pyspark/sql/tests/test_group.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_pandas_cogrouped_map.py", "additions": "4", "deletions": "5", "changes": "9"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/sql/tests/test_pandas_grouped_map.py", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 1, 3]}, {"file": {"name": "python/pyspark/sql/tests/test_pandas_map.py", "additions": "1", "deletions": "3", "changes": "4"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/sql/tests/test_pandas_udf.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_pandas_udf_grouped_agg.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_pandas_udf_scalar.py", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 1, 3]}, {"file": {"name": "python/pyspark/sql/tests/test_pandas_udf_typehints.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/sql/tests/test_pandas_udf_window.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_readwriter.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/sql/tests/test_serde.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_session.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_streaming.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_types.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 2, 4]}, {"file": {"name": "python/pyspark/sql/tests/test_udf.py", "additions": "0", "deletions": "2", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_utils.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/types.py", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 1, 4]}, {"file": {"name": "python/pyspark/storagelevel.py", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/streaming/dstream.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/streaming/tests/test_context.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/streaming/tests/test_dstream.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/streaming/tests/test_kinesis.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/streaming/tests/test_listener.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/testing/utils.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/tests/test_appsubmit.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/tests/test_broadcast.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/tests/test_conf.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/tests/test_context.py", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 2, 4]}, {"file": {"name": "python/pyspark/tests/test_daemon.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/tests/test_join.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/tests/test_pin_thread.py", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/tests/test_profiler.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/tests/test_rdd.py", "additions": "2", "deletions": "3", "changes": "5"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/tests/test_rddbarrier.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/tests/test_readwrite.py", "additions": "1", "deletions": "3", "changes": "4"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/tests/test_serializers.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/tests/test_shuffle.py", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/tests/test_taskcontext.py", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/tests/test_util.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/tests/test_worker.py", "additions": "2", "deletions": "3", "changes": "5"}, "updated": [0, 1, 2]}, {"file": {"name": "resource-managers/kubernetes/docker/src/main/dockerfiles/spark/decom.sh", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala", "additions": "11", "deletions": "16", "changes": "27"}, "updated": [1, 1, 3]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/tests/decommissioning.py", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/SupportsAtomicPartitionManagement.java", "additions": "85", "deletions": "0", "changes": "85"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/SupportsPartitionManagement.java", "additions": "115", "deletions": "0", "changes": "115"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala-2.13/org/apache/spark/sql/catalyst/expressions/ExpressionSet.scala", "additions": "7", "deletions": "1", "changes": "8"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/AlreadyExistException.scala", "additions": "24", "deletions": "6", "changes": "30"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala", "additions": "14", "deletions": "6", "changes": "20"}, "updated": [2, 3, 14]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/NoSuchItemException.scala", "additions": "24", "deletions": "9", "changes": "33"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/UnsupportedOperationChecker.scala", "additions": "7", "deletions": "1", "changes": "8"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package.scala", "additions": "32", "deletions": "14", "changes": "46"}, "updated": [1, 2, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [1, 3, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/ApproxCountDistinctForIntervals.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/HyperLogLogPlusPlus.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/interfaces.scala", "additions": "8", "deletions": "2", "changes": "10"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala", "additions": "9", "deletions": "8", "changes": "17"}, "updated": [1, 1, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/complexTypeCreator.scala", "additions": "75", "deletions": "35", "changes": "110"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/higherOrderFunctions.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JacksonParser.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/ComplexTypes.scala", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "27", "deletions": "5", "changes": "32"}, "updated": [3, 3, 12]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/UpdateFields.scala", "additions": "8", "deletions": "8", "changes": "16"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [1, 1, 10]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/planning/patterns.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreeNode.scala", "additions": "14", "deletions": "9", "changes": "23"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "13", "deletions": "5", "changes": "18"}, "updated": [1, 6, 19]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/types/Metadata.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/util/SchemaUtils.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/RandomDataGenerator.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisSuite.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CastSuite.scala", "additions": "9", "deletions": "8", "changes": "17"}, "updated": [1, 3, 4]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/BinaryComparisonSimplificationSuite.scala", "additions": "1", "deletions": "3", "changes": "4"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/CombineUpdateFieldsSuite.scala", "additions": "19", "deletions": "22", "changes": "41"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/EliminateAggregateFilterSuite.scala", "additions": "75", "deletions": "0", "changes": "75"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/FilterPushdownSuite.scala", "additions": "21", "deletions": "36", "changes": "57"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/JoinReorderSuite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/PullupCorrelatedPredicatesSuite.scala", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ReplaceNullWithFalseInPredicateSuite.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/SimplifyCastsSuite.scala", "additions": "5", "deletions": "4", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/SimplifyConditionalSuite.scala", "additions": "11", "deletions": "12", "changes": "23"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/StarJoinReorderSuite.scala", "additions": "11", "deletions": "8", "changes": "19"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/complexTypesSuite.scala", "additions": "51", "deletions": "30", "changes": "81"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/InMemoryAtomicPartitionTable.scala", "additions": "76", "deletions": "0", "changes": "76"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/InMemoryPartitionTable.scala", "additions": "95", "deletions": "0", "changes": "95"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/SupportsAtomicPartitionManagementSuite.scala", "additions": "126", "deletions": "0", "changes": "126"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/SupportsPartitionManagementSuite.scala", "additions": "143", "deletions": "0", "changes": "143"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/util/SchemaUtilsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedPlainValuesReader.java", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedRleValuesReader.java", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OffHeapColumnVector.java", "additions": "26", "deletions": "0", "changes": "26"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OnHeapColumnVector.java", "additions": "24", "deletions": "0", "changes": "24"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/WritableColumnVector.java", "additions": "12", "deletions": "0", "changes": "12"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/Column.scala", "additions": "68", "deletions": "18", "changes": "86"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/BaseScriptTransformationExec.scala", "additions": "201", "deletions": "36", "changes": "237"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/Columnar.scala", "additions": "14", "deletions": "3", "changes": "17"}, "updated": [0, 2, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala", "additions": "4", "deletions": "6", "changes": "10"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 2, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/RemoveRedundantProjects.scala", "additions": "99", "deletions": "0", "changes": "99"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala", "additions": "8", "deletions": "4", "changes": "12"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "54", "deletions": "15", "changes": "69"}, "updated": [0, 3, 6]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/EliminateNullAwareAntiJoin.scala", "additions": "41", "deletions": "0", "changes": "41"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/QueryStageExec.scala", "additions": "5", "deletions": "3", "changes": "8"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala", "additions": "7", "deletions": "5", "changes": "12"}, "updated": [1, 2, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DaysWritable.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala", "additions": "8", "deletions": "3", "changes": "11"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFiltersBase.scala", "additions": "43", "deletions": "7", "changes": "50"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala", "additions": "1", "deletions": "3", "changes": "4"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/orc/OrcScanBuilder.scala", "additions": "3", "deletions": "4", "changes": "7"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/dynamicpruning/PlanDynamicPruningFilters.scala", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastHashJoinExec.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashJoin.scala", "additions": "30", "deletions": "12", "changes": "42"}, "updated": [0, 1, 6]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashedRelation.scala", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/joins/ShuffledHashJoinExec.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala", "additions": "42", "deletions": "6", "changes": "48"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/functions.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 6]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/jdbc/PostgresDialect.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/cast.sql", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/datetime.sql", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/interval.sql", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/datetime.sql.out", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "15", "deletions": "1", "changes": "16"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/cast.sql.out", "additions": "41", "deletions": "1", "changes": "42"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/datetime-legacy.sql.out", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/datetime.sql.out", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "209", "deletions": "172", "changes": "381"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "128", "deletions": "183", "changes": "311"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "15", "deletions": "1", "changes": "16"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/test_script.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/CTEHintSuite.scala", "additions": "168", "deletions": "0", "changes": "168"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/ColumnExpressionSuite.scala", "additions": "350", "deletions": "1", "changes": "351"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala", "additions": "0", "deletions": "12", "changes": "12"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/TPCDSQuerySuite.scala", "additions": "25", "deletions": "2", "changes": "27"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/TPCDSTableStats.scala", "additions": "503", "deletions": "0", "changes": "503"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/BaseScriptTransformationSuite.scala", "additions": "382", "deletions": "0", "changes": "382"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/ColumnarRulesSuite.scala", "additions": "63", "deletions": "0", "changes": "63"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/RemoveRedundantProjectsSuite.scala", "additions": "133", "deletions": "0", "changes": "133"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/SQLViewSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/TestUncaughtExceptionHandler.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "34", "deletions": "1", "changes": "35"}, "updated": [0, 3, 7]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/FileBasedDataSourceTest.scala", "additions": "39", "deletions": "1", "changes": "40"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategySuite.scala", "additions": "30", "deletions": "0", "changes": "30"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcTest.scala", "additions": "22", "deletions": "0", "changes": "22"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala", "additions": "1", "deletions": "28", "changes": "29"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/joins/HashedRelationSuite.scala", "additions": "12", "deletions": "0", "changes": "12"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/vectorized/ColumnarBatchSuite.scala", "additions": "102", "deletions": "14", "changes": "116"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/streaming/FileStreamSourceSuite.scala", "additions": "149", "deletions": "10", "changes": "159"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingQuerySuite.scala", "additions": "50", "deletions": "1", "changes": "51"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/v1.2/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFilters.scala", "additions": "12", "deletions": "16", "changes": "28"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/v1.2/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcFilterSuite.scala", "additions": "160", "deletions": "116", "changes": "276"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/v2.3/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFilters.scala", "additions": "12", "deletions": "16", "changes": "28"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/v2.3/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcFilterSuite.scala", "additions": "163", "deletions": "119", "changes": "282"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveInspectors.scala", "additions": "5", "deletions": "2", "changes": "7"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveStrategies.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala", "additions": "28", "deletions": "5", "changes": "33"}, "updated": [1, 3, 4]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/HiveScriptTransformationExec.scala", "additions": "98", "deletions": "149", "changes": "247"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/InsertIntoHiveTable.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/HiveSQLViewSuite.scala", "additions": "21", "deletions": "1", "changes": "22"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/HiveScriptTransformationSuite.scala", "additions": "183", "deletions": "152", "changes": "335"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/HiveSerDeReadWriteSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [2, 2, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/SQLQuerySuite.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [1, 1, 3]}, {"file": {"name": "streaming/src/main/scala/org/apache/spark/streaming/scheduler/ExecutorAllocationManager.scala", "additions": "9", "deletions": "1", "changes": "10"}, "updated": [1, 1, 1]}, {"file": {"name": "streaming/src/test/scala/org/apache/spark/streaming/scheduler/ExecutorAllocationManagerSuite.scala", "additions": "39", "deletions": "12", "changes": "51"}, "updated": [1, 1, 1]}]}
{"author": "mingjialiu", "sha": "", "commit_date": "2020/09/10 03:39:34", "commit_message": "Revert \"[Spark 32708] Query optimization fails to reuse exchange with DataSourceV2\"\n\nThis reverts commit dd0fb242277184abda5c6a4cb03bdec4e930e736.", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Relation.scala", "additions": "2", "deletions": "3", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2StringFormat.scala", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 0, 0]}]}
{"author": "bowenli86", "sha": "", "commit_date": "2020/09/12 22:46:06", "commit_message": "[SPARK-32865][DOC] python section in quickstart page doesn't display SPARK_VERSION correctly", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "docs/quick-start.md", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "Ted-Jiang", "sha": "", "commit_date": "2020/08/26 01:46:10", "commit_message": "[SPARK-32620][SQL] Reset the numPartitions metric when DPP is enabled\n\n### What changes were proposed in this pull request?\n\nThis pr reset the `numPartitions` metric when DPP is enabled. Otherwise, it is always a [static value](https://github.com/apache/spark/blob/18cac6a9f0bf4a6d449393f1ee84004623b3c893/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala#L215).\n\n### Why are the changes needed?\n\nFix metric issue.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nUnit test and manual test\n\nFor [this test case](https://github.com/apache/spark/blob/18cac6a9f0bf4a6d449393f1ee84004623b3c893/sql/core/src/test/scala/org/apache/spark/sql/DynamicPartitionPruningSuite.scala#L252-L280).\n\nBefore this pr:\n![image](https://user-images.githubusercontent.com/5399861/90301798-9310b480-ded4-11ea-9294-49bcaba46f83.png)\n\nAfter this pr:\n![image](https://user-images.githubusercontent.com/5399861/90301709-0fef5e80-ded4-11ea-942d-4d45d1dd15bc.png)\n\nCloses #29436 from wangyum/SPARK-32620.\n\nAuthored-by: Yuming Wang <yumwang@ebay.com>\nSigned-off-by: Yuming Wang <wgyumg@gmail.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DynamicPartitionPruningSuite.scala", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [1, 1, 3]}]}
{"author": "erenavsarogullari", "sha": "", "commit_date": "2020/09/03 15:49:17", "commit_message": "[SPARK-32788][SQL] non-partitioned table scan should not have partition filter\n\n### What changes were proposed in this pull request?\n\nThis PR fixes a bug `FileSourceStrategy`, which generates partition filters even if the table is not partitioned. This can confuse `FileSourceScanExec`, which mistakenly think the table is partitioned and tries to update the `numPartitions` metrics, and cause a failure. We should not generate partition filters for non-partitioned table.\n\n### Why are the changes needed?\n\nThe bug was exposed by https://github.com/apache/spark/pull/29436.\n\n### Does this PR introduce _any_ user-facing change?\n\nYes, fix a bug.\n\n### How was this patch tested?\n\nnew test\n\nCloses #29637 from cloud-fan/refactor.\n\nAuthored-by: Wenchen Fan <wenchen@databricks.com>\nSigned-off-by: Yuming Wang <yumwang@ebay.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala", "additions": "3", "deletions": "4", "changes": "7"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala", "additions": "7", "deletions": "3", "changes": "10"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [2, 2, 6]}]}
{"author": "javierivanov", "sha": "", "commit_date": "2021/09/05 13:23:05", "commit_message": "[SPARK-36613][SQL][SS] Use EnumSet as the implementation of Table.capabilities method return value\n\n### What changes were proposed in this pull request?\nThe `Table.capabilities` method return a `java.util.Set` of `TableCapability` enumeration type, which is implemented using `java.util.HashSet` now. Such Set can be replaced `with java.util.EnumSet` because `EnumSet` implementations can be much more efficient compared to other sets.\n\n### Why are the changes needed?\nUse more appropriate data structures.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\n\n- Pass GA or Jenkins Tests.\n- Add a new benchmark to compare `create` and `contains` operation between `EnumSet` and `HashSet`\n\nCloses #33867 from LuciferYang/SPARK-36613.\n\nAuthored-by: yangjie01 <yangjie01@baidu.com>\nSigned-off-by: Sean Owen <srowen@gmail.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaSourceProvider.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/catalyst/benchmarks/EnumTypeSetBenchmark-jdk11-results.txt", "additions": "104", "deletions": "0", "changes": "104"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/benchmarks/EnumTypeSetBenchmark-results.txt", "additions": "104", "deletions": "0", "changes": "104"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/V1Table.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/CreateTablePartitioningValidationSuite.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/EnumTypeSetBenchmark.scala", "additions": "176", "deletions": "0", "changes": "176"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryTable.scala", "additions": "2", "deletions": "3", "changes": "5"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/noop/NoopDataSource.scala", "additions": "2", "deletions": "4", "changes": "6"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/FileTable.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCTable.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/console.scala", "additions": "1", "deletions": "3", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/memory.scala", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/ForeachWriterTable.scala", "additions": "1", "deletions": "3", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/RateStreamProvider.scala", "additions": "1", "deletions": "3", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/TextSocketSourceProvider.scala", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/memory.scala", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/java/test/org/apache/spark/sql/connector/JavaSimpleBatchTable.java", "additions": "2", "deletions": "3", "changes": "5"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/java/test/org/apache/spark/sql/connector/JavaSimpleWritableDataSource.java", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2Suite.scala", "additions": "1", "deletions": "3", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/FileDataSourceV2FallBackSuite.scala", "additions": "2", "deletions": "3", "changes": "5"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/LocalScanSuite.scala", "additions": "2", "deletions": "3", "changes": "5"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/SimpleWritableDataSource.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/TableCapabilityCheckSuite.scala", "additions": "5", "deletions": "3", "changes": "8"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/V1ReadFallbackSuite.scala", "additions": "1", "deletions": "3", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/V1WriteFallbackSuite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/streaming/sources/StreamingDataSourceV2Suite.scala", "additions": "8", "deletions": "9", "changes": "17"}, "updated": [2, 2, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/streaming/test/DataStreamTableAPISuite.scala", "additions": "3", "deletions": "4", "changes": "7"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/streaming/util/BlockOnStopSource.scala", "additions": "1", "deletions": "3", "changes": "4"}, "updated": [1, 1, 1]}]}
{"author": "Louiszr", "sha": "", "commit_date": "2020/08/16 13:43:23", "commit_message": "Merge remote-tracking branch 'upstream/master'", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/CodeGeneratorWithInterpretedFallback.scala", "additions": "2", "deletions": "3", "changes": "5"}, "updated": [1, 1, 1]}]}
{"author": "unirt", "sha": "", "commit_date": "2020/08/26 18:24:35", "commit_message": "Revert \"[SPARK-32481][CORE][SQL] Support truncate table to move data to trash\"\n\nThis reverts commit 5c077f05805bda1d0db3476ebe32624034d4066c.", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/util/Utils.scala", "additions": "1", "deletions": "22", "changes": "23"}, "updated": [2, 2, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "0", "deletions": "13", "changes": "13"}, "updated": [3, 5, 21]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala", "additions": "1", "deletions": "3", "changes": "4"}, "updated": [2, 2, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/DDLSuite.scala", "additions": "0", "deletions": "72", "changes": "72"}, "updated": [2, 3, 4]}]}
{"author": "titsuki", "sha": "", "commit_date": "2020/11/11 15:13:17", "commit_message": "[SPARK-33415][PYTHON][SQL] Don't encode JVM response in Column.__repr__\n\n### What changes were proposed in this pull request?\n\nRemoves encoding of the JVM response in `pyspark.sql.column.Column.__repr__`.\n\n### Why are the changes needed?\n\nAPI consistency and improved readability of the expressions.\n\n### Does this PR introduce _any_ user-facing change?\n\nBefore this change\n\n    col(\"abc\")\n    col(\"w\u0105\u017c\")\n\nresult in\n\n    Column<b'abc'>\n    Column<b'w\\xc4\\x85\\xc5\\xbc'>\n\nAfter this change we'll get\n\n    Column<'abc'>\n    Column<'w\u0105\u017c'>\n\n### How was this patch tested?\n\nExisting tests and manual inspection.\n\nCloses #30322 from zero323/SPARK-33415.\n\nAuthored-by: zero323 <mszymkiewicz@gmail.com>\nSigned-off-by: HyukjinKwon <gurwls223@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/sql/column.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 2, 3]}, {"file": {"name": "python/pyspark/sql/tests/test_column.py", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [1, 1, 3]}]}
{"author": "ptkool", "sha": "", "commit_date": "2020/09/18 13:24:33", "commit_message": "[SPARK-32936][SQL] Pass all `external/avro` module UTs in Scala 2.13\n\n### What changes were proposed in this pull request?\nThis pr fix all 14 failed cases in `external/avro` module in Scala 2.13, the main change of this pr as follow:\n\n- Manual call `toSeq` in `AvroDeserializer#newWriter` and `SchemaConverters#toSqlTypeHelper` method because the object  type for case match is `ArrayBuffer` not `Seq` in Scala 2.13\n\n- Specified `Seq` to `s.c.Seq` when we call `Row.get(i).asInstanceOf[Seq]` because the data maybe `mutable.ArraySeq` but `Seq` is `immutable.Seq` in Scala 2.13\n\n### Why are the changes needed?\nWe need to support a Scala 2.13 build.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\n\n- Scala 2.12: Pass the Jenkins or GitHub Action\n\n- Scala 2.13: Pass 2.13 Build GitHub Action and do the following:\n\n```\ndev/change-scala-version.sh 2.13\nmvn clean install -DskipTests  -pl external/avro -Pscala-2.13 -am\nmvn clean test -pl external/avro -Pscala-2.13\n```\n\n**Before**\n```\nTests: succeeded 197, failed 14, canceled 0, ignored 2, pending 0\n*** 14 TESTS FAILED ***\n```\n\n**After**\n\n```\nTests: succeeded 211, failed 0, canceled 0, ignored 2, pending 0\nAll tests passed.\n```\n\nCloses #29801 from LuciferYang/fix-external-avro-213.\n\nAuthored-by: yangjie01 <yangjie01@baidu.com>\nSigned-off-by: HyukjinKwon <gurwls223@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "external/avro/src/main/scala/org/apache/spark/sql/avro/AvroDeserializer.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 3]}, {"file": {"name": "external/avro/src/main/scala/org/apache/spark/sql/avro/SchemaConverters.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 2]}, {"file": {"name": "external/avro/src/test/scala/org/apache/spark/sql/avro/AvroSuite.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 1, 3]}]}
{"author": "fuwhu", "sha": "", "commit_date": "2020/09/06 11:23:12", "commit_message": "[SPARK-32548][SQL] - Add Application attemptId support to SQL Rest API\n\n### What changes were proposed in this pull request?\nCurrently, Spark Public Rest APIs support Application attemptId except SQL API. This causes `no such app: application_X` issue when the application has `attemptId` (e.g: YARN cluster mode).\n\nPlease find existing and supported Rest endpoints with attemptId.\n```\n// Existing Rest Endpoints\napplications/{appId}/sql\napplications/{appId}/sql/{executionId}\n\n// Rest Endpoints required support\napplications/{appId}/{attemptId}/sql\napplications/{appId}/{attemptId}/sql/{executionId}\n```\nAlso fixing following compile warning on `SqlResourceSuite`:\n```\n[WARNING] [Warn] ~/spark/sql/core/src/test/scala/org/apache/spark/status/api/v1/sql/SqlResourceSuite.scala:67: Reference to uninitialized value edges\n```\n### Why are the changes needed?\nThis causes `no such app: application_X` issue when the application has `attemptId`.\n\n### Does this PR introduce _any_ user-facing change?\nNot yet because SQL Rest API is being planned to release with `Spark 3.1`.\n\n### How was this patch tested?\n1. New Unit tests are added for existing Rest endpoints. `attemptId` seems not coming in `local-mode` and coming in `YARN cluster mode` so could not be added for `attemptId` case (Suggestions are welcome).\n2. Also, patch has been tested manually through both Spark Core and History Server Rest APIs.\n\nCloses #29364 from erenavsarogullari/SPARK-32548.\n\nAuthored-by: Eren Avsarogullari <erenavsarogullari@gmail.com>\nSigned-off-by: Gengliang Wang <gengliang.wang@databricks.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/status/api/v1/sql/ApiSqlRootResource.scala", "additions": "7", "deletions": "2", "changes": "9"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/status/api/v1/sql/SqlResourceSuite.scala", "additions": "2", "deletions": "3", "changes": "5"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/status/api/v1/sql/SqlResourceWithActualMetricsSuite.scala", "additions": "127", "deletions": "0", "changes": "127"}, "updated": [1, 1, 1]}]}
{"author": "William1104", "sha": "", "commit_date": "2021/04/20 03:11:40", "commit_message": "[SPARK-35080][SQL] Only allow a subset of correlated equality predicates when a subquery is aggregated\n\nThis PR updated the `foundNonEqualCorrelatedPred` logic for correlated subqueries in `CheckAnalysis` to only allow correlated equality predicates that guarantee one-to-one mapping between inner and outer attributes, instead of all equality predicates.\n\nTo fix correctness bugs. Before this fix Spark can give wrong results for certain correlated subqueries that pass CheckAnalysis:\nExample 1:\n```sql\ncreate or replace view t1(c) as values ('a'), ('b')\ncreate or replace view t2(c) as values ('ab'), ('abc'), ('bc')\n\nselect c, (select count(*) from t2 where t1.c = substring(t2.c, 1, 1)) from t1\n```\nCorrect results: [(a, 2), (b, 1)]\nSpark results:\n```\n+---+-----------------+\n|c  |scalarsubquery(c)|\n+---+-----------------+\n|a  |1                |\n|a  |1                |\n|b  |1                |\n+---+-----------------+\n```\nExample 2:\n```sql\ncreate or replace view t1(a, b) as values (0, 6), (1, 5), (2, 4), (3, 3);\ncreate or replace view t2(c) as values (6);\n\nselect c, (select count(*) from t1 where a + b = c) from t2;\n```\nCorrect results: [(6, 4)]\nSpark results:\n```\n+---+-----------------+\n|c  |scalarsubquery(c)|\n+---+-----------------+\n|6  |1                |\n|6  |1                |\n|6  |1                |\n|6  |1                |\n+---+-----------------+\n```\nYes. Users will not be able to run queries that contain unsupported correlated equality predicates.\n\nAdded unit tests.\n\nCloses #32179 from allisonwang-db/spark-35080-subquery-bug.\n\nLead-authored-by: allisonwang-db <66282705+allisonwang-db@users.noreply.github.com>\nCo-authored-by: Wenchen Fan <cloud0fan@gmail.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit bad4b6f025de4946112a0897892a97d5ae6822cf)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala", "additions": "65", "deletions": "12", "changes": "77"}, "updated": [2, 4, 9]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala", "additions": "24", "deletions": "0", "changes": "24"}, "updated": [1, 3, 4]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/udf/udf-except.sql.out", "additions": "10", "deletions": "2", "changes": "12"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala", "additions": "10", "deletions": "1", "changes": "11"}, "updated": [1, 2, 4]}]}
{"author": "nikunjb", "sha": "", "commit_date": "2020/08/24 05:06:08", "commit_message": "[SPARK-32352][SQL][FOLLOW-UP][TEST-HADOOP2.7][TEST-HIVE1.2] Exclude partition columns from data columns\n\n### What changes were proposed in this pull request?\n\nThis PR fixes a bug of #29406. #29406 partially pushes down data filter even if it mixed in partition filters. But in some cases partition columns might be in data columns too. It will possibly push down a predicate with partition column to datasource.\n\n### Why are the changes needed?\n\nThe test \"org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite.save()/load() - partitioned table - simple queries - partition columns in data\" is currently failed with hive-1.2 profile in master branch.\n\n```\n[info] - save()/load() - partitioned table - simple queries - partition columns in data *** FAILED *** (1 second, 457 milliseconds)\n[info]   java.util.NoSuchElementException: key not found: p1\n[info]   at scala.collection.immutable.Map$Map2.apply(Map.scala:138)\n[info]   at org.apache.spark.sql.hive.orc.OrcFilters$.buildLeafSearchArgument(OrcFilters.scala:250)\n[info]   at org.apache.spark.sql.hive.orc.OrcFilters$.convertibleFiltersHelper$1(OrcFilters.scala:143)\n[info]   at org.apache.spark.sql.hive.orc.OrcFilters$.$anonfun$convertibleFilters$4(OrcFilters.scala:146)\n[info]   at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)\n[info]   at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n[info]   at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n[info]   at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n[info]   at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)\n[info]   at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)\n[info]   at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)\n[info]   at org.apache.spark.sql.hive.orc.OrcFilters$.convertibleFilters(OrcFilters.scala:145)\n[info]   at org.apache.spark.sql.hive.orc.OrcFilters$.createFilter(OrcFilters.scala:83)\n[info]   at org.apache.spark.sql.hive.orc.OrcFileFormat.buildReader(OrcFileFormat.scala:142)\n```\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nUnit test.\n\nCloses #29526 from viirya/SPARK-32352-followup.\n\nAuthored-by: Liang-Chi Hsieh <viirya@gmail.com>\nSigned-off-by: Takeshi Yamamuro <yamamuro@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [1, 2, 2]}]}
{"author": "colinmjj", "sha": "", "commit_date": "2020/09/24 03:10:01", "commit_message": "[SPARK-32971][K8S][FOLLOWUP] Add `.toSeq` for Scala 2.13 compilation\n\n### What changes were proposed in this pull request?\n\nThis is a follow-up to fix Scala 2.13 compilation at Kubernetes module.\n\n### Why are the changes needed?\n\nTo fix Scala 2.13 compilation.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nPass the GitHub Action Scala 2.13 compilation job.\n\nCloses #29859 from dongjoon-hyun/SPARK-32971-2.\n\nAuthored-by: Dongjoon Hyun <dhyun@apple.com>\nSigned-off-by: Dongjoon Hyun <dhyun@apple.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/MountVolumesFeatureStep.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [2, 2, 3]}]}
{"author": "wangmiao1981", "sha": "", "commit_date": "2021/02/13 15:32:55", "commit_message": "[SPARK-33434][PYTHON][DOCS] Added RuntimeConfig to PySpark docs\n\n### What changes were proposed in this pull request?\nDocumentation for `SparkSession.conf.isModifiable` is missing from the Python API site, so we added a Configuration section to the Spark SQL page to expose docs for the `RuntimeConfig` class (the class containing `isModifiable`). Then a `:class:` reference to `RuntimeConfig` was added to the `SparkSession.conf` docstring to create a link there as well.\n\n### Why are the changes needed?\nNo docs were generated for `pyspark.sql.conf.RuntimeConfig`.\n\n### Does this PR introduce _any_ user-facing change?\nYes--a new Configuration section to the Spark SQL page and a `Returns` section of the `SparkSession.conf` docstring, so this will now show a link to the `pyspark.sql.conf.RuntimeConfig` page. This is a change compared to both the released Spark version and the unreleased master branch.\n\n### How was this patch tested?\nFirst built the Python docs:\n```bash\ncd $SPARK_HOME/docs\nSKIP_SCALADOC=1 SKIP_RDOC=1 SKIP_SQLDOC=1 jekyll serve\n```\nThen verified all pages and links:\n1. Configuration link displayed on the API Reference page, and it clicks through to Spark SQL page:\nhttp://localhost:4000/api/python/reference/index.html\n![image](https://user-images.githubusercontent.com/1160861/107601918-a2f02380-6bed-11eb-9b8f-974a0681a2a9.png)\n\n2. Configuration section displayed on the Spark SQL page, and the RuntimeConfig link clicks through to the RuntimeConfig page:\nhttp://localhost:4000/api/python/reference/pyspark.sql.html#configuration\n![image](https://user-images.githubusercontent.com/1160861/107602058-0d08c880-6bee-11eb-8cbb-ad8c47588085.png)**\n\n3. RuntimeConfig page displayed:\nhttp://localhost:4000/api/python/reference/api/pyspark.sql.conf.RuntimeConfig.html\n![image](https://user-images.githubusercontent.com/1160861/107602278-94eed280-6bee-11eb-95fc-445ea62ac1a4.png)\n\n4. SparkSession.conf page displays the RuntimeConfig link, and it navigates to the RuntimeConfig page:\nhttp://localhost:4000/api/python/reference/api/pyspark.sql.SparkSession.conf.html\n![image](https://user-images.githubusercontent.com/1160861/107602435-1f373680-6bef-11eb-985a-b72432464940.png)\n\nCloses #31483 from Eric-Lemmon/SPARK-33434-document-isModifiable.\n\nAuthored-by: Eric Lemmon <eric@lemmon.cc>\nSigned-off-by: Sean Owen <srowen@gmail.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/docs/source/reference/pyspark.sql.rst", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [1, 2, 2]}, {"file": {"name": "python/pyspark/sql/session.py", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [1, 1, 2]}]}
{"author": "sujithjay", "sha": "", "commit_date": "2020/10/14 03:13:54", "commit_message": "[SPARK-33134][SQL] Return partial results only for root JSON objects\n\n### What changes were proposed in this pull request?\nIn the PR, I propose to restrict the partial result feature only by root JSON objects. JSON datasource as well as `from_json()` will return `null` for malformed nested JSON objects.\n\n### Why are the changes needed?\n1. To not raise exception to users in the PERMISSIVE mode\n2. To fix a regression and to have the same behavior as Spark 2.4.x has\n3. Current implementation of partial result is supposed to work only for root (top-level) JSON objects, and not tested for bad nested complex JSON fields.\n\n### Does this PR introduce _any_ user-facing change?\nYes. Before the changes, the code below:\n```scala\n    val pokerhand_raw = Seq(\"\"\"[{\"cards\": [19], \"playerId\": 123456}]\"\"\").toDF(\"events\")\n    val event = new StructType().add(\"playerId\", LongType).add(\"cards\", ArrayType(new StructType().add(\"id\", LongType).add(\"rank\", StringType)))\n    val pokerhand_events = pokerhand_raw.select(from_json($\"events\", ArrayType(event)).as(\"event\"))\n    pokerhand_events.show\n```\nthrows the exception even in the default **PERMISSIVE** mode:\n```java\njava.lang.ClassCastException: java.lang.Long cannot be cast to org.apache.spark.sql.catalyst.util.ArrayData\n  at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow.getArray(rows.scala:48)\n  at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow.getArray$(rows.scala:48)\n  at org.apache.spark.sql.catalyst.expressions.GenericInternalRow.getArray(rows.scala:195)\n```\n\nAfter the changes:\n```\n+-----+\n|event|\n+-----+\n| null|\n+-----+\n```\n\n### How was this patch tested?\nAdded a test to `JsonFunctionsSuite`.\n\nCloses #30031 from MaxGekk/json-skip-row-wrong-schema.\n\nAuthored-by: Max Gekk <max.gekk@gmail.com>\nSigned-off-by: HyukjinKwon <gurwls223@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JacksonParser.scala", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/JsonFunctionsSuite.scala", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [1, 1, 1]}]}
{"author": "heuermh", "sha": "", "commit_date": "2021/01/06 17:28:22", "commit_message": "[SPARK-34022][DOCS][FOLLOW-UP] Fix typo in SQL built-in function docs\n\n### What changes were proposed in this pull request?\n\nThis PR is a follow-up of #31061. It fixes a typo in a document: `Finctions` -> `Functions`\n\n### Why are the changes needed?\n\nMake the change better documented.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nN/A\n\nCloses #31069 from kiszk/SPARK-34022-followup.\n\nAuthored-by: Kazuaki Ishizaki <ishizaki@jp.ibm.com>\nSigned-off-by: Dongjoon Hyun <dhyun@apple.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/gen-sql-api-docs.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [2, 2, 2]}]}
{"author": "rongou", "sha": "", "commit_date": "2020/11/19 21:36:45", "commit_message": "[MINOR] Structured Streaming statistics page indent fix\n\n### What changes were proposed in this pull request?\nStructured Streaming statistics page code contains an indentation issue. This PR fixes it.\n\n### Why are the changes needed?\nIndent fix.\n\n### Does this PR introduce _any_ user-facing change?\nNo.\n\n### How was this patch tested?\nExisting unit tests.\n\nCloses #30434 from gaborgsomogyi/STAT-INDENT-FIX.\n\nAuthored-by: Gabor Somogyi <gabor.g.somogyi@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/streaming/ui/StreamingQueryStatisticsPage.scala", "additions": "27", "deletions": "27", "changes": "54"}, "updated": [1, 2, 2]}]}
{"author": "techaddict", "sha": "", "commit_date": "2020/10/05 16:30:27", "commit_message": "[SPARK-33038][SQL] Combine AQE initial and current plan string when two plans are the same\n\n### What changes were proposed in this pull request?\nThis PR combines the current plan and the initial plan in the AQE query plan string when the two plans are the same. It also removes the `== Current Plan ==` and `== Initial Plan ==` headers:\n\nBefore\n```scala\nAdaptiveSparkPlan isFinalPlan=false\n+- == Current Plan ==\n   SortMergeJoin [key#13], [a#23], Inner\n   :- Sort [key#13 ASC NULLS FIRST], false, 0\n   :  +- Exchange hashpartitioning(key#13, 5), true, [id=#94]\n            ...\n+- == Initial Plan ==\n   SortMergeJoin [key#13], [a#23], Inner\n   :- Sort [key#13 ASC NULLS FIRST], false, 0\n   :  +- Exchange hashpartitioning(key#13, 5), true, [id=#94]\n            ...\n```\nAfter\n```scala\nAdaptiveSparkPlan isFinalPlan=false\n+- SortMergeJoin [key#13], [a#23], Inner\n   :- Sort [key#13 ASC NULLS FIRST], false, 0\n   :  +- Exchange hashpartitioning(key#13, 5), true, [id=#94]\n            ...\n```\nFor SQL `EXPLAIN` output:\nBefore\n```scala\nAdaptiveSparkPlan (8)\n+- == Current Plan ==\n   Sort (7)\n   +- Exchange (6)\n      ...\n+- == Initial Plan ==\n   Sort (7)\n   +- Exchange (6)\n      ...\n```\nAfter\n```scala\nAdaptiveSparkPlan (8)\n+- Sort (7)\n   +- Exchange (6)\n      ...\n```\n\n### Why are the changes needed?\nTo simplify the AQE plan string by removing the redundant plan information.\n\n### Does this PR introduce _any_ user-facing change?\nYes.\n\n### How was this patch tested?\nModified the existing unit test.\n\nCloses #29915 from allisonwang-db/aqe-explain.\n\nAuthored-by: allisonwang-db <66282705+allisonwang-db@users.noreply.github.com>\nSigned-off-by: Xiao Li <gatorsmile@gmail.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "32", "deletions": "18", "changes": "50"}, "updated": [1, 1, 6]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "13", "deletions": "110", "changes": "123"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 1, 8]}]}
{"author": "tcondie", "sha": "", "commit_date": "2020/09/01 13:04:24", "commit_message": "[SPARK-32761][SQL] Allow aggregating multiple foldable distinct expressions\n\n### What changes were proposed in this pull request?\nFor queries with multiple foldable distinct columns, since they will be eliminated during\nexecution, it's not mandatory to let `RewriteDistinctAggregates` handle this case. And\nin the current code, `RewriteDistinctAggregates` *dose* miss some \"aggregating with\nmultiple foldable distinct expressions\" cases.\nFor example: `select count(distinct 2), count(distinct 2, 3)` will be missed.\n\nBut in the planner, this will trigger an error that \"multiple distinct expressions\" are not allowed.\nAs the foldable distinct columns can be eliminated finally, we can allow this in the aggregation\nplanner check.\n\n### Why are the changes needed?\nbug fix\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nadded test case\n\nCloses #29607 from linhongliu-db/SPARK-32761.\n\nAuthored-by: Linhong Liu <linhong.liu@databricks.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [1, 3, 9]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [1, 2, 5]}]}
{"author": "frreiss", "sha": "", "commit_date": "2020/11/04 16:35:10", "commit_message": "[SPARK-33338][SQL] GROUP BY using literal map should not fail\n\n### What changes were proposed in this pull request?\n\nThis PR aims to fix `semanticEquals` works correctly on `GetMapValue` expressions having literal maps with `ArrayBasedMapData` and `GenericArrayData`.\n\n### Why are the changes needed?\n\nThis is a regression from Apache Spark 1.6.x.\n```scala\nscala> sc.version\nres1: String = 1.6.3\n\nscala> sqlContext.sql(\"SELECT map('k1', 'v1')[k] FROM t GROUP BY map('k1', 'v1')[k]\").show\n+---+\n|_c0|\n+---+\n| v1|\n+---+\n```\n\nApache Spark 2.x ~ 3.0.1 raise`RuntimeException` for the following queries.\n```sql\nCREATE TABLE t USING ORC AS SELECT map('k1', 'v1') m, 'k1' k\nSELECT map('k1', 'v1')[k] FROM t GROUP BY 1\nSELECT map('k1', 'v1')[k] FROM t GROUP BY map('k1', 'v1')[k]\nSELECT map('k1', 'v1')[k] a FROM t GROUP BY a\n```\n\n**BEFORE**\n```scala\nCaused by: java.lang.RuntimeException: Couldn't find k#3 in [keys: [k1], values: [v1][k#3]#6]\n\tat scala.sys.package$.error(package.scala:27)\n\tat org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1$$anonfun$applyOrElse$1.apply(BoundAttribute.scala:85)\n\tat org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1$$anonfun$applyOrElse$1.apply(BoundAttribute.scala:79)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n```\n\n**AFTER**\n```sql\nspark-sql> SELECT map('k1', 'v1')[k] FROM t GROUP BY 1;\nv1\nTime taken: 1.278 seconds, Fetched 1 row(s)\nspark-sql> SELECT map('k1', 'v1')[k] FROM t GROUP BY map('k1', 'v1')[k];\nv1\nTime taken: 0.313 seconds, Fetched 1 row(s)\nspark-sql> SELECT map('k1', 'v1')[k] a FROM t GROUP BY a;\nv1\nTime taken: 0.265 seconds, Fetched 1 row(s)\n```\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nPass the CIs with the newly added test case.\n\nCloses #30246 from dongjoon-hyun/SPARK-33338.\n\nAuthored-by: Dongjoon Hyun <dhyun@apple.com>\nSigned-off-by: Dongjoon Hyun <dhyun@apple.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/literals.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 2, 2]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ComplexTypeSuite.scala", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala", "additions": "12", "deletions": "0", "changes": "12"}, "updated": [1, 2, 3]}]}
{"author": "wesleydias", "sha": "", "commit_date": "2020/10/26 12:41:56", "commit_message": "[SPARK-33204][UI] The 'Event Timeline' area cannot be opened when a spark application has some failed jobs\n\n### What changes were proposed in this pull request?\nThe page returned by /jobs in Spark UI will  store the detail information of each job in javascript like this:\n```javascript\n{\n  'className': 'executor added',\n  'group': 'executors',\n  'start': new Date(1602834008978),\n  'content': '<div class=\"executor-event-content\"' +\n    'data-toggle=\"tooltip\" data-placement=\"top\"' +\n    'data-title=\"Executor 3<br>' +\n    'Added at 2020/10/16 15:40:08\"' +\n    'data-html=\"true\">Executor 3 added</div>'\n}\n```\nif an application has a failed job, the failure reason corresponding to the job will be stored in the ` content`  field in the javascript . if the failure  reason contains the character: **'**,   the  javascript code will throw an exception to cause the `event timeline url` had no response \uff0c The following is an example of error json:\n```javascript\n{\n  'className': 'executor removed',\n  'group': 'executors',\n  'start': new Date(1602925908654),\n  'content': '<div class=\"executor-event-content\"' +\n    'data-toggle=\"tooltip\" data-placement=\"top\"' +\n    'data-title=\"Executor 2<br>' +\n    'Removed at 2020/10/17 17:11:48' +\n    '<br>Reason: Container from a bad node: ...   20/10/17 16:00:42 WARN ShutdownHookManager: ShutdownHook **'$anon$2'** timeout...\"' +\n    'data-html=\"true\">Executor 2 removed</div>'\n}\n```\n\nSo we need to considier this special case , if the returned job info contains the character:**'**, just remove it\n\n### Why are the changes needed?\n\nEnsure that the UI page can function normally\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nThis pr only  fixes an exception in a special case, manual test result as blows:\n\n![fixed](https://user-images.githubusercontent.com/52202080/96711638-74490580-13d0-11eb-93e0-b44d9ed5da5c.gif)\n\nCloses #30119 from akiyamaneko/timeline_view_cannot_open.\n\nAuthored-by: neko <echohlne@gmail.com>\nSigned-off-by: Gengliang Wang <gengliang.wang@databricks.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 1, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 1, 1]}]}
{"author": "huaxingao", "sha": "c92ea97c02a4f008446cf88481af9359a86f510d", "commit_date": "2021/08/04 15:50:20", "commit_message": "[SPARK-34952][SQL] Aggregate (Min/Max/Count) push down for Parquet", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.HiveParquetSourceSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [1, 3, 17]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetUtils.scala", "additions": "219", "deletions": "0", "changes": "219"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetPartitionReaderFactory.scala", "additions": "101", "deletions": "22", "changes": "123"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetScan.scala", "additions": "31", "deletions": "3", "changes": "34"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetScanBuilder.scala", "additions": "94", "deletions": "7", "changes": "101"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/FileScanSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetAggregatePushDownSuite.scala", "additions": "520", "deletions": "0", "changes": "520"}, "updated": [0, 0, 0]}]}
{"author": "huaxingao", "sha": "1965f773825e4f6cd14b71d7f3d32550daaad4c5", "commit_date": "2021/09/19 22:36:37", "commit_message": "Migrate CreateTableStatement to v2 command framework", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.DSV2SQLInsertTestSuite", "org.apache.spark.sql.connector.DataSourceV2SQLSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 8]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveCatalogs.scala", "additions": "5", "deletions": "10", "changes": "15"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala", "additions": "7", "deletions": "3", "changes": "10"}, "updated": [0, 2, 8]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/statements.scala", "additions": "0", "deletions": "19", "changes": "19"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2Commands.scala", "additions": "23", "deletions": "4", "changes": "27"}, "updated": [0, 0, 5]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/CatalogV2Util.scala", "additions": "2", "deletions": "7", "changes": "9"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala", "additions": "8", "deletions": "12", "changes": "20"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/rules.scala", "additions": "46", "deletions": "0", "changes": "46"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala", "additions": "7", "deletions": "2", "changes": "9"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala", "additions": "9", "deletions": "6", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/PlanResolutionSuite.scala", "additions": "26", "deletions": "19", "changes": "45"}, "updated": [0, 0, 2]}]}
{"author": "huaxingao", "sha": "14a819aedc631746f037f3bf7e07b65215d1cbd8", "commit_date": "2021/08/16 17:45:03", "commit_message": "[SPARK-36526][SQL] DSV2 Index Support: Add supportsIndex interface", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/index/SupportsIndex.java", "additions": "84", "deletions": "0", "changes": "84"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/index/TableIndex.java", "additions": "90", "deletions": "0", "changes": "90"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/AlreadyExistException.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/NoSuchItemException.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}]}
{"author": "AngersZhuuuu", "sha": "71f6b17c91b92a85913f816085290a0212e46939", "commit_date": "2021/08/25 11:27:56", "commit_message": "fix ut", "title": "[SPARK-36579][SQL] Make spark source stagingDir can use user defined", "body": "### What changes were proposed in this pull request?\r\nMake spark source stagingDir can use user defined like hive\r\n\r\n### Why are the changes needed?\r\nMake spark source stagingDir can use user defined then we can do some optimization on this\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nUser can define staging dir by `spark.sql.source.stagingDir`\r\n\r\n### How was this patch tested?\r\nAdded UT\r\n", "failed_tests": ["org.apache.spark.sql.avro.AvroV1Suite", "org.apache.spark.sql.avro.AvroV2Suite", "org.apache.spark.ml.source.libsvm.LibSVMRelationSuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.execution.HiveDDLSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.InsertSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.execution.datasources.csv.CSVLegacyTimeParserSuite", "org.apache.spark.sql.execution.datasources.text.TextV1Suite", "org.apache.spark.sql.execution.datasources.text.TextV2Suite", "org.apache.spark.sql.execution.datasources.csv.CSVv1Suite", "org.apache.spark.sql.execution.datasources.orc.OrcSourceSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormatV1Suite", "org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormatV2Suite", "org.apache.spark.sql.execution.datasources.json.JsonV1Suite", "org.apache.spark.sql.execution.datasources.json.JsonV2Suite", "org.apache.spark.sql.execution.datasources.json.JsonLegacyTimeParserSuite", "org.apache.spark.sql.execution.datasources.csv.CSVv2Suite", "org.apache.spark.sql.sources.InsertSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/internal/io/FileCommitProtocol.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "12", "deletions": "0", "changes": "12"}, "updated": [0, 2, 18]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala", "additions": "2", "deletions": "11", "changes": "13"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SQLHadoopMapReduceCommitProtocol.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/util/SQLFileCommitProtocolUtils.scala", "additions": "115", "deletions": "0", "changes": "115"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/sources/InsertSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/sources/PartitionedWriteSuite.scala", "additions": "27", "deletions": "0", "changes": "27"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/SaveAsHiveFile.scala", "additions": "7", "deletions": "89", "changes": "96"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/InsertSuite.scala", "additions": "13", "deletions": "8", "changes": "21"}, "updated": [0, 0, 0]}]}
{"author": "AngersZhuuuu", "sha": "892b557875158c6e127de081cff98d06b18e14bf", "commit_date": "2021/08/25 02:25:50", "commit_message": "update", "title": " [SPARK-36563][SQL] dynamicPartitionOverwrite can direct rename to targetPath instead of partition path one by one when targetPath is empty", "body": "### What changes were proposed in this pull request?\r\nWhen target path is empty, we can directly rename stagingDir to targetPath avoid to rename path one by one\r\n\r\n\r\n### Why are the changes needed?\r\nOptimize file commit logic\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nUser can set `spark.sql.source.stagingDir` to enable  direct rename to targetPath instead of partition path one by one when targetPath is empty for dynamicPartitionOverWrite\r\n\r\n\r\n### How was this patch tested?\r\nAdded UT\r\n", "failed_tests": ["org.apache.spark.shuffle.KubernetesLocalDiskShuffleDataIOSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala", "additions": "28", "deletions": "18", "changes": "46"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "12", "deletions": "0", "changes": "12"}, "updated": [0, 2, 18]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SQLHadoopMapReduceCommitProtocol.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/sources/PartitionedWriteSuite.scala", "additions": "27", "deletions": "0", "changes": "27"}, "updated": [0, 0, 0]}]}
{"author": "AngersZhuuuu", "sha": "2f7b98bbfb17eb68f66500ee23030684bb7046fe", "commit_date": "2021/08/12 03:09:27", "commit_message": "[SPARK-36485][SQL] Support cast type constructed string as year month interval", "title": "[SPARK-36485][SQL] Support cast type constructed string as year month interval", "body": "### What changes were proposed in this pull request?\r\nSpark support type constructed string as  year month interval such as \r\n```\r\ninterval '1 year 2 month'\r\ninterval '3 year'\r\n```\r\nAnd PGSQL support\r\n```\r\ncast('1 year 2 month' as interval year)\r\ncast('1 year 2 month' as interval year to month)\r\ncast('1 year 2 month' as interval month)\r\n```\r\netc, spark can support this too.\r\n\r\n\r\n### Why are the changes needed?\r\nSupport cast  same type constructed string as year month interval\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nuser can cast such as `1 year 2 month` string as year month interval\r\n\r\n### How was this patch tested?\r\nAdded UT", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/IntervalUtils.scala", "additions": "29", "deletions": "2", "changes": "31"}, "updated": [0, 1, 14]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/cast.sql", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/cast.sql.out", "additions": "57", "deletions": "1", "changes": "58"}, "updated": [0, 0, 1]}]}
{"author": "AngersZhuuuu", "sha": "91cdc0a6bd992f72d1773b37c5f8b811271aad1c", "commit_date": "2021/08/12 03:53:43", "commit_message": "[SPARK-36486][SQL] Support cast type constructed same string to day time interval", "title": "[SPARK-36486][SQL] Support cast type constructed same string to day time interval", "body": "### What changes were proposed in this pull request?\r\nSpark support type constructed string as  day time interval such as \r\n```\r\ninterval '1 day 2 hour'\r\n```\r\nAnd PGSQL support\r\n```\r\ncast('1 day 2 hour' as interval day)\r\ncast('1 day 2 hour' as interval day to hour)\r\n```\r\netc, spark can support this too.\r\n\r\n\r\n### Why are the changes needed?\r\nSupport cast  same type constructed string as day time interval\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nuser can cast such as `1 day 2 hour` string as day time interval\r\n\r\n### How was this patch tested?\r\nAdded UT", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/IntervalUtils.scala", "additions": "51", "deletions": "2", "changes": "53"}, "updated": [0, 1, 14]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/cast.sql", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/cast.sql.out", "additions": "113", "deletions": "1", "changes": "114"}, "updated": [0, 0, 1]}]}
{"author": "AngersZhuuuu", "sha": "34f9d1ad28c7e9f3cfbae47be72349e45c855fe6", "commit_date": "2021/08/31 09:39:53", "commit_message": "[SPARK-36624][YARN] In yarn client mode, when ApplicationMaster failed with KILLED/FAILED, driver should exit with code not 0", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "docs/running-on-yarn.md", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/config.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/yarn/src/main/scala/org/apache/spark/scheduler/cluster/YarnClientSchedulerBackend.scala", "additions": "9", "deletions": "1", "changes": "10"}, "updated": [0, 0, 0]}]}
{"author": "AngersZhuuuu", "sha": "8058f28c46b122d023b09857ffaec55f02ba09b6", "commit_date": "2021/09/08 06:17:18", "commit_message": "[SPARK-36691][PYSPARK] PythonRunner failed should pass error message to ApplicationMaster too", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/SparkException.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 2, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/PythonRunner.scala", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/Utils.scala", "additions": "75", "deletions": "0", "changes": "75"}, "updated": [1, 1, 5]}, {"file": {"name": "resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala", "additions": "3", "deletions": "4", "changes": "7"}, "updated": [0, 1, 1]}]}
{"author": "AngersZhuuuu", "sha": "d90816e4798ad0ada5af2a7f5e60a405e6b8e98d", "commit_date": "2021/08/18 08:31:54", "commit_message": "[SPARK-36540][YARN]YARN-CLIENT mode should check Shutdown message when AMEndpoint disconencted", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "docs/running-on-yarn.md", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala", "additions": "13", "deletions": "2", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/config.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/yarn/src/main/scala/org/apache/spark/scheduler/cluster/YarnClientSchedulerBackend.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/yarn/src/main/scala/org/apache/spark/scheduler/cluster/YarnSchedulerBackend.scala", "additions": "11", "deletions": "2", "changes": "13"}, "updated": [0, 0, 0]}]}
{"author": "AngersZhuuuu", "sha": "b33ad90675910357f976266099883f8c7067441e", "commit_date": "2021/08/11 02:50:19", "commit_message": "[SPARK-36437][CORE] Add non-unified memory peak metrics", "title": "", "body": "", "failed_tests": ["org.apache.spark.deploy.history.HistoryServerSuite", "org.apache.spark.util.JsonProtocolSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/metrics/ExecutorMetricType.scala", "additions": "12", "deletions": "0", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/api/v1/PrometheusResource.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/complete_stage_list_json_expectation.json", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/excludeOnFailure_for_stage_expectation.json", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/excludeOnFailure_node_for_stage_expectation.json", "additions": "12", "deletions": "0", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/executor_list_json_expectation.json", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/executor_list_with_executor_metrics_json_expectation.json", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/executor_memory_usage_expectation.json", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/executor_node_excludeOnFailure_expectation.json", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/executor_node_excludeOnFailure_unexcluding_expectation.json", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/failed_stage_list_json_expectation.json", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/one_stage_attempt_json_details_with_failed_task_expectation.json", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/one_stage_attempt_json_expectation.json", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/one_stage_json_expectation.json", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/one_stage_json_with_details_expectation.json", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/stage_list_json_expectation.json", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/stage_list_with_accumulable_json_expectation.json", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/stage_list_with_peak_metrics_expectation.json", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/stage_with_accumulable_json_expectation.json", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/stage_with_peak_metrics_expectation.json", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/stage_with_summaries_expectation.json", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/util/JsonProtocolSuite.scala", "additions": "19", "deletions": "8", "changes": "27"}, "updated": [0, 0, 0]}, {"file": {"name": "docs/monitoring.md", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 1]}]}
{"author": "Peng-Lei", "sha": "414aabe1938954d3bf56a7f93d23b6ef469a1622", "commit_date": "2021/09/23 07:12:40", "commit_message": "add draft", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.avro.AvroV1Suite", "org.apache.spark.sql.avro.AvroV2Suite", "org.apache.spark.sql.HiveCharVarcharTestSuite", "org.apache.spark.sql.execution.datasources.FileIndexSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetRebaseDatetimeV2Suite", "org.apache.spark.sql.execution.datasources.parquet.ParquetRebaseDatetimeV1Suite", "org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite", "org.apache.spark.sql.FileSourceCharVarcharTestSuite", "org.apache.spark.sql.sources.InsertSuite"], "files": [{"file": {"name": "R/pkg/tests/fulltests/test_sparkSQL.R", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 2, 3]}, {"file": {"name": "core/src/main/resources/error/error-classes.json", "additions": "70", "deletions": "0", "changes": "70"}, "updated": [0, 2, 4]}, {"file": {"name": "core/src/main/scala/org/apache/spark/SparkException.scala", "additions": "5", "deletions": "3", "changes": "8"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala", "additions": "69", "deletions": "78", "changes": "147"}, "updated": [0, 2, 4]}]}
{"author": "Peng-Lei", "sha": "21a94544c539ee2896be81f3988104820efc0068", "commit_date": "2021/09/24 07:02:49", "commit_message": "add draft", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "docs/sql-ref-ansi-compliance.md", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveCatalogs.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 1, 7]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/statements.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2Commands.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/CatalogManager.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [0, 0, 0]}]}
{"author": "Peng-Lei", "sha": "7912e186729193d315ca01641147e386224bb970", "commit_date": "2021/09/16 08:29:05", "commit_message": "add draft", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.DynamicPartitionPruningV2Suite", "org.apache.spark.sql.DSV2SQLInsertTestSuite", "org.apache.spark.sql.connector.DataSourceV2SQLSuite", "org.apache.spark.sql.DynamicPartitionPruningV2Suite"], "files": [{"file": {"name": "R/pkg/tests/fulltests/test_sparkSQL.R", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [1, 2, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 1, 9]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/internal/CatalogImpl.scala", "additions": "58", "deletions": "23", "changes": "81"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala", "additions": "44", "deletions": "0", "changes": "44"}, "updated": [0, 0, 0]}]}
{"author": "Peng-Lei", "sha": "67d3622b9f836b39da812259ad5d284d5e62c233", "commit_date": "2021/07/14 05:59:17", "commit_message": "Keep consistent with the namespace naming rule", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/CatalogManager.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 7, 16]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 1, 2]}]}
{"author": "Peng-Lei", "sha": "759dd0a179e3a38e6bc8c3b692ac82df0c7ef8bb", "commit_date": "2021/07/01 13:42:53", "commit_message": "Add the show catalogs feature", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "docs/sql-ref-ansi-compliance.md", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 5]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveCatalogs.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [1, 3, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [1, 5, 15]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/statements.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 3, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2Commands.scala", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [1, 3, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/CatalogManager.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [1, 3, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ShowCatalogsExec.scala", "additions": "47", "deletions": "0", "changes": "47"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala", "additions": "21", "deletions": "0", "changes": "21"}, "updated": [1, 1, 1]}]}
{"author": "itholic", "sha": "d930f8925720672cb6120ff73826b662f88bc2a0", "commit_date": "2021/09/27 10:35:34", "commit_message": "[SPARK-36438] Support list-like Python objects for Series comparison", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.data_type_ops.test_categorical_ops"], "files": [{"file": {"name": "python/pyspark/pandas/base.py", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [1, 3, 3]}, {"file": {"name": "python/pyspark/pandas/series.py", "additions": "17", "deletions": "1", "changes": "18"}, "updated": [1, 5, 7]}, {"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames.py", "additions": "69", "deletions": "0", "changes": "69"}, "updated": [0, 3, 5]}, {"file": {"name": "python/pyspark/pandas/tests/test_series.py", "additions": "36", "deletions": "0", "changes": "36"}, "updated": [0, 4, 9]}]}
{"author": "itholic", "sha": "0b79e4aef9b8832accebd18d4d4a8135a6721fec", "commit_date": "2021/09/27 07:38:27", "commit_message": "Implement MultiIndex.equal_levels", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.indexes.test_base"], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/indexing.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 3]}, {"file": {"name": "python/pyspark/pandas/indexes/multi.py", "additions": "37", "deletions": "0", "changes": "37"}, "updated": [1, 1, 2]}, {"file": {"name": "python/pyspark/pandas/tests/indexes/test_base.py", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 5]}, {"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames.py", "additions": "37", "deletions": "0", "changes": "37"}, "updated": [0, 3, 5]}]}
{"author": "ueshin", "sha": "0a43396ce3da47024db39f27ffcc9f28911cf1ab", "commit_date": "2021/09/24 20:45:41", "commit_message": "Inline most of type hint files under pyspark/sql/pandas folder", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/sql/pandas/_typing/protocols/frame.pyi", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/pandas/conversion.py", "additions": "88", "deletions": "32", "changes": "120"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/sql/pandas/conversion.pyi", "additions": "0", "deletions": "59", "changes": "59"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/sql/pandas/group_ops.py", "additions": "32", "deletions": "14", "changes": "46"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/pandas/group_ops.pyi", "additions": "0", "deletions": "49", "changes": "49"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/pandas/map_ops.py", "additions": "11", "deletions": "3", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/pandas/map_ops.pyi", "additions": "0", "deletions": "30", "changes": "30"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/pandas/typehints.py", "additions": "20", "deletions": "4", "changes": "24"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/pandas/types.py", "additions": "28", "deletions": "14", "changes": "42"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/sql/pandas/utils.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}]}
{"author": "ueshin", "sha": "cd0f7070b4a504d2aba57d7e4b71fcc225731603", "commit_date": "2021/09/21 00:08:22", "commit_message": "Implement ps.merge_asof.", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/namespace.py", "additions": "497", "deletions": "0", "changes": "497"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_reshape.py", "additions": "138", "deletions": "0", "changes": "138"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/dataframe.py", "additions": "117", "deletions": "0", "changes": "117"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 0, 8]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/DeduplicateRelations.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 2, 9]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteAsOfJoin.scala", "additions": "89", "deletions": "0", "changes": "89"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/joinTypes.scala", "additions": "21", "deletions": "0", "changes": "21"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala", "additions": "116", "deletions": "0", "changes": "116"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/rules/RuleIdCollection.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreePatterns.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/RewriteAsOfJoinSuite.scala", "additions": "289", "deletions": "0", "changes": "289"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala", "additions": "90", "deletions": "27", "changes": "117"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameAsOfJoinSuite.scala", "additions": "169", "deletions": "0", "changes": "169"}, "updated": [0, 0, 0]}]}
{"author": "dongjoon-hyun", "sha": "7a581fc89fa38f921def1de4924bdae9df9d647e", "commit_date": "2021/09/24 02:33:48", "commit_message": "[SPARK-36837][BUILD] Upgrade Kafka to 3.0.0", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.kafka010.KafkaRelationSuite", "org.apache.spark.sql.kafka010.KafkaRelationSuite", "org.apache.spark.sql.kafka010.KafkaRelationSuite", "org.apache.spark.sql.kafka010.KafkaRelationSuite"], "files": [{"file": {"name": "external/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaTestUtils.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "external/kafka-0-10/src/test/scala/org/apache/spark/streaming/kafka010/KafkaRDDSuite.scala", "additions": "11", "deletions": "4", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "external/kafka-0-10/src/test/scala/org/apache/spark/streaming/kafka010/KafkaTestUtils.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "pom.xml", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [2, 12, 26]}]}
{"author": "c21", "sha": "b22479a85a4d7992b81305df27af50c9915dabf0", "commit_date": "2021/09/17 19:00:08", "commit_message": "Ignore duplicated join keys when building relation for LEFT/ANTI hash join", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.TPCHPlanStabilitySuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilitySuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilityWithStatsSuite", "org.apache.spark.sql.execution.debug.DebuggingSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/BroadcastExchangeExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastHashJoinExec.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashJoin.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashedRelation.scala", "additions": "24", "deletions": "12", "changes": "36"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/joins/ShuffledHashJoinExec.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q10.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q10/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q19.sf100/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q19/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q27.sf100/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q27/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q3.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q3/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q34.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q34/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q42.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q42/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q43.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q43/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q46.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q46/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q52.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q52/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q53.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q53/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q55.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q55/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q59.sf100/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q59/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q63.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q63/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q65.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q65/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q68.sf100/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q68/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q7.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q7/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q73.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q73/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q79.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q79/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q89.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q89/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q98.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q98/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q1.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q1/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q10.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q10/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q11.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q11/explain.txt", "additions": "9", "deletions": "9", "changes": "18"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q12.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q12/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q13.sf100/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q13/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a.sf100/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a/explain.txt", "additions": "9", "deletions": "9", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b.sf100/explain.txt", "additions": "8", "deletions": "8", "changes": "16"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b/explain.txt", "additions": "11", "deletions": "11", "changes": "22"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q15.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q15/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q16.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q16/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q17.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q17/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q18.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q18/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q19.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q19/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q2.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q2/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q20.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q20/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q21.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q21/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q22.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q22/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23a.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23a/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23b.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23b/explain.txt", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24a.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24a/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24b.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24b/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q25.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q25/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q26.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q26/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q27.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q27/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q29.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q29/explain.txt", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q3.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q3/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q30.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q30/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q31.sf100/explain.txt", "additions": "8", "deletions": "8", "changes": "16"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q31/explain.txt", "additions": "9", "deletions": "9", "changes": "18"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q32.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q32/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q33.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q33/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q35.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q35/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q36.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q36/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q37.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q37/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q38.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q38/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q39a.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q39a/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q39b.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q39b/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q4.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q4/explain.txt", "additions": "13", "deletions": "13", "changes": "26"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q40.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q40/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q41.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q41/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q42.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q42/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q43.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q43/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q44.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q44/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q45.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q45/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q46.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q46/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q47.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q47/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q48.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q48/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q49.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q49/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q5.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q5/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q50.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q50/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q51.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q51/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q52.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q52/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q53.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q53/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q54.sf100/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q54/explain.txt", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q55.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q55/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q56.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q56/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q57.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q57/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q58.sf100/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q58/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q59.sf100/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q59/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q60.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q60/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q61.sf100/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q61/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q62.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q62/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q63.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q63/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q65.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q65/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q66.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q66/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q67.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q67/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q68.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q68/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q69.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q69/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q7.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q7/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q70.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q70/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q71.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q71/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q72.sf100/explain.txt", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q72/explain.txt", "additions": "9", "deletions": "9", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q73.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q73/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q76.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q76/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q77.sf100/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q77/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q79.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q79/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q8.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q8/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q80.sf100/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q80/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q81.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q81/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q82.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q82/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q83.sf100/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q83/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q84.sf100/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q84/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q85.sf100/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q85/explain.txt", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q86.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q86/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q87.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q87/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q88.sf100/explain.txt", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q88/explain.txt", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q89.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q89/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q90.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q90/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q91.sf100/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q91/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q92.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q92/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q93.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q93/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q94.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q94/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q95.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q95/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q96.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q96/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q97.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q97/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q98.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q98/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q99.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q99/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q10a.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q10a/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q11.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q11/explain.txt", "additions": "9", "deletions": "9", "changes": "18"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q12.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q12/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14.sf100/explain.txt", "additions": "8", "deletions": "8", "changes": "16"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14/explain.txt", "additions": "11", "deletions": "11", "changes": "22"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a.sf100/explain.txt", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a/explain.txt", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q18a.sf100/explain.txt", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q18a/explain.txt", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q20.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q20/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q22.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q22/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q22a.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q22a/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q24.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q24/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q27a.sf100/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q27a/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q34.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q34/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35a.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35a/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q36a.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q36a/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q47.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q47/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q49.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q49/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q51a.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q51a/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q57.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q57/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q5a.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q5a/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q6.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q6/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q64.sf100/explain.txt", "additions": "9", "deletions": "9", "changes": "18"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q64/explain.txt", "additions": "13", "deletions": "13", "changes": "26"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q67a.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q67a/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q70a.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q70a/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q72.sf100/explain.txt", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q72/explain.txt", "additions": "9", "deletions": "9", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q74.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q74/explain.txt", "additions": "9", "deletions": "9", "changes": "18"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q75.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q75/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q77a.sf100/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q77a/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q78.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q78/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q80a.sf100/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q80a/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q86a.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q86a/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q98.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "LucaCanali", "sha": "903feb290e31e898705e177c7b3fb8c477567eee", "commit_date": "2021/07/28 09:28:16", "commit_message": "Implement SQL metrics for Python UDF.", "title": "[SPARK-34265][WIP][PYTHON] Instrument Python UDFs using SQL metrics ", "body": "### What changes are proposed in this pull request?\r\n\r\nThis proposes to add SQLMetrics instrumentation for Python UDF execution.\r\nThe proposed metrics are:\r\n\r\n- data sent to Python workers\r\n- data returned from Python workers\r\n- number of rows processed\r\n\r\n\r\n### Why are the changes needed?\r\nThis aims at improving monitoring and performance troubleshooting of Python UDFs.\r\nIn particular as an aid to answer performance-related questions such as:\r\nwhy is the UDF slow?, how much work it has done so far?, etc.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nSQL metrics are made available in the WEB UI.  \r\nSee the following examples:  \r\n\r\n![image1](https://user-images.githubusercontent.com/5243162/127323340-f0132da1-e19c-4d81-b5dc-a534ea9346ee.png)\r\n  \r\n![image2](https://issues.apache.org/jira/secure/attachment/13031153/Python_UDF_instrumentation_lite_BatchEvalPython.png)\r\n\r\n### How was this patch tested?\r\n\r\nManually tested + a Python unit test has been added.\r\n\r\nExample code used for testing:\r\n\r\n```\r\nfrom pyspark.sql.functions import col, pandas_udf\r\nimport time\r\n\r\n@pandas_udf(\"long\")\r\ndef test_pandas(col1):\r\n  time.sleep(0.02)\r\n  return col1 * col1\r\n\r\nspark.udf.register(\"test_pandas\", test_pandas)\r\nspark.sql(\"select rand(42)*rand(51)*rand(12) col1 from range(10000000)\").createOrReplaceTempView(\"t1\")\r\nspark.sql(\"select max(test_pandas(col1)) from t1\").collect()\r\n```\r\n\r\nThis is used to test with more data pushed to the Python workers\r\n\r\n```\r\nfrom pyspark.sql.functions import col, pandas_udf\r\nimport time\r\n\r\n@pandas_udf(\"long\")\r\ndef test_pandas(col1,col2,col3,col4,col5,col6,col7,col8,col9,col10,col11,col12,col13,col14,col15,col16,col17):\r\n  time.sleep(0.02)\r\n  return col1\r\n\r\nspark.udf.register(\"test_pandas\", test_pandas)\r\nspark.sql(\"select rand(42)*rand(51)*rand(12) col1 from range(10000000)\").createOrReplaceTempView(\"t1\")\r\nspark.sql(\"select max(test_pandas(col1,col1+1,col1+2,col1+3,col1+4,col1+5,col1+6,col1+7,col1+8,col1+9,col1+10,col1+11,col1+12,col1+13,col1+14,col1+15,col1+16)) from t1\").collect()\r\n```\r\n\r\nThis is for testing Python UDF (non pandas)\r\n\r\n`from pyspark.sql.functions import udf; spark.range(100).select(udf(lambda x: x/1)(\"id\")).collect()`\r\n  ", "failed_tests": ["org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/sparktestsupport/modules.py", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [1, 1, 10]}, {"file": {"name": "docs/web-ui.md", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [1, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_pandas_sqlmetrics.py", "additions": "58", "deletions": "0", "changes": "58"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/python/AggregateInPandasExec.scala", "additions": "5", "deletions": "2", "changes": "7"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowEvalPythonExec.scala", "additions": "5", "deletions": "2", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowPythonRunner.scala", "additions": "8", "deletions": "1", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/python/BatchEvalPythonExec.scala", "additions": "7", "deletions": "2", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/python/CoGroupedArrowPythonRunner.scala", "additions": "8", "deletions": "1", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/python/FlatMapCoGroupsInPandasExec.scala", "additions": "5", "deletions": "2", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/python/FlatMapGroupsInPandasExec.scala", "additions": "5", "deletions": "2", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/python/MapInPandasExec.scala", "additions": "5", "deletions": "2", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/python/PythonArrowOutput.scala", "additions": "10", "deletions": "1", "changes": "11"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/python/PythonSQLMetrics.scala", "additions": "37", "deletions": "0", "changes": "37"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/python/PythonUDFRunner.scala", "additions": "8", "deletions": "1", "changes": "9"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/python/WindowInPandasExec.scala", "additions": "5", "deletions": "2", "changes": "7"}, "updated": [0, 0, 0]}]}
{"author": "peter-toth", "sha": "d86d2c48a3e6244fc091ae09cb9377ade98f66b0", "commit_date": "2021/04/21 17:40:19", "commit_message": "[SPARK-34079][SQL] Merging non-correlated scalar subqueries to multi-column scalar subqueries for better reuse", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueries.scala", "additions": "416", "deletions": "0", "changes": "416"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala", "additions": "32", "deletions": "1", "changes": "33"}, "updated": [2, 8, 16]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreePatterns.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [1, 3, 3]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueriesSuite.scala", "additions": "282", "deletions": "0", "changes": "282"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap.java", "additions": "2", "deletions": "7", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "8", "deletions": "3", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/PlanAdaptiveSubqueries.scala", "additions": "18", "deletions": "6", "changes": "24"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/ReuseAdaptiveSubquery.scala", "additions": "5", "deletions": "3", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala", "additions": "2", "deletions": "8", "changes": "10"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "0", "deletions": "9", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "21", "deletions": "0", "changes": "21"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/reuse/ReuseExchangeAndSubquery.scala", "additions": "10", "deletions": "3", "changes": "13"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala", "additions": "18", "deletions": "2", "changes": "20"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9.sf100/explain.txt", "additions": "183", "deletions": "598", "changes": "781"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9.sf100/simplified.txt", "additions": "65", "deletions": "170", "changes": "235"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9/explain.txt", "additions": "183", "deletions": "598", "changes": "781"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9/simplified.txt", "additions": "65", "deletions": "170", "changes": "235"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala", "additions": "72", "deletions": "0", "changes": "72"}, "updated": [1, 3, 10]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala", "additions": "37", "deletions": "0", "changes": "37"}, "updated": [1, 2, 4]}]}
{"author": "planga82", "sha": "533cb7ca056e746ce0d4047662e0ae5574d47c36", "commit_date": "2021/08/15 22:47:03", "commit_message": "Fix style", "title": "[SPARK-36453][SQL] Improve consistency processing floating point special literals", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nSpecial literals in floating point are not consistent between cast and json expressions\r\n```\r\nscala> spark.sql(\"SELECT CAST('+Inf' as Double)\").show\r\n+--------------------+                                                        \r\n|CAST(+Inf AS DOUBLE)|\r\n+--------------------+\r\n|            Infinity|\r\n+--------------------+\r\n```\r\n```\r\nscala> val schema =  StructType(StructField(\"a\", DoubleType) :: Nil)\r\n\r\nscala> Seq(\"\"\"{\"a\" : \"+Inf\"}\"\"\").toDF(\"col1\").select(from_json(col(\"col1\"),schema)).show\r\n+---------------+\r\n|from_json(col1)|\r\n+---------------+\r\n|         {null}|\r\n+---------------+\r\n\r\nscala> Seq(\"\"\"{\"a\" : \"+Inf\"}\"\"\").toDF(\"col\").withColumn(\"col\", from_json(col(\"col\"), StructType.fromDDL(\"a DOUBLE\"))).write.json(\"/tmp/jsontests12345\")\r\nscala> spark.read.schema(StructType(Seq(StructField(\"col\",schema)))).json(\"/tmp/jsontests12345\").show\r\n+------+\r\n|   col|\r\n+------+\r\n|{null}|\r\n+------+\r\n```\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nImprove consistency between operations\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nYes, we are going to support the same special literal in Cast and Json expressions\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nUnit testing and manual testing", "failed_tests": ["pyspark.pandas.tests.data_type_ops.test_string_ops", "pyspark.pandas.tests.test_dataframe", "org.apache.spark.sql.catalyst.expressions.AnsiCastSuite", "org.apache.spark.sql.catalyst.expressions.AnsiCastSuite", "org.apache.spark.sql.catalyst.expressions.CastSuite", "org.apache.spark.sql.catalyst.expressions.TryCastSuite", "org.apache.spark.sql.catalyst.expressions.CastSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SQLQuerySuite", "org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite", "org.apache.spark.sql.DynamicPartitionPruningV1Suite", "org.apache.spark.sql.DynamicPartitionPruningV2Suite", "org.apache.spark.sql.DynamicPartitionPruningV1Suite", "org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite", "org.apache.spark.sql.DataFrameWindowFunctionsSuite", "org.apache.spark.sql.SubquerySuite", "org.apache.spark.sql.DynamicPartitionPruningV2Suite", "org.apache.spark.sql.DataFrameSuite", "org.apache.spark.sql.ColumnExpressionSuite", "org.apache.spark.sql.execution.datasources.json.JsonV1Suite", "org.apache.spark.sql.execution.datasources.json.JsonV2Suite", "org.apache.spark.sql.execution.datasources.json.JsonLegacyTimeParserSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala", "additions": "5", "deletions": "20", "changes": "25"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ExprUtils.scala", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JacksonParser.scala", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [0, 4, 7]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/json/JacksonParserSuite.scala", "additions": "27", "deletions": "12", "changes": "39"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "28", "deletions": "0", "changes": "28"}, "updated": [1, 2, 11]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/json/JsonSuite.scala", "additions": "0", "deletions": "17", "changes": "17"}, "updated": [0, 1, 3]}]}
{"author": "planga82", "sha": "02a265d4eb38a1b562db6ce02e56f739a16d9ce7", "commit_date": "2021/09/08 22:53:55", "commit_message": "Implementation & tests", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite", "org.apache.spark.sql.DataFrameSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 2, 18]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [1, 1, 8]}]}
{"author": "dgd-contributor", "sha": "b4bf4d2eb5962a8f5c4baec92927f960c4a2cfb8", "commit_date": "2021/08/19 09:06:26", "commit_message": "resolve suggested change", "title": "[SPARK-36099][CORE] Grouping exception in core/util", "body": "### What changes were proposed in this pull request?\r\nThis PR group exception messages in core/src/main/scala/org/apache/spark/util\r\n\r\n### Why are the changes needed?\r\nIt will largely help with standardization of error messages and its maintenance.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo. Error messages remain unchanged.\r\n\r\n### How was this patch tested?\r\nNo new tests - pass all original tests to make sure it doesn't break any existing behavior.", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/errors/SparkCoreErrors.scala", "additions": "152", "deletions": "5", "changes": "157"}, "updated": [0, 2, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala", "additions": "5", "deletions": "8", "changes": "13"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/DependencyUtils.scala", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/KeyLock.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/ListenerBus.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/NextIterator.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/SerializableBuffer.scala", "additions": "5", "deletions": "3", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/ThreadUtils.scala", "additions": "5", "deletions": "4", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/Utils.scala", "additions": "18", "deletions": "30", "changes": "48"}, "updated": [0, 1, 6]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/collection/AppendOnlyMap.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/collection/CompactBuffer.scala", "additions": "5", "deletions": "4", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala", "additions": "6", "deletions": "5", "changes": "11"}, "updated": [0, 0, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/collection/ImmutableBitSet.scala", "additions": "8", "deletions": "6", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/collection/MedianHeap.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/collection/OpenHashSet.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/collection/PartitionedPairBuffer.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/collection/PrimitiveVector.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/io/ChunkedByteBuffer.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/logging/DriverLogger.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/random/RandomSampler.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}]}
{"author": "dgd-contributor", "sha": "a89af71e932f932e05a22caca449cd9b4872d865", "commit_date": "2021/08/19 05:22:39", "commit_message": "resolve suggested change", "title": "[SPARK-36100][CORE] Grouping exception in core/status", "body": "### What changes were proposed in this pull request?\r\nThis PR group exception messages in core/src/main/scala/org/apache/spark/status\r\n\r\n### Why are the changes needed?\r\nIt will largely help with standardization of error messages and its maintenance.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo. Error messages remain unchanged.\r\n\r\n### How was this patch tested?\r\nNo new tests - pass all original tests to make sure it doesn't break any existing behavior.", "failed_tests": ["org.apache.spark.deploy.history.HistoryServerSuite", "org.apache.spark.status.AppStatusListenerSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/errors/SparkCoreErrors.scala", "additions": "103", "deletions": "0", "changes": "103"}, "updated": [0, 2, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusStore.scala", "additions": "4", "deletions": "5", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/KVUtils.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/LiveEntity.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/api/v1/ApiRootResource.scala", "additions": "9", "deletions": "8", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/api/v1/OneApplicationResource.scala", "additions": "12", "deletions": "12", "changes": "24"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/api/v1/SimpleDateParam.scala", "additions": "3", "deletions": "9", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/api/v1/StagesResource.scala", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [0, 0, 0]}]}
{"author": "dgd-contributor", "sha": "909fe78c89b3e603ea7130b83ce640a9957c8aa5", "commit_date": "2021/08/30 03:51:19", "commit_message": "[SPARK-36303]: Refactor fourteenth set of 20 query execution errors to use error classes", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/resources/error/error-classes.json", "additions": "73", "deletions": "0", "changes": "73"}, "updated": [0, 1, 5]}, {"file": {"name": "core/src/main/scala/org/apache/spark/SparkException.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [1, 3, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/CatalogNotFoundException.scala", "additions": "13", "deletions": "1", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala", "additions": "69", "deletions": "31", "changes": "100"}, "updated": [0, 1, 6]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 2, 14]}, {"file": {"name": "sql/catalyst/src/test/java/org/apache/spark/sql/connector/catalog/CatalogLoadingSuite.java", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/RuntimeConfig.scala", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [0, 0, 0]}]}
{"author": "dgd-contributor", "sha": "6227f49e2f407b9e07c03b3c124eb679248065c1", "commit_date": "2021/08/19 08:26:13", "commit_message": "[SPARK-36296]: Refactor seventh set of 20 query execution errors to use error classes", "title": "", "body": "", "failed_tests": ["org.apache.spark.SparkThrowableSuite"], "files": [{"file": {"name": "core/src/main/resources/error/error-classes.json", "additions": "65", "deletions": "0", "changes": "65"}, "updated": [0, 1, 5]}, {"file": {"name": "core/src/main/scala/org/apache/spark/SparkException.scala", "additions": "21", "deletions": "7", "changes": "28"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala", "additions": "38", "deletions": "31", "changes": "69"}, "updated": [0, 0, 8]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 4]}]}
{"author": "dgd-contributor", "sha": "271ad2d90bdf7a078a3068f796de404397c7634a", "commit_date": "2021/09/16 07:54:52", "commit_message": "[SPARK-36711] Support multi-index in new syntax", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_typedef", "pyspark.pandas.tests.indexes.test_datetime"], "files": [{"file": {"name": "python/pyspark/pandas/accessors.py", "additions": "36", "deletions": "18", "changes": "54"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "16", "deletions": "8", "changes": "24"}, "updated": [1, 3, 11]}, {"file": {"name": "python/pyspark/pandas/groupby.py", "additions": "15", "deletions": "8", "changes": "23"}, "updated": [0, 1, 10]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "26", "deletions": "0", "changes": "26"}, "updated": [0, 1, 6]}, {"file": {"name": "python/pyspark/pandas/typedef/typehints.py", "additions": "159", "deletions": "87", "changes": "246"}, "updated": [1, 2, 2]}]}
{"author": "dgd-contributor", "sha": "65ad6074f481bfc8665684d1f96905c7aca47ba3", "commit_date": "2021/08/27 09:54:40", "commit_message": "[SPARK-36292]: Refactor third set of 20 query execution errors to use error classes", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "core/src/main/resources/error/error-classes.json", "additions": "71", "deletions": "2", "changes": "73"}, "updated": [1, 2, 5]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [2, 4, 14]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala", "additions": "41", "deletions": "25", "changes": "66"}, "updated": [1, 1, 6]}]}
{"author": "dgd-contributor", "sha": "8a3b657745e5dedd40445e965d079233bd4beacc", "commit_date": "2021/09/22 09:45:16", "commit_message": "[SPARK-36742][PYTHON] Fix ps.to_datetime with plurals of keys like years, months, days", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_namespace", "pyspark.pandas.tests.test_series"], "files": [{"file": {"name": "python/pyspark/pandas/namespace.py", "additions": "23", "deletions": "2", "changes": "25"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_namespace.py", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [0, 0, 1]}]}
{"author": "dgd-contributor", "sha": "0a4b38e04b79991e327c9b38f3111b8e6636cab5", "commit_date": "2021/09/24 02:10:42", "commit_message": "[SPARK-36302][SQL] Refactor thirteenth set of 20 query execution errors to use error classes", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/resources/error/error-classes.json", "additions": "68", "deletions": "0", "changes": "68"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala", "additions": "73", "deletions": "57", "changes": "130"}, "updated": [0, 1, 4]}]}
{"author": "dgd-contributor", "sha": "37a2a0de0b71c10e117987d778d80e1d2ea1c822", "commit_date": "2021/08/25 08:02:14", "commit_message": "Add needed classes from pr 36107", "title": "", "body": "", "failed_tests": ["org.apache.spark.SparkThrowableSuite", "org.apache.spark.sql.catalyst.expressions.StringExpressionsSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "core/src/main/resources/error/error-classes.json", "additions": "64", "deletions": "0", "changes": "64"}, "updated": [1, 2, 6]}, {"file": {"name": "core/src/main/scala/org/apache/spark/SparkException.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [1, 1, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala", "additions": "25", "deletions": "25", "changes": "50"}, "updated": [1, 1, 8]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 5, 11]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 5, 9]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/regexp-functions.sql.out", "additions": "12", "deletions": "12", "changes": "24"}, "updated": [0, 0, 0]}]}
{"author": "dgd-contributor", "sha": "b658e3535c7c8e4b6b78163e5fd2069820f6256a", "commit_date": "2021/09/21 01:51:17", "commit_message": "[SPARK-36293][SQL] Refactor fourth set of 20 query execution errors to use error classes", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/resources/error/error-classes.json", "additions": "51", "deletions": "0", "changes": "51"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/AnalysisException.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala", "additions": "28", "deletions": "27", "changes": "55"}, "updated": [1, 2, 4]}]}
{"author": "dgd-contributor", "sha": "838f41303b58701ced288152bb95f8938ebacc66", "commit_date": "2021/07/28 15:38:09", "commit_message": "[SPARK-36096][CORE] Grouping exception in core/resource", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/errors/SparkCoreErrors.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [1, 1, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/resource/ResourceProfile.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/resource/ResourceUtils.scala", "additions": "5", "deletions": "4", "changes": "9"}, "updated": [0, 0, 0]}]}
{"author": "dgd-contributor", "sha": "d5f894eed43d9b0b4f41c846c7c2aca25a74c2dd", "commit_date": "2021/08/25 13:02:40", "commit_message": "[SPARK-36402][PYTHON] Implement series.combine", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_ops_on_diff_frames"], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/series.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 9]}, {"file": {"name": "python/pyspark/pandas/missing/series.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/series.py", "additions": "168", "deletions": "0", "changes": "168"}, "updated": [0, 0, 10]}, {"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames.py", "additions": "38", "deletions": "0", "changes": "38"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_series.py", "additions": "61", "deletions": "0", "changes": "61"}, "updated": [0, 1, 4]}]}
{"author": "dgd-contributor", "sha": "909e6a50094fa288da5de891abd9258d8ae05751", "commit_date": "2021/08/23 15:12:18", "commit_message": "[SPARK-36396] Implement_DataFrame.cov\n\nupdate", "title": "", "body": "", "failed_tests": ["pyspark.pandas.frame"], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/frame.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "180", "deletions": "0", "changes": "180"}, "updated": [0, 0, 13]}, {"file": {"name": "python/pyspark/pandas/missing/frame.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "57", "deletions": "1", "changes": "58"}, "updated": [0, 0, 6]}]}
{"author": "dgd-contributor", "sha": "6f699396ad96604ef69b7cc8aa2745ee470580fe", "commit_date": "2021/09/09 01:23:16", "commit_message": "[SPARK-36671][PYTHON] Support Series.__and__ and Series.__or__ for integral", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.data_type_ops.test_num_ops", "pyspark.pandas.tests.test_ops_on_diff_frames"], "files": [{"file": {"name": "python/pyspark/pandas/data_type_ops/base.py", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [1, 1, 6]}, {"file": {"name": "python/pyspark/pandas/data_type_ops/boolean_ops.py", "additions": "13", "deletions": "4", "changes": "17"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/data_type_ops/num_ops.py", "additions": "50", "deletions": "0", "changes": "50"}, "updated": [0, 0, 5]}, {"file": {"name": "python/pyspark/pandas/tests/data_type_ops/test_num_ops.py", "additions": "68", "deletions": "22", "changes": "90"}, "updated": [1, 1, 6]}, {"file": {"name": "python/pyspark/pandas/tests/data_type_ops/testing_utils.py", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames.py", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 2, 2]}]}
{"author": "dgd-contributor", "sha": "d096bd616a63d5b887a4bc05fa7967c628ab7027", "commit_date": "2021/08/17 09:04:19", "commit_message": "[SPARK-36304]: Refactor fifteenth set of 20 query execution errors to use error classes", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.util.ArrowUtilsSuite", "org.apache.spark.sql.catalyst.encoders.EncoderErrorMessageSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.execution.streaming.sources.TextSocketStreamSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.execution.streaming.sources.RateStreamProviderSuite"], "files": [{"file": {"name": "core/src/main/resources/error/error-classes.json", "additions": "76", "deletions": "2", "changes": "78"}, "updated": [1, 1, 5]}, {"file": {"name": "core/src/main/scala/org/apache/spark/SparkException.scala", "additions": "16", "deletions": "1", "changes": "17"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala", "additions": "69", "deletions": "39", "changes": "108"}, "updated": [0, 0, 9]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/RowTest.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/encoders/EncoderErrorMessageSuite.scala", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/util/ArrowUtilsSuite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/sources/RateStreamProviderSuite.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/sources/TextSocketStreamSuite.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}]}
{"author": "dgd-contributor", "sha": "711ff22baafa74f254b7feb5db3bc461fbd910a3", "commit_date": "2021/07/27 13:56:28", "commit_message": "[SPARK-36102][CORE] Grouping exception in core/deploy", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/FaultToleranceTest.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/RRunner.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/SparkSubmitArguments.scala", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/StandaloneResourceUtils.scala", "additions": "2", "deletions": "5", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/history/ApplicationCache.scala", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/history/EventLogFileWriters.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala", "additions": "7", "deletions": "11", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/master/Master.scala", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionClient.scala", "additions": "13", "deletions": "14", "changes": "27"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolException.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolMessage.scala", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolRequest.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/security/HadoopFSDelegationTokenProvider.scala", "additions": "3", "deletions": "4", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/worker/DriverRunner.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala", "additions": "5", "deletions": "4", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/worker/ui/LogPage.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/errors/SparkCoreErrors.scala", "additions": "202", "deletions": "2", "changes": "204"}, "updated": [0, 0, 0]}]}
{"author": "venkata91", "sha": "ac1659e156eca5899e1eff765698c9986eec5d4c", "commit_date": "2020/06/10 21:02:55", "commit_message": "Basic speculation metrics summary for a stage", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/stagepage.js", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/stagespage-template.html", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/webui.css", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusListener.scala", "additions": "19", "deletions": "0", "changes": "19"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusStore.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [1, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/LiveEntity.scala", "additions": "26", "deletions": "0", "changes": "26"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/api/v1/api.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/storeTypes.scala", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/application_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/completed_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/limit_app_list_json_expectation.json", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/minDate_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/minEndDate_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/stage_with_speculation_summary_expectation.json", "additions": "507", "deletions": "0", "changes": "507"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/spark-events/application_1628109047826_1317105", "additions": "52", "deletions": "0", "changes": "52"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/deploy/history/HistoryServerSuite.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 1, 4]}, {"file": {"name": "core/src/test/scala/org/apache/spark/status/AppStatusListenerSuite.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/status/AppStatusStoreSuite.scala", "additions": "56", "deletions": "1", "changes": "57"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/ui/StagePageSuite.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "dev/.rat-excludes", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 2]}]}
{"author": "venkata91", "sha": "5b611bbb0857c43747088721c48590d151361e45", "commit_date": "2021/09/02 04:53:49", "commit_message": "SPARK-33701: Adaptive shuffle merge finalization", "title": "", "body": "", "failed_tests": ["org.apache.spark.shuffle.ShuffleBlockPusherSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/Dependency.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/SparkEnv.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 1, 6]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala", "additions": "190", "deletions": "58", "changes": "248"}, "updated": [0, 0, 4]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerEvent.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessage.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [1, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/shuffle/ShuffleBlockPusher.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 7]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/ThreadUtils.scala", "additions": "9", "deletions": "1", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/DAGSchedulerSuite.scala", "additions": "161", "deletions": "3", "changes": "164"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/test/scala/org/apache/spark/shuffle/ShuffleBlockPusherSuite.scala", "additions": "34", "deletions": "0", "changes": "34"}, "updated": [0, 0, 5]}]}
{"author": "ulysses-you", "sha": "5beb51810dfec69964e570d90d0634e5a8e0499d", "commit_date": "2021/08/13 14:26:41", "commit_message": "fix", "title": "[SPARK-36321][K8S] Do not fail application in kubernetes if name is too long", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nUse short string as executor pod name prefix if app name is long.\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nIf we have a long spark app name and start with k8s master, we will get the execption.\r\n```\r\njava.lang.IllegalArgumentException: 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa-89fe2f7ae71c3570' in spark.kubernetes.executor.podNamePrefix is invalid. must conform https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#dns-label-names and the value length <= 47\r\n\tat org.apache.spark.internal.config.TypedConfigBuilder.$anonfun$checkValue$1(ConfigBuilder.scala:108)\r\n\tat org.apache.spark.internal.config.TypedConfigBuilder.$anonfun$transform$1(ConfigBuilder.scala:101)\r\n\tat scala.Option.map(Option.scala:230)\r\n\tat org.apache.spark.internal.config.OptionalConfigEntry.readFrom(ConfigEntry.scala:239)\r\n\tat org.apache.spark.internal.config.OptionalConfigEntry.readFrom(ConfigEntry.scala:214)\r\n\tat org.apache.spark.SparkConf.get(SparkConf.scala:261)\r\n\tat org.apache.spark.deploy.k8s.KubernetesConf.get(KubernetesConf.scala:67)\r\n\tat org.apache.spark.deploy.k8s.KubernetesExecutorConf.<init>(KubernetesConf.scala:147)\r\n\tat org.apache.spark.deploy.k8s.KubernetesConf$.createExecutorConf(KubernetesConf.scala:231)\r\n\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$requestNewExecutors$2(ExecutorPodsAllocator.scala:367)\r\n```\r\nUse app name as the executor pod name is the Spark internal behavior and we should not make application failure.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nyes\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nAdd test\r\n\r\nthe new log:\r\n```\r\n21/07/28 09:35:53 INFO SparkEnv: Registering OutputCommitCoordinator\r\n21/07/28 09:35:54 INFO Utils: Successfully started service 'SparkUI' on port 41926.\r\n21/07/28 09:35:54 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://:41926\r\n21/07/28 09:35:54 WARN KubernetesClusterManager: Use spark-c460617aeac0fda9 as the executor pod's name prefix due to spark.app.name is too long. Please set 'spark.kubernetes.executor.podNamePrefix' if you need a custom executor pod's name prefix.\r\n21/07/28 09:35:54 INFO SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file\r\n21/07/28 09:35:55 WARN Utils: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.\r\n```\r\n\r\nverify the config:\r\n![image](https://user-images.githubusercontent.com/12025282/127258223-fbcaaac8-451d-4c55-8c09-e802511a510d.png)\r\n\r\nverify the executor pod name\r\n![image](https://user-images.githubusercontent.com/12025282/127258284-be15b862-b826-4440-9a11-023d69c61fc4.png)\r\n", "failed_tests": [], "files": [{"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPi.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPiWithoutAppName.scala", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "4", "deletions": "14", "changes": "18"}, "updated": [1, 1, 5]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala", "additions": "19", "deletions": "7", "changes": "26"}, "updated": [0, 0, 3]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesUtils.scala", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala", "additions": "10", "deletions": "6", "changes": "16"}, "updated": [0, 0, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterManager.scala", "additions": "10", "deletions": "2", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/KubernetesConfSuite.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 0, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 3]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/BasicTestsSuite.scala", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 1, 1]}]}
{"author": "ulysses-you", "sha": "4ed226f66643523f3661b53a28517383cf1f0eb5", "commit_date": "2021/09/22 06:39:11", "commit_message": "Support broadcast nested loop join hint for equi-join", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "docs/sql-ref-syntax-qry-select-hints.md", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/hints.scala", "additions": "10", "deletions": "1", "changes": "11"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "14", "deletions": "2", "changes": "16"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/LogicalQueryStageStrategy.scala", "additions": "12", "deletions": "6", "changes": "18"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/JoinHintSuite.scala", "additions": "38", "deletions": "0", "changes": "38"}, "updated": [0, 0, 2]}]}
{"author": "ulysses-you", "sha": "a846ecd5221bc4b21416c9c52552cdaa0e683d0d", "commit_date": "2021/04/07 10:02:05", "commit_message": "Support coalesce partition through union", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.execution.CoalesceShufflePartitionsSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "78", "deletions": "52", "changes": "130"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/CoalesceShufflePartitionsSuite.scala", "additions": "1", "deletions": "3", "changes": "4"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "86", "deletions": "1", "changes": "87"}, "updated": [0, 1, 5]}]}
{"author": "xinrong-databricks", "sha": "8b7b395c0faa78d8809489eaf6f88a013fcfe2a0", "commit_date": "2021/08/10 21:37:08", "commit_message": "empty", "title": "[WIP][SPARK-36397][PYTHON] Implement DataFrame.mode", "body": "### What changes were proposed in this pull request?\r\nImplement DataFrame.mode (along index axis).\r\n\r\n\r\n### Why are the changes needed?\r\nGet the mode(s) of each element along the selected axis is a common functionality, which is supported in pandas. We should support that.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes. `DataFrame.mode` can be used now.\r\n\r\n```py\r\n>>> psdf = ps.DataFrame(\r\n...     [(\"bird\", 2, 2), (\"mammal\", 4, np.nan), (\"arthropod\", 8, 0), (\"bird\", 2, np.nan)],\r\n...     index=(\"falcon\", \"horse\", \"spider\", \"ostrich\"),\r\n...     columns=(\"species\", \"legs\", \"wings\"),\r\n... )\r\n>>> psdf\r\n           species  legs  wings                                                 \r\nfalcon        bird     2    2.0\r\nhorse       mammal     4    NaN\r\nspider   arthropod     8    0.0\r\nostrich       bird     2    NaN\r\n\r\n>>> psdf.mode()\r\n  species  legs  wings\r\n0    bird   2.0    0.0\r\n1    None   NaN    2.0\r\n\r\n>>> psdf.mode(dropna=False)\r\n  species  legs  wings\r\n0    bird     2    NaN\r\n\r\n>>> psdf.mode(numeric_only=True)\r\n   legs  wings\r\n0   2.0    0.0\r\n1   NaN    2.0\r\n```\r\n\r\n### How was this patch tested?\r\nUnit tests.\r\n", "failed_tests": ["pyspark.pandas.tests.test_dataframe"], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/frame.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "103", "deletions": "1", "changes": "104"}, "updated": [0, 5, 20]}, {"file": {"name": "python/pyspark/pandas/missing/frame.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [0, 2, 6]}]}
{"author": "xinrong-databricks", "sha": "87172874344406fac65abb26f072c06855674b7f", "commit_date": "2021/08/31 22:56:40", "commit_message": "test case", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "74", "deletions": "18", "changes": "92"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}]}
{"author": "viirya", "sha": "190fa2b796454125d83a90309b17a1f970e90fe0", "commit_date": "2021/09/20 16:46:14", "commit_message": "Remove unnecessary broadcast check.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "13", "deletions": "0", "changes": "13"}, "updated": [0, 2, 8]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/PlanAdaptiveDynamicPruningFilters.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/PlanAdaptiveSubqueries.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/dynamicpruning/PartitionPruning.scala", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 2, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala", "additions": "14", "deletions": "9", "changes": "23"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DynamicPartitionPruningSuite.scala", "additions": "6", "deletions": "5", "changes": "11"}, "updated": [0, 2, 3]}]}
{"author": "yaooqinn", "sha": "9e8b227e91070c35b98247acdbf4caf022cfaf72", "commit_date": "2021/08/20 13:52:21", "commit_message": "address comments", "title": "[SPARK-36477][SQL] Inferring schema from JSON file shall handle CharConversionException/MalformedInputException", "body": "\r\n<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\nWhen set `ignoreCorruptFiles=true`, reading JSON still fails with corrupt files during inferring schema.\r\n\r\n```scala\r\njava.io.CharConversionException: Unsupported UCS-4 endianness (2143) detected\r\n\tat com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.reportWeirdUCS4(ByteSourceJsonBootstrapper.java:504)\r\n\tat com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.checkUTF32(ByteSourceJsonBootstrapper.java:471)\r\n\tat com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.detectEncoding(ByteSourceJsonBootstrapper.java:144)\r\n\tat com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.constructParser(ByteSourceJsonBootstrapper.java:247)\r\n\tat com.fasterxml.jackson.core.JsonFactory._createParser(JsonFactory.java:1528)\r\n\tat com.fasterxml.jackson.core.JsonFactory.createParser(JsonFactory.java:1034)\r\n\tat org.apache.spark.sql.catalyst.json.CreateJacksonParser$.internalRow(CreateJacksonParser.scala:86)\r\n\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.$anonfun$inferFromDataset$4(JsonDataSource.scala:107)\r\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$3(JsonInferSchema.scala:66)\r\n\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2621)\r\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$2(JsonInferSchema.scala:66)\r\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\r\n\tat scala.collection.TraversableOnce.reduceLeft(TraversableOnce.scala:190)\r\n\tat scala.collection.TraversableOnce.reduceLeft$(TraversableOnce.scala:183)\r\n\tat scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1429)\r\n\tat scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:208)\r\n\tat scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:207)\r\n\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1429)\r\n\tat scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:215)\r\n\tat scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:215)\r\n\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1429)\r\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:81)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n```\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\nbugfix\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nyes, ignoreCorruptFiles will ignore JSON files corrupted\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\n\r\nnew test", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JsonInferSchema.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/json/JsonSuite.scala", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [1, 1, 4]}]}
{"author": "yaooqinn", "sha": "623dc4659e016505d1245bf7637c12d499aa947d", "commit_date": "2021/08/11 04:22:48", "commit_message": "Update sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala\n\nCo-authored-by: Hyukjin Kwon <gurwls223@gmail.com>", "title": "[SPARK-36180][SQL] Store TIMESTAMP_NTZ into hive catalog as TIMESTAMP", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\nThis PR fix a issue that HMS can not recognize timestamp_ntz by mapping timestamp_ntz to `timestamp` of hive\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\nThe hive 2.3.9 does not have 2 timestamp or a type named timestamp_ntz.\r\nFYI, In hive 3.0, the will be a timestamp with local timezone added.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\nno, timestamp_ntz is new and not public yet\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\n\r\nnew test", "failed_tests": [], "files": [{"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala", "additions": "15", "deletions": "14", "changes": "29"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/SQLQuerySuite.scala", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [0, 1, 2]}]}
{"author": "yaooqinn", "sha": "a3b7d08983115c84225ce52f8db3dd16efd5471e", "commit_date": "2021/09/03 07:38:14", "commit_message": "[SPARK-36662][SQL] special timestamps support for path filters -  modifiedBefore/modifiedAfter", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "docs/sql-data-sources-generic-options.md", "additions": "11", "deletions": "4", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 2, 12]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/pathFilters.scala", "additions": "6", "deletions": "3", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/PathFilterSuite.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [0, 0, 0]}]}
{"author": "yaooqinn", "sha": "b989aa3821dbaf6cacccde886ed1051068d43cf5", "commit_date": "2021/09/01 08:19:44", "commit_message": "[SPARK-36634][SQL] Support access and read parquet file by column index", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.execution.datasources.parquet.ParquetV2SchemaPruningSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV1SchemaPruningSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 1, 11]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport.scala", "additions": "54", "deletions": "21", "changes": "75"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRecordMaterializer.scala", "additions": "6", "deletions": "2", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "27", "deletions": "13", "changes": "40"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "85", "deletions": "0", "changes": "85"}, "updated": [0, 0, 0]}]}
{"author": "LuciferYang", "sha": "a7eff43e2f65385754e2accc016bc51281f4c594", "commit_date": "2021/08/16 06:24:44", "commit_message": "support orc file meta cache", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "36", "deletions": "0", "changes": "36"}, "updated": [0, 3, 17]}, {"file": {"name": "sql/core/benchmarks/FileMetaCacheReadBenchmark-jdk11-results.txt", "additions": "95", "deletions": "0", "changes": "95"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/benchmarks/FileMetaCacheReadBenchmark-results.txt", "additions": "95", "deletions": "0", "changes": "95"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnarBatchReader.java", "additions": "9", "deletions": "1", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileMetaCacheManager.scala", "additions": "94", "deletions": "0", "changes": "94"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFileFormat.scala", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [0, 2, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFileMeta.scala", "additions": "46", "deletions": "0", "changes": "46"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/orc/OrcPartitionReaderFactory.scala", "additions": "18", "deletions": "3", "changes": "21"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/FileMetaCacheSuite.scala", "additions": "80", "deletions": "0", "changes": "80"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/benchmark/FileMetaCacheReadBenchmark.scala", "additions": "128", "deletions": "0", "changes": "128"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/internal/SQLConfSuite.scala", "additions": "24", "deletions": "0", "changes": "24"}, "updated": [0, 1, 1]}]}
{"author": "LuciferYang", "sha": "f200546c1e6df47210d9d68bb55d11a9cc529035", "commit_date": "2021/08/26 07:59:24", "commit_message": "add Guava cache bad case", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ExternalShuffleBlockResolver.java", "additions": "8", "deletions": "1", "changes": "9"}, "updated": [0, 1, 3]}, {"file": {"name": "common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/RemoteBlockPushResolver.java", "additions": "7", "deletions": "1", "changes": "8"}, "updated": [0, 2, 8]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileStatusCache.scala", "additions": "9", "deletions": "1", "changes": "10"}, "updated": [0, 1, 2]}]}
{"author": "LuciferYang", "sha": "cc6f52ec8272d3c10a3641a495ee4be49408d58f", "commit_date": "2021/08/04 03:27:21", "commit_message": "add a new method to avoid file truncate", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java", "additions": "1", "deletions": "4", "changes": "5"}, "updated": [1, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/storage/DiskBlockObjectWriter.scala", "additions": "21", "deletions": "0", "changes": "21"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala", "additions": "1", "deletions": "6", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala", "additions": "1", "deletions": "6", "changes": "7"}, "updated": [1, 2, 3]}]}
{"author": "LuciferYang", "sha": "7d0a912471047a58b8c40240e74805b69c78b758", "commit_date": "2021/09/14 06:09:47", "commit_message": "Replace some guava api usage with j.u.Objects api", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "common/kvstore/src/main/java/org/apache/spark/util/kvstore/InMemoryStore.java", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "common/kvstore/src/main/java/org/apache/spark/util/kvstore/KVStoreView.java", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "common/kvstore/src/main/java/org/apache/spark/util/kvstore/LevelDBTypeInfo.java", "additions": "6", "deletions": "4", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "common/network-common/src/main/java/org/apache/spark/network/client/TransportClient.java", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 1]}, {"file": {"name": "common/network-common/src/main/java/org/apache/spark/network/client/TransportClientFactory.java", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "common/network-common/src/main/java/org/apache/spark/network/crypto/AuthEngine.java", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 2]}, {"file": {"name": "common/network-common/src/main/java/org/apache/spark/network/protocol/AbstractMessage.java", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "common/network-common/src/main/java/org/apache/spark/network/protocol/MergedBlockMetaRequest.java", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [0, 0, 1]}, {"file": {"name": "common/network-common/src/main/java/org/apache/spark/network/protocol/MergedBlockMetaSuccess.java", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "common/network-common/src/main/java/org/apache/spark/network/sasl/SparkSaslServer.java", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "common/network-common/src/main/java/org/apache/spark/network/server/BlockPushNonFatalFailure.java", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 2]}, {"file": {"name": "common/network-common/src/main/java/org/apache/spark/network/server/OneForOneStreamManager.java", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 1]}, {"file": {"name": "common/network-common/src/main/java/org/apache/spark/network/server/TransportServer.java", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "common/network-common/src/main/java/org/apache/spark/network/util/JavaUtils.java", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "common/network-common/src/main/java/org/apache/spark/network/util/LimitedInputStream.java", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ExternalBlockHandler.java", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 4]}, {"file": {"name": "common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/MergedBlockMeta.java", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/protocol/AbstractFetchShuffleBlocks.java", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/protocol/BlockPushReturnCode.java", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 0, 1]}, {"file": {"name": "common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/protocol/FinalizeShuffleMerge.java", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/protocol/MergeStatuses.java", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 1]}, {"file": {"name": "common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/protocol/PushBlockStream.java", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/protocol/mesos/RegisterDriver.java", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "common/network-yarn/src/main/java/org/apache/spark/network/yarn/YarnShuffleService.java", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/java/org/apache/spark/api/java/Optional.java", "additions": "2", "deletions": "4", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/rdd/RDDOperationScope.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "mllib/src/main/scala/org/apache/spark/mllib/linalg/Matrices.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "mllib/src/main/scala/org/apache/spark/mllib/linalg/distributed/BlockMatrix.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "mllib/src/main/scala/org/apache/spark/mllib/tree/model/InformationGainStats.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "mllib/src/main/scala/org/apache/spark/mllib/tree/model/Predict.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala", "additions": "2", "deletions": "3", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/IdentifierImpl.java", "additions": "2", "deletions": "4", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/BatchScanExec.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/java/test/org/apache/spark/sql/JavaDatasetSuite.java", "additions": "12", "deletions": "13", "changes": "25"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveShim.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}]}
{"author": "LuciferYang", "sha": "104b1256a23300b7f7912c7bf37fc7b14ac5099c", "commit_date": "2020/11/24 11:39:06", "commit_message": "add simple support for Parquet", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.execution.datasources.parquet.ParquetV2QuerySuite", "org.apache.spark.sql.execution.datasources.orc.OrcV2QuerySuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [1, 10, 27]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase.java", "additions": "38", "deletions": "8", "changes": "46"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileMetaCacheManager.scala", "additions": "87", "deletions": "0", "changes": "87"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala", "additions": "15", "deletions": "1", "changes": "16"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileMeta.scala", "additions": "45", "deletions": "0", "changes": "45"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetPartitionReaderFactory.scala", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "34", "deletions": "1", "changes": "35"}, "updated": [0, 0, 0]}]}
{"author": "dnskr", "sha": "7d4d4e39a31a815f97b55228f1123c7f0edb2afa", "commit_date": "2021/08/14 10:33:16", "commit_message": "Merge branch 'apache:master' into docs/SPARK-36510", "title": "[SPARK-36510][DOCS] Add spark.redaction.string.regex property to the docs", "body": "### What changes were proposed in this pull request?\r\nThe PR fixes [SPARK-36510](https://issues.apache.org/jira/browse/SPARK-36510) by adding missing `spark.redaction.string.regex` property to the docs\r\n\r\n### Why are the changes needed?\r\nThe property referred by `spark.sql.redaction.string.regex` description as its default value\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\nNot needed for docs\r\n", "failed_tests": [], "files": [{"file": {"name": "docs/configuration.md", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [1, 2, 3]}]}
{"author": "ocworld", "sha": "554127b11daaab6377e69cff7956f5c6c38c2d62", "commit_date": "2021/04/29 12:38:14", "commit_message": "[SPARK-35084] supporting \"--packages\" in k8s cluster mode", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala", "additions": "12", "deletions": "11", "changes": "23"}, "updated": [1, 3, 10]}]}
{"author": "beliefer", "sha": "7df29e5120f2473579f65e6ddab89cd878ca7ca0", "commit_date": "2021/09/01 08:45:24", "commit_message": "DivideYMInterval should consider ansi mode.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/intervalExpressions.scala", "additions": "45", "deletions": "10", "changes": "55"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 10]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 9]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/ColumnExpressionSuite.scala", "additions": "26", "deletions": "2", "changes": "28"}, "updated": [0, 0, 0]}]}
{"author": "beliefer", "sha": "fc088707c35365c8f6c9c804f73b4b7374a6415a", "commit_date": "2021/07/30 11:15:07", "commit_message": "Support TimestampNTZ type in Orc file source", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.orc.HiveOrcQuerySuite", "pyspark.pandas.mlflow"], "files": [{"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcAtomicColumnVector.java", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcDeserializer.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFileFormat.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcOutputWriter.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcSerializer.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcUtils.scala", "additions": "47", "deletions": "15", "changes": "62"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/package.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/FileBasedDataSourceSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcQuerySuite.scala", "additions": "42", "deletions": "0", "changes": "42"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/orc/OrcFileFormat.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/orc/HiveOrcSourceSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 0]}]}
{"author": "beliefer", "sha": "86ea00d27e9979d6057a39ac7c0711131a6b6c26", "commit_date": "2021/07/27 07:37:12", "commit_message": "Refactor first set of 20 query parsing errors to use error classes", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.parser.DDLParserSuite"], "files": [{"file": {"name": "core/src/main/resources/error/error-classes.json", "additions": "52", "deletions": "0", "changes": "52"}, "updated": [0, 2, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala", "additions": "0", "deletions": "2", "changes": "2"}, "updated": [0, 2, 22]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala", "additions": "22", "deletions": "29", "changes": "51"}, "updated": [0, 2, 7]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisTest.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala", "additions": "8", "deletions": "3", "changes": "11"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/ErrorParserSuite.scala", "additions": "26", "deletions": "9", "changes": "35"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala", "additions": "15", "deletions": "8", "changes": "23"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/SparkScriptTransformationSuite.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/ShowPartitionsParserSuite.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/TruncateTableParserSuite.scala", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [0, 0, 0]}]}
{"author": "wangyum", "sha": "3e482ffa6a779aeebe67494f9a766d17ac2e2899", "commit_date": "2021/09/24 06:32:59", "commit_message": "Support DPP if there is no selective predicate on the filtering side", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.TPCDSModifiedPlanStabilitySuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilityWithStatsSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/dynamicpruning/PartitionPruning.scala", "additions": "9", "deletions": "14", "changes": "23"}, "updated": [0, 2, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DynamicPartitionPruningSuite.scala", "additions": "27", "deletions": "0", "changes": "27"}, "updated": [0, 2, 3]}]}
{"author": "wangyum", "sha": "83a4dbdd610d0c35e4e460eea58edcf7776daee5", "commit_date": "2021/09/09 13:38:17", "commit_message": "Remove the Sort if it is the child of RepartitionByExpression", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 10]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/CollapseRepartitionSuite.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 0, 0]}]}
{"author": "wangyum", "sha": "1929f30e3335b90b9e78357f76e247167e69b901", "commit_date": "2021/09/05 09:17:19", "commit_message": "Dynamic Bloom Filter pruning", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite"], "files": [{"file": {"name": "common/sketch/src/main/java/org/apache/spark/util/sketch/BloomFilterImpl.java", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 2, 19]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/DynamicPruning.scala", "additions": "58", "deletions": "1", "changes": "59"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/BuildBloomFilter.scala", "additions": "127", "deletions": "0", "changes": "127"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/predicates.scala", "additions": "76", "deletions": "0", "changes": "76"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 2, 10]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "26", "deletions": "0", "changes": "26"}, "updated": [1, 1, 12]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [1, 2, 6]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SubqueryAdaptiveShuffleExec.scala", "additions": "49", "deletions": "0", "changes": "49"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AQEOptimizer.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/InsertAdaptiveSparkPlan.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeBloomFilterJoin.scala", "additions": "48", "deletions": "0", "changes": "48"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/PlanAdaptiveDynamicPruningFilters.scala", "additions": "83", "deletions": "41", "changes": "124"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/PlanAdaptiveSubqueries.scala", "additions": "5", "deletions": "3", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/dynamicpruning/DynamicBloomFilterPruning.scala", "additions": "191", "deletions": "0", "changes": "191"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/dynamicpruning/DynamicPruningHelper.scala", "additions": "107", "deletions": "0", "changes": "107"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/dynamicpruning/PartitionPruning.scala", "additions": "8", "deletions": "70", "changes": "78"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/dynamicpruning/PlanDynamicPruningFilters.scala", "additions": "8", "deletions": "3", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala", "additions": "57", "deletions": "1", "changes": "58"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q12.sf100/explain.txt", "additions": "121", "deletions": "93", "changes": "214"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q12.sf100/simplified.txt", "additions": "11", "deletions": "6", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q20.sf100/explain.txt", "additions": "121", "deletions": "93", "changes": "214"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q20.sf100/simplified.txt", "additions": "11", "deletions": "6", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q37.sf100/explain.txt", "additions": "37", "deletions": "9", "changes": "46"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q37.sf100/simplified.txt", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q50.sf100/explain.txt", "additions": "121", "deletions": "37", "changes": "158"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q50.sf100/simplified.txt", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q81.sf100/explain.txt", "additions": "128", "deletions": "100", "changes": "228"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q81.sf100/simplified.txt", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q82.sf100/explain.txt", "additions": "37", "deletions": "9", "changes": "46"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q82.sf100/simplified.txt", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q93.sf100/explain.txt", "additions": "98", "deletions": "34", "changes": "132"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q93.sf100/simplified.txt", "additions": "14", "deletions": "3", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q98.sf100/explain.txt", "additions": "129", "deletions": "101", "changes": "230"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q98.sf100/simplified.txt", "additions": "11", "deletions": "6", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q12.sf100/explain.txt", "additions": "121", "deletions": "93", "changes": "214"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q12.sf100/simplified.txt", "additions": "11", "deletions": "6", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q20.sf100/explain.txt", "additions": "121", "deletions": "93", "changes": "214"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q20.sf100/simplified.txt", "additions": "11", "deletions": "6", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q98.sf100/explain.txt", "additions": "125", "deletions": "97", "changes": "222"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q98.sf100/simplified.txt", "additions": "11", "deletions": "6", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DynamicBloomFilterJoinPruningSuite.scala", "additions": "225", "deletions": "0", "changes": "225"}, "updated": [0, 0, 0]}]}
{"author": "wangyum", "sha": "3643e3f26ac8755a4da1e83b36e246cb16b48275", "commit_date": "2020/05/26 09:42:15", "commit_message": "Infer IsNotNull for all children of NullIntolerant expression", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "38", "deletions": "22", "changes": "60"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/InferFiltersFromConstraintsSuite.scala", "additions": "32", "deletions": "11", "changes": "43"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q59.sf100/explain.txt", "additions": "98", "deletions": "80", "changes": "178"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q59.sf100/simplified.txt", "additions": "8", "deletions": "3", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q59/explain.txt", "additions": "98", "deletions": "80", "changes": "178"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q59/simplified.txt", "additions": "8", "deletions": "3", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q2.sf100/explain.txt", "additions": "133", "deletions": "54", "changes": "187"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q2.sf100/simplified.txt", "additions": "25", "deletions": "2", "changes": "27"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q2/explain.txt", "additions": "133", "deletions": "54", "changes": "187"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q2/simplified.txt", "additions": "25", "deletions": "2", "changes": "27"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24a.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24a/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24b.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24b/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q59.sf100/explain.txt", "additions": "133", "deletions": "73", "changes": "206"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q59.sf100/simplified.txt", "additions": "18", "deletions": "3", "changes": "21"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q59/explain.txt", "additions": "133", "deletions": "73", "changes": "206"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q59/simplified.txt", "additions": "18", "deletions": "3", "changes": "21"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q8.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q8/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q24.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q24/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/JoinSuite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}
{"author": "wangyum", "sha": "824ba80a0a71bb8c5079c45ec5de6bc9ae198699", "commit_date": "2021/07/10 12:43:25", "commit_message": "First commit", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.JoinSuite", "org.apache.spark.sql.streaming.StreamingOuterJoinSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.streaming.StreamingLeftSemiJoinSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/BroadcastJoinOuterJoinStreamSide.scala", "additions": "48", "deletions": "0", "changes": "48"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 7]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 2, 9]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/BroadcastJoinOuterJoinStreamSideSuite.scala", "additions": "108", "deletions": "0", "changes": "108"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 1, 6]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a.sf100/explain.txt", "additions": "634", "deletions": "533", "changes": "1167"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a.sf100/simplified.txt", "additions": "97", "deletions": "70", "changes": "167"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b.sf100/explain.txt", "additions": "569", "deletions": "468", "changes": "1037"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b.sf100/simplified.txt", "additions": "90", "deletions": "63", "changes": "153"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23a/explain.txt", "additions": "275", "deletions": "245", "changes": "520"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23a/simplified.txt", "additions": "75", "deletions": "67", "changes": "142"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23b/explain.txt", "additions": "364", "deletions": "319", "changes": "683"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23b/simplified.txt", "additions": "93", "deletions": "81", "changes": "174"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q40/explain.txt", "additions": "123", "deletions": "108", "changes": "231"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q40/simplified.txt", "additions": "17", "deletions": "13", "changes": "30"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q72/explain.txt", "additions": "65", "deletions": "50", "changes": "115"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q72/simplified.txt", "additions": "13", "deletions": "9", "changes": "22"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q80/explain.txt", "additions": "462", "deletions": "417", "changes": "879"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q80/simplified.txt", "additions": "65", "deletions": "53", "changes": "118"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14.sf100/explain.txt", "additions": "569", "deletions": "468", "changes": "1037"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14.sf100/simplified.txt", "additions": "90", "deletions": "63", "changes": "153"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a.sf100/explain.txt", "additions": "778", "deletions": "677", "changes": "1455"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a.sf100/simplified.txt", "additions": "114", "deletions": "87", "changes": "201"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q72/explain.txt", "additions": "65", "deletions": "50", "changes": "115"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q72/simplified.txt", "additions": "13", "deletions": "9", "changes": "22"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q75/explain.txt", "additions": "614", "deletions": "455", "changes": "1069"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q75/simplified.txt", "additions": "108", "deletions": "66", "changes": "174"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q78/explain.txt", "additions": "320", "deletions": "275", "changes": "595"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q78/simplified.txt", "additions": "51", "deletions": "39", "changes": "90"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q80a/explain.txt", "additions": "532", "deletions": "487", "changes": "1019"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q80a/simplified.txt", "additions": "73", "deletions": "61", "changes": "134"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala", "additions": "21", "deletions": "1", "changes": "22"}, "updated": [0, 3, 8]}]}
{"author": "wangyum", "sha": "65df34649227d1f04065cc76a49778770b433536", "commit_date": "2021/07/21 14:51:57", "commit_message": "Deduplicate the right side of left semi/anti join", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/DeduplicateRightSideOfLeftSemiAntiJoin.scala", "additions": "56", "deletions": "0", "changes": "56"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [2, 2, 13]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/Statistics.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/DeduplicateRightSideOfLeftSemiAntiJoinSuite.scala", "additions": "98", "deletions": "0", "changes": "98"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AQEOptimizer.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/QueryStageExec.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q10.sf100/explain.txt", "additions": "119", "deletions": "103", "changes": "222"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q10.sf100/simplified.txt", "additions": "63", "deletions": "63", "changes": "126"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q10.sf100/explain.txt", "additions": "220", "deletions": "204", "changes": "424"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q10.sf100/simplified.txt", "additions": "48", "deletions": "48", "changes": "96"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a.sf100/explain.txt", "additions": "614", "deletions": "646", "changes": "1260"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a.sf100/simplified.txt", "additions": "143", "deletions": "185", "changes": "328"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b.sf100/explain.txt", "additions": "559", "deletions": "576", "changes": "1135"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b.sf100/simplified.txt", "additions": "151", "deletions": "184", "changes": "335"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q35.sf100/explain.txt", "additions": "230", "deletions": "214", "changes": "444"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q35.sf100/simplified.txt", "additions": "51", "deletions": "51", "changes": "102"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q69.sf100/explain.txt", "additions": "221", "deletions": "183", "changes": "404"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q69.sf100/simplified.txt", "additions": "56", "deletions": "60", "changes": "116"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q95.sf100/explain.txt", "additions": "134", "deletions": "92", "changes": "226"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q95.sf100/simplified.txt", "additions": "42", "deletions": "36", "changes": "78"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q10a.sf100/explain.txt", "additions": "199", "deletions": "183", "changes": "382"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q10a.sf100/simplified.txt", "additions": "42", "deletions": "42", "changes": "84"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14.sf100/explain.txt", "additions": "559", "deletions": "576", "changes": "1135"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14.sf100/simplified.txt", "additions": "151", "deletions": "184", "changes": "335"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a.sf100/explain.txt", "additions": "742", "deletions": "774", "changes": "1516"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a.sf100/simplified.txt", "additions": "158", "deletions": "200", "changes": "358"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35.sf100/explain.txt", "additions": "230", "deletions": "214", "changes": "444"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35.sf100/simplified.txt", "additions": "51", "deletions": "51", "changes": "102"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35a.sf100/explain.txt", "additions": "219", "deletions": "203", "changes": "422"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35a.sf100/simplified.txt", "additions": "49", "deletions": "49", "changes": "98"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [1, 5, 17]}]}
{"author": "wangyum", "sha": "07eda27de9622d97f807a7891af824e203e6b8ff", "commit_date": "2021/07/26 15:19:57", "commit_message": "Push down join condition evaluation", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.optimizer.PushDownJoinConditionEvaluationSuite", "org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite", "org.apache.spark.sql.streaming.StreamingOuterJoinSuite", "org.apache.spark.sql.streaming.StreamingInnerJoinSuite", "org.apache.spark.sql.streaming.StreamingLeftSemiJoinSuite", "org.apache.spark.sql.streaming.StreamingFullOuterJoinSuite", "org.apache.spark.sql.execution.PlannerSuite", "org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 3, 13]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/PullOutJoinCondition.scala", "additions": "75", "deletions": "0", "changes": "75"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/PullOutJoinConditionSuite.scala", "additions": "92", "deletions": "0", "changes": "92"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q58.sf100/explain.txt", "additions": "289", "deletions": "278", "changes": "567"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q58.sf100/simplified.txt", "additions": "24", "deletions": "20", "changes": "44"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q58/explain.txt", "additions": "278", "deletions": "267", "changes": "545"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q58/simplified.txt", "additions": "32", "deletions": "28", "changes": "60"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q83.sf100/explain.txt", "additions": "278", "deletions": "264", "changes": "542"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q83.sf100/simplified.txt", "additions": "27", "deletions": "23", "changes": "50"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q83/explain.txt", "additions": "259", "deletions": "245", "changes": "504"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q83/simplified.txt", "additions": "33", "deletions": "29", "changes": "62"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/JoinSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/PlannerSuite.scala", "additions": "0", "deletions": "8", "changes": "8"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/WholeStageCodegenSuite.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}]}
{"author": "wangyum", "sha": "eb71b8ae8de1fb737eea170e920c24127bcc2b95", "commit_date": "2021/07/17 15:56:31", "commit_message": "Remove the aggregation from left semi/anti join if the same aggregation has already been done on left side", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RemoveRedundantAggregates.scala", "additions": "6", "deletions": "2", "changes": "8"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/DistinctAttributesVisitor.scala", "additions": "100", "deletions": "0", "changes": "100"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/LogicalPlan.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/LogicalPlanDistinctAttributes.scala", "additions": "33", "deletions": "0", "changes": "33"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/RemoveRedundantAggregatesSuite.scala", "additions": "110", "deletions": "11", "changes": "121"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/plans/logical/DistinctAttributesVisitorSuite.scala", "additions": "112", "deletions": "0", "changes": "112"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a.sf100/explain.txt", "additions": "460", "deletions": "481", "changes": "941"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a.sf100/simplified.txt", "additions": "113", "deletions": "118", "changes": "231"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a/explain.txt", "additions": "229", "deletions": "245", "changes": "474"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a/simplified.txt", "additions": "62", "deletions": "64", "changes": "126"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b.sf100/explain.txt", "additions": "393", "deletions": "414", "changes": "807"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b.sf100/simplified.txt", "additions": "106", "deletions": "111", "changes": "217"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b/explain.txt", "additions": "213", "deletions": "229", "changes": "442"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b/simplified.txt", "additions": "62", "deletions": "64", "changes": "126"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q38.sf100/explain.txt", "additions": "143", "deletions": "190", "changes": "333"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q38.sf100/simplified.txt", "additions": "112", "deletions": "125", "changes": "237"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q38/explain.txt", "additions": "79", "deletions": "106", "changes": "185"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q38/simplified.txt", "additions": "60", "deletions": "63", "changes": "123"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q87.sf100/explain.txt", "additions": "143", "deletions": "190", "changes": "333"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q87.sf100/simplified.txt", "additions": "112", "deletions": "125", "changes": "237"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q87/explain.txt", "additions": "79", "deletions": "106", "changes": "185"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q87/simplified.txt", "additions": "60", "deletions": "63", "changes": "123"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14.sf100/explain.txt", "additions": "393", "deletions": "414", "changes": "807"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14.sf100/simplified.txt", "additions": "106", "deletions": "111", "changes": "217"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14/explain.txt", "additions": "213", "deletions": "229", "changes": "442"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14/simplified.txt", "additions": "62", "deletions": "64", "changes": "126"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a.sf100/explain.txt", "additions": "602", "deletions": "623", "changes": "1225"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a.sf100/simplified.txt", "additions": "128", "deletions": "133", "changes": "261"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a/explain.txt", "additions": "279", "deletions": "295", "changes": "574"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a/simplified.txt", "additions": "62", "deletions": "64", "changes": "126"}, "updated": [0, 0, 3]}]}
{"author": "xuzikun2003", "sha": "876c38309d42d3dddb23b5695a363cf517860ebf", "commit_date": "2021/08/19 16:24:47", "commit_message": "Merge branch 'master' of https://github.com/xuzikun2003/spark into Victor/Cp_fix", "title": "[SPARK-36493] Skip Retrieving keytab with SparkFiles.get if keytab found in the CWD of Yarn Container", "body": "### Why are the changes needed?\r\nCurrently we have the following logic to deal with the JDBC keytab provided by the `--files` option\r\n\r\n`if (keytabParam != null && FilenameUtils.getPath(keytabParam).isEmpty)`\r\n`{`     \r\n`    ` `val result = SparkFiles.get(keytabParam)`     \r\n`    ` `logDebug(s\"Keytab path not found, assuming --files, file name used on executor: $result\")`     \r\n`    ` `result` \r\n`}` ` else {`\r\n`    ` `logDebug(\"Keytab path found, assuming manual upload\")`\r\n`    ` `keytabParam`\r\n`}`\r\n\r\n\r\n\r\nSpark has already created the soft link in the current working directory of driver (application master) for the file submitted by the `--files` option in cluster mode. Here is an example.\r\n`testusera1.keytab -> /var/opt/hadoop/temp/nm-local-dir/usercache/testusera1/appcache/application_1628584679772_0003/filecache/12/testusera1.keytab`\r\n\r\n\r\n\r\nMoreover, SparkFiles.get will get a wrong path of keytab for the driver in cluster mode. In cluster mode, the keytab is available at the following location for both the driver and executors\r\n`/var/opt/hadoop/temp/nm-local-dir/usercache/testusera1/appcache/application_1628584679772_0003/container_1628584679772_0030_01_000001/testusera1.keytab`\r\nwhile `SparkFiles.get` brings the following wrong location\r\n`/var/opt/hadoop/temp/nm-local-dir/usercache/testusera1/appcache/application_1628584679772_0003/spark-8fb0f437-c842-4a9f-9612-39de40082e40/userFiles-5075388b-0928-4bc3-a498-7f6c84b27808/testusera1.keytab`\r\n\r\nSo there is no need to call the function `SparkFiles.get` to get the absolute path of the keytab file if it exists at the current working directory of Yarn container. We can directly use the variable `keytabParam` as the keytab file path.\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nWe need to check whether the keytab exists in the current directory. If it is in the current directory, we do not need to call SparkFiles.get to obtain the file. To check whether the keytab exists in the current directory, we use the function call `!Files.exists(Paths.get(keytabParam))`.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nNo\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nExisting unit tests and manual integration tests.", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 0, 2]}]}
{"author": "cxzl25", "sha": "bec1e6d7ba85b3486ca7b2e16ecd453c1b98dd6e", "commit_date": "2021/09/18 17:24:39", "commit_message": "Pass queryExecution name in CLI when only select query.", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkSQLDriver.scala", "additions": "6", "deletions": "2", "changes": "8"}, "updated": [0, 0, 0]}]}
{"author": "cxzl25", "sha": "7ec3ccce0e3b7ac445b7ce8224c2eac9bab420bd", "commit_date": "2021/06/28 11:49:59", "commit_message": "fix conflicts", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/ExternalCatalogUtils.scala", "additions": "27", "deletions": "17", "changes": "44"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "13", "deletions": "0", "changes": "13"}, "updated": [1, 2, 12]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 7]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveShim.scala", "additions": "68", "deletions": "11", "changes": "79"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/client/HivePartitionFilteringSuite.scala", "additions": "58", "deletions": "1", "changes": "59"}, "updated": [0, 0, 1]}]}
{"author": "cxzl25", "sha": "41d4c4f72f65c8bf2c388d2980240b445522313f", "commit_date": "2021/08/20 07:13:45", "commit_message": "Propagation cause when UDF reflection fails", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveSessionCatalog.scala", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 0, 0]}]}
{"author": "cxzl25", "sha": "d3086a535794dcd4984d30ea66063f8eab5ad95a", "commit_date": "2021/08/03 08:53:06", "commit_message": "Replace SessionState.close with SessionState.detachSession", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/hive-thriftserver/src/main/java/org/apache/hive/service/cli/session/HiveSessionImpl.java", "additions": "3", "deletions": "15", "changes": "18"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkSQLCLIDriver.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/CliSuite.scala", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveUtils.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 1, 4]}]}
{"author": "fhygh", "sha": "691bb5c0add0420ca51eacfb41b384eefb9637cc", "commit_date": "2021/08/17 06:39:37", "commit_message": "[SPARK-36518][Deploy] Spark should support distribute directory to\ncluster", "title": "[SPARK-36518][Deploy] Spark should support distribute directory to cluster", "body": "\r\n### What changes were proposed in this pull request?\r\nsupport distribute directory to cluster via --files\r\nbefore:\r\n[root@kwephispra41893 spark]# ll /opt/ygh/testdir/\r\ntotal 8\r\ndrwxr-xr-x 2 root root 4096 Aug 17 16:07 dd1\r\ndrwxr-xr-x 2 root root 4096 Aug 17 16:07 dd2\r\n-rw-r--r-- 1 root root    0 Aug 17 16:07 t1.txt\r\n-rw-r--r-- 1 root root    0 Aug 17 16:07 t2.txt\r\n-rw-r--r-- 1 root root    0 Aug 17 16:07 t3.conf\r\n\r\nspark-shell --master yarn --files file:///opt/ygh/testdir\r\n![image](https://user-images.githubusercontent.com/25889738/129689226-d63cc7f6-c529-4c6f-a94d-d48c062dbf29.png)\r\n\r\nafter:\r\nspark-shell --master yarn --files file:///opt/ygh/testdir\r\n![image](https://user-images.githubusercontent.com/25889738/129689406-432c796c-52e5-49c1-8016-9eec151257fb.png)\r\n\r\n\r\n### Why are the changes needed?\r\nwhen we submit spark application we can't directly distribute a directory to cluster\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo, user do not need change any code\r\n\r\n\r\n### How was this patch tested?\r\ntested by existing UT\r\n", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/util/DependencyUtils.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "fhygh", "sha": "e604f1ba111d208526ec3d49799fed063dc5c990", "commit_date": "2021/09/01 07:22:47", "commit_message": "[SPARK-36604][SQL] timestamp type column stats result consistent with\nthe time zone", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/interface.scala", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/StatisticsCollectionSuite.scala", "additions": "39", "deletions": "12", "changes": "51"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/StatisticsSuite.scala", "additions": "29", "deletions": "19", "changes": "48"}, "updated": [0, 0, 0]}]}
{"author": "ekoifman", "sha": "af5a71ab3d61056f0fa309e89868d57d584d0994", "commit_date": "2021/08/04 18:18:29", "commit_message": "[SPARK-36416][SQL] Add SQL metrics to AdaptiveSparkPlanExec for BHJs and Skew joins\n[SPARK-36416][SQL] fix typo", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "32", "deletions": "0", "changes": "32"}, "updated": [1, 3, 9]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "56", "deletions": "1", "changes": "57"}, "updated": [1, 4, 16]}]}
{"author": "tanelk", "sha": "38d98ed9e613f181018bb6266d89ab772db84b4c", "commit_date": "2021/08/17 05:51:07", "commit_message": "Merge remote-tracking branch 'origin/master' into SPARK-36496_remove_grouping_literals\n\n# Conflicts:\n#\tsql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "title": "[SPARK-36496][SQL] Remove literals from grouping expressions when using the DataFrame withColumn API", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\nMove the `RemoveLiteralFromGroupExpressions` and `RemoveRepetitionFromGroupExpressions` rules from a separate batch to the `operatorOptimizationBatch`.\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nThe `RemoveLiteralFromGroupExpressions` does not work in some cases if it is in a separate batch.\r\nThe added UT would fail with:\r\n```\r\n[info] - SPARK-36496: Remove literals from grouping expressions *** FAILED *** (2 seconds, 955 milliseconds)\r\n[info]   == FAIL: Plans do not match ===\r\n[info]   !Aggregate [*id#0L, null], [*id#0L, null AS a#0, count(1) AS count#0L]   Aggregate [*id#0L], [*id#0L, null AS a#0, count(1) AS count#0L]\r\n[info]    +- Range (0, 100, step=1, splits=Some(2))                               +- Range (0, 100, step=1, splits=Some(2)) (PlanTest.scala:174)\r\n```\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nNo\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nNew UT", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "4", "deletions": "5", "changes": "9"}, "updated": [0, 5, 14]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [1, 2, 11]}]}
{"author": "brandondahler", "sha": "2314d1d41f26973a27f6576aeec2f7c6de6dff4f", "commit_date": "2021/06/14 15:09:40", "commit_message": "[SPARK-35739][SQL] Add Java-compatible Dataset.join overloads", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala", "additions": "79", "deletions": "2", "changes": "81"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameJoinSuite.scala", "additions": "55", "deletions": "0", "changes": "55"}, "updated": [0, 0, 0]}]}
{"author": "kazuyukitanimura", "sha": "931c9061a99e6f63bd9fc3371a9fe7ac189c423c", "commit_date": "2021/08/31 06:50:05", "commit_message": "[SPARK-36665][SQL] Add more Not operator simplifications\n\n ### What changes were proposed in this pull request?\nThis PR proposes to add more Not operator simplifications in `BooleanSimplification` by applying the following rules\n  - Not(null) == null\n    - e.g. IsNull(Not(...)) can be IsNull(...)\n  - (Not(a) = b) == (a = Not(b))\n    - e.g. Not(...) = true can be (...) = false\n  - (a != b) == (a = Not(b))\n    - e.g. (...) != true can be (...) = false\n\n ### Why are the changes needed?\nThe following query does not push down the filter in the current implementation\n```\nSELECT * FROM t WHERE (not boolean_col) <=> null\n```\nalthough the following equivalent query pushes down the filter as expected.\n```\nSELECT * FROM t WHERE boolean_col <=> null\n```\nThat is because the first query creates `IsNull(Not(boolean_col))` in the current implementation, which should be able to get simplified further to `IsNull(boolean_col)`\nThis PR helps optimizing such cases.\n\n ### Does this PR introduce _any_ user-facing change?\nNo\n\n ### How was this patch tested?\nAdded unit tests\n```\nbuild/sbt \"testOnly *BooleanSimplificationSuite  -- -z SPARK-36665\"\n```", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.optimizer.BooleanSimplificationSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV2FilterSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV1FilterSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 2, 11]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/expressions.scala", "additions": "78", "deletions": "0", "changes": "78"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/rules/RuleIdCollection.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NotPropagationSuite.scala", "additions": "176", "deletions": "0", "changes": "176"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NullDownPropagationSuite.scala", "additions": "59", "deletions": "0", "changes": "59"}, "updated": [0, 0, 0]}]}
{"author": "zengruios", "sha": "e4794d404bb8bb140a9bd5470fee55ab1e1589c9", "commit_date": "2021/08/12 12:14:01", "commit_message": "[SPARK-36494]Add param bucketSpec when create LogicalRelation for the hive table in HiveMetastoreCatalog.covertToLogicalRelation.", "title": "[SPARK-36494]Add param bucketSpec when create LogicalRelation for the hive table in HiveMetastoreCatalog.covertToLogicalRelation", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nAdd param bucketSpec when create LogicalRelation for the hive table in HiveMetastoreCatalog.covertToLogicalRelation\r\n\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nIf bucketSpec is not used, SortMergeJoin will do unnecessary shuffle.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nYes. See in SPARK-36494.\r\n\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nTest it in my develop enviroinment.\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMetastoreCatalog.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 1]}]}
{"author": "MyeongKim", "sha": "8389a0f7c7febe1d3c9489ce18b61a1501f6f0d1", "commit_date": "2021/07/25 17:59:42", "commit_message": "Support TPCDSQueryBenchmark in Benchmarks", "title": "", "body": "", "failed_tests": ["org.apache.spark.scheduler.BasicSchedulerIntegrationSuite"], "files": [{"file": {"name": ".github/workflows/benchmark.yml", "additions": "72", "deletions": "4", "changes": "76"}, "updated": [0, 0, 3]}, {"file": {"name": ".github/workflows/build_and_test.yml", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [3, 9, 22]}, {"file": {"name": "core/src/test/scala/org/apache/spark/benchmark/Benchmarks.scala", "additions": "10", "deletions": "2", "changes": "12"}, "updated": [0, 0, 0]}]}
{"author": "sunchao", "sha": "c2ec1f8122ea9b871a1679571f0b20009be39cb7", "commit_date": "2021/09/03 20:48:52", "commit_message": "wip", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "LICENSE-binary", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "assembly/pom.xml", "additions": "19", "deletions": "1", "changes": "20"}, "updated": [0, 0, 0]}, {"file": {"name": "dev/deps/spark-deps-hadoop-2.7-hive-2.3", "additions": "6", "deletions": "26", "changes": "32"}, "updated": [1, 3, 15]}, {"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "7", "deletions": "27", "changes": "34"}, "updated": [1, 3, 15]}, {"file": {"name": "pom.xml", "additions": "152", "deletions": "3", "changes": "155"}, "updated": [1, 4, 23]}, {"file": {"name": "project/SparkBuild.scala", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [0, 2, 6]}, {"file": {"name": "sql/hive-shaded/pom.xml", "additions": "238", "deletions": "0", "changes": "238"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive-thriftserver/pom.xml", "additions": "9", "deletions": "16", "changes": "25"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/pom.xml", "additions": "29", "deletions": "66", "changes": "95"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/IsolatedClientLoader.scala", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}]}
{"author": "sunchao", "sha": "622467ba22d2c8e7a97523b9d0985716642603cc", "commit_date": "2021/06/09 19:26:29", "commit_message": "wip", "title": "", "body": "", "failed_tests": ["pyspark.sql.tests.test_pandas_cogrouped_map", "org.apache.spark.sql.JoinSuite", "org.apache.spark.sql.execution.RemoveRedundantSortsSuite", "org.apache.spark.sql.execution.WholeStageCodegenSuite", "org.apache.spark.sql.execution.CoalesceShufflePartitionsSuite", "org.apache.spark.sql.DataFrameSuite", "org.apache.spark.sql.execution.RemoveRedundantSortsSuite", "org.apache.spark.sql.execution.exchange.EnsureRequirementsSuite", "org.apache.spark.sql.execution.PlannerSuite"], "files": [{"file": {"name": "python/pyspark/sql/tests/test_pandas_cogrouped_map.py", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/physical/partitioning.scala", "additions": "188", "deletions": "29", "changes": "217"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/DistributionSuite.scala", "additions": "0", "deletions": "38", "changes": "38"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AQEUtils.scala", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/bucketing/DisableUnnecessaryBucketedScan.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ValidateRequirements.scala", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/joins/ShuffledJoin.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/objects.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinExec.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q17.sf100/explain.txt", "additions": "155", "deletions": "170", "changes": "325"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q17.sf100/simplified.txt", "additions": "74", "deletions": "81", "changes": "155"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q25.sf100/explain.txt", "additions": "155", "deletions": "170", "changes": "325"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q25.sf100/simplified.txt", "additions": "74", "deletions": "81", "changes": "155"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q29.sf100/explain.txt", "additions": "173", "deletions": "188", "changes": "361"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q29.sf100/simplified.txt", "additions": "75", "deletions": "82", "changes": "157"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q47.sf100/explain.txt", "additions": "110", "deletions": "125", "changes": "235"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q47.sf100/simplified.txt", "additions": "80", "deletions": "89", "changes": "169"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q57.sf100/explain.txt", "additions": "110", "deletions": "125", "changes": "235"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q57.sf100/simplified.txt", "additions": "80", "deletions": "89", "changes": "169"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q72.sf100/explain.txt", "additions": "197", "deletions": "207", "changes": "404"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q72.sf100/simplified.txt", "additions": "105", "deletions": "111", "changes": "216"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q24.sf100/explain.txt", "additions": "61", "deletions": "43", "changes": "104"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q24.sf100/simplified.txt", "additions": "36", "deletions": "33", "changes": "69"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q47.sf100/explain.txt", "additions": "110", "deletions": "125", "changes": "235"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q47.sf100/simplified.txt", "additions": "80", "deletions": "89", "changes": "169"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q51a.sf100/explain.txt", "additions": "249", "deletions": "259", "changes": "508"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q51a.sf100/simplified.txt", "additions": "100", "deletions": "106", "changes": "206"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q57.sf100/explain.txt", "additions": "110", "deletions": "125", "changes": "235"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q57.sf100/simplified.txt", "additions": "80", "deletions": "89", "changes": "169"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q64/explain.txt", "additions": "414", "deletions": "424", "changes": "838"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q64/simplified.txt", "additions": "259", "deletions": "265", "changes": "524"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q72.sf100/explain.txt", "additions": "197", "deletions": "207", "changes": "404"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q72.sf100/simplified.txt", "additions": "105", "deletions": "111", "changes": "216"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/PlannerSuite.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/exchange/EnsureRequirementsSuite.scala", "additions": "181", "deletions": "2", "changes": "183"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/sources/BucketedReadSuite.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingJoinSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}]}
{"author": "sunchao", "sha": "5f9f186ab3c4e403dddd0a34d8f8852193af0bc7", "commit_date": "2021/06/15 21:48:11", "commit_message": "initial commit", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.execution.datasources.parquet.ParquetVectorizedSuite", "org.apache.spark.sql.FileBasedDataSourceSuite"], "files": [{"file": {"name": "pom.xml", "additions": "21", "deletions": "0", "changes": "21"}, "updated": [1, 7, 32]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/vectorized/ColumnVector.java", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [1, 2, 13]}, {"file": {"name": "sql/core/pom.xml", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/parquet/io/ColumnIOUtil.java", "additions": "38", "deletions": "0", "changes": "38"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnarBatchReader.java", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetColumn.java", "additions": "318", "deletions": "0", "changes": "318"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetReadState.java", "additions": "41", "deletions": "18", "changes": "59"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase.java", "additions": "84", "deletions": "22", "changes": "106"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader.java", "additions": "63", "deletions": "32", "changes": "95"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader.java", "additions": "114", "deletions": "57", "changes": "171"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedRleValuesReader.java", "additions": "341", "deletions": "24", "changes": "365"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OffHeapColumnVector.java", "additions": "20", "deletions": "1", "changes": "21"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OnHeapColumnVector.java", "additions": "21", "deletions": "1", "changes": "22"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/WritableColumnVector.java", "additions": "64", "deletions": "6", "changes": "70"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala", "additions": "20", "deletions": "3", "changes": "23"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "5", "deletions": "2", "changes": "7"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "138", "deletions": "38", "changes": "176"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetType.scala", "additions": "68", "deletions": "0", "changes": "68"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/java/org/apache/parquet/column/page/TestDataPage.java", "additions": "44", "deletions": "0", "changes": "44"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/benchmark/DataSourceReadBenchmark.scala", "additions": "119", "deletions": "3", "changes": "122"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/FileBasedDataSourceTest.scala", "additions": "6", "deletions": "2", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcTest.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcV1SchemaPruningSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcV2SchemaPruningSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetColumnIndexSuite.scala", "additions": "14", "deletions": "1", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "326", "deletions": "0", "changes": "326"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaPruningSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetTest.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetVectorizedSuite.scala", "additions": "751", "deletions": "0", "changes": "751"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/vectorized/ColumnVectorSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/vectorized/ColumnarBatchSuite.scala", "additions": "15", "deletions": "2", "changes": "17"}, "updated": [0, 0, 0]}]}
{"author": "sunchao", "sha": "4bf4533b8c7e9c7b2f264ede6a14cad52c33b194", "commit_date": "2021/06/30 20:22:02", "commit_message": "initial commit", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.data_type_ops.test_binary_ops", "pyspark.pandas.tests.indexes.test_base", "org.apache.spark.sql.execution.datasources.AvroReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.avro.AvroScanSuite", "org.apache.spark.sql.avro.AvroScanSuite", "org.apache.spark.sql.avro.AvroScanSuite", "org.apache.spark.sql.avro.AvroScanSuite", "org.apache.spark.sql.avro.AvroScanSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.avro.AvroCatalystDataConversionSuite", "org.apache.spark.sql.avro.AvroCatalystDataConversionSuite", "org.apache.spark.sql.avro.AvroCatalystDataConversionSuite", "org.apache.spark.sql.avro.AvroCatalystDataConversionSuite", "org.apache.spark.sql.avro.AvroCatalystDataConversionSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.avro.AvroV1Suite", "org.apache.spark.sql.avro.AvroSuite", "org.apache.spark.sql.avro.AvroSuite", "org.apache.spark.sql.avro.AvroSuite", "org.apache.spark.sql.avro.AvroSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.avro.DeprecatedAvroFunctionsSuite", "org.apache.spark.sql.avro.DeprecatedAvroFunctionsSuite", "org.apache.spark.sql.avro.DeprecatedAvroFunctionsSuite", "org.apache.spark.sql.avro.DeprecatedAvroFunctionsSuite", "org.apache.spark.sql.avro.DeprecatedAvroFunctionsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.avro.AvroV1LogicalTypeSuite", "org.apache.spark.sql.avro.AvroLogicalTypeSuite", "org.apache.spark.sql.avro.AvroLogicalTypeSuite", "org.apache.spark.sql.avro.AvroLogicalTypeSuite", "org.apache.spark.sql.avro.AvroLogicalTypeSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.avro.AvroSchemaHelperSuite", "org.apache.spark.sql.avro.AvroSchemaHelperSuite", "org.apache.spark.sql.avro.AvroSchemaHelperSuite", "org.apache.spark.sql.avro.AvroSchemaHelperSuite", "org.apache.spark.sql.avro.AvroSchemaHelperSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.avro.AvroFunctionsSuite", "org.apache.spark.sql.avro.AvroFunctionsSuite", "org.apache.spark.sql.avro.AvroFunctionsSuite", "org.apache.spark.sql.avro.AvroFunctionsSuite", "org.apache.spark.sql.avro.AvroFunctionsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.avro.AvroRowReaderSuite", "org.apache.spark.sql.avro.AvroRowReaderSuite", "org.apache.spark.sql.avro.AvroRowReaderSuite", "org.apache.spark.sql.avro.AvroRowReaderSuite", "org.apache.spark.sql.avro.AvroRowReaderSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.avro.AvroV2Suite", "org.apache.spark.sql.avro.AvroSuite", "org.apache.spark.sql.avro.AvroSuite", "org.apache.spark.sql.avro.AvroSuite", "org.apache.spark.sql.avro.AvroSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.avro.AvroV2LogicalTypeSuite", "org.apache.spark.sql.avro.AvroLogicalTypeSuite", "org.apache.spark.sql.avro.AvroLogicalTypeSuite", "org.apache.spark.sql.avro.AvroLogicalTypeSuite", "org.apache.spark.sql.avro.AvroLogicalTypeSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.avro.JavaAvroFunctionsSuite", "org.apache.spark.sql.avro.JavaAvroFunctionsSuite", "org.apache.spark.sql.avro.JavaAvroFunctionsSuite", "org.apache.spark.sql.execution.datasources.AvroReadSchemaSuite", "org.apache.spark.sql.avro.AvroScanSuite", "org.apache.spark.sql.avro.AvroCatalystDataConversionSuite", "org.apache.spark.sql.avro.AvroV1Suite", "org.apache.spark.sql.avro.DeprecatedAvroFunctionsSuite", "org.apache.spark.sql.avro.AvroV1LogicalTypeSuite", "org.apache.spark.sql.avro.AvroSchemaHelperSuite", "org.apache.spark.sql.avro.AvroFunctionsSuite", "org.apache.spark.sql.avro.AvroRowReaderSuite", "org.apache.spark.sql.avro.AvroV2Suite", "org.apache.spark.sql.avro.AvroV2LogicalTypeSuite", "org.apache.spark.sql.catalyst.analysis.V2OverwriteByExpressionANSIAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2ANSIAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2ANSIAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2ANSIAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.analysis.PullOutNondeterministicSuite", "org.apache.spark.sql.catalyst.analysis.PullOutNondeterministicSuite", "org.apache.spark.sql.catalyst.analysis.PullOutNondeterministicSuite", "org.apache.spark.sql.catalyst.analysis.PullOutNondeterministicSuite", "org.apache.spark.sql.catalyst.analysis.PullOutNondeterministicSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.analysis.V2OverwritePartitionsDynamicANSIAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2ANSIAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2ANSIAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2ANSIAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.analysis.V2AppendDataStrictAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2StrictAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2StrictAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2StrictAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite", "org.apache.spark.sql.catalyst.catalog.ExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.analysis.ResolveHintsSuite", "org.apache.spark.sql.catalyst.analysis.ResolveHintsSuite", "org.apache.spark.sql.catalyst.analysis.ResolveHintsSuite", "org.apache.spark.sql.catalyst.analysis.ResolveHintsSuite", "org.apache.spark.sql.catalyst.analysis.ResolveHintsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.catalog.InMemorySessionCatalogSuite", "org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite", "org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.analysis.ResolveGroupingAnalyticsSuite", "org.apache.spark.sql.catalyst.analysis.ResolveGroupingAnalyticsSuite", "org.apache.spark.sql.catalyst.analysis.ResolveGroupingAnalyticsSuite", "org.apache.spark.sql.catalyst.analysis.ResolveGroupingAnalyticsSuite", "org.apache.spark.sql.catalyst.analysis.ResolveGroupingAnalyticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisExceptionPositionSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisExceptionPositionSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisExceptionPositionSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisExceptionPositionSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisExceptionPositionSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisExceptionPositionSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisExceptionPositionSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.analysis.ResolveNaturalJoinSuite", "org.apache.spark.sql.catalyst.analysis.ResolveNaturalJoinSuite", "org.apache.spark.sql.catalyst.analysis.ResolveNaturalJoinSuite", "org.apache.spark.sql.catalyst.analysis.ResolveNaturalJoinSuite", "org.apache.spark.sql.catalyst.analysis.ResolveNaturalJoinSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.analysis.V2AppendDataANSIAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2ANSIAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2ANSIAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2ANSIAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.analysis.V2OverwritePartitionsDynamicStrictAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2StrictAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2StrictAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2StrictAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.expressions.SelectedFieldSuite", "org.apache.spark.sql.catalyst.expressions.SelectedFieldSuite", "org.apache.spark.sql.catalyst.expressions.SelectedFieldSuite", "org.apache.spark.sql.catalyst.expressions.SelectedFieldSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.catalog.ExternalCatalogEventSuite", "org.apache.spark.sql.catalyst.catalog.ExternalCatalogEventSuite", "org.apache.spark.sql.catalyst.catalog.ExternalCatalogEventSuite", "org.apache.spark.sql.catalyst.catalog.ExternalCatalogEventSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite", "org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite", "org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite", "org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite", "org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite", "org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite", "org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite", "org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.analysis.SubstituteUnresolvedOrdinalsSuite", "org.apache.spark.sql.catalyst.analysis.SubstituteUnresolvedOrdinalsSuite", "org.apache.spark.sql.catalyst.analysis.SubstituteUnresolvedOrdinalsSuite", "org.apache.spark.sql.catalyst.analysis.SubstituteUnresolvedOrdinalsSuite", "org.apache.spark.sql.catalyst.analysis.SubstituteUnresolvedOrdinalsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.analysis.V2OverwriteByExpressionStrictAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2StrictAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2StrictAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2StrictAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.analysis.ExtractGeneratorSuite", "org.apache.spark.sql.catalyst.analysis.ExtractGeneratorSuite", "org.apache.spark.sql.catalyst.analysis.ExtractGeneratorSuite", "org.apache.spark.sql.catalyst.analysis.ExtractGeneratorSuite", "org.apache.spark.sql.catalyst.analysis.ExtractGeneratorSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.analysis.ResolvedUuidExpressionsSuite", "org.apache.spark.sql.catalyst.analysis.ResolvedUuidExpressionsSuite", "org.apache.spark.sql.catalyst.analysis.ResolvedUuidExpressionsSuite", "org.apache.spark.sql.catalyst.analysis.CreateTablePartitioningValidationSuite", "org.apache.spark.sql.catalyst.analysis.CreateTablePartitioningValidationSuite", "org.apache.spark.sql.catalyst.analysis.CreateTablePartitioningValidationSuite", "org.apache.spark.sql.catalyst.analysis.CreateTablePartitioningValidationSuite", "org.apache.spark.sql.catalyst.analysis.CreateTablePartitioningValidationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.analysis.ResolveAliasesSuite", "org.apache.spark.sql.catalyst.analysis.ResolveAliasesSuite", "org.apache.spark.sql.catalyst.analysis.ResolveAliasesSuite", "org.apache.spark.sql.catalyst.analysis.ResolveAliasesSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.analysis.LookupFunctionsSuite", "org.apache.spark.sql.catalyst.analysis.LookupFunctionsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.analysis.ResolveSubquerySuite", "org.apache.spark.sql.catalyst.analysis.ResolveSubquerySuite", "org.apache.spark.sql.catalyst.analysis.ResolveSubquerySuite", "org.apache.spark.sql.catalyst.analysis.ResolveSubquerySuite", "org.apache.spark.sql.catalyst.analysis.ResolveSubquerySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisExternalCatalogSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisExternalCatalogSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisExternalCatalogSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.optimizer.AggregateOptimizeSuite", "org.apache.spark.sql.catalyst.optimizer.AggregateOptimizeSuite", "org.apache.spark.sql.catalyst.optimizer.AggregateOptimizeSuite", "org.apache.spark.sql.catalyst.optimizer.EliminateSortsSuite", "org.apache.spark.sql.catalyst.optimizer.EliminateSortsSuite", "org.apache.spark.sql.catalyst.optimizer.EliminateSortsSuite", "org.apache.spark.sql.catalyst.analysis.TableLookupCacheSuite", "org.apache.spark.sql.catalyst.analysis.TableLookupCacheSuite", "org.apache.spark.sql.catalyst.analysis.TableLookupCacheSuite", "org.apache.spark.sql.catalyst.analysis.TableLookupCacheSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.analysis.TableLookupCacheSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.connector.catalog.CatalogManagerSuite", "org.apache.spark.sql.connector.catalog.CatalogManagerSuite", "org.apache.spark.sql.connector.catalog.CatalogManagerSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.expressions.codegen.BufferHolderSparkSubmitSuite", "org.apache.spark.sql.catalyst.analysis.V2OverwriteByExpressionANSIAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.PullOutNondeterministicSuite", "org.apache.spark.sql.catalyst.analysis.V2OverwritePartitionsDynamicANSIAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisSuite", "org.apache.spark.sql.catalyst.analysis.V2AppendDataStrictAnalysisSuite", "org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite", "org.apache.spark.sql.catalyst.analysis.ResolveHintsSuite", "org.apache.spark.sql.catalyst.catalog.InMemorySessionCatalogSuite", "org.apache.spark.sql.catalyst.analysis.ResolveGroupingAnalyticsSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisExceptionPositionSuite", "org.apache.spark.sql.catalyst.analysis.ResolveNaturalJoinSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite", "org.apache.spark.sql.catalyst.analysis.V2AppendDataANSIAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.V2OverwritePartitionsDynamicStrictAnalysisSuite", "org.apache.spark.sql.catalyst.expressions.SelectedFieldSuite", "org.apache.spark.sql.catalyst.catalog.ExternalCatalogEventSuite", "org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite", "org.apache.spark.sql.catalyst.analysis.SubstituteUnresolvedOrdinalsSuite", "org.apache.spark.sql.catalyst.analysis.V2OverwriteByExpressionStrictAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.ExtractGeneratorSuite", "org.apache.spark.sql.catalyst.analysis.ResolvedUuidExpressionsSuite", "org.apache.spark.sql.catalyst.analysis.CreateTablePartitioningValidationSuite", "org.apache.spark.sql.catalyst.analysis.ResolveAliasesSuite", "org.apache.spark.sql.catalyst.analysis.LookupFunctionsSuite", "org.apache.spark.sql.catalyst.analysis.ResolveSubquerySuite", "org.apache.spark.sql.catalyst.analysis.AnalysisExternalCatalogSuite", "org.apache.spark.sql.catalyst.optimizer.AggregateOptimizeSuite", "org.apache.spark.sql.catalyst.optimizer.EliminateSortsSuite", "org.apache.spark.sql.catalyst.analysis.TableLookupCacheSuite", "org.apache.spark.sql.connector.catalog.CatalogManagerSuite", "org.apache.spark.streaming.kafka010.KafkaRDDSuite", "org.apache.spark.streaming.kafka010.KafkaRDDSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.streaming.kafka010.DirectKafkaStreamSuite", "org.apache.spark.streaming.kafka010.DirectKafkaStreamSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.streaming.kafka010.JavaDirectKafkaStreamSuite", "org.apache.spark.streaming.kafka010.JavaKafkaRDDSuite", "org.apache.spark.streaming.kafka010.JavaDirectKafkaStreamSuite", "org.apache.spark.streaming.kafka010.JavaKafkaRDDSuite", "org.apache.spark.streaming.kafka010.KafkaRDDSuite", "org.apache.spark.streaming.kafka010.DirectKafkaStreamSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.HiveParquetSuite", "org.apache.spark.sql.hive.HiveParquetSuite", "org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite", "org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite", "org.apache.spark.sql.hive.orc.HiveOrcQuerySuite", "org.apache.spark.sql.hive.orc.HiveOrcQuerySuite", "org.apache.spark.sql.hive.execution.command.ShowPartitionsSuite", "org.apache.spark.sql.hive.execution.command.ShowPartitionsSuite", "org.apache.spark.sql.hive.execution.command.ShowNamespacesSuite", "org.apache.spark.sql.hive.execution.command.ShowNamespacesSuite", "org.apache.spark.sql.hive.execution.HiveDDLSuite", "org.apache.spark.sql.hive.execution.HiveDDLSuite", "org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite", "org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite", "org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite", "org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite", "org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite", "org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite", "org.apache.spark.sql.hive.execution.command.ShowTablesSuite", "org.apache.spark.sql.hive.execution.command.ShowTablesSuite", "org.apache.spark.sql.hive.execution.HiveQuerySuite", "org.apache.spark.sql.hive.execution.HiveQuerySuite", "org.apache.spark.sql.hive.orc.HiveOrcPartitionDiscoverySuite", "org.apache.spark.sql.hive.orc.HiveOrcPartitionDiscoverySuite", "org.apache.spark.sql.hive.InsertSuite", "org.apache.spark.sql.hive.InsertSuite", "org.apache.spark.sql.hive.CachedTableSuite", "org.apache.spark.sql.hive.CachedTableSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.sources.DisableUnnecessaryBucketedScanWithHiveSupportSuite", "org.apache.spark.sql.sources.DisableUnnecessaryBucketedScanWithHiveSupportSuite", "org.apache.spark.sql.hive.CompressionCodecSuite", "org.apache.spark.sql.hive.CompressionCodecSuite", "org.apache.spark.sql.hive.execution.WindowQuerySuite", "org.apache.spark.sql.hive.execution.WindowQuerySuite", "org.apache.spark.sql.hive.HiveExternalSessionCatalogSuite", "org.apache.spark.sql.hive.HiveExternalSessionCatalogSuite", "org.apache.spark.sql.hive.execution.HiveSQLViewSuite", "org.apache.spark.sql.hive.execution.HiveSQLViewSuite", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.ListTablesSuite", "org.apache.spark.sql.hive.ListTablesSuite", "org.apache.spark.sql.sources.BucketedWriteWithHiveSupportSuite", "org.apache.spark.sql.sources.BucketedWriteWithHiveSupportSuite", "org.apache.spark.sql.hive.execution.HashUDAQuerySuite", "org.apache.spark.sql.hive.execution.UDAQuerySuite", "org.apache.spark.sql.hive.execution.HashUDAQuerySuite", "org.apache.spark.sql.hive.ParquetEncryptionSuite", "org.apache.spark.sql.hive.ParquetEncryptionSuite", "org.apache.spark.sql.hive.execution.PruneHiveTablePartitionsSuite", "org.apache.spark.sql.hive.execution.PruneHiveTablePartitionsSuite", "org.apache.spark.sql.hive.HiveUtilsSuite", "org.apache.spark.sql.hive.HiveUtilsSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveUDAFSuite", "org.apache.spark.sql.hive.execution.HiveUDAFSuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.command.AlterTableRenamePartitionSuite", "org.apache.spark.sql.hive.execution.command.AlterTableRenamePartitionSuite", "org.apache.spark.sql.hive.HiveSessionStateSuite", "org.apache.spark.sql.hive.HiveSessionStateSuite", "org.apache.spark.sql.hive.HiveVariableSubstitutionSuite", "org.apache.spark.sql.hive.HiveVariableSubstitutionSuite", "org.apache.spark.sql.hive.execution.HiveSerDeReadWriteSuite", "org.apache.spark.sql.hive.execution.HiveSerDeReadWriteSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.UDFSuite", "org.apache.spark.sql.hive.UDFSuite", "org.apache.spark.sql.hive.execution.HashAggregationQueryWithControlledFallbackSuite", "org.apache.spark.sql.hive.execution.AggregationQuerySuite", "org.apache.spark.sql.hive.execution.HashAggregationQueryWithControlledFallbackSuite", "org.apache.spark.sql.hive.HiveUserDefinedTypeSuite", "org.apache.spark.sql.hive.HiveUserDefinedTypeSuite", "org.apache.spark.sql.HiveCharVarcharTestSuite", "org.apache.spark.sql.HiveCharVarcharTestSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.BigDataBenchmarkSuite", "org.apache.spark.sql.hive.execution.BigDataBenchmarkSuite", "org.apache.spark.sql.hive.execution.ObjectHashAggregateSuite", "org.apache.spark.sql.hive.execution.ObjectHashAggregateSuite", "org.apache.spark.sql.hive.execution.command.TruncateTableSuite", "org.apache.spark.sql.hive.execution.command.TruncateTableSuite", "org.apache.spark.sql.hive.execution.command.AlterTableRenameSuite", "org.apache.spark.sql.hive.execution.command.AlterTableRenameSuite", "org.apache.spark.sql.hive.execution.SQLMetricsSuite", "org.apache.spark.sql.hive.execution.SQLMetricsSuite", "org.apache.spark.sql.hive.OptimizeHiveMetadataOnlyQuerySuite", "org.apache.spark.sql.hive.OptimizeHiveMetadataOnlyQuerySuite", "org.apache.spark.sql.hive.execution.HiveCommandSuite", "org.apache.spark.sql.hive.execution.HiveCommandSuite", "org.apache.spark.sql.hive.QueryPartitionSuite", "org.apache.spark.sql.hive.QueryPartitionSuite", "org.apache.spark.sql.hive.execution.HashUDAQueryWithControlledFallbackSuite", "org.apache.spark.sql.hive.execution.UDAQuerySuite", "org.apache.spark.sql.hive.execution.HashUDAQueryWithControlledFallbackSuite", "org.apache.spark.sql.hive.ParquetHiveCompatibilitySuite", "org.apache.spark.sql.hive.ParquetHiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.command.MsckRepairTableSuite", "org.apache.spark.sql.hive.execution.command.MsckRepairTableSuite", "org.apache.spark.sql.hive.HiveUDFDynamicLoadSuite", "org.apache.spark.sql.hive.HiveUDFDynamicLoadSuite", "org.apache.spark.sql.hive.TestHiveSuite", "org.apache.spark.sql.hive.TestHiveSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.HiveSQLInsertTestSuite", "org.apache.spark.sql.hive.HiveSQLInsertTestSuite", "org.apache.spark.sql.sources.CommitFailureTestRelationSuite", "org.apache.spark.sql.sources.CommitFailureTestRelationSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.execution.HiveWindowFunctionQueryFileSuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveWindowFunctionQueryFileSuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveWindowFunctionQueryFileSuite", "org.apache.spark.sql.hive.HiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.HiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.ErrorPositionSuite", "org.apache.spark.sql.hive.ErrorPositionSuite", "org.apache.spark.sql.hive.execution.HiveScriptTransformationSuite", "org.apache.spark.sql.hive.execution.HiveScriptTransformationSuite", "org.apache.spark.sql.hive.execution.HashAggregationQuerySuite", "org.apache.spark.sql.hive.execution.AggregationQuerySuite", "org.apache.spark.sql.hive.execution.HashAggregationQuerySuite", "org.apache.spark.sql.hive.execution.command.AlterTableRecoverPartitionsSuite", "org.apache.spark.sql.hive.execution.command.AlterTableRecoverPartitionsSuite", "org.apache.spark.sql.hive.execution.HiveExplainSuite", "org.apache.spark.sql.hive.execution.HiveExplainSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.execution.command.DropTableSuite", "org.apache.spark.sql.hive.execution.command.DropTableSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.execution.command.AlterTableDropPartitionSuite", "org.apache.spark.sql.hive.execution.command.AlterTableDropPartitionSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.HiveCharVarcharDDLTestSuite", "org.apache.spark.sql.HiveCharVarcharDDLTestSuite", "org.apache.spark.sql.hive.execution.command.AlterTableAddPartitionSuite", "org.apache.spark.sql.hive.execution.command.AlterTableAddPartitionSuite", "org.apache.spark.sql.hive.HiveExternalCatalogVersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.HiveParquetSuite", "org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite", "org.apache.spark.sql.hive.orc.HiveOrcQuerySuite", "org.apache.spark.sql.hive.execution.command.ShowPartitionsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.execution.command.ShowNamespacesSuite", "org.apache.spark.sql.hive.execution.HiveDDLSuite", "org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite", "org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite", "org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite", "org.apache.spark.sql.hive.execution.command.ShowTablesSuite", "org.apache.spark.sql.hive.execution.HiveQuerySuite", "org.apache.spark.sql.hive.orc.HiveOrcPartitionDiscoverySuite", "org.apache.spark.sql.hive.InsertSuite", "org.apache.spark.sql.hive.CachedTableSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.sources.DisableUnnecessaryBucketedScanWithHiveSupportSuite", "org.apache.spark.sql.hive.CompressionCodecSuite", "org.apache.spark.sql.hive.execution.WindowQuerySuite", "org.apache.spark.sql.hive.HiveExternalSessionCatalogSuite", "org.apache.spark.sql.hive.execution.HiveSQLViewSuite", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.ListTablesSuite", "org.apache.spark.sql.sources.BucketedWriteWithHiveSupportSuite", "org.apache.spark.sql.hive.execution.HashUDAQuerySuite", "org.apache.spark.sql.hive.ParquetEncryptionSuite", "org.apache.spark.sql.hive.execution.PruneHiveTablePartitionsSuite", "org.apache.spark.sql.hive.HiveUtilsSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveUDAFSuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.command.AlterTableRenamePartitionSuite", "org.apache.spark.sql.hive.HiveSessionStateSuite", "org.apache.spark.sql.hive.HiveVariableSubstitutionSuite", "org.apache.spark.sql.hive.execution.HiveSerDeReadWriteSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.UDFSuite", "org.apache.spark.sql.hive.execution.HashAggregationQueryWithControlledFallbackSuite", "org.apache.spark.sql.hive.HiveUserDefinedTypeSuite", "org.apache.spark.sql.HiveCharVarcharTestSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.BigDataBenchmarkSuite", "org.apache.spark.sql.hive.execution.ObjectHashAggregateSuite", "org.apache.spark.sql.hive.execution.command.TruncateTableSuite", "org.apache.spark.sql.hive.execution.command.AlterTableRenameSuite", "org.apache.spark.sql.hive.execution.SQLMetricsSuite", "org.apache.spark.sql.hive.OptimizeHiveMetadataOnlyQuerySuite", "org.apache.spark.sql.hive.execution.HiveCommandSuite", "org.apache.spark.sql.hive.QueryPartitionSuite", "org.apache.spark.sql.hive.execution.HashUDAQueryWithControlledFallbackSuite", "org.apache.spark.sql.hive.ParquetHiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.command.MsckRepairTableSuite", "org.apache.spark.sql.hive.HiveUDFDynamicLoadSuite", "org.apache.spark.sql.hive.TestHiveSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.HiveSQLInsertTestSuite", "org.apache.spark.sql.sources.CommitFailureTestRelationSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.execution.HiveWindowFunctionQueryFileSuite", "org.apache.spark.sql.hive.HiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.ErrorPositionSuite", "org.apache.spark.sql.hive.execution.HiveScriptTransformationSuite", "org.apache.spark.sql.hive.execution.HashAggregationQuerySuite", "org.apache.spark.sql.hive.execution.command.AlterTableRecoverPartitionsSuite", "org.apache.spark.sql.hive.execution.HiveExplainSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.execution.command.DropTableSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.execution.command.AlterTableDropPartitionSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.HiveCharVarcharDDLTestSuite", "org.apache.spark.sql.hive.execution.command.AlterTableAddPartitionSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.HiveParquetSuite", "org.apache.spark.sql.hive.HiveParquetSuite", "org.apache.spark.sql.hive.HiveContextCompatibilitySuite", "org.apache.spark.sql.hive.HiveContextCompatibilitySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite", "org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite", "org.apache.spark.sql.hive.orc.HiveOrcQuerySuite", "org.apache.spark.sql.hive.orc.HiveOrcQuerySuite", "org.apache.spark.sql.hive.execution.command.ShowPartitionsSuite", "org.apache.spark.sql.hive.execution.command.ShowPartitionsSuite", "org.apache.spark.sql.hive.execution.command.ShowNamespacesSuite", "org.apache.spark.sql.hive.execution.command.ShowNamespacesSuite", "org.apache.spark.sql.hive.client.HiveClientUserNameSuite", "org.apache.spark.sql.hive.client.HiveClientUserNameSuite", "org.apache.spark.sql.hive.client.HiveClientUserNameSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.client.HiveClientUserNameSuites.runNestedSuite", "org.apache.spark.sql.hive.client.HiveClientUserNameSuite", "org.apache.spark.sql.hive.execution.HiveResolutionSuite", "org.apache.spark.sql.hive.execution.HiveResolutionSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.execution.HiveDDLSuite", "org.apache.spark.sql.hive.execution.HiveDDLSuite", "org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite", "org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite", "org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite", "org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite", "org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite", "org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite", "org.apache.spark.sql.hive.execution.command.ShowTablesSuite", "org.apache.spark.sql.hive.execution.command.ShowTablesSuite", "org.apache.spark.sql.hive.execution.HiveQuerySuite", "org.apache.spark.sql.hive.execution.HiveQuerySuite", "org.apache.spark.sql.hive.orc.HiveOrcPartitionDiscoverySuite", "org.apache.spark.sql.hive.orc.HiveOrcPartitionDiscoverySuite", "org.apache.spark.sql.hive.InsertSuite", "org.apache.spark.sql.hive.InsertSuite", "org.apache.spark.sql.hive.CachedTableSuite", "org.apache.spark.sql.hive.CachedTableSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.sources.DisableUnnecessaryBucketedScanWithHiveSupportSuite", "org.apache.spark.sql.sources.DisableUnnecessaryBucketedScanWithHiveSupportSuite", "org.apache.spark.sql.hive.CompressionCodecSuite", "org.apache.spark.sql.hive.CompressionCodecSuite", "org.apache.spark.sql.hive.execution.WindowQuerySuite", "org.apache.spark.sql.hive.execution.WindowQuerySuite", "org.apache.spark.sql.hive.HiveExternalSessionCatalogSuite", "org.apache.spark.sql.hive.HiveExternalSessionCatalogSuite", "org.apache.spark.sql.hive.execution.HiveSQLViewSuite", "org.apache.spark.sql.hive.execution.HiveSQLViewSuite", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.ListTablesSuite", "org.apache.spark.sql.hive.ListTablesSuite", "org.apache.spark.sql.sources.BucketedWriteWithHiveSupportSuite", "org.apache.spark.sql.sources.BucketedWriteWithHiveSupportSuite", "org.apache.spark.sql.hive.execution.HashUDAQuerySuite", "org.apache.spark.sql.hive.execution.UDAQuerySuite", "org.apache.spark.sql.hive.execution.HashUDAQuerySuite", "org.apache.spark.sql.hive.ParquetEncryptionSuite", "org.apache.spark.sql.hive.ParquetEncryptionSuite", "org.apache.spark.sql.hive.execution.PruneHiveTablePartitionsSuite", "org.apache.spark.sql.hive.execution.PruneHiveTablePartitionsSuite", "org.apache.spark.sql.hive.HiveUtilsSuite", "org.apache.spark.sql.hive.HiveUtilsSuite", "org.apache.spark.sql.hive.HiveShimSuite", "org.apache.spark.sql.hive.HiveShimSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveSharedStateSuite", "org.apache.spark.sql.hive.HiveSharedStateSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.client.HadoopVersionInfoSuite", "org.apache.spark.sql.hive.client.HadoopVersionInfoSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveSerDeSuite", "org.apache.spark.sql.hive.execution.HiveSerDeSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.execution.HiveUDAFSuite", "org.apache.spark.sql.hive.execution.HiveUDAFSuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.command.AlterTableRenamePartitionSuite", "org.apache.spark.sql.hive.execution.command.AlterTableRenamePartitionSuite", "org.apache.spark.sql.hive.HiveSessionStateSuite", "org.apache.spark.sql.hive.HiveSessionStateSuite", "org.apache.spark.sql.hive.HiveVariableSubstitutionSuite", "org.apache.spark.sql.hive.HiveVariableSubstitutionSuite", "org.apache.spark.sql.hive.execution.HiveSerDeReadWriteSuite", "org.apache.spark.sql.hive.execution.HiveSerDeReadWriteSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite", "org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite", "org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite", "org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.UDFSuite", "org.apache.spark.sql.hive.UDFSuite", "org.apache.spark.sql.hive.execution.HashAggregationQueryWithControlledFallbackSuite", "org.apache.spark.sql.hive.execution.AggregationQuerySuite", "org.apache.spark.sql.hive.execution.HashAggregationQueryWithControlledFallbackSuite", "org.apache.spark.sql.hive.HiveUserDefinedTypeSuite", "org.apache.spark.sql.hive.HiveUserDefinedTypeSuite", "org.apache.spark.sql.HiveCharVarcharTestSuite", "org.apache.spark.sql.HiveCharVarcharTestSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.BigDataBenchmarkSuite", "org.apache.spark.sql.hive.execution.BigDataBenchmarkSuite", "org.apache.spark.sql.hive.execution.ObjectHashAggregateSuite", "org.apache.spark.sql.hive.execution.ObjectHashAggregateSuite", "org.apache.spark.sql.hive.execution.command.TruncateTableSuite", "org.apache.spark.sql.hive.execution.command.TruncateTableSuite", "org.apache.spark.sql.hive.execution.command.AlterTableRenameSuite", "org.apache.spark.sql.hive.execution.command.AlterTableRenameSuite", "org.apache.spark.sql.hive.execution.SQLMetricsSuite", "org.apache.spark.sql.hive.execution.SQLMetricsSuite", "org.apache.spark.sql.hive.OptimizeHiveMetadataOnlyQuerySuite", "org.apache.spark.sql.hive.OptimizeHiveMetadataOnlyQuerySuite", "org.apache.spark.sql.hive.execution.HiveCommandSuite", "org.apache.spark.sql.hive.execution.HiveCommandSuite", "org.apache.spark.sql.hive.QueryPartitionSuite", "org.apache.spark.sql.hive.QueryPartitionSuite", "org.apache.spark.sql.hive.execution.HashUDAQueryWithControlledFallbackSuite", "org.apache.spark.sql.hive.execution.UDAQuerySuite", "org.apache.spark.sql.hive.execution.HashUDAQueryWithControlledFallbackSuite", "org.apache.spark.sql.hive.ParquetHiveCompatibilitySuite", "org.apache.spark.sql.hive.ParquetHiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.command.MsckRepairTableSuite", "org.apache.spark.sql.hive.execution.command.MsckRepairTableSuite", "org.apache.spark.sql.hive.HiveUDFDynamicLoadSuite", "org.apache.spark.sql.hive.HiveUDFDynamicLoadSuite", "org.apache.spark.sql.hive.TestHiveSuite", "org.apache.spark.sql.hive.TestHiveSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.HiveSQLInsertTestSuite", "org.apache.spark.sql.hive.HiveSQLInsertTestSuite", "org.apache.spark.sql.sources.CommitFailureTestRelationSuite", "org.apache.spark.sql.sources.CommitFailureTestRelationSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.client.HivePartitionFilteringSuite", "org.apache.spark.sql.hive.client.HivePartitionFilteringSuite", "org.apache.spark.sql.hive.client.HivePartitionFilteringSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.client.HivePartitionFilteringSuites.runNestedSuite", "org.apache.spark.sql.hive.client.HivePartitionFilteringSuite", "org.apache.spark.sql.hive.execution.HiveWindowFunctionQueryFileSuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveWindowFunctionQueryFileSuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveWindowFunctionQueryFileSuite", "org.apache.spark.sql.hive.HiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.HiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.ErrorPositionSuite", "org.apache.spark.sql.hive.ErrorPositionSuite", "org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.execution.HiveScriptTransformationSuite", "org.apache.spark.sql.hive.execution.HiveScriptTransformationSuite", "org.apache.spark.sql.hive.execution.HashAggregationQuerySuite", "org.apache.spark.sql.hive.execution.AggregationQuerySuite", "org.apache.spark.sql.hive.execution.HashAggregationQuerySuite", "org.apache.spark.sql.hive.execution.command.AlterTableRecoverPartitionsSuite", "org.apache.spark.sql.hive.execution.command.AlterTableRecoverPartitionsSuite", "org.apache.spark.sql.hive.execution.HiveExplainSuite", "org.apache.spark.sql.hive.execution.HiveExplainSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.execution.command.DropTableSuite", "org.apache.spark.sql.hive.execution.command.DropTableSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.execution.command.AlterTableDropPartitionSuite", "org.apache.spark.sql.hive.execution.command.AlterTableDropPartitionSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.hive.execution.PruningSuite", "org.apache.spark.sql.hive.execution.PruningSuite", "org.apache.spark.sql.hive.execution.PruningSuite", "org.apache.spark.sql.hive.execution.PruningSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.HiveCharVarcharDDLTestSuite", "org.apache.spark.sql.HiveCharVarcharDDLTestSuite", "org.apache.spark.sql.hive.execution.command.AlterTableAddPartitionSuite", "org.apache.spark.sql.hive.execution.command.AlterTableAddPartitionSuite", "org.apache.spark.sql.hive.JavaDataFrameSuite", "org.apache.spark.sql.hive.JavaDataFrameSuite", "org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite", "org.apache.spark.sql.hive.JavaDataFrameSuite", "org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.HiveParquetSuite", "org.apache.spark.sql.hive.HiveContextCompatibilitySuite", "org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite", "org.apache.spark.sql.hive.orc.HiveOrcQuerySuite", "org.apache.spark.sql.hive.execution.command.ShowPartitionsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.execution.command.ShowNamespacesSuite", "org.apache.spark.sql.hive.client.HiveClientUserNameSuite", "org.apache.spark.sql.hive.execution.HiveResolutionSuite", "org.apache.spark.sql.hive.execution.HiveDDLSuite", "org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite", "org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite", "org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite", "org.apache.spark.sql.hive.execution.command.ShowTablesSuite", "org.apache.spark.sql.hive.execution.HiveQuerySuite", "org.apache.spark.sql.hive.orc.HiveOrcPartitionDiscoverySuite", "org.apache.spark.sql.hive.InsertSuite", "org.apache.spark.sql.hive.CachedTableSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.sources.DisableUnnecessaryBucketedScanWithHiveSupportSuite", "org.apache.spark.sql.hive.CompressionCodecSuite", "org.apache.spark.sql.hive.execution.WindowQuerySuite", "org.apache.spark.sql.hive.HiveExternalSessionCatalogSuite", "org.apache.spark.sql.hive.execution.HiveSQLViewSuite", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.ListTablesSuite", "org.apache.spark.sql.sources.BucketedWriteWithHiveSupportSuite", "org.apache.spark.sql.hive.execution.HashUDAQuerySuite", "org.apache.spark.sql.hive.ParquetEncryptionSuite", "org.apache.spark.sql.hive.execution.PruneHiveTablePartitionsSuite", "org.apache.spark.sql.hive.HiveUtilsSuite", "org.apache.spark.sql.hive.HiveShimSuite", "org.apache.spark.sql.hive.HiveSharedStateSuite", "org.apache.spark.sql.hive.client.HadoopVersionInfoSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveSerDeSuite", "org.apache.spark.sql.hive.execution.HiveUDAFSuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.command.AlterTableRenamePartitionSuite", "org.apache.spark.sql.hive.HiveSessionStateSuite", "org.apache.spark.sql.hive.HiveVariableSubstitutionSuite", "org.apache.spark.sql.hive.execution.HiveSerDeReadWriteSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.UDFSuite", "org.apache.spark.sql.hive.execution.HashAggregationQueryWithControlledFallbackSuite", "org.apache.spark.sql.hive.HiveUserDefinedTypeSuite", "org.apache.spark.sql.HiveCharVarcharTestSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.BigDataBenchmarkSuite", "org.apache.spark.sql.hive.execution.ObjectHashAggregateSuite", "org.apache.spark.sql.hive.execution.command.TruncateTableSuite", "org.apache.spark.sql.hive.execution.command.AlterTableRenameSuite", "org.apache.spark.sql.hive.execution.SQLMetricsSuite", "org.apache.spark.sql.hive.OptimizeHiveMetadataOnlyQuerySuite", "org.apache.spark.sql.hive.execution.HiveCommandSuite", "org.apache.spark.sql.hive.QueryPartitionSuite", "org.apache.spark.sql.hive.execution.HashUDAQueryWithControlledFallbackSuite", "org.apache.spark.sql.hive.ParquetHiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.command.MsckRepairTableSuite", "org.apache.spark.sql.hive.HiveUDFDynamicLoadSuite", "org.apache.spark.sql.hive.TestHiveSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.HiveSQLInsertTestSuite", "org.apache.spark.sql.sources.CommitFailureTestRelationSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.client.HivePartitionFilteringSuite", "org.apache.spark.sql.hive.execution.HiveWindowFunctionQueryFileSuite", "org.apache.spark.sql.hive.HiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.ErrorPositionSuite", "org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite", "org.apache.spark.sql.hive.execution.HiveScriptTransformationSuite", "org.apache.spark.sql.hive.execution.HashAggregationQuerySuite", "org.apache.spark.sql.hive.execution.command.AlterTableRecoverPartitionsSuite", "org.apache.spark.sql.hive.execution.HiveExplainSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.execution.command.DropTableSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.execution.command.AlterTableDropPartitionSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.hive.execution.PruningSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.HiveCharVarcharDDLTestSuite", "org.apache.spark.sql.hive.execution.command.AlterTableAddPartitionSuite", "org.apache.spark.sql.execution.datasources.BasicWriteTaskStatsTrackerSuite", "org.apache.spark.sql.execution.datasources.BasicWriteTaskStatsTrackerSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV2SchemaPruningSuite", "org.apache.spark.sql.execution.datasources.SchemaPruningSuite", "org.apache.spark.sql.execution.datasources.SchemaPruningSuite", "org.apache.spark.sql.execution.datasources.SchemaPruningSuite", "org.apache.spark.sql.execution.datasources.SchemaPruningSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.TPCDSQuerySuite", "org.apache.spark.sql.TPCDSQuerySuite", "org.apache.spark.sql.TPCDSQuerySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV2FilterSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetFilterSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetFilterSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetFilterSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetFilterSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV1FilterSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetFilterSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetFilterSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetFilterSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetFilterSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.ExpressionsSchemaSuite", "org.apache.spark.sql.ExpressionsSchemaSuite", "org.apache.spark.sql.ExpressionsSchemaSuite", "org.apache.spark.sql.ExpressionsSchemaSuite", "org.apache.spark.sql.ExpressionsSchemaSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.TPCDSQueryWithStatsSuite", "org.apache.spark.sql.TPCDSQuerySuite", "org.apache.spark.sql.TPCDSQuerySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV1SchemaPruningSuite", "org.apache.spark.sql.execution.datasources.SchemaPruningSuite", "org.apache.spark.sql.execution.datasources.SchemaPruningSuite", "org.apache.spark.sql.execution.datasources.SchemaPruningSuite", "org.apache.spark.sql.execution.datasources.SchemaPruningSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.TPCDSQueryANSISuite", "org.apache.spark.sql.TPCDSQuerySuite", "org.apache.spark.sql.TPCDSQuerySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.BasicWriteTaskStatsTrackerSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV2SchemaPruningSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.TPCDSQuerySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV2FilterSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV1FilterSuite", "org.apache.spark.sql.ExpressionsSchemaSuite", "org.apache.spark.sql.TPCDSQueryWithStatsSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV1SchemaPruningSuite", "org.apache.spark.sql.TPCDSQueryANSISuite", "org.apache.spark.sql.SQLQuerySuite", "org.apache.spark.sql.SQLQuerySuite", "org.apache.spark.sql.SQLQuerySuite", "org.apache.spark.sql.SQLQuerySuite", "org.apache.spark.sql.SQLQuerySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.MemorySourceStressSuite", "org.apache.spark.sql.streaming.MemorySourceStressSuite", "org.apache.spark.sql.streaming.MemorySourceStressSuite", "org.apache.spark.sql.streaming.MemorySourceStressSuite", "org.apache.spark.sql.streaming.MemorySourceStressSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.HiveResultSuite", "org.apache.spark.sql.execution.HiveResultSuite", "org.apache.spark.sql.execution.HiveResultSuite", "org.apache.spark.sql.execution.HiveResultSuite", "org.apache.spark.sql.execution.HiveResultSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderSuite", "org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderSuite", "org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderSuite", "org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderSuite", "org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.DSV2CharVarcharDDLTestSuite", "org.apache.spark.sql.execution.command.DSV2CharVarcharDDLTestSuite", "org.apache.spark.sql.execution.command.DSV2CharVarcharDDLTestSuite", "org.apache.spark.sql.execution.command.DSV2CharVarcharDDLTestSuite", "org.apache.spark.sql.execution.command.DSV2CharVarcharDDLTestSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.SparkPlanSuite", "org.apache.spark.sql.execution.SparkPlanSuite", "org.apache.spark.sql.execution.SparkPlanSuite", "org.apache.spark.sql.execution.SparkPlanSuite", "org.apache.spark.sql.execution.SparkPlanSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.StreamingAggregationSuite", "org.apache.spark.sql.streaming.StreamingAggregationSuite", "org.apache.spark.sql.streaming.StreamingAggregationSuite", "org.apache.spark.sql.streaming.StreamingAggregationSuite", "org.apache.spark.sql.streaming.StreamingAggregationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DateFunctionsSuite", "org.apache.spark.sql.DateFunctionsSuite", "org.apache.spark.sql.DateFunctionsSuite", "org.apache.spark.sql.DateFunctionsSuite", "org.apache.spark.sql.DateFunctionsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.NestedDataSourceV1Suite", "org.apache.spark.sql.NestedDataSourceV1Suite", "org.apache.spark.sql.NestedDataSourceV1Suite", "org.apache.spark.sql.NestedDataSourceV1Suite", "org.apache.spark.sql.NestedDataSourceV1Suite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.state.StateStoreSuite", "org.apache.spark.sql.execution.streaming.state.StateStoreSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.state.StateStoreSuite", "org.apache.spark.sql.execution.streaming.state.StateStoreSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.state.StateStoreSuite", "org.apache.spark.sql.execution.streaming.state.StateStoreSuite", "org.apache.spark.sql.execution.streaming.sources.ForeachBatchSinkSuite", "org.apache.spark.sql.execution.streaming.sources.ForeachBatchSinkSuite", "org.apache.spark.sql.execution.streaming.sources.ForeachBatchSinkSuite", "org.apache.spark.sql.execution.streaming.sources.ForeachBatchSinkSuite", "org.apache.spark.sql.execution.streaming.sources.ForeachBatchSinkSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.jdbc.JDBCV2Suite", "org.apache.spark.sql.jdbc.JDBCV2Suite", "org.apache.spark.sql.jdbc.JDBCV2Suite", "org.apache.spark.sql.jdbc.JDBCV2Suite", "org.apache.spark.sql.jdbc.JDBCV2Suite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.continuous.ContinuousQueryStatusAndProgressSuite", "org.apache.spark.sql.streaming.continuous.ContinuousSuite", "org.apache.spark.sql.streaming.continuous.ContinuousSuite", "org.apache.spark.sql.streaming.continuous.ContinuousSuite", "org.apache.spark.sql.streaming.continuous.ContinuousSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite", "org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite", "org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite", "org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite", "org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRenameSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRenameSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRenameSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRenameSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRenameSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.v2.V2SessionCatalogTableSuite", "org.apache.spark.sql.execution.datasources.v2.V2SessionCatalogBaseSuite", "org.apache.spark.sql.execution.datasources.v2.V2SessionCatalogBaseSuite", "org.apache.spark.sql.execution.datasources.v2.V2SessionCatalogBaseSuite", "org.apache.spark.sql.execution.datasources.v2.V2SessionCatalogTableSuite", "org.apache.spark.sql.execution.datasources.v2.V2SessionCatalogBaseSuite", "org.apache.spark.sql.execution.datasources.v2.V2SessionCatalogBaseSuite", "org.apache.spark.sql.execution.datasources.BasicWriteTaskStatsTrackerSuite", "org.apache.spark.sql.execution.datasources.BasicWriteTaskStatsTrackerSuite", "org.apache.spark.sql.execution.arrow.ArrowConvertersSuite", "org.apache.spark.sql.execution.arrow.ArrowConvertersSuite", "org.apache.spark.sql.execution.arrow.ArrowConvertersSuite", "org.apache.spark.sql.execution.arrow.ArrowConvertersSuite", "org.apache.spark.sql.execution.arrow.ArrowConvertersSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.v2.AlterTableDropPartitionSuite", "org.apache.spark.sql.execution.command.v2.AlterTableDropPartitionSuite", "org.apache.spark.sql.execution.command.v2.AlterTableDropPartitionSuite", "org.apache.spark.sql.execution.command.v2.AlterTableDropPartitionSuite", "org.apache.spark.sql.execution.command.v2.AlterTableDropPartitionSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DynamicPartitionPruningV1Suite", "org.apache.spark.sql.DynamicPartitionPruningSuite", "org.apache.spark.sql.DynamicPartitionPruningSuite", "org.apache.spark.sql.DynamicPartitionPruningSuite", "org.apache.spark.sql.DynamicPartitionPruningSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite", "org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite", "org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite", "org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite", "org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.v1.ShowPartitionsSuite", "org.apache.spark.sql.execution.command.v1.ShowPartitionsSuite", "org.apache.spark.sql.execution.command.v1.ShowPartitionsSuite", "org.apache.spark.sql.execution.command.v1.ShowPartitionsSuite", "org.apache.spark.sql.execution.command.v1.ShowPartitionsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.ReplaceNullWithFalseInPredicateEndToEndSuite", "org.apache.spark.sql.ReplaceNullWithFalseInPredicateEndToEndSuite", "org.apache.spark.sql.ReplaceNullWithFalseInPredicateEndToEndSuite", "org.apache.spark.sql.ReplaceNullWithFalseInPredicateEndToEndSuite", "org.apache.spark.sql.ReplaceNullWithFalseInPredicateEndToEndSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.continuous.ContinuousEpochBacklogSuite", "org.apache.spark.sql.streaming.continuous.ContinuousEpochBacklogSuite", "org.apache.spark.sql.streaming.continuous.ContinuousSuite", "org.apache.spark.sql.streaming.continuous.ContinuousSuite", "org.apache.spark.sql.streaming.continuous.ContinuousSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DeprecatedAPISuite", "org.apache.spark.sql.DeprecatedAPISuite", "org.apache.spark.sql.DeprecatedAPISuite", "org.apache.spark.sql.DeprecatedAPISuite", "org.apache.spark.sql.DeprecatedAPISuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.MergedParquetReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.MergedParquetReadSchemaSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.StreamingQueryListenersConfSuite", "org.apache.spark.sql.streaming.StreamingQueryListenersConfSuite", "org.apache.spark.sql.streaming.StreamingQueryListenersConfSuite", "org.apache.spark.sql.streaming.StreamingQueryListenersConfSuite", "org.apache.spark.sql.streaming.StreamingQueryListenersConfSuite", "org.apache.spark.sql.streaming.StreamingQueryListenersConfSuite", "org.apache.spark.sql.streaming.StreamingQueryListenersConfSuite", "org.apache.spark.sql.execution.datasources.OrcReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.OrcReadSchemaSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.jdbc.JDBCSuite", "org.apache.spark.sql.jdbc.JDBCSuite", "org.apache.spark.sql.jdbc.JDBCSuite", "org.apache.spark.sql.jdbc.JDBCSuite", "org.apache.spark.sql.jdbc.JDBCSuite", "org.apache.spark.sql.jdbc.JDBCSuite", "org.apache.spark.sql.jdbc.JDBCSuite", "org.apache.spark.sql.execution.datasources.HeaderCSVReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCTableCatalogSuite", "org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCTableCatalogSuite", "org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCTableCatalogSuite", "org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCTableCatalogSuite", "org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCTableCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DataFrameJoinSuite", "org.apache.spark.sql.DataFrameJoinSuite", "org.apache.spark.sql.DataFrameJoinSuite", "org.apache.spark.sql.DataFrameJoinSuite", "org.apache.spark.sql.DataFrameJoinSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.SQLWindowFunctionSuite", "org.apache.spark.sql.execution.SQLWindowFunctionSuite", "org.apache.spark.sql.execution.SQLWindowFunctionSuite", "org.apache.spark.sql.execution.SQLWindowFunctionSuite", "org.apache.spark.sql.execution.SQLWindowFunctionSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.continuous.EpochCoordinatorSuite", "org.apache.spark.sql.streaming.continuous.EpochCoordinatorSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.sources.StreamingDataSourceV2Suite", "org.apache.spark.sql.streaming.sources.StreamingDataSourceV2Suite", "org.apache.spark.sql.streaming.sources.StreamingDataSourceV2Suite", "org.apache.spark.sql.streaming.sources.StreamingDataSourceV2Suite", "org.apache.spark.sql.streaming.sources.StreamingDataSourceV2Suite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.execution.command.v2.AlterTableRecoverPartitionsSuite", "org.apache.spark.sql.execution.command.v2.AlterTableRecoverPartitionsSuite", "org.apache.spark.sql.execution.command.v2.AlterTableRecoverPartitionsSuite", "org.apache.spark.sql.execution.command.v2.AlterTableRecoverPartitionsSuite", "org.apache.spark.sql.execution.command.v2.AlterTableRecoverPartitionsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.TypedImperativeAggregateSuite", "org.apache.spark.sql.TypedImperativeAggregateSuite", "org.apache.spark.sql.TypedImperativeAggregateSuite", "org.apache.spark.sql.TypedImperativeAggregateSuite", "org.apache.spark.sql.TypedImperativeAggregateSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.binaryfile.BinaryFileFormatSuite", "org.apache.spark.sql.execution.datasources.binaryfile.BinaryFileFormatSuite", "org.apache.spark.sql.execution.datasources.binaryfile.BinaryFileFormatSuite", "org.apache.spark.sql.execution.datasources.binaryfile.BinaryFileFormatSuite", "org.apache.spark.sql.execution.datasources.binaryfile.BinaryFileFormatSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.FileIndexSuite", "org.apache.spark.sql.execution.datasources.FileIndexSuite", "org.apache.spark.sql.execution.datasources.FileIndexSuite", "org.apache.spark.sql.execution.datasources.FileIndexSuite", "org.apache.spark.sql.execution.datasources.FileIndexSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetRebaseDatetimeV2Suite", "org.apache.spark.sql.execution.datasources.parquet.ParquetRebaseDatetimeSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetRebaseDatetimeSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetRebaseDatetimeSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetRebaseDatetimeSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DatasetPrimitiveSuite", "org.apache.spark.sql.DatasetPrimitiveSuite", "org.apache.spark.sql.DatasetPrimitiveSuite", "org.apache.spark.sql.DatasetPrimitiveSuite", "org.apache.spark.sql.DatasetPrimitiveSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.text.WholeTextFileV2Suite", "org.apache.spark.sql.execution.datasources.text.WholeTextFileSuite", "org.apache.spark.sql.execution.datasources.text.WholeTextFileSuite", "org.apache.spark.sql.execution.datasources.text.WholeTextFileSuite", "org.apache.spark.sql.execution.datasources.text.WholeTextFileSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetColumnIndexSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetColumnIndexSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetColumnIndexSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetColumnIndexSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetColumnIndexSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.PersistedViewTestSuite", "org.apache.spark.sql.execution.PersistedViewTestSuite", "org.apache.spark.sql.execution.PersistedViewTestSuite", "org.apache.spark.sql.execution.PersistedViewTestSuite", "org.apache.spark.sql.execution.PersistedViewTestSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.connector.DataSourceV2FunctionSuite", "org.apache.spark.sql.connector.DataSourceV2FunctionSuite", "org.apache.spark.sql.connector.DataSourceV2FunctionSuite", "org.apache.spark.sql.connector.DataSourceV2FunctionSuite", "org.apache.spark.sql.connector.DataSourceV2FunctionSuite", "org.apache.spark.sql.connector.DataSourceV2FunctionSuite", "org.apache.spark.sql.connector.DataSourceV2FunctionSuite", "org.apache.spark.sql.execution.streaming.UpdatingSessionsIteratorSuite", "org.apache.spark.sql.execution.streaming.UpdatingSessionsIteratorSuite", "org.apache.spark.sql.execution.streaming.UpdatingSessionsIteratorSuite", "org.apache.spark.sql.execution.streaming.UpdatingSessionsIteratorSuite", "org.apache.spark.sql.execution.streaming.UpdatingSessionsIteratorSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.connector.V1ReadFallbackWithCatalogSuite", "org.apache.spark.sql.connector.V1ReadFallbackSuite", "org.apache.spark.sql.connector.V1ReadFallbackSuite", "org.apache.spark.sql.connector.V1ReadFallbackSuite", "org.apache.spark.sql.connector.V1ReadFallbackWithCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.connector.V1WriteFallbackSessionCatalogSuite", "org.apache.spark.sql.execution.streaming.state.StateSchemaCompatibilityCheckerSuite", "org.apache.spark.sql.execution.streaming.state.StateSchemaCompatibilityCheckerSuite", "org.apache.spark.sql.execution.streaming.state.StateSchemaCompatibilityCheckerSuite", "org.apache.spark.sql.execution.streaming.state.StateSchemaCompatibilityCheckerSuite", "org.apache.spark.sql.execution.streaming.state.StateSchemaCompatibilityCheckerSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.internal.CatalogSuite", "org.apache.spark.sql.internal.CatalogSuite", "org.apache.spark.sql.internal.CatalogSuite", "org.apache.spark.sql.internal.CatalogSuite", "org.apache.spark.sql.internal.CatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.connector.WriteDistributionAndOrderingSuite", "org.apache.spark.sql.connector.WriteDistributionAndOrderingSuite", "org.apache.spark.sql.connector.WriteDistributionAndOrderingSuite", "org.apache.spark.sql.connector.WriteDistributionAndOrderingSuite", "org.apache.spark.sql.connector.WriteDistributionAndOrderingSuite", "org.apache.spark.sql.connector.WriteDistributionAndOrderingSuite", "org.apache.spark.sql.connector.WriteDistributionAndOrderingSuite", "org.apache.spark.sql.streaming.continuous.ContinuousQueuedDataReaderSuite", "org.apache.spark.sql.streaming.continuous.ContinuousQueuedDataReaderSuite", "org.apache.spark.sql.streaming.continuous.ContinuousQueuedDataReaderSuite", "org.apache.spark.sql.streaming.continuous.ContinuousQueuedDataReaderSuite", "org.apache.spark.sql.streaming.continuous.ContinuousQueuedDataReaderSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.GeneratorFunctionSuite", "org.apache.spark.sql.GeneratorFunctionSuite", "org.apache.spark.sql.GeneratorFunctionSuite", "org.apache.spark.sql.GeneratorFunctionSuite", "org.apache.spark.sql.GeneratorFunctionSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.csv.CSVLegacyTimeParserSuite", "org.apache.spark.sql.execution.datasources.csv.CSVSuite", "org.apache.spark.sql.execution.datasources.csv.CSVSuite", "org.apache.spark.sql.execution.datasources.csv.CSVSuite", "org.apache.spark.sql.execution.datasources.csv.CSVSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorSuite", "org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorSuite", "org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.DataSourceStrategySuite", "org.apache.spark.sql.execution.datasources.DataSourceStrategySuite", "org.apache.spark.sql.execution.datasources.DataSourceStrategySuite", "org.apache.spark.sql.execution.datasources.DataSourceStrategySuite", "org.apache.spark.sql.execution.datasources.DataSourceStrategySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.FileStreamSourceSuite", "org.apache.spark.sql.streaming.FileStreamSourceSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.JoinHintSuite", "org.apache.spark.sql.JoinHintSuite", "org.apache.spark.sql.JoinHintSuite", "org.apache.spark.sql.JoinHintSuite", "org.apache.spark.sql.JoinHintSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DynamicPartitionPruningV2Suite", "org.apache.spark.sql.DynamicPartitionPruningSuite", "org.apache.spark.sql.DynamicPartitionPruningSuite", "org.apache.spark.sql.DynamicPartitionPruningSuite", "org.apache.spark.sql.DynamicPartitionPruningSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.StreamingSessionWindowSuite", "org.apache.spark.sql.streaming.StreamingSessionWindowSuite", "org.apache.spark.sql.streaming.StreamingSessionWindowSuite", "org.apache.spark.sql.streaming.StreamingSessionWindowSuite", "org.apache.spark.sql.streaming.StreamingSessionWindowSuite", "org.apache.spark.sql.streaming.StreamingSessionWindowSuite", "org.apache.spark.sql.streaming.StreamingSessionWindowSuite", "org.apache.spark.sql.DSV2SQLInsertTestSuite", "org.apache.spark.sql.DSV2SQLInsertTestSuite", "org.apache.spark.sql.DSV2SQLInsertTestSuite", "org.apache.spark.sql.DSV2SQLInsertTestSuite", "org.apache.spark.sql.DSV2SQLInsertTestSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.python.ExtractPythonUDFsSuite", "org.apache.spark.sql.execution.python.ExtractPythonUDFsSuite", "org.apache.spark.sql.execution.python.ExtractPythonUDFsSuite", "org.apache.spark.sql.execution.python.ExtractPythonUDFsSuite", "org.apache.spark.sql.execution.python.ExtractPythonUDFsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DynamicPartitionPruningV1Suite", "org.apache.spark.sql.DynamicPartitionPruningSuite", "org.apache.spark.sql.DynamicPartitionPruningSuite", "org.apache.spark.sql.DynamicPartitionPruningSuite", "org.apache.spark.sql.DynamicPartitionPruningSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite", "org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite", "org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite", "org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite", "org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.ui.SQLAppStatusListenerSuite", "org.apache.spark.sql.execution.ui.SQLAppStatusListenerSuite", "org.apache.spark.sql.execution.ui.SQLAppStatusListenerSuite", "org.apache.spark.sql.execution.ui.SQLAppStatusListenerSuite", "org.apache.spark.sql.execution.ui.SQLAppStatusListenerSuite", "org.apache.spark.sql.execution.ui.SQLAppStatusListenerSuite", "org.apache.spark.sql.execution.ui.SQLAppStatusListenerSuite", "org.apache.spark.sql.execution.datasources.orc.OrcV1FilterSuite", "org.apache.spark.sql.execution.datasources.orc.OrcFilterSuite", "org.apache.spark.sql.execution.datasources.orc.OrcFilterSuite", "org.apache.spark.sql.execution.datasources.orc.OrcFilterSuite", "org.apache.spark.sql.execution.datasources.orc.OrcFilterSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.connector.DataSourceV2SQLSuite", "org.apache.spark.sql.DatasetCacheSuite", "org.apache.spark.sql.DatasetCacheSuite", "org.apache.spark.sql.DatasetCacheSuite", "org.apache.spark.sql.DatasetCacheSuite", "org.apache.spark.sql.DatasetCacheSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.JoinSuite", "org.apache.spark.sql.JoinSuite", "org.apache.spark.sql.JoinSuite", "org.apache.spark.sql.JoinSuite", "org.apache.spark.sql.JoinSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.joins.BroadcastJoinSuite", "org.apache.spark.sql.execution.joins.BroadcastJoinSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.UDFSuite", "org.apache.spark.sql.UDFSuite", "org.apache.spark.sql.UDFSuite", "org.apache.spark.sql.UDFSuite", "org.apache.spark.sql.UDFSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRecoverPartitionsSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRecoverPartitionsSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRecoverPartitionsSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRecoverPartitionsSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRecoverPartitionsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.CachedTableSuite", "org.apache.spark.sql.CachedTableSuite", "org.apache.spark.sql.CachedTableSuite", "org.apache.spark.sql.CachedTableSuite", "org.apache.spark.sql.CachedTableSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.ShowTablesParserSuite", "org.apache.spark.sql.execution.command.ShowTablesParserSuite", "org.apache.spark.sql.execution.command.ShowTablesParserSuite", "org.apache.spark.sql.execution.command.ShowTablesParserSuite", "org.apache.spark.sql.execution.command.ShowTablesParserSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.sources.PathOptionSuite", "org.apache.spark.sql.sources.PathOptionSuite", "org.apache.spark.sql.sources.PathOptionSuite", "org.apache.spark.sql.sources.PathOptionSuite", "org.apache.spark.sql.sources.PathOptionSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.AlterTableAddPartitionParserSuite", "org.apache.spark.sql.execution.command.AlterTableAddPartitionParserSuite", "org.apache.spark.sql.execution.command.AlterTableAddPartitionParserSuite", "org.apache.spark.sql.execution.command.AlterTableAddPartitionParserSuite", "org.apache.spark.sql.execution.command.AlterTableAddPartitionParserSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaInferenceSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.GlobalTempViewTestSuite", "org.apache.spark.sql.execution.GlobalTempViewTestSuite", "org.apache.spark.sql.execution.GlobalTempViewTestSuite", "org.apache.spark.sql.execution.GlobalTempViewTestSuite", "org.apache.spark.sql.execution.GlobalTempViewTestSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.SerializationSuite", "org.apache.spark.sql.SerializationSuite", "org.apache.spark.sql.SerializationSuite", "org.apache.spark.sql.SerializationSuite", "org.apache.spark.sql.SerializationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DataFrameWindowFunctionsSuite", "org.apache.spark.sql.DataFrameWindowFunctionsSuite", "org.apache.spark.sql.DataFrameWindowFunctionsSuite", "org.apache.spark.sql.DataFrameWindowFunctionsSuite", "org.apache.spark.sql.DataFrameWindowFunctionsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.text.TextV1Suite", "org.apache.spark.sql.execution.datasources.text.TextSuite", "org.apache.spark.sql.execution.datasources.text.TextSuite", "org.apache.spark.sql.execution.datasources.text.TextSuite", "org.apache.spark.sql.execution.datasources.text.TextSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.sources.BucketedWriteWithoutHiveSupportSuite", "org.apache.spark.sql.sources.BucketedWriteWithoutHiveSupportSuite", "org.apache.spark.sql.sources.BucketedWriteWithoutHiveSupportSuite", "org.apache.spark.sql.sources.BucketedWriteWithoutHiveSupportSuite", "org.apache.spark.sql.sources.BucketedWriteWithoutHiveSupportSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetAvroCompatibilitySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetAvroCompatibilitySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetAvroCompatibilitySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetAvroCompatibilitySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetAvroCompatibilitySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.sources.CreateTableAsSelectSuite", "org.apache.spark.sql.sources.CreateTableAsSelectSuite", "org.apache.spark.sql.sources.CreateTableAsSelectSuite", "org.apache.spark.sql.sources.CreateTableAsSelectSuite", "org.apache.spark.sql.sources.CreateTableAsSelectSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.ColumnarRulesSuite", "org.apache.spark.sql.execution.ColumnarRulesSuite", "org.apache.spark.sql.execution.ColumnarRulesSuite", "org.apache.spark.sql.execution.ColumnarRulesSuite", "org.apache.spark.sql.execution.ColumnarRulesSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.SQLContextSuite", "org.apache.spark.sql.SQLContextSuite", "org.apache.spark.sql.SQLContextSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.UnwrapCastInComparisonEndToEndSuite", "org.apache.spark.sql.UnwrapCastInComparisonEndToEndSuite", "org.apache.spark.sql.UnwrapCastInComparisonEndToEndSuite", "org.apache.spark.sql.UnwrapCastInComparisonEndToEndSuite", "org.apache.spark.sql.UnwrapCastInComparisonEndToEndSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.SetCommandSuite", "org.apache.spark.sql.SetCommandSuite", "org.apache.spark.sql.SetCommandSuite", "org.apache.spark.sql.SetCommandSuite", "org.apache.spark.sql.SetCommandSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.StatisticsCollectionSuite", "org.apache.spark.sql.StatisticsCollectionSuite", "org.apache.spark.sql.StatisticsCollectionSuite", "org.apache.spark.sql.StatisticsCollectionSuite", "org.apache.spark.sql.StatisticsCollectionSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.SSBQuerySuite", "org.apache.spark.sql.SSBQuerySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.StreamingQueryListenerSuite", "org.apache.spark.sql.streaming.StreamingQueryListenerSuite", "org.apache.spark.sql.streaming.StreamingQueryListenerSuite", "org.apache.spark.sql.streaming.StreamingQueryListenerSuite", "org.apache.spark.sql.streaming.StreamingQueryListenerSuite", "org.apache.spark.sql.streaming.StreamingQueryListenerSuite", "org.apache.spark.sql.streaming.StreamingQueryListenerSuite", "org.apache.spark.sql.PlanStabilitySuite", "org.apache.spark.sql.PlanStabilitySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.FileStreamSinkV1Suite", "org.apache.spark.sql.streaming.FileStreamSinkSuite", "org.apache.spark.sql.streaming.FileStreamSinkSuite", "org.apache.spark.sql.streaming.FileStreamSinkSuite", "org.apache.spark.sql.streaming.FileStreamSinkSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.StreamingOuterJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.test.DataFrameReaderWriterSuite", "org.apache.spark.sql.test.DataFrameReaderWriterSuite", "org.apache.spark.sql.test.DataFrameReaderWriterSuite", "org.apache.spark.sql.test.DataFrameReaderWriterSuite", "org.apache.spark.sql.test.DataFrameReaderWriterSuite", "org.apache.spark.sql.test.DataFrameReaderWriterSuite", "org.apache.spark.sql.test.DataFrameReaderWriterSuite", "org.apache.spark.sql.StringFunctionsSuite", "org.apache.spark.sql.StringFunctionsSuite", "org.apache.spark.sql.StringFunctionsSuite", "org.apache.spark.sql.StringFunctionsSuite", "org.apache.spark.sql.StringFunctionsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.NestedDataSourceV2Suite", "org.apache.spark.sql.NestedDataSourceV2Suite", "org.apache.spark.sql.NestedDataSourceV2Suite", "org.apache.spark.sql.NestedDataSourceV2Suite", "org.apache.spark.sql.NestedDataSourceV2Suite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.AlterTableRecoverPartitionsParserSuite", "org.apache.spark.sql.execution.command.AlterTableRecoverPartitionsParserSuite", "org.apache.spark.sql.execution.command.AlterTableRecoverPartitionsParserSuite", "org.apache.spark.sql.execution.command.AlterTableRecoverPartitionsParserSuite", "org.apache.spark.sql.execution.command.AlterTableRecoverPartitionsParserSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetRebaseDatetimeV1Suite", "org.apache.spark.sql.execution.datasources.parquet.ParquetRebaseDatetimeSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetRebaseDatetimeSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetRebaseDatetimeSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetRebaseDatetimeSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.sources.PrunedScanSuite", "org.apache.spark.sql.sources.PrunedScanSuite", "org.apache.spark.sql.sources.PrunedScanSuite", "org.apache.spark.sql.sources.PrunedScanSuite", "org.apache.spark.sql.sources.PrunedScanSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.TruncateTableParserSuite", "org.apache.spark.sql.execution.command.TruncateTableParserSuite", "org.apache.spark.sql.execution.command.TruncateTableParserSuite", "org.apache.spark.sql.execution.command.TruncateTableParserSuite", "org.apache.spark.sql.execution.command.TruncateTableParserSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.CompactibleFileStreamLogSuite", "org.apache.spark.sql.execution.streaming.CompactibleFileStreamLogSuite", "org.apache.spark.sql.execution.streaming.CompactibleFileStreamLogSuite", "org.apache.spark.sql.execution.streaming.CompactibleFileStreamLogSuite", "org.apache.spark.sql.execution.streaming.CompactibleFileStreamLogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.StreamingQueryManagerSuite", "org.apache.spark.sql.streaming.StreamingQueryManagerSuite", "org.apache.spark.sql.streaming.StreamingQueryManagerSuite", "org.apache.spark.sql.streaming.StreamingQueryManagerSuite", "org.apache.spark.sql.streaming.StreamingQueryManagerSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.noop.NoopStreamSuite", "org.apache.spark.sql.execution.datasources.noop.NoopStreamSuite", "org.apache.spark.sql.execution.datasources.noop.NoopStreamSuite", "org.apache.spark.sql.execution.datasources.noop.NoopStreamSuite", "org.apache.spark.sql.execution.datasources.noop.NoopStreamSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.CsvFunctionsSuite", "org.apache.spark.sql.CsvFunctionsSuite", "org.apache.spark.sql.CsvFunctionsSuite", "org.apache.spark.sql.CsvFunctionsSuite", "org.apache.spark.sql.CsvFunctionsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.connector.DataSourceV2DataFrameSessionCatalogSuite", "org.apache.spark.sql.execution.SortSuite", "org.apache.spark.sql.execution.SortSuite", "org.apache.spark.sql.execution.SortSuite", "org.apache.spark.sql.execution.SortSuite", "org.apache.spark.sql.execution.SortSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.SparkSessionBuilderSuite", "org.apache.spark.sql.SparkSessionBuilderSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.continuous.ContinuousStressSuite", "org.apache.spark.sql.streaming.continuous.ContinuousSuite", "org.apache.spark.sql.streaming.continuous.ContinuousSuite", "org.apache.spark.sql.streaming.continuous.ContinuousSuite", "org.apache.spark.sql.streaming.continuous.ContinuousSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.columnar.RefCountedTestCachedBatchSerializerSuite", "org.apache.spark.sql.execution.columnar.RefCountedTestCachedBatchSerializerSuite", "org.apache.spark.sql.execution.columnar.RefCountedTestCachedBatchSerializerSuite", "org.apache.spark.sql.execution.columnar.RefCountedTestCachedBatchSerializerSuite", "org.apache.spark.sql.execution.columnar.RefCountedTestCachedBatchSerializerSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV1QuerySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.sources.FilteredScanSuite", "org.apache.spark.sql.sources.FilteredScanSuite", "org.apache.spark.sql.sources.FilteredScanSuite", "org.apache.spark.sql.sources.FilteredScanSuite", "org.apache.spark.sql.sources.FilteredScanSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.BasicCharVarcharTestSuite", "org.apache.spark.sql.BasicCharVarcharTestSuite", "org.apache.spark.sql.BasicCharVarcharTestSuite", "org.apache.spark.sql.BasicCharVarcharTestSuite", "org.apache.spark.sql.BasicCharVarcharTestSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.SubquerySuite", "org.apache.spark.sql.SubquerySuite", "org.apache.spark.sql.SubquerySuite", "org.apache.spark.sql.SubquerySuite", "org.apache.spark.sql.SubquerySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.ShowPartitionsParserSuite", "org.apache.spark.sql.execution.command.ShowPartitionsParserSuite", "org.apache.spark.sql.execution.command.ShowPartitionsParserSuite", "org.apache.spark.sql.execution.command.ShowPartitionsParserSuite", "org.apache.spark.sql.execution.command.ShowPartitionsParserSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.InMemoryTableMetricSuite", "org.apache.spark.sql.execution.datasources.InMemoryTableMetricSuite", "org.apache.spark.sql.execution.datasources.InMemoryTableMetricSuite", "org.apache.spark.sql.execution.datasources.InMemoryTableMetricSuite", "org.apache.spark.sql.execution.datasources.InMemoryTableMetricSuite", "org.apache.spark.sql.execution.datasources.InMemoryTableMetricSuite", "org.apache.spark.sql.execution.datasources.InMemoryTableMetricSuite", "org.apache.spark.sql.execution.ui.AllExecutionsPageSuite", "org.apache.spark.sql.execution.ui.AllExecutionsPageSuite", "org.apache.spark.sql.execution.ui.AllExecutionsPageSuite", "org.apache.spark.sql.execution.ui.AllExecutionsPageSuite", "org.apache.spark.sql.execution.ui.AllExecutionsPageSuite", "org.apache.spark.sql.execution.ui.AllExecutionsPageSuite", "org.apache.spark.sql.execution.ui.AllExecutionsPageSuite", "org.apache.spark.sql.sources.TableScanSuite", "org.apache.spark.sql.sources.TableScanSuite", "org.apache.spark.sql.sources.TableScanSuite", "org.apache.spark.sql.sources.TableScanSuite", "org.apache.spark.sql.sources.TableScanSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.StreamingDeduplicationSuite", "org.apache.spark.sql.streaming.StreamingDeduplicationSuite", "org.apache.spark.sql.streaming.StreamingDeduplicationSuite", "org.apache.spark.sql.streaming.StreamingDeduplicationSuite", "org.apache.spark.sql.streaming.StreamingDeduplicationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.v1.MsckRepairTableSuite", "org.apache.spark.sql.execution.command.v1.MsckRepairTableSuite", "org.apache.spark.sql.execution.command.v1.MsckRepairTableSuite", "org.apache.spark.sql.execution.command.v1.MsckRepairTableSuite", "org.apache.spark.sql.execution.command.v1.MsckRepairTableSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.MetadataCacheV2Suite", "org.apache.spark.sql.MetadataCacheSuite", "org.apache.spark.sql.MetadataCacheSuite", "org.apache.spark.sql.MetadataCacheSuite", "org.apache.spark.sql.MetadataCacheSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.AggregatingAccumulatorSuite", "org.apache.spark.sql.execution.AggregatingAccumulatorSuite", "org.apache.spark.sql.execution.AggregatingAccumulatorSuite", "org.apache.spark.sql.execution.AggregatingAccumulatorSuite", "org.apache.spark.sql.execution.AggregatingAccumulatorSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.state.StateStoreCompatibilitySuite", "org.apache.spark.sql.execution.streaming.state.StateStoreCompatibilitySuite", "org.apache.spark.sql.execution.streaming.state.StateStoreCompatibilitySuite", "org.apache.spark.sql.execution.streaming.state.StateStoreCompatibilitySuite", "org.apache.spark.sql.execution.streaming.state.StateStoreCompatibilitySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.internal.SQLConfSuite", "org.apache.spark.sql.internal.SQLConfSuite", "org.apache.spark.sql.internal.SQLConfSuite", "org.apache.spark.sql.internal.SQLConfSuite", "org.apache.spark.sql.internal.SQLConfSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.jdbc.JDBCNestedDataSourceSuite", "org.apache.spark.sql.jdbc.JDBCNestedDataSourceSuite", "org.apache.spark.sql.jdbc.JDBCNestedDataSourceSuite", "org.apache.spark.sql.jdbc.JDBCNestedDataSourceSuite", "org.apache.spark.sql.jdbc.JDBCNestedDataSourceSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.DataSourceSuite", "org.apache.spark.sql.execution.datasources.DataSourceSuite", "org.apache.spark.sql.execution.datasources.DataSourceSuite", "org.apache.spark.sql.execution.datasources.DataSourceSuite", "org.apache.spark.sql.execution.datasources.DataSourceSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.text.TextV2Suite", "org.apache.spark.sql.execution.datasources.text.TextSuite", "org.apache.spark.sql.execution.datasources.text.TextSuite", "org.apache.spark.sql.execution.datasources.text.TextSuite", "org.apache.spark.sql.execution.datasources.text.TextSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.MathFunctionsSuite", "org.apache.spark.sql.MathFunctionsSuite", "org.apache.spark.sql.MathFunctionsSuite", "org.apache.spark.sql.MathFunctionsSuite", "org.apache.spark.sql.MathFunctionsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.DeprecatedStreamingAggregationSuite", "org.apache.spark.sql.streaming.DeprecatedStreamingAggregationSuite", "org.apache.spark.sql.streaming.DeprecatedStreamingAggregationSuite", "org.apache.spark.sql.streaming.DeprecatedStreamingAggregationSuite", "org.apache.spark.sql.streaming.DeprecatedStreamingAggregationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.sources.TextSocketStreamSuite", "org.apache.spark.sql.execution.streaming.sources.TextSocketStreamSuite", "org.apache.spark.sql.execution.streaming.sources.TextSocketStreamSuite", "org.apache.spark.sql.execution.streaming.sources.TextSocketStreamSuite", "org.apache.spark.sql.execution.streaming.sources.TextSocketStreamSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.CTEHintSuite", "org.apache.spark.sql.CTEHintSuite", "org.apache.spark.sql.CTEHintSuite", "org.apache.spark.sql.CTEHintSuite", "org.apache.spark.sql.CTEHintSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.ParquetReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ParquetReadSchemaSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.joins.InnerJoinSuite", "org.apache.spark.sql.execution.joins.InnerJoinSuite", "org.apache.spark.sql.execution.joins.InnerJoinSuite", "org.apache.spark.sql.execution.joins.InnerJoinSuite", "org.apache.spark.sql.execution.joins.InnerJoinSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.RowDataSourceStrategySuite", "org.apache.spark.sql.execution.datasources.RowDataSourceStrategySuite", "org.apache.spark.sql.execution.datasources.RowDataSourceStrategySuite", "org.apache.spark.sql.execution.datasources.RowDataSourceStrategySuite", "org.apache.spark.sql.execution.datasources.RowDataSourceStrategySuite", "org.apache.spark.sql.execution.datasources.RowDataSourceStrategySuite", "org.apache.spark.sql.execution.datasources.RowDataSourceStrategySuite", "org.apache.spark.sql.execution.GlobalTempViewSuite", "org.apache.spark.sql.execution.GlobalTempViewSuite", "org.apache.spark.sql.execution.GlobalTempViewSuite", "org.apache.spark.sql.execution.GlobalTempViewSuite", "org.apache.spark.sql.execution.GlobalTempViewSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRecoverPartitionsParallelSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRecoverPartitionsParallelSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRecoverPartitionsParallelSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRecoverPartitionsParallelSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRecoverPartitionsParallelSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.ui.SQLAppStatusListenerMemoryLeakSuite", "org.apache.spark.sql.execution.ui.SQLAppStatusListenerMemoryLeakSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.orc.OrcPartitionDiscoverySuite", "org.apache.spark.sql.execution.datasources.orc.OrcPartitionDiscoverySuite", "org.apache.spark.sql.execution.datasources.orc.OrcPartitionDiscoverySuite", "org.apache.spark.sql.execution.datasources.orc.OrcPartitionDiscoverySuite", "org.apache.spark.sql.execution.datasources.orc.OrcPartitionDiscoverySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.RemoveRedundantSortsSuite", "org.apache.spark.sql.execution.RemoveRedundantSortsSuite", "org.apache.spark.sql.execution.RemoveRedundantSortsSuite", "org.apache.spark.sql.execution.RemoveRedundantSortsSuite", "org.apache.spark.sql.execution.RemoveRedundantSortsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.HadoopFileLinesReaderSuite", "org.apache.spark.sql.execution.datasources.HadoopFileLinesReaderSuite", "org.apache.spark.sql.execution.datasources.HadoopFileLinesReaderSuite", "org.apache.spark.sql.execution.datasources.HadoopFileLinesReaderSuite", "org.apache.spark.sql.execution.datasources.HadoopFileLinesReaderSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DynamicPartitionPruningV2Suite", "org.apache.spark.sql.DynamicPartitionPruningSuite", "org.apache.spark.sql.DynamicPartitionPruningSuite", "org.apache.spark.sql.DynamicPartitionPruningSuite", "org.apache.spark.sql.DynamicPartitionPruningSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.test.GenericFunSpecSuite", "org.apache.spark.sql.test.GenericFunSpecSuite", "org.apache.spark.sql.test.GenericFunSpecSuite", "org.apache.spark.sql.test.GenericFunSpecSuite", "org.apache.spark.sql.test.GenericFunSpecSuite", "org.apache.spark.sql.connector.SupportsCatalogOptionsSuite", "org.apache.spark.sql.connector.SupportsCatalogOptionsSuite", "org.apache.spark.sql.connector.SupportsCatalogOptionsSuite", "org.apache.spark.sql.connector.SupportsCatalogOptionsSuite", "org.apache.spark.sql.connector.SupportsCatalogOptionsSuite", "org.apache.spark.sql.connector.SupportsCatalogOptionsSuite", "org.apache.spark.sql.connector.SupportsCatalogOptionsSuite", "org.apache.spark.sql.DatasetAggregatorSuite", "org.apache.spark.sql.DatasetAggregatorSuite", "org.apache.spark.sql.DatasetAggregatorSuite", "org.apache.spark.sql.DatasetAggregatorSuite", "org.apache.spark.sql.DatasetAggregatorSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.v2.AlterTableAddPartitionSuite", "org.apache.spark.sql.execution.command.v2.AlterTableAddPartitionSuite", "org.apache.spark.sql.execution.command.v2.AlterTableAddPartitionSuite", "org.apache.spark.sql.execution.command.v2.AlterTableAddPartitionSuite", "org.apache.spark.sql.execution.command.v2.AlterTableAddPartitionSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.v2.ShowPartitionsSuite", "org.apache.spark.sql.execution.command.v2.ShowPartitionsSuite", "org.apache.spark.sql.execution.command.v2.ShowPartitionsSuite", "org.apache.spark.sql.execution.command.v2.ShowPartitionsSuite", "org.apache.spark.sql.execution.command.v2.ShowPartitionsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.sources.DisableUnnecessaryBucketedScanWithoutHiveSupportSuite", "org.apache.spark.sql.sources.DisableUnnecessaryBucketedScanWithoutHiveSupportSuite", "org.apache.spark.sql.sources.DisableUnnecessaryBucketedScanWithoutHiveSupportSuite", "org.apache.spark.sql.sources.DisableUnnecessaryBucketedScanWithoutHiveSupportSuite", "org.apache.spark.sql.sources.DisableUnnecessaryBucketedScanWithoutHiveSupportSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.v1.ShowNamespacesSuite", "org.apache.spark.sql.execution.command.v1.ShowNamespacesSuite", "org.apache.spark.sql.execution.command.v1.ShowNamespacesSuite", "org.apache.spark.sql.execution.command.v1.ShowNamespacesSuite", "org.apache.spark.sql.execution.command.v1.ShowNamespacesSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.v1.ShowTablesSuite", "org.apache.spark.sql.execution.command.v1.ShowTablesSuite", "org.apache.spark.sql.execution.command.v1.ShowTablesSuite", "org.apache.spark.sql.execution.command.v1.ShowTablesSuite", "org.apache.spark.sql.execution.command.v1.ShowTablesSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.test.GenericFlatSpecSuite", "org.apache.spark.sql.test.GenericFlatSpecSuite", "org.apache.spark.sql.test.GenericFlatSpecSuite", "org.apache.spark.sql.test.GenericFlatSpecSuite", "org.apache.spark.sql.test.GenericFlatSpecSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.RemoveRedundantProjectsSuite", "org.apache.spark.sql.execution.RemoveRedundantProjectsSuite", "org.apache.spark.sql.execution.RemoveRedundantProjectsSuite", "org.apache.spark.sql.execution.RemoveRedundantProjectsSuite", "org.apache.spark.sql.execution.RemoveRedundantProjectsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManagerSuite", "org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManagerSuite", "org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManagerSuite", "org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManagerSuite", "org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManagerSuite", "org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManagerSuite", "org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManagerSuite", "org.apache.spark.sql.streaming.ui.UISeleniumSuite", "org.apache.spark.sql.streaming.ui.UISeleniumSuite", "org.apache.spark.sql.streaming.ui.UISeleniumSuite", "org.apache.spark.sql.streaming.ui.UISeleniumSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.WholeStageCodegenSuite", "org.apache.spark.sql.execution.WholeStageCodegenSuite", "org.apache.spark.sql.execution.WholeStageCodegenSuite", "org.apache.spark.sql.execution.WholeStageCodegenSuite", "org.apache.spark.sql.execution.WholeStageCodegenSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.CountMinSketchAggQuerySuite", "org.apache.spark.sql.CountMinSketchAggQuerySuite", "org.apache.spark.sql.CountMinSketchAggQuerySuite", "org.apache.spark.sql.CountMinSketchAggQuerySuite", "org.apache.spark.sql.CountMinSketchAggQuerySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.SQLExecutionSuite", "org.apache.spark.sql.execution.SQLExecutionSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.DataSourceScanExecRedactionSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.test.DataStreamTableAPISuite", "org.apache.spark.sql.streaming.test.DataStreamTableAPISuite", "org.apache.spark.sql.streaming.test.DataStreamTableAPISuite", "org.apache.spark.sql.streaming.test.DataStreamTableAPISuite", "org.apache.spark.sql.streaming.test.DataStreamTableAPISuite", "org.apache.spark.sql.streaming.test.DataStreamTableAPISuite", "org.apache.spark.sql.streaming.test.DataStreamTableAPISuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DataFramePivotSuite", "org.apache.spark.sql.DataFramePivotSuite", "org.apache.spark.sql.DataFramePivotSuite", "org.apache.spark.sql.DataFramePivotSuite", "org.apache.spark.sql.DataFramePivotSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.FileStreamStressSuite", "org.apache.spark.sql.streaming.FileStreamStressSuite", "org.apache.spark.sql.streaming.FileStreamStressSuite", "org.apache.spark.sql.streaming.FileStreamStressSuite", "org.apache.spark.sql.streaming.FileStreamStressSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.columnar.CachedBatchSerializerSuite", "org.apache.spark.sql.execution.columnar.CachedBatchSerializerSuite", "org.apache.spark.sql.execution.columnar.CachedBatchSerializerSuite", "org.apache.spark.sql.execution.columnar.CachedBatchSerializerSuite", "org.apache.spark.sql.execution.columnar.CachedBatchSerializerSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.VectorizedParquetReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.VectorizedParquetReadSchemaSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.state.RocksDBSuite", "org.apache.spark.sql.execution.streaming.state.RocksDBSuite", "org.apache.spark.sql.execution.streaming.state.RocksDBSuite", "org.apache.spark.sql.execution.streaming.state.RocksDBSuite", "org.apache.spark.sql.execution.streaming.state.RocksDBSuite", "org.apache.spark.sql.execution.streaming.state.RocksDBSuite", "org.apache.spark.sql.execution.streaming.state.RocksDBSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.text.WholeTextFileV1Suite", "org.apache.spark.sql.execution.datasources.text.WholeTextFileSuite", "org.apache.spark.sql.execution.datasources.text.WholeTextFileSuite", "org.apache.spark.sql.execution.datasources.text.WholeTextFileSuite", "org.apache.spark.sql.execution.datasources.text.WholeTextFileSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.v1.AlterTableAddPartitionSuite", "org.apache.spark.sql.execution.command.v1.AlterTableAddPartitionSuite", "org.apache.spark.sql.execution.command.v1.AlterTableAddPartitionSuite", "org.apache.spark.sql.execution.command.v1.AlterTableAddPartitionSuite", "org.apache.spark.sql.execution.command.v1.AlterTableAddPartitionSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.ui.StreamingQueryStatusListenerSuite", "org.apache.spark.sql.streaming.ui.StreamingQueryStatusListenerSuite", "org.apache.spark.sql.streaming.ui.StreamingQueryStatusListenerSuite", "org.apache.spark.sql.streaming.ui.StreamingQueryStatusListenerSuite", "org.apache.spark.sql.streaming.ui.StreamingQueryStatusListenerSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.UserDefinedTypeSuite", "org.apache.spark.sql.UserDefinedTypeSuite", "org.apache.spark.sql.UserDefinedTypeSuite", "org.apache.spark.sql.UserDefinedTypeSuite", "org.apache.spark.sql.UserDefinedTypeSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.DropTableParserSuite", "org.apache.spark.sql.execution.command.DropTableParserSuite", "org.apache.spark.sql.execution.command.DropTableParserSuite", "org.apache.spark.sql.execution.command.DropTableParserSuite", "org.apache.spark.sql.execution.command.DropTableParserSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DSV2CharVarcharTestSuite", "org.apache.spark.sql.DSV2CharVarcharTestSuite", "org.apache.spark.sql.DSV2CharVarcharTestSuite", "org.apache.spark.sql.DSV2CharVarcharTestSuite", "org.apache.spark.sql.DSV2CharVarcharTestSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.SparkSessionExtensionSuite", "org.apache.spark.sql.SparkSessionExtensionSuite", "org.apache.spark.sql.SparkSessionExtensionSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DataFrameSelfJoinSuite", "org.apache.spark.sql.DataFrameSelfJoinSuite", "org.apache.spark.sql.DataFrameSelfJoinSuite", "org.apache.spark.sql.DataFrameSelfJoinSuite", "org.apache.spark.sql.DataFrameSelfJoinSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.StreamingInnerJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.connector.V1ReadFallbackWithDataFrameReaderSuite", "org.apache.spark.sql.connector.V1ReadFallbackSuite", "org.apache.spark.sql.connector.V1ReadFallbackSuite", "org.apache.spark.sql.connector.V1ReadFallbackSuite", "org.apache.spark.sql.connector.V1ReadFallbackSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.CoalesceShufflePartitionsSuite", "org.apache.spark.sql.execution.CoalesceShufflePartitionsSuite", "org.apache.spark.sql.execution.CoalesceShufflePartitionsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.CSVReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.ApproximatePercentileQuerySuite", "org.apache.spark.sql.ApproximatePercentileQuerySuite", "org.apache.spark.sql.ApproximatePercentileQuerySuite", "org.apache.spark.sql.ApproximatePercentileQuerySuite", "org.apache.spark.sql.ApproximatePercentileQuerySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DataFrameSuite", "org.apache.spark.sql.DataFrameSuite", "org.apache.spark.sql.DataFrameSuite", "org.apache.spark.sql.DataFrameSuite", "org.apache.spark.sql.DataFrameSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DataFrameNaFunctionsSuite", "org.apache.spark.sql.DataFrameNaFunctionsSuite", "org.apache.spark.sql.DataFrameNaFunctionsSuite", "org.apache.spark.sql.DataFrameNaFunctionsSuite", "org.apache.spark.sql.DataFrameNaFunctionsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.orc.OrcFilterSuite", "org.apache.spark.sql.execution.datasources.orc.OrcFilterSuite", "org.apache.spark.sql.execution.datasources.orc.OrcFilterSuite", "org.apache.spark.sql.execution.datasources.orc.OrcFilterSuite", "org.apache.spark.sql.execution.datasources.orc.OrcFilterSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.noop.NoopSuite", "org.apache.spark.sql.execution.datasources.noop.NoopSuite", "org.apache.spark.sql.execution.datasources.noop.NoopSuite", "org.apache.spark.sql.execution.datasources.noop.NoopSuite", "org.apache.spark.sql.execution.datasources.noop.NoopSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.sources.SaveLoadSuite", "org.apache.spark.sql.sources.SaveLoadSuite", "org.apache.spark.sql.sources.SaveLoadSuite", "org.apache.spark.sql.sources.SaveLoadSuite", "org.apache.spark.sql.sources.SaveLoadSuite", "org.apache.spark.sql.sources.SaveLoadSuite", "org.apache.spark.sql.sources.SaveLoadSuite", "org.apache.spark.sql.FileScanSuite", "org.apache.spark.sql.FileScanSuite", "org.apache.spark.sql.FileScanSuite", "org.apache.spark.sql.FileScanSuite", "org.apache.spark.sql.FileScanSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.MicroBatchExecutionSuite", "org.apache.spark.sql.execution.streaming.MicroBatchExecutionSuite", "org.apache.spark.sql.execution.streaming.MicroBatchExecutionSuite", "org.apache.spark.sql.execution.streaming.MicroBatchExecutionSuite", "org.apache.spark.sql.execution.streaming.MicroBatchExecutionSuite", "org.apache.spark.sql.execution.streaming.MicroBatchExecutionSuite", "org.apache.spark.sql.execution.streaming.MicroBatchExecutionSuite", "org.apache.spark.sql.execution.datasources.csv.CSVv1Suite", "org.apache.spark.sql.execution.datasources.csv.CSVSuite", "org.apache.spark.sql.execution.datasources.csv.CSVSuite", "org.apache.spark.sql.execution.datasources.csv.CSVSuite", "org.apache.spark.sql.execution.datasources.csv.CSVSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.FileBasedDataSourceSuite", "org.apache.spark.sql.FileBasedDataSourceSuite", "org.apache.spark.sql.FileBasedDataSourceSuite", "org.apache.spark.sql.FileBasedDataSourceSuite", "org.apache.spark.sql.FileBasedDataSourceSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.ExtraStrategiesSuite", "org.apache.spark.sql.ExtraStrategiesSuite", "org.apache.spark.sql.ExtraStrategiesSuite", "org.apache.spark.sql.ExtraStrategiesSuite", "org.apache.spark.sql.ExtraStrategiesSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.orc.OrcV1SchemaPruningSuite", "org.apache.spark.sql.execution.datasources.SchemaPruningSuite", "org.apache.spark.sql.execution.datasources.SchemaPruningSuite", "org.apache.spark.sql.execution.datasources.SchemaPruningSuite", "org.apache.spark.sql.execution.datasources.SchemaPruningSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.connector.DataSourceV2SQLSessionCatalogSuite", "org.apache.spark.sql.execution.streaming.OffsetSeqLogSuite", "org.apache.spark.sql.execution.streaming.OffsetSeqLogSuite", "org.apache.spark.sql.execution.streaming.OffsetSeqLogSuite", "org.apache.spark.sql.execution.streaming.OffsetSeqLogSuite", "org.apache.spark.sql.execution.streaming.OffsetSeqLogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.internal.DeprecatedCreateExternalTableSuite", "org.apache.spark.sql.internal.DeprecatedCreateExternalTableSuite", "org.apache.spark.sql.internal.DeprecatedCreateExternalTableSuite", "org.apache.spark.sql.internal.DeprecatedCreateExternalTableSuite", "org.apache.spark.sql.internal.DeprecatedCreateExternalTableSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.QueryExecutionSuite", "org.apache.spark.sql.execution.QueryExecutionSuite", "org.apache.spark.sql.execution.QueryExecutionSuite", "org.apache.spark.sql.execution.QueryExecutionSuite", "org.apache.spark.sql.execution.QueryExecutionSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.ApproxCountDistinctForIntervalsQuerySuite", "org.apache.spark.sql.ApproxCountDistinctForIntervalsQuerySuite", "org.apache.spark.sql.ApproxCountDistinctForIntervalsQuerySuite", "org.apache.spark.sql.ApproxCountDistinctForIntervalsQuerySuite", "org.apache.spark.sql.ApproxCountDistinctForIntervalsQuerySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DeprecatedDatasetAggregatorSuite", "org.apache.spark.sql.DeprecatedDatasetAggregatorSuite", "org.apache.spark.sql.DeprecatedDatasetAggregatorSuite", "org.apache.spark.sql.DeprecatedDatasetAggregatorSuite", "org.apache.spark.sql.DeprecatedDatasetAggregatorSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.LocalTempViewTestSuite", "org.apache.spark.sql.execution.LocalTempViewTestSuite", "org.apache.spark.sql.execution.LocalTempViewTestSuite", "org.apache.spark.sql.execution.LocalTempViewTestSuite", "org.apache.spark.sql.execution.LocalTempViewTestSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.python.BatchEvalPythonExecSuite", "org.apache.spark.sql.execution.python.BatchEvalPythonExecSuite", "org.apache.spark.sql.execution.python.BatchEvalPythonExecSuite", "org.apache.spark.sql.execution.python.BatchEvalPythonExecSuite", "org.apache.spark.sql.execution.python.BatchEvalPythonExecSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV1PartitionDiscoverySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetPartitionDiscoverySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetPartitionDiscoverySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetPartitionDiscoverySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetPartitionDiscoverySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQuerySuite", "org.apache.spark.sql.TPCDSQuerySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.PathFilterStrategySuite", "org.apache.spark.sql.execution.datasources.PathFilterStrategySuite", "org.apache.spark.sql.execution.datasources.PathFilterStrategySuite", "org.apache.spark.sql.execution.datasources.PathFilterStrategySuite", "org.apache.spark.sql.execution.datasources.PathFilterStrategySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.ui.StreamingQueryPageSuite", "org.apache.spark.sql.streaming.ui.StreamingQueryPageSuite", "org.apache.spark.sql.streaming.ui.StreamingQueryPageSuite", "org.apache.spark.sql.streaming.ui.StreamingQueryPageSuite", "org.apache.spark.sql.streaming.ui.StreamingQueryPageSuite", "org.apache.spark.sql.streaming.ui.StreamingQueryPageSuite", "org.apache.spark.sql.streaming.ui.StreamingQueryPageSuite", "org.apache.spark.sql.execution.command.v2.MsckRepairTableSuite", "org.apache.spark.sql.execution.command.v2.MsckRepairTableSuite", "org.apache.spark.sql.execution.command.v2.MsckRepairTableSuite", "org.apache.spark.sql.execution.command.v2.MsckRepairTableSuite", "org.apache.spark.sql.execution.command.v2.MsckRepairTableSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.internal.SharedStateSuite", "org.apache.spark.sql.internal.SharedStateSuite", "org.apache.spark.sql.internal.SharedStateSuite", "org.apache.spark.sql.internal.SharedStateSuite", "org.apache.spark.sql.internal.SharedStateSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.metric.SQLMetricsSuite", "org.apache.spark.sql.execution.metric.SQLMetricsSuite", "org.apache.spark.sql.execution.metric.SQLMetricsSuite", "org.apache.spark.sql.execution.metric.SQLMetricsSuite", "org.apache.spark.sql.execution.metric.SQLMetricsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.MemorySinkSuite", "org.apache.spark.sql.execution.streaming.MemorySinkSuite", "org.apache.spark.sql.execution.streaming.MemorySinkSuite", "org.apache.spark.sql.execution.streaming.MemorySinkSuite", "org.apache.spark.sql.execution.streaming.MemorySinkSuite", "org.apache.spark.sql.execution.streaming.MemorySinkSuite", "org.apache.spark.sql.execution.streaming.MemorySinkSuite", "org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommandSuite", "org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommandSuite", "org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommandSuite", "org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommandSuite", "org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommandSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.v2.AlterTableRenamePartitionSuite", "org.apache.spark.sql.execution.command.v2.AlterTableRenamePartitionSuite", "org.apache.spark.sql.execution.command.v2.AlterTableRenamePartitionSuite", "org.apache.spark.sql.execution.command.v2.AlterTableRenamePartitionSuite", "org.apache.spark.sql.execution.command.v2.AlterTableRenamePartitionSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DataFrameComplexTypeSuite", "org.apache.spark.sql.DataFrameComplexTypeSuite", "org.apache.spark.sql.DataFrameComplexTypeSuite", "org.apache.spark.sql.DataFrameComplexTypeSuite", "org.apache.spark.sql.DataFrameComplexTypeSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.TPCHQuerySuite", "org.apache.spark.sql.TPCHQuerySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.ScalaReflectionRelationSuite", "org.apache.spark.sql.ScalaReflectionRelationSuite", "org.apache.spark.sql.ScalaReflectionRelationSuite", "org.apache.spark.sql.ScalaReflectionRelationSuite", "org.apache.spark.sql.ScalaReflectionRelationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetEncodingSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetEncodingSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetEncodingSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetEncodingSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetEncodingSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.v2.ShowNamespacesSuite", "org.apache.spark.sql.execution.command.v2.ShowNamespacesSuite", "org.apache.spark.sql.execution.command.v2.ShowNamespacesSuite", "org.apache.spark.sql.execution.command.v2.ShowNamespacesSuite", "org.apache.spark.sql.execution.command.v2.ShowNamespacesSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.expressions.ExpressionInfoSuite", "org.apache.spark.sql.expressions.ExpressionInfoSuite", "org.apache.spark.sql.expressions.ExpressionInfoSuite", "org.apache.spark.sql.expressions.ExpressionInfoSuite", "org.apache.spark.sql.expressions.ExpressionInfoSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.columnar.InMemoryColumnarQuerySuite", "org.apache.spark.sql.execution.columnar.InMemoryColumnarQuerySuite", "org.apache.spark.sql.execution.columnar.InMemoryColumnarQuerySuite", "org.apache.spark.sql.execution.columnar.InMemoryColumnarQuerySuite", "org.apache.spark.sql.execution.columnar.InMemoryColumnarQuerySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.v1.TruncateTableSuite", "org.apache.spark.sql.execution.command.v1.TruncateTableSuite", "org.apache.spark.sql.execution.command.v1.TruncateTableSuite", "org.apache.spark.sql.execution.command.v1.TruncateTableSuite", "org.apache.spark.sql.execution.command.v1.TruncateTableSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.debug.DebuggingSuite", "org.apache.spark.sql.execution.debug.DebuggingSuite", "org.apache.spark.sql.execution.debug.DebuggingSuite", "org.apache.spark.sql.execution.debug.DebuggingSuite", "org.apache.spark.sql.execution.debug.DebuggingSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.RemoveRedundantSortsSuite", "org.apache.spark.sql.execution.RemoveRedundantSortsSuite", "org.apache.spark.sql.execution.RemoveRedundantSortsSuite", "org.apache.spark.sql.execution.RemoveRedundantSortsSuite", "org.apache.spark.sql.execution.RemoveRedundantSortsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DataFrameWriterV2Suite", "org.apache.spark.sql.DataFrameWriterV2Suite", "org.apache.spark.sql.DataFrameWriterV2Suite", "org.apache.spark.sql.DataFrameWriterV2Suite", "org.apache.spark.sql.DataFrameWriterV2Suite", "org.apache.spark.sql.DataFrameWriterV2Suite", "org.apache.spark.sql.DataFrameWriterV2Suite", "org.apache.spark.sql.execution.streaming.FileSystemBasedCheckpointFileManagerSuite", "org.apache.spark.sql.execution.streaming.FileSystemBasedCheckpointFileManagerSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.connector.V1WriteFallbackSuite", "org.apache.spark.sql.connector.V1WriteFallbackSuite", "org.apache.spark.sql.connector.V1WriteFallbackSuite", "org.apache.spark.sql.connector.V1WriteFallbackSuite", "org.apache.spark.sql.connector.V1WriteFallbackSuite", "org.apache.spark.sql.connector.V1WriteFallbackSuite", "org.apache.spark.sql.connector.V1WriteFallbackSuite", "org.apache.spark.sql.execution.SQLJsonProtocolSuite", "org.apache.spark.sql.execution.SQLJsonProtocolSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.FileSourceSQLInsertTestSuite", "org.apache.spark.sql.FileSourceSQLInsertTestSuite", "org.apache.spark.sql.FileSourceSQLInsertTestSuite", "org.apache.spark.sql.FileSourceSQLInsertTestSuite", "org.apache.spark.sql.FileSourceSQLInsertTestSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.orc.OrcV1QuerySuite", "org.apache.spark.sql.execution.datasources.orc.OrcQuerySuite", "org.apache.spark.sql.execution.datasources.orc.OrcQuerySuite", "org.apache.spark.sql.execution.datasources.orc.OrcQuerySuite", "org.apache.spark.sql.execution.datasources.orc.OrcQuerySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.aggregate.SortBasedAggregationStoreSuite", "org.apache.spark.sql.execution.aggregate.SortBasedAggregationStoreSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.sources.RateStreamProviderSuite", "org.apache.spark.sql.execution.streaming.sources.RateStreamProviderSuite", "org.apache.spark.sql.execution.streaming.sources.RateStreamProviderSuite", "org.apache.spark.sql.execution.streaming.sources.RateStreamProviderSuite", "org.apache.spark.sql.execution.streaming.sources.RateStreamProviderSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.util.DataFrameCallbackSuite", "org.apache.spark.sql.util.DataFrameCallbackSuite", "org.apache.spark.sql.util.DataFrameCallbackSuite", "org.apache.spark.sql.util.DataFrameCallbackSuite", "org.apache.spark.sql.util.DataFrameCallbackSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.v2.FileTableSuite", "org.apache.spark.sql.execution.datasources.v2.FileTableSuite", "org.apache.spark.sql.execution.datasources.v2.FileTableSuite", "org.apache.spark.sql.execution.datasources.v2.FileTableSuite", "org.apache.spark.sql.execution.datasources.v2.FileTableSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.JsonReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.FlatMapGroupsWithStateSuite", "org.apache.spark.sql.streaming.FlatMapGroupsWithStateSuite", "org.apache.spark.sql.streaming.FlatMapGroupsWithStateSuite", "org.apache.spark.sql.streaming.FlatMapGroupsWithStateSuite", "org.apache.spark.sql.streaming.FlatMapGroupsWithStateSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArraySuite", "org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArraySuite", "org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArraySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.ConfigBehaviorSuite", "org.apache.spark.sql.ConfigBehaviorSuite", "org.apache.spark.sql.ConfigBehaviorSuite", "org.apache.spark.sql.ConfigBehaviorSuite", "org.apache.spark.sql.ConfigBehaviorSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DatasetSuite", "org.apache.spark.sql.DatasetSuite", "org.apache.spark.sql.DatasetSuite", "org.apache.spark.sql.DatasetSuite", "org.apache.spark.sql.DatasetSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DataFrameTungstenSuite", "org.apache.spark.sql.DataFrameTungstenSuite", "org.apache.spark.sql.DataFrameTungstenSuite", "org.apache.spark.sql.DataFrameTungstenSuite", "org.apache.spark.sql.DataFrameTungstenSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreSuite", "org.apache.spark.sql.execution.streaming.state.StateStoreSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreSuite", "org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreSuite", "org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreSuite", "org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMapSuite", "org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMapSuite", "org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMapSuite", "org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMapSuite", "org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMapSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.connector.FileDataSourceV2FallBackSuite", "org.apache.spark.sql.connector.FileDataSourceV2FallBackSuite", "org.apache.spark.sql.connector.FileDataSourceV2FallBackSuite", "org.apache.spark.sql.connector.FileDataSourceV2FallBackSuite", "org.apache.spark.sql.connector.FileDataSourceV2FallBackSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite", "org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite", "org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite", "org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite", "org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerSuite", "org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerSuite", "org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerSuite", "org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerSuite", "org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.test.GenericWordSpecSuite", "org.apache.spark.sql.test.GenericWordSpecSuite", "org.apache.spark.sql.test.GenericWordSpecSuite", "org.apache.spark.sql.test.GenericWordSpecSuite", "org.apache.spark.sql.test.GenericWordSpecSuite", "org.apache.spark.sql.ProductAggSuite", "org.apache.spark.sql.ProductAggSuite", "org.apache.spark.sql.ProductAggSuite", "org.apache.spark.sql.ProductAggSuite", "org.apache.spark.sql.ProductAggSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.bucketing.CoalesceBucketsInJoinSuite", "org.apache.spark.sql.execution.bucketing.CoalesceBucketsInJoinSuite", "org.apache.spark.sql.execution.bucketing.CoalesceBucketsInJoinSuite", "org.apache.spark.sql.execution.bucketing.CoalesceBucketsInJoinSuite", "org.apache.spark.sql.execution.bucketing.CoalesceBucketsInJoinSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilitySuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilitySuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilitySuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilitySuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilitySuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilitySuite", "org.apache.spark.sql.PlanStabilitySuite", "org.apache.spark.sql.PlanStabilitySuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilitySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.QueryTestSuite", "org.apache.spark.sql.QueryTestSuite", "org.apache.spark.sql.QueryTestSuite", "org.apache.spark.sql.QueryTestSuite", "org.apache.spark.sql.QueryTestSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.FileSourceCharVarcharTestSuite", "org.apache.spark.sql.FileSourceCharVarcharTestSuite", "org.apache.spark.sql.FileSourceCharVarcharTestSuite", "org.apache.spark.sql.FileSourceCharVarcharTestSuite", "org.apache.spark.sql.FileSourceCharVarcharTestSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.MiscFunctionsSuite", "org.apache.spark.sql.MiscFunctionsSuite", "org.apache.spark.sql.MiscFunctionsSuite", "org.apache.spark.sql.MiscFunctionsSuite", "org.apache.spark.sql.MiscFunctionsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.FileFormatWriterSuite", "org.apache.spark.sql.execution.datasources.FileFormatWriterSuite", "org.apache.spark.sql.execution.datasources.FileFormatWriterSuite", "org.apache.spark.sql.execution.datasources.FileFormatWriterSuite", "org.apache.spark.sql.execution.datasources.FileFormatWriterSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.orc.OrcSourceSuite", "org.apache.spark.sql.execution.datasources.orc.OrcSourceSuite", "org.apache.spark.sql.execution.datasources.orc.OrcSourceSuite", "org.apache.spark.sql.execution.datasources.orc.OrcSourceSuite", "org.apache.spark.sql.execution.datasources.orc.OrcSourceSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.StreamingLeftSemiJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.streaming.EventTimeWatermarkSuite", "org.apache.spark.sql.streaming.EventTimeWatermarkSuite", "org.apache.spark.sql.streaming.EventTimeWatermarkSuite", "org.apache.spark.sql.streaming.EventTimeWatermarkSuite", "org.apache.spark.sql.streaming.EventTimeWatermarkSuite", "org.apache.spark.sql.streaming.EventTimeWatermarkSuite", "org.apache.spark.sql.streaming.EventTimeWatermarkSuite", "org.apache.spark.sql.execution.ui.SparkPlanInfoSuite", "org.apache.spark.sql.execution.ui.SparkPlanInfoSuite", "org.apache.spark.sql.execution.ui.SparkPlanInfoSuite", "org.apache.spark.sql.execution.ui.SparkPlanInfoSuite", "org.apache.spark.sql.execution.ui.SparkPlanInfoSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV2PartitionDiscoverySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetPartitionDiscoverySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetPartitionDiscoverySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetPartitionDiscoverySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetPartitionDiscoverySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.StreamMetadataSuite", "org.apache.spark.sql.execution.streaming.StreamMetadataSuite", "org.apache.spark.sql.execution.streaming.StreamMetadataSuite", "org.apache.spark.sql.execution.streaming.StreamMetadataSuite", "org.apache.spark.sql.execution.streaming.StreamMetadataSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetThriftCompatibilitySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetThriftCompatibilitySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetThriftCompatibilitySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetThriftCompatibilitySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetThriftCompatibilitySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.sources.ConsoleWriteSupportSuite", "org.apache.spark.sql.execution.streaming.sources.ConsoleWriteSupportSuite", "org.apache.spark.sql.execution.streaming.sources.ConsoleWriteSupportSuite", "org.apache.spark.sql.execution.streaming.sources.ConsoleWriteSupportSuite", "org.apache.spark.sql.execution.streaming.sources.ConsoleWriteSupportSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.SparkScriptTransformationSuite", "org.apache.spark.sql.execution.SparkScriptTransformationSuite", "org.apache.spark.sql.execution.SparkScriptTransformationSuite", "org.apache.spark.sql.execution.SparkScriptTransformationSuite", "org.apache.spark.sql.execution.SparkScriptTransformationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.connector.TableCapabilityCheckSuite", "org.apache.spark.sql.connector.TableCapabilityCheckSuite", "org.apache.spark.sql.connector.TableCapabilityCheckSuite", "org.apache.spark.sql.connector.TableCapabilityCheckSuite", "org.apache.spark.sql.connector.TableCapabilityCheckSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.orc.OrcV2SchemaPruningSuite", "org.apache.spark.sql.execution.datasources.SchemaPruningSuite", "org.apache.spark.sql.execution.datasources.SchemaPruningSuite", "org.apache.spark.sql.execution.datasources.SchemaPruningSuite", "org.apache.spark.sql.execution.datasources.SchemaPruningSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.AlterTableRenamePartitionParserSuite", "org.apache.spark.sql.execution.command.AlterTableRenamePartitionParserSuite", "org.apache.spark.sql.execution.command.AlterTableRenamePartitionParserSuite", "org.apache.spark.sql.execution.command.AlterTableRenamePartitionParserSuite", "org.apache.spark.sql.execution.command.AlterTableRenamePartitionParserSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.sources.PartitionedWriteSuite", "org.apache.spark.sql.sources.PartitionedWriteSuite", "org.apache.spark.sql.sources.PartitionedWriteSuite", "org.apache.spark.sql.sources.PartitionedWriteSuite", "org.apache.spark.sql.sources.PartitionedWriteSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.orc.OrcEncryptionSuite", "org.apache.spark.sql.execution.datasources.orc.OrcEncryptionSuite", "org.apache.spark.sql.execution.datasources.orc.OrcEncryptionSuite", "org.apache.spark.sql.execution.datasources.orc.OrcEncryptionSuite", "org.apache.spark.sql.execution.datasources.orc.OrcEncryptionSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.internal.ExecutorSideSQLConfSuite", "org.apache.spark.sql.internal.ExecutorSideSQLConfSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.internal.SQLConfGetterSuite", "org.apache.spark.sql.internal.SQLConfGetterSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV2QuerySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.json.JsonParsingOptionsSuite", "org.apache.spark.sql.execution.datasources.json.JsonParsingOptionsSuite", "org.apache.spark.sql.execution.datasources.json.JsonParsingOptionsSuite", "org.apache.spark.sql.execution.datasources.json.JsonParsingOptionsSuite", "org.apache.spark.sql.execution.datasources.json.JsonParsingOptionsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.JsonFunctionsSuite", "org.apache.spark.sql.JsonFunctionsSuite", "org.apache.spark.sql.JsonFunctionsSuite", "org.apache.spark.sql.JsonFunctionsSuite", "org.apache.spark.sql.JsonFunctionsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.v2.AlterTableRenameSuite", "org.apache.spark.sql.execution.command.v2.AlterTableRenameSuite", "org.apache.spark.sql.execution.command.v2.AlterTableRenameSuite", "org.apache.spark.sql.execution.command.v2.AlterTableRenameSuite", "org.apache.spark.sql.execution.command.v2.AlterTableRenameSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.test.DataStreamReaderWriterSuite", "org.apache.spark.sql.streaming.test.DataStreamReaderWriterSuite", "org.apache.spark.sql.streaming.test.DataStreamReaderWriterSuite", "org.apache.spark.sql.streaming.test.DataStreamReaderWriterSuite", "org.apache.spark.sql.streaming.test.DataStreamReaderWriterSuite", "org.apache.spark.sql.streaming.test.DataStreamReaderWriterSuite", "org.apache.spark.sql.streaming.test.DataStreamReaderWriterSuite", "org.apache.spark.sql.streaming.StreamingSymmetricHashJoinHelperSuite", "org.apache.spark.sql.streaming.StreamingSymmetricHashJoinHelperSuite", "org.apache.spark.sql.streaming.StreamingSymmetricHashJoinHelperSuite", "org.apache.spark.sql.streaming.StreamingSymmetricHashJoinHelperSuite", "org.apache.spark.sql.streaming.StreamingSymmetricHashJoinHelperSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormatV1Suite", "org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormatSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormatSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormatSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormatSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.PlanStabilitySuite", "org.apache.spark.sql.PlanStabilitySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.DataSourceV2ScanExecRedactionSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.orc.OrcV1PartitionDiscoverySuite", "org.apache.spark.sql.execution.datasources.orc.OrcV1PartitionDiscoverySuite", "org.apache.spark.sql.execution.datasources.orc.OrcV1PartitionDiscoverySuite", "org.apache.spark.sql.execution.datasources.orc.OrcV1PartitionDiscoverySuite", "org.apache.spark.sql.execution.datasources.orc.OrcV1PartitionDiscoverySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.test.TestSparkSessionSuite", "org.apache.spark.sql.test.TestSparkSessionSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.connector.DataSourceV2DataFrameSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetInteroperabilitySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetInteroperabilitySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetInteroperabilitySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetInteroperabilitySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetInteroperabilitySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.QueryPlanningTrackerEndToEndSuite", "org.apache.spark.sql.execution.QueryPlanningTrackerEndToEndSuite", "org.apache.spark.sql.execution.QueryPlanningTrackerEndToEndSuite", "org.apache.spark.sql.execution.QueryPlanningTrackerEndToEndSuite", "org.apache.spark.sql.execution.QueryPlanningTrackerEndToEndSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetProtobufCompatibilitySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetProtobufCompatibilitySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetProtobufCompatibilitySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetProtobufCompatibilitySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetProtobufCompatibilitySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.exchange.EnsureRequirementsSuite", "org.apache.spark.sql.execution.exchange.EnsureRequirementsSuite", "org.apache.spark.sql.execution.exchange.EnsureRequirementsSuite", "org.apache.spark.sql.execution.exchange.EnsureRequirementsSuite", "org.apache.spark.sql.execution.exchange.EnsureRequirementsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.SimpleSQLViewSuite", "org.apache.spark.sql.execution.SimpleSQLViewSuite", "org.apache.spark.sql.execution.SimpleSQLViewSuite", "org.apache.spark.sql.execution.SimpleSQLViewSuite", "org.apache.spark.sql.execution.SimpleSQLViewSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.v1.DropTableSuite", "org.apache.spark.sql.execution.command.v1.DropTableSuite", "org.apache.spark.sql.execution.command.v1.DropTableSuite", "org.apache.spark.sql.execution.command.v1.DropTableSuite", "org.apache.spark.sql.execution.command.v1.DropTableSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.joins.ExistenceJoinSuite", "org.apache.spark.sql.execution.joins.ExistenceJoinSuite", "org.apache.spark.sql.execution.joins.ExistenceJoinSuite", "org.apache.spark.sql.execution.joins.ExistenceJoinSuite", "org.apache.spark.sql.execution.joins.ExistenceJoinSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.SameResultSuite", "org.apache.spark.sql.execution.SameResultSuite", "org.apache.spark.sql.execution.SameResultSuite", "org.apache.spark.sql.execution.SameResultSuite", "org.apache.spark.sql.execution.SameResultSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.StreamingFullOuterJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.DataFrameStatSuite", "org.apache.spark.sql.DataFrameStatSuite", "org.apache.spark.sql.DataFrameStatSuite", "org.apache.spark.sql.DataFrameStatSuite", "org.apache.spark.sql.DataFrameStatSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.XPathFunctionsSuite", "org.apache.spark.sql.XPathFunctionsSuite", "org.apache.spark.sql.XPathFunctionsSuite", "org.apache.spark.sql.XPathFunctionsSuite", "org.apache.spark.sql.XPathFunctionsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.FileStreamSourceStressTestSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DataFrameWindowFramesSuite", "org.apache.spark.sql.DataFrameWindowFramesSuite", "org.apache.spark.sql.DataFrameWindowFramesSuite", "org.apache.spark.sql.DataFrameWindowFramesSuite", "org.apache.spark.sql.DataFrameWindowFramesSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.status.api.v1.sql.SqlResourceWithActualMetricsSuite", "org.apache.spark.status.api.v1.sql.SqlResourceWithActualMetricsSuite", "org.apache.spark.status.api.v1.sql.SqlResourceWithActualMetricsSuite", "org.apache.spark.status.api.v1.sql.SqlResourceWithActualMetricsSuite", "org.apache.spark.status.api.v1.sql.SqlResourceWithActualMetricsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.state.StateStoreRDDSuite", "org.apache.spark.sql.execution.streaming.state.StateStoreRDDSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.state.StateStoreRDDSuite", "org.apache.spark.sql.execution.streaming.state.StateStoreRDDSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.state.StateStoreRDDSuite", "org.apache.spark.sql.execution.streaming.state.StateStoreRDDSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite", "org.apache.spark.sql.sources.DisableUnnecessaryBucketedScanWithoutHiveSupportSuite", "org.apache.spark.sql.sources.DisableUnnecessaryBucketedScanWithoutHiveSupportSuite", "org.apache.spark.sql.sources.DisableUnnecessaryBucketedScanWithoutHiveSupportSuite", "org.apache.spark.sql.sources.DisableUnnecessaryBucketedScanWithoutHiveSupportSuite", "org.apache.spark.sql.sources.DisableUnnecessaryBucketedScanWithoutHiveSupportSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.PathFilterSuite", "org.apache.spark.sql.execution.datasources.PathFilterSuite", "org.apache.spark.sql.execution.datasources.PathFilterSuite", "org.apache.spark.sql.execution.datasources.PathFilterSuite", "org.apache.spark.sql.execution.datasources.PathFilterSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRenamePartitionSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRenamePartitionSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRenamePartitionSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRenamePartitionSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRenamePartitionSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.state.StreamingSessionWindowStateManagerSuite", "org.apache.spark.sql.execution.streaming.state.StreamingSessionWindowStateManagerSuite", "org.apache.spark.sql.execution.streaming.state.StreamingSessionWindowStateManagerSuite", "org.apache.spark.sql.execution.streaming.state.StreamingSessionWindowStateManagerSuite", "org.apache.spark.sql.execution.streaming.state.StreamingSessionWindowStateManagerSuite", "org.apache.spark.sql.execution.streaming.state.StreamingSessionWindowStateManagerSuite", "org.apache.spark.sql.execution.streaming.state.StreamingSessionWindowStateManagerSuite", "org.apache.spark.sql.execution.datasources.FileSourceStrategySuite", "org.apache.spark.sql.execution.datasources.FileSourceStrategySuite", "org.apache.spark.sql.execution.datasources.FileSourceStrategySuite", "org.apache.spark.sql.execution.datasources.FileSourceStrategySuite", "org.apache.spark.sql.execution.datasources.FileSourceStrategySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.SessionStateSuite", "org.apache.spark.sql.SessionStateSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.connector.LocalScanSuite", "org.apache.spark.sql.connector.LocalScanSuite", "org.apache.spark.sql.connector.LocalScanSuite", "org.apache.spark.sql.connector.LocalScanSuite", "org.apache.spark.sql.connector.LocalScanSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetCompressionCodecPrecedenceSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetCompressionCodecPrecedenceSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetCompressionCodecPrecedenceSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetCompressionCodecPrecedenceSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetCompressionCodecPrecedenceSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.UnsafeRowSerializerSuite", "org.apache.spark.sql.execution.UnsafeRowSerializerSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.v2.ShowTablesSuite", "org.apache.spark.sql.execution.command.v2.ShowTablesSuite", "org.apache.spark.sql.execution.command.v2.ShowTablesSuite", "org.apache.spark.sql.execution.command.v2.ShowTablesSuite", "org.apache.spark.sql.execution.command.v2.ShowTablesSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.BroadcastExchangeSuite", "org.apache.spark.sql.execution.BroadcastExchangeSuite", "org.apache.spark.sql.execution.BroadcastExchangeSuite", "org.apache.spark.sql.execution.BroadcastExchangeSuite", "org.apache.spark.sql.execution.BroadcastExchangeSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.OptimizeMetadataOnlyQuerySuite", "org.apache.spark.sql.execution.OptimizeMetadataOnlyQuerySuite", "org.apache.spark.sql.execution.OptimizeMetadataOnlyQuerySuite", "org.apache.spark.sql.execution.OptimizeMetadataOnlyQuerySuite", "org.apache.spark.sql.execution.OptimizeMetadataOnlyQuerySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.MergedOrcReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.StreamingQuerySuite", "org.apache.spark.sql.streaming.StreamingQuerySuite", "org.apache.spark.sql.streaming.StreamingQuerySuite", "org.apache.spark.sql.streaming.StreamingQuerySuite", "org.apache.spark.sql.streaming.StreamingQuerySuite", "org.apache.spark.sql.streaming.StreamingQuerySuite", "org.apache.spark.sql.streaming.StreamingQuerySuite", "org.apache.spark.sql.execution.datasources.jdbc.connection.DisallowedConnectionProviderSuite", "org.apache.spark.sql.execution.datasources.jdbc.connection.DisallowedConnectionProviderSuite", "org.apache.spark.sql.execution.datasources.jdbc.connection.DisallowedConnectionProviderSuite", "org.apache.spark.sql.execution.datasources.jdbc.connection.DisallowedConnectionProviderSuite", "org.apache.spark.sql.execution.datasources.jdbc.connection.DisallowedConnectionProviderSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.FileStreamSinkV2Suite", "org.apache.spark.sql.streaming.FileStreamSinkSuite", "org.apache.spark.sql.streaming.FileStreamSinkSuite", "org.apache.spark.sql.streaming.FileStreamSinkSuite", "org.apache.spark.sql.streaming.FileStreamSinkSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.AlterTableDropPartitionParserSuite", "org.apache.spark.sql.execution.command.AlterTableDropPartitionParserSuite", "org.apache.spark.sql.execution.command.AlterTableDropPartitionParserSuite", "org.apache.spark.sql.execution.command.AlterTableDropPartitionParserSuite", "org.apache.spark.sql.execution.command.AlterTableDropPartitionParserSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.execution.datasources.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.execution.datasources.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.execution.datasources.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.execution.datasources.PruneFileSourcePartitionsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetCommitterSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetCommitterSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.DDLParserSuite", "org.apache.spark.sql.execution.command.DDLParserSuite", "org.apache.spark.sql.execution.command.DDLParserSuite", "org.apache.spark.sql.execution.command.DDLParserSuite", "org.apache.spark.sql.execution.command.DDLParserSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.v2.V2SessionCatalogNamespaceSuite", "org.apache.spark.sql.execution.datasources.v2.V2SessionCatalogBaseSuite", "org.apache.spark.sql.execution.datasources.v2.V2SessionCatalogBaseSuite", "org.apache.spark.sql.execution.datasources.v2.V2SessionCatalogBaseSuite", "org.apache.spark.sql.execution.datasources.v2.V2SessionCatalogBaseSuite", "org.apache.spark.sql.execution.datasources.v2.V2SessionCatalogBaseSuite", "org.apache.spark.sql.execution.datasources.v2.V2SessionCatalogBaseSuite", "org.apache.spark.sql.sources.ExternalCommandRunnerSuite", "org.apache.spark.sql.sources.ExternalCommandRunnerSuite", "org.apache.spark.sql.sources.ExternalCommandRunnerSuite", "org.apache.spark.sql.sources.ExternalCommandRunnerSuite", "org.apache.spark.sql.sources.ExternalCommandRunnerSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.v2.DropTableSuite", "org.apache.spark.sql.execution.command.v2.DropTableSuite", "org.apache.spark.sql.execution.command.v2.DropTableSuite", "org.apache.spark.sql.execution.command.v2.DropTableSuite", "org.apache.spark.sql.execution.command.v2.DropTableSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.RowSuite", "org.apache.spark.sql.RowSuite", "org.apache.spark.sql.RowSuite", "org.apache.spark.sql.RowSuite", "org.apache.spark.sql.RowSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.v2.TruncateTableSuite", "org.apache.spark.sql.execution.command.v2.TruncateTableSuite", "org.apache.spark.sql.execution.command.v2.TruncateTableSuite", "org.apache.spark.sql.execution.command.v2.TruncateTableSuite", "org.apache.spark.sql.execution.command.v2.TruncateTableSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.RemoveRedundantProjectsSuite", "org.apache.spark.sql.execution.RemoveRedundantProjectsSuite", "org.apache.spark.sql.execution.RemoveRedundantProjectsSuite", "org.apache.spark.sql.execution.RemoveRedundantProjectsSuite", "org.apache.spark.sql.execution.RemoveRedundantProjectsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.connector.V2CommandsCaseSensitivitySuite", "org.apache.spark.sql.connector.V2CommandsCaseSensitivitySuite", "org.apache.spark.sql.connector.V2CommandsCaseSensitivitySuite", "org.apache.spark.sql.connector.V2CommandsCaseSensitivitySuite", "org.apache.spark.sql.connector.V2CommandsCaseSensitivitySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.sources.ResolvedDataSourceSuite", "org.apache.spark.sql.sources.ResolvedDataSourceSuite", "org.apache.spark.sql.sources.ResolvedDataSourceSuite", "org.apache.spark.sql.sources.ResolvedDataSourceSuite", "org.apache.spark.sql.sources.ResolvedDataSourceSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.sources.ForeachWriterSuite", "org.apache.spark.sql.execution.streaming.sources.ForeachWriterSuite", "org.apache.spark.sql.execution.streaming.sources.ForeachWriterSuite", "org.apache.spark.sql.execution.streaming.sources.ForeachWriterSuite", "org.apache.spark.sql.execution.streaming.sources.ForeachWriterSuite", "org.apache.spark.sql.execution.streaming.sources.ForeachWriterSuite", "org.apache.spark.sql.execution.streaming.sources.ForeachWriterSuite", "org.apache.spark.sql.execution.python.PythonUDFSuite", "org.apache.spark.sql.execution.python.PythonUDFSuite", "org.apache.spark.sql.execution.python.PythonUDFSuite", "org.apache.spark.sql.execution.python.PythonUDFSuite", "org.apache.spark.sql.execution.python.PythonUDFSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreIntegrationSuite", "org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreIntegrationSuite", "org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreIntegrationSuite", "org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreIntegrationSuite", "org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreIntegrationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.ReuseExchangeAndSubquerySuite", "org.apache.spark.sql.execution.ReuseExchangeAndSubquerySuite", "org.apache.spark.sql.execution.ReuseExchangeAndSubquerySuite", "org.apache.spark.sql.execution.ReuseExchangeAndSubquerySuite", "org.apache.spark.sql.execution.ReuseExchangeAndSubquerySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DataFrameFunctionsSuite", "org.apache.spark.sql.DataFrameFunctionsSuite", "org.apache.spark.sql.DataFrameFunctionsSuite", "org.apache.spark.sql.DataFrameFunctionsSuite", "org.apache.spark.sql.DataFrameFunctionsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.orc.OrcV2QuerySuite", "org.apache.spark.sql.execution.datasources.orc.OrcQuerySuite", "org.apache.spark.sql.execution.datasources.orc.OrcQuerySuite", "org.apache.spark.sql.execution.datasources.orc.OrcQuerySuite", "org.apache.spark.sql.execution.datasources.orc.OrcQuerySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilityWithStatsSuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilityWithStatsSuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilityWithStatsSuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilityWithStatsSuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilityWithStatsSuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilityWithStatsSuite", "org.apache.spark.sql.PlanStabilitySuite", "org.apache.spark.sql.PlanStabilitySuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilityWithStatsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormatV2Suite", "org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormatSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormatSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormatSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormatSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.MetadataCacheV1Suite", "org.apache.spark.sql.MetadataCacheSuite", "org.apache.spark.sql.MetadataCacheSuite", "org.apache.spark.sql.MetadataCacheSuite", "org.apache.spark.sql.MetadataCacheSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.continuous.ContinuousMetaSuite", "org.apache.spark.sql.streaming.continuous.ContinuousMetaSuite", "org.apache.spark.sql.streaming.continuous.ContinuousSuite", "org.apache.spark.sql.streaming.continuous.ContinuousSuite", "org.apache.spark.sql.streaming.continuous.ContinuousSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.ColumnExpressionSuite", "org.apache.spark.sql.ColumnExpressionSuite", "org.apache.spark.sql.ColumnExpressionSuite", "org.apache.spark.sql.ColumnExpressionSuite", "org.apache.spark.sql.ColumnExpressionSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DataFrameSessionWindowingSuite", "org.apache.spark.sql.DataFrameSessionWindowingSuite", "org.apache.spark.sql.DataFrameSessionWindowingSuite", "org.apache.spark.sql.DataFrameSessionWindowingSuite", "org.apache.spark.sql.DataFrameSessionWindowingSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.util.ExecutionListenerManagerSuite", "org.apache.spark.sql.util.ExecutionListenerManagerSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DataFrameImplicitsSuite", "org.apache.spark.sql.DataFrameImplicitsSuite", "org.apache.spark.sql.DataFrameImplicitsSuite", "org.apache.spark.sql.DataFrameImplicitsSuite", "org.apache.spark.sql.DataFrameImplicitsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.HDFSMetadataLogSuite", "org.apache.spark.sql.execution.streaming.HDFSMetadataLogSuite", "org.apache.spark.sql.execution.streaming.HDFSMetadataLogSuite", "org.apache.spark.sql.execution.streaming.HDFSMetadataLogSuite", "org.apache.spark.sql.execution.streaming.HDFSMetadataLogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.ExchangeSuite", "org.apache.spark.sql.execution.ExchangeSuite", "org.apache.spark.sql.execution.ExchangeSuite", "org.apache.spark.sql.execution.ExchangeSuite", "org.apache.spark.sql.execution.ExchangeSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DatasetOptimizationSuite", "org.apache.spark.sql.DatasetOptimizationSuite", "org.apache.spark.sql.DatasetOptimizationSuite", "org.apache.spark.sql.DatasetOptimizationSuite", "org.apache.spark.sql.DatasetOptimizationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.continuous.ContinuousSuite", "org.apache.spark.sql.streaming.continuous.ContinuousSuite", "org.apache.spark.sql.streaming.continuous.ContinuousSuite", "org.apache.spark.sql.streaming.continuous.ContinuousSuite", "org.apache.spark.sql.streaming.continuous.ContinuousSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.joins.BroadcastJoinSuite", "org.apache.spark.sql.execution.joins.BroadcastJoinSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.json.JsonV1Suite", "org.apache.spark.sql.execution.datasources.json.JsonSuite", "org.apache.spark.sql.execution.datasources.json.JsonSuite", "org.apache.spark.sql.execution.datasources.json.JsonSuite", "org.apache.spark.sql.execution.datasources.json.JsonSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.v1.AlterTableDropPartitionSuite", "org.apache.spark.sql.execution.command.v1.AlterTableDropPartitionSuite", "org.apache.spark.sql.execution.command.v1.AlterTableDropPartitionSuite", "org.apache.spark.sql.execution.command.v1.AlterTableDropPartitionSuite", "org.apache.spark.sql.execution.command.v1.AlterTableDropPartitionSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.state.FlatMapGroupsWithStateExecHelperSuite", "org.apache.spark.sql.execution.streaming.state.FlatMapGroupsWithStateExecHelperSuite", "org.apache.spark.sql.execution.streaming.state.FlatMapGroupsWithStateExecHelperSuite", "org.apache.spark.sql.execution.streaming.state.FlatMapGroupsWithStateExecHelperSuite", "org.apache.spark.sql.execution.streaming.state.FlatMapGroupsWithStateExecHelperSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.ShufflePartitionsUtilSuite", "org.apache.spark.sql.execution.ShufflePartitionsUtilSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.PlanStabilitySuite", "org.apache.spark.sql.PlanStabilitySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.connector.DataSourceV2Suite", "org.apache.spark.sql.connector.DataSourceV2Suite", "org.apache.spark.sql.connector.DataSourceV2Suite", "org.apache.spark.sql.connector.DataSourceV2Suite", "org.apache.spark.sql.connector.DataSourceV2Suite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.ExplainSuite", "org.apache.spark.sql.ExplainSuite", "org.apache.spark.sql.ExplainSuite", "org.apache.spark.sql.ExplainSuite", "org.apache.spark.sql.ExplainSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.TakeOrderedAndProjectSuite", "org.apache.spark.sql.execution.TakeOrderedAndProjectSuite", "org.apache.spark.sql.execution.TakeOrderedAndProjectSuite", "org.apache.spark.sql.execution.TakeOrderedAndProjectSuite", "org.apache.spark.sql.execution.TakeOrderedAndProjectSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.api.r.SQLUtilsSuite", "org.apache.spark.sql.api.r.SQLUtilsSuite", "org.apache.spark.sql.api.r.SQLUtilsSuite", "org.apache.spark.sql.api.r.SQLUtilsSuite", "org.apache.spark.sql.api.r.SQLUtilsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.PlanStabilitySuite", "org.apache.spark.sql.PlanStabilitySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.sources.DDLSourceLoadSuite", "org.apache.spark.sql.sources.DDLSourceLoadSuite", "org.apache.spark.sql.sources.DDLSourceLoadSuite", "org.apache.spark.sql.sources.DDLSourceLoadSuite", "org.apache.spark.sql.sources.DDLSourceLoadSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.StreamingStateStoreFormatCompatibilitySuite", "org.apache.spark.sql.streaming.StreamingStateStoreFormatCompatibilitySuite", "org.apache.spark.sql.streaming.StreamingStateStoreFormatCompatibilitySuite", "org.apache.spark.sql.streaming.StreamingStateStoreFormatCompatibilitySuite", "org.apache.spark.sql.streaming.StreamingStateStoreFormatCompatibilitySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.ui.StreamingQueryHistorySuite", "org.apache.spark.sql.streaming.ui.StreamingQueryHistorySuite", "org.apache.spark.sql.streaming.ui.StreamingQueryHistorySuite", "org.apache.spark.sql.streaming.ui.StreamingQueryHistorySuite", "org.apache.spark.sql.streaming.ui.StreamingQueryHistorySuite", "org.apache.spark.sql.streaming.ui.StreamingQueryHistorySuite", "org.apache.spark.sql.streaming.ui.StreamingQueryHistorySuite", "org.apache.spark.sql.execution.datasources.json.JsonV2Suite", "org.apache.spark.sql.execution.datasources.json.JsonSuite", "org.apache.spark.sql.execution.datasources.json.JsonSuite", "org.apache.spark.sql.execution.datasources.json.JsonSuite", "org.apache.spark.sql.execution.datasources.json.JsonSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.SimpleShowCreateTableSuite", "org.apache.spark.sql.SimpleShowCreateTableSuite", "org.apache.spark.sql.SimpleShowCreateTableSuite", "org.apache.spark.sql.SimpleShowCreateTableSuite", "org.apache.spark.sql.SimpleShowCreateTableSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.json.JsonLegacyTimeParserSuite", "org.apache.spark.sql.execution.datasources.json.JsonSuite", "org.apache.spark.sql.execution.datasources.json.JsonSuite", "org.apache.spark.sql.execution.datasources.json.JsonSuite", "org.apache.spark.sql.execution.datasources.json.JsonSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.debug.DebuggingSuite", "org.apache.spark.sql.execution.debug.DebuggingSuite", "org.apache.spark.sql.execution.debug.DebuggingSuite", "org.apache.spark.sql.execution.debug.DebuggingSuite", "org.apache.spark.sql.execution.debug.DebuggingSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.csv.CSVv2Suite", "org.apache.spark.sql.execution.datasources.csv.CSVSuite", "org.apache.spark.sql.execution.datasources.csv.CSVSuite", "org.apache.spark.sql.execution.datasources.csv.CSVSuite", "org.apache.spark.sql.execution.datasources.csv.CSVSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.StreamSuite", "org.apache.spark.sql.streaming.StreamSuite", "org.apache.spark.sql.streaming.StreamSuite", "org.apache.spark.sql.streaming.StreamSuite", "org.apache.spark.sql.streaming.StreamSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DataFrameHintSuite", "org.apache.spark.sql.DataFrameHintSuite", "org.apache.spark.sql.DataFrameHintSuite", "org.apache.spark.sql.DataFrameHintSuite", "org.apache.spark.sql.DataFrameHintSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DataFrameRangeSuite", "org.apache.spark.sql.DataFrameRangeSuite", "org.apache.spark.sql.DataFrameRangeSuite", "org.apache.spark.sql.DataFrameRangeSuite", "org.apache.spark.sql.DataFrameRangeSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DataFrameTimeWindowingSuite", "org.apache.spark.sql.DataFrameTimeWindowingSuite", "org.apache.spark.sql.DataFrameTimeWindowingSuite", "org.apache.spark.sql.DataFrameTimeWindowingSuite", "org.apache.spark.sql.DataFrameTimeWindowingSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.FileSourceCharVarcharDDLTestSuite", "org.apache.spark.sql.execution.command.FileSourceCharVarcharDDLTestSuite", "org.apache.spark.sql.execution.command.FileSourceCharVarcharDDLTestSuite", "org.apache.spark.sql.execution.command.FileSourceCharVarcharDDLTestSuite", "org.apache.spark.sql.execution.command.FileSourceCharVarcharDDLTestSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.CheckpointFileManagerSuite", "org.apache.spark.sql.execution.streaming.CheckpointFileManagerSuite", "org.apache.spark.sql.execution.streaming.CheckpointFileManagerSuite", "org.apache.spark.sql.execution.streaming.CheckpointFileManagerSuite", "org.apache.spark.sql.execution.streaming.CheckpointFileManagerSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.IntervalFunctionsSuite", "org.apache.spark.sql.IntervalFunctionsSuite", "org.apache.spark.sql.IntervalFunctionsSuite", "org.apache.spark.sql.IntervalFunctionsSuite", "org.apache.spark.sql.IntervalFunctionsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.DeprecatedWholeStageCodegenSuite", "org.apache.spark.sql.execution.DeprecatedWholeStageCodegenSuite", "org.apache.spark.sql.execution.DeprecatedWholeStageCodegenSuite", "org.apache.spark.sql.execution.DeprecatedWholeStageCodegenSuite", "org.apache.spark.sql.execution.DeprecatedWholeStageCodegenSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.sources.InsertSuite", "org.apache.spark.sql.sources.InsertSuite", "org.apache.spark.sql.sources.InsertSuite", "org.apache.spark.sql.sources.InsertSuite", "org.apache.spark.sql.sources.InsertSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DataFrameSetOperationsSuite", "org.apache.spark.sql.DataFrameSetOperationsSuite", "org.apache.spark.sql.DataFrameSetOperationsSuite", "org.apache.spark.sql.DataFrameSetOperationsSuite", "org.apache.spark.sql.DataFrameSetOperationsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.SparkPlannerSuite", "org.apache.spark.sql.execution.SparkPlannerSuite", "org.apache.spark.sql.execution.SparkPlannerSuite", "org.apache.spark.sql.execution.SparkPlannerSuite", "org.apache.spark.sql.execution.SparkPlannerSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DatasetSerializerRegistratorSuite", "org.apache.spark.sql.DatasetSerializerRegistratorSuite", "org.apache.spark.sql.DatasetSerializerRegistratorSuite", "org.apache.spark.sql.DatasetSerializerRegistratorSuite", "org.apache.spark.sql.DatasetSerializerRegistratorSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.ShowNamespacesParserSuite", "org.apache.spark.sql.execution.command.ShowNamespacesParserSuite", "org.apache.spark.sql.execution.command.ShowNamespacesParserSuite", "org.apache.spark.sql.execution.command.ShowNamespacesParserSuite", "org.apache.spark.sql.execution.command.ShowNamespacesParserSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.StreamingQueryStatusAndProgressSuite", "org.apache.spark.sql.streaming.StreamingQueryStatusAndProgressSuite", "org.apache.spark.sql.streaming.StreamingQueryStatusAndProgressSuite", "org.apache.spark.sql.streaming.StreamingQueryStatusAndProgressSuite", "org.apache.spark.sql.streaming.StreamingQueryStatusAndProgressSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.columnar.PartitionBatchPruningSuite", "org.apache.spark.sql.execution.columnar.PartitionBatchPruningSuite", "org.apache.spark.sql.execution.columnar.PartitionBatchPruningSuite", "org.apache.spark.sql.execution.columnar.PartitionBatchPruningSuite", "org.apache.spark.sql.execution.columnar.PartitionBatchPruningSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.FileStreamSinkLogSuite", "org.apache.spark.sql.execution.streaming.FileStreamSinkLogSuite", "org.apache.spark.sql.execution.streaming.FileStreamSinkLogSuite", "org.apache.spark.sql.execution.streaming.FileStreamSinkLogSuite", "org.apache.spark.sql.execution.streaming.FileStreamSinkLogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.VectorizedOrcReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.VectorizedOrcReadSchemaSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.ComplexTypesSuite", "org.apache.spark.sql.ComplexTypesSuite", "org.apache.spark.sql.ComplexTypesSuite", "org.apache.spark.sql.ComplexTypesSuite", "org.apache.spark.sql.ComplexTypesSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.joins.HashedRelationSuite", "org.apache.spark.sql.execution.joins.HashedRelationSuite", "org.apache.spark.sql.execution.joins.HashedRelationSuite", "org.apache.spark.sql.execution.joins.HashedRelationSuite", "org.apache.spark.sql.execution.joins.HashedRelationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.jdbc.JDBCWriteSuite", "org.apache.spark.sql.jdbc.JDBCWriteSuite", "org.apache.spark.sql.jdbc.JDBCWriteSuite", "org.apache.spark.sql.jdbc.JDBCWriteSuite", "org.apache.spark.sql.jdbc.JDBCWriteSuite", "org.apache.spark.sql.jdbc.JDBCWriteSuite", "org.apache.spark.sql.jdbc.JDBCWriteSuite", "org.apache.spark.sql.ExplainSuite", "org.apache.spark.sql.ExplainSuite", "org.apache.spark.sql.ExplainSuite", "org.apache.spark.sql.ExplainSuite", "org.apache.spark.sql.ExplainSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.joins.OuterJoinSuite", "org.apache.spark.sql.execution.joins.OuterJoinSuite", "org.apache.spark.sql.execution.joins.OuterJoinSuite", "org.apache.spark.sql.execution.joins.OuterJoinSuite", "org.apache.spark.sql.execution.joins.OuterJoinSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.PlannerSuite", "org.apache.spark.sql.execution.PlannerSuite", "org.apache.spark.sql.execution.PlannerSuite", "org.apache.spark.sql.execution.PlannerSuite", "org.apache.spark.sql.execution.PlannerSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.orc.OrcColumnarBatchReaderSuite", "org.apache.spark.sql.execution.datasources.orc.OrcColumnarBatchReaderSuite", "org.apache.spark.sql.execution.datasources.orc.OrcColumnarBatchReaderSuite", "org.apache.spark.sql.execution.datasources.orc.OrcColumnarBatchReaderSuite", "org.apache.spark.sql.execution.datasources.orc.OrcColumnarBatchReaderSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.JavaUDFSuite", "org.apache.spark.sql.JavaUDFSuite", "org.apache.spark.sql.JavaUDFSuite", "org.apache.spark.sql.JavaUDFSuite", "org.apache.spark.sql.JavaUDFSuite", "org.apache.spark.sql.JavaUDFSuite", "org.apache.spark.sql.JavaUDFSuite", "org.apache.spark.sql.JavaUDFSuite", "org.apache.spark.sql.JavaUDFSuite", "org.apache.spark.sql.JavaUDFSuite", "org.apache.spark.sql.JavaUDFSuite", "org.apache.spark.sql.JavaUDFSuite", "org.apache.spark.sql.JavaUDFSuite", "org.apache.spark.sql.JavaUDFSuite", "org.apache.spark.sql.JavaUDFSuite", "org.apache.spark.sql.JavaUDFSuite", "org.apache.spark.sql.JavaDataFrameReaderWriterSuite", "org.apache.spark.sql.JavaDataFrameReaderWriterSuite", "org.apache.spark.sql.JavaDataFrameReaderWriterSuite", "org.apache.spark.sql.JavaDataFrameReaderWriterSuite", "org.apache.spark.sql.JavaDataFrameReaderWriterSuite", "org.apache.spark.sql.JavaDataFrameReaderWriterSuite", "org.apache.spark.sql.JavaDataFrameReaderWriterSuite", "org.apache.spark.sql.JavaDataFrameReaderWriterSuite", "org.apache.spark.sql.JavaDataFrameReaderWriterSuite", "org.apache.spark.sql.JavaDatasetAggregatorSuite", "org.apache.spark.sql.JavaDatasetAggregatorSuite", "org.apache.spark.sql.JavaDatasetAggregatorSuite", "org.apache.spark.sql.JavaDatasetAggregatorSuite", "org.apache.spark.sql.JavaDatasetAggregatorSuite", "org.apache.spark.sql.JavaDatasetAggregatorSuite", "org.apache.spark.sql.JavaDatasetAggregatorSuite", "org.apache.spark.sql.JavaDatasetAggregatorSuite", "org.apache.spark.sql.JavaUDAFSuite", "org.apache.spark.sql.JavaUDAFSuite", "org.apache.spark.sql.JavaDatasetAggregatorSuite", "org.apache.spark.sql.JavaDatasetAggregatorSuite", "org.apache.spark.sql.JavaDatasetAggregatorSuite", "org.apache.spark.sql.JavaDatasetAggregatorSuite", "org.apache.spark.sql.JavaDatasetAggregatorSuite", "org.apache.spark.sql.JavaDatasetAggregatorSuite", "org.apache.spark.sql.JavaDatasetAggregatorSuite", "org.apache.spark.sql.JavaDatasetAggregatorSuite", "org.apache.spark.sql.JavaDatasetAggregatorSuite", "org.apache.spark.sql.JavaDatasetAggregatorSuite", "org.apache.spark.sql.JavaApplySchemaSuite", "org.apache.spark.sql.JavaApplySchemaSuite", "org.apache.spark.sql.JavaApplySchemaSuite", "org.apache.spark.sql.JavaApplySchemaSuite", "org.apache.spark.sql.JavaApplySchemaSuite", "org.apache.spark.sql.JavaApplySchemaSuite", "org.apache.spark.sql.JavaColumnExpressionSuite", "org.apache.spark.sql.JavaColumnExpressionSuite", "org.apache.spark.sql.JavaColumnExpressionSuite", "org.apache.spark.sql.JavaColumnExpressionSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite", "org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite", "org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite", "org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite", "org.apache.spark.sql.JavaSaveLoadSuite", "org.apache.spark.sql.JavaSaveLoadSuite", "org.apache.spark.sql.JavaSaveLoadSuite", "org.apache.spark.sql.JavaSaveLoadSuite", "org.apache.spark.sql.JavaHigherOrderFunctionsSuite", "org.apache.spark.sql.JavaHigherOrderFunctionsSuite", "org.apache.spark.sql.JavaHigherOrderFunctionsSuite", "org.apache.spark.sql.JavaHigherOrderFunctionsSuite", "org.apache.spark.sql.JavaHigherOrderFunctionsSuite", "org.apache.spark.sql.JavaHigherOrderFunctionsSuite", "org.apache.spark.sql.JavaHigherOrderFunctionsSuite", "org.apache.spark.sql.JavaHigherOrderFunctionsSuite", "org.apache.spark.sql.JavaHigherOrderFunctionsSuite", "org.apache.spark.sql.JavaHigherOrderFunctionsSuite", "org.apache.spark.sql.JavaHigherOrderFunctionsSuite", "org.apache.spark.sql.JavaHigherOrderFunctionsSuite", "org.apache.spark.sql.JavaHigherOrderFunctionsSuite", "org.apache.spark.sql.JavaHigherOrderFunctionsSuite", "org.apache.spark.sql.JavaHigherOrderFunctionsSuite", "org.apache.spark.sql.JavaHigherOrderFunctionsSuite", "org.apache.spark.sql.JavaHigherOrderFunctionsSuite", "org.apache.spark.sql.JavaHigherOrderFunctionsSuite", "org.apache.spark.sql.JavaHigherOrderFunctionsSuite", "org.apache.spark.sql.JavaHigherOrderFunctionsSuite", "org.apache.spark.sql.JavaBeanDeserializationSuite", "org.apache.spark.sql.JavaBeanDeserializationSuite", "org.apache.spark.sql.JavaBeanDeserializationSuite", "org.apache.spark.sql.JavaBeanDeserializationSuite", "org.apache.spark.sql.JavaBeanDeserializationSuite", "org.apache.spark.sql.JavaBeanDeserializationSuite", "org.apache.spark.sql.JavaBeanDeserializationSuite", "org.apache.spark.sql.JavaBeanDeserializationSuite", "org.apache.spark.sql.JavaBeanDeserializationSuite", "org.apache.spark.sql.JavaBeanDeserializationSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameWriterV2Suite", "org.apache.spark.sql.JavaDataFrameWriterV2Suite", "org.apache.spark.sql.JavaDataFrameWriterV2Suite", "org.apache.spark.sql.JavaDataFrameWriterV2Suite", "org.apache.spark.sql.JavaDataFrameWriterV2Suite", "org.apache.spark.sql.JavaDataFrameWriterV2Suite", "org.apache.spark.sql.JavaDataFrameWriterV2Suite", "org.apache.spark.sql.JavaDataFrameWriterV2Suite", "org.apache.spark.sql.JavaDataFrameWriterV2Suite", "org.apache.spark.sql.JavaDataFrameWriterV2Suite", "org.apache.spark.sql.JavaDataFrameWriterV2Suite", "org.apache.spark.sql.JavaDataFrameWriterV2Suite", "org.apache.spark.sql.JavaUDFSuite", "org.apache.spark.sql.execution.datasources.BasicWriteJobStatsTrackerMetricSuite", "org.apache.spark.sql.JavaDataFrameReaderWriterSuite", "org.apache.spark.sql.Java8DatasetAggregatorSuite", "org.apache.spark.sql.JavaUDAFSuite", "org.apache.spark.sql.JavaDatasetAggregatorSuite", "org.apache.spark.sql.JavaApplySchemaSuite", "org.apache.spark.sql.JavaColumnExpressionSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite", "org.apache.spark.sql.JavaSaveLoadSuite", "org.apache.spark.sql.JavaHigherOrderFunctionsSuite", "org.apache.spark.sql.JavaBeanDeserializationSuite", "org.apache.spark.sql.execution.WholeStageCodegenSparkSubmitSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameWriterV2Suite", "org.apache.spark.sql.SQLQuerySuite", "org.apache.spark.sql.streaming.MemorySourceStressSuite", "org.apache.spark.sql.execution.HiveResultSuite", "org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderSuite", "org.apache.spark.sql.execution.command.DSV2CharVarcharDDLTestSuite", "org.apache.spark.sql.execution.SparkPlanSuite", "org.apache.spark.sql.streaming.StreamingAggregationSuite", "org.apache.spark.sql.DateFunctionsSuite", "org.apache.spark.sql.NestedDataSourceV1Suite", "org.apache.spark.sql.execution.streaming.state.StateStoreSuite", "org.apache.spark.sql.execution.streaming.sources.ForeachBatchSinkSuite", "org.apache.spark.sql.jdbc.JDBCV2Suite", "org.apache.spark.sql.streaming.continuous.ContinuousQueryStatusAndProgressSuite", "org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRenameSuite", "org.apache.spark.sql.execution.datasources.v2.V2SessionCatalogTableSuite", "org.apache.spark.sql.execution.datasources.BasicWriteTaskStatsTrackerSuite", "org.apache.spark.sql.execution.arrow.ArrowConvertersSuite", "org.apache.spark.sql.execution.command.v2.AlterTableDropPartitionSuite", "org.apache.spark.sql.DynamicPartitionPruningV1Suite", "org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite", "org.apache.spark.sql.execution.command.v1.ShowPartitionsSuite", "org.apache.spark.sql.ReplaceNullWithFalseInPredicateEndToEndSuite", "org.apache.spark.sql.streaming.continuous.ContinuousEpochBacklogSuite", "org.apache.spark.sql.DeprecatedAPISuite", "org.apache.spark.sql.execution.datasources.MergedParquetReadSchemaSuite", "org.apache.spark.sql.streaming.StreamingQueryListenersConfSuite", "org.apache.spark.sql.execution.datasources.OrcReadSchemaSuite", "org.apache.spark.sql.jdbc.JDBCSuite", "org.apache.spark.sql.execution.datasources.HeaderCSVReadSchemaSuite", "org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCTableCatalogSuite", "org.apache.spark.sql.DataFrameJoinSuite", "org.apache.spark.sql.execution.SQLWindowFunctionSuite", "org.apache.spark.sql.streaming.continuous.EpochCoordinatorSuite", "org.apache.spark.sql.streaming.sources.StreamingDataSourceV2Suite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.execution.command.v2.AlterTableRecoverPartitionsSuite", "org.apache.spark.sql.TypedImperativeAggregateSuite", "org.apache.spark.sql.execution.datasources.binaryfile.BinaryFileFormatSuite", "org.apache.spark.sql.execution.datasources.FileIndexSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetRebaseDatetimeV2Suite", "org.apache.spark.sql.DatasetPrimitiveSuite", "org.apache.spark.sql.execution.datasources.text.WholeTextFileV2Suite", "org.apache.spark.sql.execution.datasources.parquet.ParquetColumnIndexSuite", "org.apache.spark.sql.execution.PersistedViewTestSuite", "org.apache.spark.sql.connector.DataSourceV2FunctionSuite", "org.apache.spark.sql.execution.streaming.UpdatingSessionsIteratorSuite", "org.apache.spark.sql.connector.V1ReadFallbackWithCatalogSuite", "org.apache.spark.sql.connector.V1WriteFallbackSessionCatalogSuite", "org.apache.spark.sql.execution.streaming.state.StateSchemaCompatibilityCheckerSuite", "org.apache.spark.sql.internal.CatalogSuite", "org.apache.spark.sql.connector.WriteDistributionAndOrderingSuite", "org.apache.spark.sql.streaming.continuous.ContinuousQueuedDataReaderSuite", "org.apache.spark.sql.GeneratorFunctionSuite", "org.apache.spark.sql.execution.datasources.csv.CSVLegacyTimeParserSuite", "org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorSuite", "org.apache.spark.sql.execution.datasources.DataSourceStrategySuite", "org.apache.spark.sql.streaming.FileStreamSourceSuite", "org.apache.spark.sql.JoinHintSuite", "org.apache.spark.sql.DynamicPartitionPruningV2Suite", "org.apache.spark.sql.streaming.StreamingSessionWindowSuite", "org.apache.spark.sql.DSV2SQLInsertTestSuite", "org.apache.spark.sql.execution.python.ExtractPythonUDFsSuite", "org.apache.spark.sql.DynamicPartitionPruningV1Suite", "org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite", "org.apache.spark.sql.execution.ui.SQLAppStatusListenerSuite", "org.apache.spark.sql.execution.datasources.orc.OrcV1FilterSuite", "org.apache.spark.sql.connector.DataSourceV2SQLSuite", "org.apache.spark.sql.DatasetCacheSuite", "org.apache.spark.sql.JoinSuite", "org.apache.spark.sql.execution.joins.BroadcastJoinSuite", "org.apache.spark.sql.UDFSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRecoverPartitionsSuite", "org.apache.spark.sql.CachedTableSuite", "org.apache.spark.sql.execution.command.ShowTablesParserSuite", "org.apache.spark.sql.sources.PathOptionSuite", "org.apache.spark.sql.execution.command.AlterTableAddPartitionParserSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaInferenceSuite", "org.apache.spark.sql.execution.GlobalTempViewTestSuite", "org.apache.spark.sql.SerializationSuite", "org.apache.spark.sql.DataFrameWindowFunctionsSuite", "org.apache.spark.sql.execution.datasources.text.TextV1Suite", "org.apache.spark.sql.sources.BucketedWriteWithoutHiveSupportSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetAvroCompatibilitySuite", "org.apache.spark.sql.sources.CreateTableAsSelectSuite", "org.apache.spark.sql.execution.ColumnarRulesSuite", "org.apache.spark.sql.SQLContextSuite", "org.apache.spark.sql.UnwrapCastInComparisonEndToEndSuite", "org.apache.spark.sql.SetCommandSuite", "org.apache.spark.sql.StatisticsCollectionSuite", "org.apache.spark.sql.SSBQuerySuite", "org.apache.spark.sql.streaming.StreamingQueryListenerSuite", "org.apache.spark.sql.streaming.FileStreamSinkV1Suite", "org.apache.spark.sql.streaming.StreamingOuterJoinSuite", "org.apache.spark.sql.test.DataFrameReaderWriterSuite", "org.apache.spark.sql.StringFunctionsSuite", "org.apache.spark.sql.NestedDataSourceV2Suite", "org.apache.spark.sql.execution.command.AlterTableRecoverPartitionsParserSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetRebaseDatetimeV1Suite", "org.apache.spark.sql.sources.PrunedScanSuite", "org.apache.spark.sql.execution.command.TruncateTableParserSuite", "org.apache.spark.sql.execution.streaming.CompactibleFileStreamLogSuite", "org.apache.spark.sql.streaming.StreamingQueryManagerSuite", "org.apache.spark.sql.execution.datasources.noop.NoopStreamSuite", "org.apache.spark.sql.CsvFunctionsSuite", "org.apache.spark.sql.connector.DataSourceV2DataFrameSessionCatalogSuite", "org.apache.spark.sql.execution.SortSuite", "org.apache.spark.sql.SparkSessionBuilderSuite", "org.apache.spark.sql.streaming.continuous.ContinuousStressSuite", "org.apache.spark.sql.execution.columnar.RefCountedTestCachedBatchSerializerSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV1QuerySuite", "org.apache.spark.sql.sources.FilteredScanSuite", "org.apache.spark.sql.BasicCharVarcharTestSuite", "org.apache.spark.sql.SubquerySuite", "org.apache.spark.sql.execution.command.ShowPartitionsParserSuite", "org.apache.spark.sql.execution.datasources.InMemoryTableMetricSuite", "org.apache.spark.sql.execution.ui.AllExecutionsPageSuite", "org.apache.spark.sql.sources.TableScanSuite", "org.apache.spark.sql.streaming.StreamingDeduplicationSuite", "org.apache.spark.sql.execution.command.v1.MsckRepairTableSuite", "org.apache.spark.sql.MetadataCacheV2Suite", "org.apache.spark.sql.execution.AggregatingAccumulatorSuite", "org.apache.spark.sql.execution.streaming.state.StateStoreCompatibilitySuite", "org.apache.spark.sql.internal.SQLConfSuite", "org.apache.spark.sql.jdbc.JDBCNestedDataSourceSuite", "org.apache.spark.sql.execution.datasources.DataSourceSuite", "org.apache.spark.sql.execution.datasources.text.TextV2Suite", "org.apache.spark.sql.MathFunctionsSuite", "org.apache.spark.sql.streaming.DeprecatedStreamingAggregationSuite", "org.apache.spark.sql.execution.streaming.sources.TextSocketStreamSuite", "org.apache.spark.sql.CTEHintSuite", "org.apache.spark.sql.execution.datasources.ParquetReadSchemaSuite", "org.apache.spark.sql.execution.joins.InnerJoinSuite", "org.apache.spark.sql.execution.datasources.RowDataSourceStrategySuite", "org.apache.spark.sql.execution.GlobalTempViewSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRecoverPartitionsParallelSuite", "org.apache.spark.sql.execution.ui.SQLAppStatusListenerMemoryLeakSuite", "org.apache.spark.sql.execution.datasources.orc.OrcPartitionDiscoverySuite", "org.apache.spark.sql.execution.RemoveRedundantSortsSuite", "org.apache.spark.sql.execution.datasources.HadoopFileLinesReaderSuite", "org.apache.spark.sql.DynamicPartitionPruningV2Suite", "org.apache.spark.sql.test.GenericFunSpecSuite", "org.apache.spark.sql.connector.SupportsCatalogOptionsSuite", "org.apache.spark.sql.DatasetAggregatorSuite", "org.apache.spark.sql.execution.command.v2.AlterTableAddPartitionSuite", "org.apache.spark.sql.execution.command.v2.ShowPartitionsSuite", "org.apache.spark.sql.sources.DisableUnnecessaryBucketedScanWithoutHiveSupportSuite", "org.apache.spark.sql.execution.command.v1.ShowNamespacesSuite", "org.apache.spark.sql.execution.command.v1.ShowTablesSuite", "org.apache.spark.sql.test.GenericFlatSpecSuite", "org.apache.spark.sql.execution.RemoveRedundantProjectsSuite", "org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManagerSuite", "org.apache.spark.sql.streaming.ui.UISeleniumSuite", "org.apache.spark.sql.execution.WholeStageCodegenSuite", "org.apache.spark.sql.CountMinSketchAggQuerySuite", "org.apache.spark.sql.execution.SQLExecutionSuite", "org.apache.spark.sql.execution.DataSourceScanExecRedactionSuite", "org.apache.spark.sql.streaming.test.DataStreamTableAPISuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.DataFramePivotSuite", "org.apache.spark.sql.streaming.FileStreamStressSuite", "org.apache.spark.sql.execution.columnar.CachedBatchSerializerSuite", "org.apache.spark.sql.execution.datasources.VectorizedParquetReadSchemaSuite", "org.apache.spark.sql.execution.streaming.state.RocksDBSuite", "org.apache.spark.sql.execution.datasources.text.WholeTextFileV1Suite", "org.apache.spark.sql.execution.command.v1.AlterTableAddPartitionSuite", "org.apache.spark.sql.streaming.ui.StreamingQueryStatusListenerSuite", "org.apache.spark.sql.UserDefinedTypeSuite", "org.apache.spark.sql.execution.command.DropTableParserSuite", "org.apache.spark.sql.DSV2CharVarcharTestSuite", "org.apache.spark.sql.SparkSessionExtensionSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite", "org.apache.spark.sql.DataFrameSelfJoinSuite", "org.apache.spark.sql.streaming.StreamingInnerJoinSuite", "org.apache.spark.sql.connector.V1ReadFallbackWithDataFrameReaderSuite", "org.apache.spark.sql.execution.CoalesceShufflePartitionsSuite", "org.apache.spark.sql.execution.datasources.CSVReadSchemaSuite", "org.apache.spark.sql.ApproximatePercentileQuerySuite", "org.apache.spark.sql.DataFrameSuite", "org.apache.spark.sql.DataFrameNaFunctionsSuite", "org.apache.spark.sql.execution.datasources.orc.OrcFilterSuite", "org.apache.spark.sql.execution.datasources.noop.NoopSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.sources.SaveLoadSuite", "org.apache.spark.sql.FileScanSuite", "org.apache.spark.sql.execution.streaming.MicroBatchExecutionSuite", "org.apache.spark.sql.execution.datasources.csv.CSVv1Suite", "org.apache.spark.sql.FileBasedDataSourceSuite", "org.apache.spark.sql.ExtraStrategiesSuite", "org.apache.spark.sql.execution.datasources.orc.OrcV1SchemaPruningSuite", "org.apache.spark.sql.connector.DataSourceV2SQLSessionCatalogSuite", "org.apache.spark.sql.execution.streaming.OffsetSeqLogSuite", "org.apache.spark.sql.internal.DeprecatedCreateExternalTableSuite", "org.apache.spark.sql.execution.QueryExecutionSuite", "org.apache.spark.sql.ApproxCountDistinctForIntervalsQuerySuite", "org.apache.spark.sql.DeprecatedDatasetAggregatorSuite", "org.apache.spark.sql.execution.LocalTempViewTestSuite", "org.apache.spark.sql.execution.python.BatchEvalPythonExecSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV1PartitionDiscoverySuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.execution.datasources.PathFilterStrategySuite", "org.apache.spark.sql.streaming.ui.StreamingQueryPageSuite", "org.apache.spark.sql.execution.command.v2.MsckRepairTableSuite", "org.apache.spark.sql.internal.SharedStateSuite", "org.apache.spark.sql.execution.metric.SQLMetricsSuite", "org.apache.spark.sql.execution.streaming.MemorySinkSuite", "org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommandSuite", "org.apache.spark.sql.execution.command.v2.AlterTableRenamePartitionSuite", "org.apache.spark.sql.DataFrameComplexTypeSuite", "org.apache.spark.sql.TPCHQuerySuite", "org.apache.spark.sql.ScalaReflectionRelationSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetEncodingSuite", "org.apache.spark.sql.execution.command.v2.ShowNamespacesSuite", "org.apache.spark.sql.expressions.ExpressionInfoSuite", "org.apache.spark.sql.execution.columnar.InMemoryColumnarQuerySuite", "org.apache.spark.sql.execution.command.v1.TruncateTableSuite", "org.apache.spark.sql.execution.debug.DebuggingSuite", "org.apache.spark.sql.execution.RemoveRedundantSortsSuite", "org.apache.spark.sql.DataFrameWriterV2Suite", "org.apache.spark.sql.execution.streaming.FileSystemBasedCheckpointFileManagerSuite", "org.apache.spark.sql.connector.V1WriteFallbackSuite", "org.apache.spark.sql.execution.SQLJsonProtocolSuite", "org.apache.spark.sql.FileSourceSQLInsertTestSuite", "org.apache.spark.sql.execution.datasources.orc.OrcV1QuerySuite", "org.apache.spark.sql.execution.aggregate.SortBasedAggregationStoreSuite", "org.apache.spark.sql.execution.streaming.sources.RateStreamProviderSuite", "org.apache.spark.sql.util.DataFrameCallbackSuite", "org.apache.spark.sql.execution.datasources.v2.FileTableSuite", "org.apache.spark.sql.execution.datasources.JsonReadSchemaSuite", "org.apache.spark.sql.streaming.FlatMapGroupsWithStateSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite", "org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArraySuite", "org.apache.spark.sql.ConfigBehaviorSuite", "org.apache.spark.sql.DatasetSuite", "org.apache.spark.sql.DataFrameTungstenSuite", "org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreSuite", "org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMapSuite", "org.apache.spark.sql.connector.FileDataSourceV2FallBackSuite", "org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite", "org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerSuite", "org.apache.spark.sql.test.GenericWordSpecSuite", "org.apache.spark.sql.ProductAggSuite", "org.apache.spark.sql.execution.bucketing.CoalesceBucketsInJoinSuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilitySuite", "org.apache.spark.sql.QueryTestSuite", "org.apache.spark.sql.FileSourceCharVarcharTestSuite", "org.apache.spark.sql.MiscFunctionsSuite", "org.apache.spark.sql.execution.datasources.FileFormatWriterSuite", "org.apache.spark.sql.execution.datasources.orc.OrcSourceSuite", "org.apache.spark.sql.streaming.StreamingLeftSemiJoinSuite", "org.apache.spark.sql.streaming.EventTimeWatermarkSuite", "org.apache.spark.sql.execution.ui.SparkPlanInfoSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV2PartitionDiscoverySuite", "org.apache.spark.sql.execution.streaming.StreamMetadataSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetThriftCompatibilitySuite", "org.apache.spark.sql.execution.streaming.sources.ConsoleWriteSupportSuite", "org.apache.spark.sql.execution.SparkScriptTransformationSuite", "org.apache.spark.sql.connector.TableCapabilityCheckSuite", "org.apache.spark.sql.execution.datasources.orc.OrcV2SchemaPruningSuite", "org.apache.spark.sql.execution.command.AlterTableRenamePartitionParserSuite", "org.apache.spark.sql.sources.PartitionedWriteSuite", "org.apache.spark.sql.execution.datasources.orc.OrcEncryptionSuite", "org.apache.spark.sql.internal.ExecutorSideSQLConfSuite", "org.apache.spark.sql.internal.SQLConfGetterSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV2QuerySuite", "org.apache.spark.sql.execution.datasources.json.JsonParsingOptionsSuite", "org.apache.spark.sql.JsonFunctionsSuite", "org.apache.spark.sql.execution.command.v2.AlterTableRenameSuite", "org.apache.spark.sql.streaming.test.DataStreamReaderWriterSuite", "org.apache.spark.sql.streaming.StreamingSymmetricHashJoinHelperSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormatV1Suite", "org.apache.spark.sql.execution.DataSourceV2ScanExecRedactionSuite", "org.apache.spark.sql.execution.datasources.orc.OrcV1PartitionDiscoverySuite", "org.apache.spark.sql.test.TestSparkSessionSuite", "org.apache.spark.sql.connector.DataSourceV2DataFrameSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetInteroperabilitySuite", "org.apache.spark.sql.execution.QueryPlanningTrackerEndToEndSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetProtobufCompatibilitySuite", "org.apache.spark.sql.execution.exchange.EnsureRequirementsSuite", "org.apache.spark.sql.execution.SimpleSQLViewSuite", "org.apache.spark.sql.execution.command.v1.DropTableSuite", "org.apache.spark.sql.execution.joins.ExistenceJoinSuite", "org.apache.spark.sql.execution.SameResultSuite", "org.apache.spark.sql.streaming.StreamingFullOuterJoinSuite", "org.apache.spark.sql.DataFrameStatSuite", "org.apache.spark.sql.XPathFunctionsSuite", "org.apache.spark.sql.streaming.FileStreamSourceStressTestSuite", "org.apache.spark.sql.DataFrameWindowFramesSuite", "org.apache.spark.status.api.v1.sql.SqlResourceWithActualMetricsSuite", "org.apache.spark.sql.execution.streaming.state.StateStoreRDDSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite", "org.apache.spark.sql.sources.DisableUnnecessaryBucketedScanWithoutHiveSupportSuite", "org.apache.spark.sql.execution.datasources.PathFilterSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRenamePartitionSuite", "org.apache.spark.sql.execution.streaming.state.StreamingSessionWindowStateManagerSuite", "org.apache.spark.sql.execution.datasources.FileSourceStrategySuite", "org.apache.spark.sql.SessionStateSuite", "org.apache.spark.sql.connector.LocalScanSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetCompressionCodecPrecedenceSuite", "org.apache.spark.sql.execution.UnsafeRowSerializerSuite", "org.apache.spark.sql.execution.command.v2.ShowTablesSuite", "org.apache.spark.sql.execution.BroadcastExchangeSuite", "org.apache.spark.sql.execution.OptimizeMetadataOnlyQuerySuite", "org.apache.spark.sql.execution.datasources.MergedOrcReadSchemaSuite", "org.apache.spark.sql.streaming.StreamingQuerySuite", "org.apache.spark.sql.execution.datasources.jdbc.connection.DisallowedConnectionProviderSuite", "org.apache.spark.sql.streaming.FileStreamSinkV2Suite", "org.apache.spark.sql.execution.command.AlterTableDropPartitionParserSuite", "org.apache.spark.sql.execution.datasources.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetCommitterSuite", "org.apache.spark.sql.execution.command.DDLParserSuite", "org.apache.spark.sql.execution.datasources.v2.V2SessionCatalogNamespaceSuite", "org.apache.spark.sql.sources.ExternalCommandRunnerSuite", "org.apache.spark.sql.execution.command.v2.DropTableSuite", "org.apache.spark.sql.RowSuite", "org.apache.spark.sql.execution.command.v2.TruncateTableSuite", "org.apache.spark.sql.execution.RemoveRedundantProjectsSuite", "org.apache.spark.sql.connector.V2CommandsCaseSensitivitySuite", "org.apache.spark.sql.sources.ResolvedDataSourceSuite", "org.apache.spark.sql.execution.streaming.sources.ForeachWriterSuite", "org.apache.spark.sql.execution.python.PythonUDFSuite", "org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreIntegrationSuite", "org.apache.spark.sql.execution.ReuseExchangeAndSubquerySuite", "org.apache.spark.sql.DataFrameFunctionsSuite", "org.apache.spark.sql.execution.datasources.orc.OrcV2QuerySuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilityWithStatsSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormatV2Suite", "org.apache.spark.sql.MetadataCacheV1Suite", "org.apache.spark.sql.streaming.continuous.ContinuousMetaSuite", "org.apache.spark.sql.ColumnExpressionSuite", "org.apache.spark.sql.DataFrameSessionWindowingSuite", "org.apache.spark.sql.util.ExecutionListenerManagerSuite", "org.apache.spark.sql.DataFrameImplicitsSuite", "org.apache.spark.sql.execution.streaming.HDFSMetadataLogSuite", "org.apache.spark.sql.execution.ExchangeSuite", "org.apache.spark.sql.DatasetOptimizationSuite", "org.apache.spark.sql.streaming.continuous.ContinuousSuite", "org.apache.spark.sql.execution.joins.BroadcastJoinSuite", "org.apache.spark.sql.execution.datasources.json.JsonV1Suite", "org.apache.spark.sql.execution.command.v1.AlterTableDropPartitionSuite", "org.apache.spark.sql.execution.streaming.state.FlatMapGroupsWithStateExecHelperSuite", "org.apache.spark.sql.execution.ShufflePartitionsUtilSuite", "org.apache.spark.sql.connector.DataSourceV2Suite", "org.apache.spark.sql.ExplainSuite", "org.apache.spark.sql.execution.TakeOrderedAndProjectSuite", "org.apache.spark.sql.api.r.SQLUtilsSuite", "org.apache.spark.sql.sources.DDLSourceLoadSuite", "org.apache.spark.sql.streaming.StreamingStateStoreFormatCompatibilitySuite", "org.apache.spark.sql.streaming.ui.StreamingQueryHistorySuite", "org.apache.spark.sql.execution.datasources.json.JsonV2Suite", "org.apache.spark.sql.SimpleShowCreateTableSuite", "org.apache.spark.sql.execution.datasources.json.JsonLegacyTimeParserSuite", "org.apache.spark.sql.execution.debug.DebuggingSuite", "org.apache.spark.sql.execution.datasources.csv.CSVv2Suite", "org.apache.spark.sql.streaming.StreamSuite", "org.apache.spark.sql.DataFrameHintSuite", "org.apache.spark.sql.DataFrameRangeSuite", "org.apache.spark.sql.DataFrameTimeWindowingSuite", "org.apache.spark.sql.execution.command.FileSourceCharVarcharDDLTestSuite", "org.apache.spark.sql.execution.streaming.CheckpointFileManagerSuite", "org.apache.spark.sql.IntervalFunctionsSuite", "org.apache.spark.sql.execution.DeprecatedWholeStageCodegenSuite", "org.apache.spark.sql.sources.InsertSuite", "org.apache.spark.sql.DataFrameSetOperationsSuite", "org.apache.spark.sql.execution.SparkPlannerSuite", "org.apache.spark.sql.DatasetSerializerRegistratorSuite", "org.apache.spark.sql.execution.command.ShowNamespacesParserSuite", "org.apache.spark.sql.streaming.StreamingQueryStatusAndProgressSuite", "org.apache.spark.sql.execution.columnar.PartitionBatchPruningSuite", "org.apache.spark.sql.execution.streaming.FileStreamSinkLogSuite", "org.apache.spark.sql.execution.datasources.VectorizedOrcReadSchemaSuite", "org.apache.spark.sql.ComplexTypesSuite", "org.apache.spark.sql.execution.joins.HashedRelationSuite", "org.apache.spark.sql.jdbc.JDBCWriteSuite", "org.apache.spark.sql.ExplainSuite", "org.apache.spark.sql.execution.joins.OuterJoinSuite", "org.apache.spark.sql.execution.PlannerSuite", "org.apache.spark.sql.execution.datasources.orc.OrcColumnarBatchReaderSuite", "org.apache.spark.sql.jdbc.v2.OracleIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.jdbc.v2.MsSqlServerIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.jdbc.DB2IntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.jdbc.v2.DB2IntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.jdbc.MariaDBKrbIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerKrbJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerKrbJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerKrbJDBCIntegrationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.jdbc.v2.PostgresNamespaceSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.jdbc.MsSqlServerIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.jdbc.OracleIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.jdbc.v2.MySQLIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.jdbc.PostgresKrbIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerKrbJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerKrbJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerKrbJDBCIntegrationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.jdbc.PostgresIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.jdbc.DB2KrbIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerKrbJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerKrbJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerKrbJDBCIntegrationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.jdbc.v2.PostgresIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.jdbc.MySQLIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.jdbc.v2.OracleIntegrationSuite", "org.apache.spark.sql.jdbc.v2.MsSqlServerIntegrationSuite", "org.apache.spark.sql.jdbc.DB2IntegrationSuite", "org.apache.spark.sql.jdbc.v2.DB2IntegrationSuite", "org.apache.spark.sql.jdbc.MariaDBKrbIntegrationSuite", "org.apache.spark.sql.jdbc.v2.PostgresNamespaceSuite", "org.apache.spark.sql.jdbc.MsSqlServerIntegrationSuite", "org.apache.spark.sql.jdbc.OracleIntegrationSuite", "org.apache.spark.sql.jdbc.v2.MySQLIntegrationSuite", "org.apache.spark.sql.jdbc.PostgresKrbIntegrationSuite", "org.apache.spark.sql.jdbc.PostgresIntegrationSuite", "org.apache.spark.sql.jdbc.DB2KrbIntegrationSuite", "org.apache.spark.sql.jdbc.v2.PostgresIntegrationSuite", "org.apache.spark.sql.jdbc.MySQLIntegrationSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [3, 6, 23]}, {"file": {"name": "dev/run-tests.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 1, 6]}, {"file": {"name": "pom.xml", "additions": "10", "deletions": "1", "changes": "11"}, "updated": [4, 10, 35]}, {"file": {"name": "resource-managers/yarn/pom.xml", "additions": "32", "deletions": "0", "changes": "32"}, "updated": [0, 1, 1]}]}
{"author": "SaurabhChawla100", "sha": "a09e37faec1be04a20785846f9a2b53c8fdfd663", "commit_date": "2021/08/09 05:49:33", "commit_message": "update the comment", "title": "[SPARK-36452][SQL]: Add the support in Spark for having group by map datatype column for the scenario that works in Hive", "body": "### What changes were proposed in this pull request?\r\nAdd the support in Spark for having group by map datatype column for the scenario that works in Hive.\r\nIn hive this scenario works fine \r\n\r\n```\r\ndescribe extended complex2;\r\nOK\r\nid                  string \r\nc1                  map<int, string>   \r\nDetailed Table Information Table(tableName:complex2, dbName:default, owner:abc, createTime:1627994412, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:string, comment:null), FieldSchema(name:c1, type:map<int,string>, comment:null)], location:/user/hive/warehouse/complex2, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1\r\n\r\nselect * from complex2;\r\nOK\r\n1 {1:\"u\"}\r\n2 {1:\"u\",2:\"uo\"}\r\n1 {1:\"u\",2:\"uo\"}\r\nTime taken: 0.363 seconds, Fetched: 3 row(s)\r\n\r\nWorking Scenario in Hive -: \r\n\r\nselect id, c1, count(*) from complex2 group by id, c1;\r\nOK\r\n1 {1:\"u\"} 1\r\n1 {1:\"u\",2:\"uo\"} 1\r\n2 {1:\"u\",2:\"uo\"} 1\r\nTime taken: 1.621 seconds, Fetched: 3 row(s)\r\n\r\nFailed Scenario in Hive -: \r\nWhen map type is present in aggregated expression \r\nselect id, max(c1), count(*) from complex2 group by id, c1; \r\nFAILED: UDFArgumentTypeException Cannot support comparison of map<> type or complex type containing map<>.\r\n```\r\nBut in spark where the group by map column failed for this scenario where the map column is used in the select without any aggregation, The one that works in hive.\r\n\r\n```\r\nscala> spark.sql(\"select id,c1, count(*) from complex2 group by id, c1\").show\r\norg.apache.spark.sql.AnalysisException: expression spark_catalog.default.complex2.`c1` cannot be used as a grouping expression because its data type map<int,string> is not an orderable data type.;\r\nAggregate [id#1, c1#2], [id#1, c1#2, count(1) AS count(1)#3L]\r\n+- SubqueryAlias spark_catalog.default.complex2\r\n +- HiveTableRelation [`default`.`complex2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#1, c1#2], Partition Cols: []]\r\nat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.failAnalysis(CheckAnalysis.scala:50)\r\n```\r\n\r\n### Why are the changes needed?\r\nThere is need to add the this scenario where grouping expression can have map type if aggregated expression does not have the that map type reference. This helps in migrating the user from hive to Spark.\r\n\r\nAfter the code change \r\n\r\n```\r\nscala> spark.sql(\"select id,c1, count(*) from complex2 group by id, c1\").show\r\n+---+-----------------+--------+                                                \r\n| id|               c1|count(1)|\r\n+---+-----------------+--------+\r\n|  1|         {1 -> u}|       1|\r\n|  2|{1 -> u, 2 -> uo}|       1|\r\n|  1|{1 -> u, 2 -> uo}|       1|\r\n+---+-----------------+--------+\r\n```\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n\r\n### How was this patch tested?\r\nAdded the unit test and also tested using spark-shell the scenario\r\n", "failed_tests": ["org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ordering.scala", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeFloatingNumbers.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "15", "deletions": "5", "changes": "20"}, "updated": [1, 1, 6]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala", "additions": "17", "deletions": "8", "changes": "25"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameAggregateSuite.scala", "additions": "27", "deletions": "0", "changes": "27"}, "updated": [0, 0, 5]}]}
{"author": "robert3005", "sha": "e3754487a8fc49622059a47e403945f2850731f2", "commit_date": "2017/06/22 15:53:30", "commit_message": "Merge existing registry with default one or configure default metric registry", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala", "additions": "66", "deletions": "19", "changes": "85"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/metrics/MetricsSystemSuite.scala", "additions": "30", "deletions": "9", "changes": "39"}, "updated": [0, 0, 0]}, {"file": {"name": "streaming/src/test/scala/org/apache/spark/streaming/StreamingContextSuite.scala", "additions": "6", "deletions": "3", "changes": "9"}, "updated": [0, 0, 0]}]}
{"author": "xwu99", "sha": "67034f284803bd10a487b5c67eb4c552ace950c3", "commit_date": "2021/09/01 13:11:10", "commit_message": "Add some utilities for profiles", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala", "additions": "72", "deletions": "12", "changes": "84"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 1, 6]}, {"file": {"name": "core/src/main/scala/org/apache/spark/resource/ResourceProfile.scala", "additions": "13", "deletions": "0", "changes": "13"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/resource/ResourceProfileManager.scala", "additions": "55", "deletions": "0", "changes": "55"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala", "additions": "14", "deletions": "3", "changes": "17"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/test/resources/log4j.properties", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/ExecutorAllocationManagerSuite.scala", "additions": "115", "deletions": "0", "changes": "115"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/TaskSchedulerImplSuite.scala", "additions": "83", "deletions": "0", "changes": "83"}, "updated": [0, 0, 0]}]}
{"author": "xkrogen", "sha": "9b58975f88eaad623febea4524b3e7a63dd99272", "commit_date": "2021/09/15 21:57:44", "commit_message": "SPARK-34378 [AVRO] Enhance AvroSerializer validation to allow extra nullable Avro fields", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.avro.AvroSerdeSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/TestUtils.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 1]}, {"file": {"name": "external/avro/src/main/scala/org/apache/spark/sql/avro/AvroSerializer.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 3]}, {"file": {"name": "external/avro/src/main/scala/org/apache/spark/sql/avro/AvroUtils.scala", "additions": "10", "deletions": "3", "changes": "13"}, "updated": [0, 0, 3]}, {"file": {"name": "external/avro/src/test/scala/org/apache/spark/sql/avro/AvroSchemaHelperSuite.scala", "additions": "24", "deletions": "1", "changes": "25"}, "updated": [0, 0, 1]}, {"file": {"name": "external/avro/src/test/scala/org/apache/spark/sql/avro/AvroSerdeSuite.scala", "additions": "26", "deletions": "15", "changes": "41"}, "updated": [0, 0, 1]}, {"file": {"name": "external/avro/src/test/scala/org/apache/spark/sql/avro/AvroSuite.scala", "additions": "26", "deletions": "0", "changes": "26"}, "updated": [0, 0, 3]}]}
{"author": "andygrove", "sha": "095a4d57b7a348d6e1dba38a6db37d8667d0482d", "commit_date": "2021/08/03 20:32:30", "commit_message": "backport aqe proposal", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite", "org.apache.spark.sql.execution.CoalesceShufflePartitionsSuite", "org.apache.spark.sql.ExplainSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/SparkSessionExtensions.scala", "additions": "21", "deletions": "1", "changes": "22"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/Columnar.scala", "additions": "11", "deletions": "6", "changes": "17"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "35", "deletions": "36", "changes": "71"}, "updated": [1, 3, 9]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 1, 2]}]}
{"author": "darcy-shen", "sha": "54553fd062798c008254ddebe3987ef983b4e2ad", "commit_date": "2021/04/25 08:13:23", "commit_message": "verify inferred schema for _create_dataframe", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/sql/session.py", "additions": "36", "deletions": "8", "changes": "44"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/tests/test_dataframe.py", "additions": "41", "deletions": "0", "changes": "41"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/tests/test_types.py", "additions": "10", "deletions": "1", "changes": "11"}, "updated": [0, 0, 0]}]}
{"author": "grarkydev", "sha": "7ebf7088f6ff608da334468137ad8336f5b0f28f", "commit_date": "2021/04/06 11:44:03", "commit_message": "Support spark application managing with spark app handle on kubernetes\n\nCo-authored-by: hongdd <hongdongdong@cmss.chinamobile.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/submit/KubernetesClientApplication.scala", "additions": "40", "deletions": "3", "changes": "43"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/submit/PodStatusWatcher.scala", "additions": "34", "deletions": "5", "changes": "39"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/submit/ClientSuite.scala", "additions": "21", "deletions": "8", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/submit/PodStatusWatcherSuite.scala", "additions": "85", "deletions": "0", "changes": "85"}, "updated": [0, 0, 0]}]}
{"author": "AngersZhuuuu", "sha": "f5ec814e4d399e2dc0cea637ff5b2e99382a3c04", "commit_date": "2021/07/23 07:14:45", "commit_message": "[SPARK-34851][CORE][SQL] Add tag for each configuration", "title": "[WIP][SPARK-34851][CORE][SQL] Add tag for each configuration", "body": "### What changes were proposed in this pull request?\r\nAdd tag for each configuration\r\n\r\n\r\n### Why are the changes needed?\r\nAdd tag for each configuration\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\n\r\n", "failed_tests": ["org.apache.spark.sql.jdbc.OracleIntegrationSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/ConfigBuilder.scala", "additions": "12", "deletions": "0", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/Deploy.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/Kryo.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/Python.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/R.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/Streaming.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/Tests.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/UI.scala", "additions": "27", "deletions": "0", "changes": "27"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/Worker.scala", "additions": "12", "deletions": "0", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "305", "deletions": "6", "changes": "311"}, "updated": [0, 2, 5]}, {"file": {"name": "launcher/src/main/java/org/apache/spark/launcher/SparkLauncher.java", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/config.scala", "additions": "54", "deletions": "1", "changes": "55"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [2, 6, 18]}]}
{"author": "AngersZhuuuu", "sha": "a0fc9b99b77f31457a372a8b6496ce430417641b", "commit_date": "2021/04/27 10:06:54", "commit_message": "[SPARK-35228][SQL] Add expression ToHiveString for keep consistent between hive/spark format in df.show and transform", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala", "additions": "406", "deletions": "358", "changes": "764"}, "updated": [0, 1, 11]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala", "additions": "54", "deletions": "0", "changes": "54"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "12", "deletions": "0", "changes": "12"}, "updated": [1, 3, 13]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala", "additions": "41", "deletions": "0", "changes": "41"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala", "additions": "7", "deletions": "1", "changes": "8"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/BaseScriptTransformationExec.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 2, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/HiveResult.scala", "additions": "20", "deletions": "9", "changes": "29"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/transform.sql.out", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [1, 4, 6]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SQLQueryTestHelper.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/BaseScriptTransformationSuite.scala", "additions": "10", "deletions": "21", "changes": "31"}, "updated": [1, 3, 4]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/HiveResultSuite.scala", "additions": "15", "deletions": "26", "changes": "41"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/benchmark/DateTimeBenchmark.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/expressions/ExpressionInfoSuite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkSQLDriver.scala", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/HiveComparisonTest.scala", "additions": "5", "deletions": "3", "changes": "8"}, "updated": [0, 0, 0]}]}
{"author": "gengliangwang", "sha": "bc94b027e839859d587271c3bcf78e0f632a5234", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "[WIP][SPARK-36182][SQL] Support TimestampNTZ type in Parquet file source", "body": "This is still WIP. I am deciding the behaviors of the Parquet reader for both schema inference and user-provided schema.", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}
{"author": "dgd-contributor", "sha": "838f41303b58701ced288152bb95f8938ebacc66", "commit_date": "2021/07/28 15:38:09", "commit_message": "[SPARK-36096][CORE] Grouping exception in core/resource", "title": "", "body": "", "failed_tests": ["org.apache.spark.scheduler.ExecutorResourceInfoSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/errors/SparkCoreErrors.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [1, 1, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/resource/ResourceProfile.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/resource/ResourceUtils.scala", "additions": "5", "deletions": "4", "changes": "9"}, "updated": [0, 0, 0]}]}
{"author": "dgd-contributor", "sha": "a89af71e932f932e05a22caca449cd9b4872d865", "commit_date": "2021/07/29 01:46:33", "commit_message": "[SPARK-36100][CORE]: group exception messages in core/status", "title": "", "body": "", "failed_tests": ["org.apache.spark.deploy.history.HistoryServerSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/errors/SparkCoreErrors.scala", "additions": "103", "deletions": "0", "changes": "103"}, "updated": [1, 1, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusStore.scala", "additions": "4", "deletions": "5", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/KVUtils.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/LiveEntity.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/api/v1/ApiRootResource.scala", "additions": "9", "deletions": "8", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/api/v1/OneApplicationResource.scala", "additions": "12", "deletions": "12", "changes": "24"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/api/v1/SimpleDateParam.scala", "additions": "3", "deletions": "9", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/api/v1/StagesResource.scala", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [0, 0, 0]}]}
{"author": "shardulm94", "sha": "f39e7c997562c67b4fa9a374e993744512ad3b84", "commit_date": "2021/07/22 21:07:10", "commit_message": "Fix compile error after revert", "title": "[SPARK-36215][SHUFFLE] Add logging for slow fetches to diagnose external shuffle service issues", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\nAdd logging to `ShuffleBlockFetcherIterator` to log \"slow\" fetches, where slow is defined by two confs: `spark.reducer.shuffleFetchSlowLogThreshold.time` and `spark.reducer.shuffleFetchSlowLogThreshold.bytesPerSec`\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\n\r\n### Why are the changes needed?\r\nCurrently we can see from the metrics that a task or stage has slow fetches, and the logs indicate *all* of the shuffle servers those tasks were fetching from, but often this is a big set (dozens or even hundreds) and narrowing down which one caused issues can be very difficult. This change makes it easier to understand which fetch is \"slow\".\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nAdds two configs `spark.reducer.shuffleFetchSlowLogThreshold.time` and `spark.reducer.shuffleFetchSlowLogThreshold.bytesPerSec`\r\n\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nAdded unit test", "failed_tests": ["pyspark.mllib.tests.test_streaming_algorithms"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/TestUtils.scala", "additions": "32", "deletions": "1", "changes": "33"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [1, 2, 5]}, {"file": {"name": "core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala", "additions": "43", "deletions": "5", "changes": "48"}, "updated": [0, 1, 4]}, {"file": {"name": "core/src/test/scala/org/apache/spark/storage/ShuffleBlockFetcherIteratorSuite.scala", "additions": "47", "deletions": "3", "changes": "50"}, "updated": [0, 0, 2]}, {"file": {"name": "docs/configuration.md", "additions": "22", "deletions": "0", "changes": "22"}, "updated": [0, 0, 1]}]}
{"author": "sunchao", "sha": "4bf4533b8c7e9c7b2f264ede6a14cad52c33b194", "commit_date": "2021/06/30 20:22:02", "commit_message": "initial commit", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [3, 6, 23]}, {"file": {"name": "dev/run-tests.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 1, 6]}, {"file": {"name": "pom.xml", "additions": "10", "deletions": "1", "changes": "11"}, "updated": [4, 10, 35]}, {"file": {"name": "resource-managers/yarn/pom.xml", "additions": "32", "deletions": "0", "changes": "32"}, "updated": [0, 1, 1]}]}
{"author": "viirya", "sha": "ba4172076f3f8030510632978a0e47d5b720617a", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}
{"author": "Yikun", "sha": "c6d4f21ca368bbc7ba4236dcd9d09904e7b82e5b", "commit_date": "2021/06/30 15:03:00", "commit_message": "Path level discover in python/run-test.py", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": ".github/workflows/build_and_test.yml", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 1, 26]}, {"file": {"name": "dev/run-tests.py", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [1, 1, 6]}, {"file": {"name": "dev/sparktestsupport/modules.py", "additions": "39", "deletions": "129", "changes": "168"}, "updated": [4, 6, 16]}, {"file": {"name": "python/pyspark/pandas/tests/indexes/test_base.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 7]}, {"file": {"name": "python/pyspark/pandas/tests/indexes/test_datetime.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 10]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 5]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 3]}, {"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 4]}, {"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames_groupby.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 4]}, {"file": {"name": "python/pyspark/pandas/tests/test_series.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [3, 3, 7]}, {"file": {"name": "python/pyspark/pandas/tests/test_stats.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 5]}, {"file": {"name": "python/pyspark/testing/utils.py", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 1]}, {"file": {"name": "python/run-tests.py", "additions": "119", "deletions": "2", "changes": "121"}, "updated": [0, 0, 1]}]}
{"author": "Yikun", "sha": "e46233af0bbd8f125a0b31f045628e239d0c8382", "commit_date": "2021/06/30 15:03:00", "commit_message": "Path level discover in python/run-test.py", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": ".github/workflows/build_and_test.yml", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 1, 26]}, {"file": {"name": "dev/run-tests.py", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [1, 1, 6]}, {"file": {"name": "dev/sparktestsupport/modules.py", "additions": "39", "deletions": "129", "changes": "168"}, "updated": [4, 6, 16]}, {"file": {"name": "python/pyspark/pandas/tests/indexes/test_base.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 7]}, {"file": {"name": "python/pyspark/pandas/tests/indexes/test_datetime.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 10]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 5]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 3]}, {"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 4]}, {"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames_groupby.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 4]}, {"file": {"name": "python/pyspark/pandas/tests/test_series.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [3, 3, 7]}, {"file": {"name": "python/pyspark/pandas/tests/test_stats.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 5]}, {"file": {"name": "python/pyspark/testing/utils.py", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 1]}, {"file": {"name": "python/run-tests.py", "additions": "119", "deletions": "2", "changes": "121"}, "updated": [0, 0, 1]}]}
{"author": "Yikun", "sha": "0915bf6bb053a7fe53c5eb7ba52a81fc26957c8b", "commit_date": "2021/06/30 15:03:00", "commit_message": "Path level discover in python/run-test.py", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": ".github/workflows/build_and_test.yml", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 1, 26]}, {"file": {"name": "dev/run-tests.py", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [1, 1, 6]}, {"file": {"name": "dev/sparktestsupport/modules.py", "additions": "40", "deletions": "130", "changes": "170"}, "updated": [4, 6, 16]}, {"file": {"name": "python/pyspark/pandas/tests/indexes/test_base.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 7]}, {"file": {"name": "python/pyspark/pandas/tests/indexes/test_datetime.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 10]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 5]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 3]}, {"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 4]}, {"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames_groupby.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 4]}, {"file": {"name": "python/pyspark/pandas/tests/test_series.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [3, 3, 7]}, {"file": {"name": "python/pyspark/pandas/tests/test_stats.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 5]}, {"file": {"name": "python/pyspark/testing/utils.py", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 1]}, {"file": {"name": "python/run-tests.py", "additions": "118", "deletions": "2", "changes": "120"}, "updated": [0, 0, 1]}]}
{"author": "Yikun", "sha": "05ff042611c27270e9b059e4fd0483c7194672c1", "commit_date": "2021/06/29 08:56:13", "commit_message": "[SPARK-35721][PYTHON] Path level discover for python unittests\n\n### What changes were proposed in this pull request?\nAdd path level discover for python unittests.\n\n### Why are the changes needed?\nNow we need to specify the python test cases by manually when we add a new testcase. Sometime, we forgot to add the testcase to module list, the testcase would not be executed.\n\nSuch as:\n- pyspark-core pyspark.tests.test_pin_thread\n\nThus we need some auto-discover way to find all testcase rather than specified every case by manually.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nAdd below code in end of `dev/sparktestsupport/modules.py`\n```python\nfor m in sorted(all_modules):\n    for g in sorted(m.python_test_goals):\n        print(m.name, g)\n```\nCompare the result before and after:\nhttps://www.diffchecker.com/iO3FvhKL\n\nCloses #32867 from Yikun/SPARK_DISCOVER_TEST.\n\nAuthored-by: Yikun Jiang <yikunkero@gmail.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": ".github/workflows/build_and_test.yml", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 25]}, {"file": {"name": "dev/run-tests.py", "additions": "9", "deletions": "1", "changes": "10"}, "updated": [1, 1, 6]}, {"file": {"name": "dev/sparktestsupport/modules.py", "additions": "86", "deletions": "140", "changes": "226"}, "updated": [3, 5, 15]}, {"file": {"name": "python/pyspark/pandas/tests/indexes/test_base.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 1, 6]}, {"file": {"name": "python/pyspark/pandas/tests/indexes/test_datetime.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 1, 5]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 1, 9]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 1, 4]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 1, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 1, 3]}, {"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames_groupby.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 1, 3]}, {"file": {"name": "python/pyspark/pandas/tests/test_series.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_stats.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 1, 4]}, {"file": {"name": "python/pyspark/testing/utils.py", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 1]}]}
{"author": "Yikun", "sha": "8e0065acce17a938482bb102afb6d99ef8d65a8a", "commit_date": "2021/06/29 11:15:11", "commit_message": "Add missing test modules check", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "dev/sparktestsupport/modules.py", "additions": "17", "deletions": "1", "changes": "18"}, "updated": [3, 5, 15]}]}
{"author": "Yikun", "sha": "33a9eaf0fd7f01f56a1f795a283a782053d4ac63", "commit_date": "2021/06/29 05:27:00", "commit_message": "discover-assert", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "dev/sparktestsupport/modules.py", "additions": "16", "deletions": "1", "changes": "17"}, "updated": [1, 3, 13]}]}
{"author": "ulysses-you", "sha": "5beb51810dfec69964e570d90d0634e5a8e0499d", "commit_date": "2021/07/28 03:14:13", "commit_message": "Do not fail application in kubernetes if name is too long", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPi.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPiWithoutAppName.scala", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "4", "deletions": "14", "changes": "18"}, "updated": [0, 3, 4]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala", "additions": "19", "deletions": "7", "changes": "26"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesUtils.scala", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala", "additions": "10", "deletions": "6", "changes": "16"}, "updated": [0, 2, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterManager.scala", "additions": "10", "deletions": "2", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/KubernetesConfSuite.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/BasicTestsSuite.scala", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "ulysses-you", "sha": "a846ecd5221bc4b21416c9c52552cdaa0e683d0d", "commit_date": "2021/04/07 10:02:05", "commit_message": "Support coalesce partition through union", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "78", "deletions": "52", "changes": "130"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/CoalesceShufflePartitionsSuite.scala", "additions": "1", "deletions": "3", "changes": "4"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "86", "deletions": "1", "changes": "87"}, "updated": [0, 1, 5]}]}
{"author": "venkata91", "sha": "ac1659e156eca5899e1eff765698c9986eec5d4c", "commit_date": "2020/06/10 21:02:55", "commit_message": "Basic speculation metrics summary for a stage", "title": "", "body": "", "failed_tests": ["pyspark.mllib.tests.test_streaming_algorithms"], "files": [{"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/stagepage.js", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/stagespage-template.html", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/webui.css", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusListener.scala", "additions": "19", "deletions": "0", "changes": "19"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusStore.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [1, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/LiveEntity.scala", "additions": "26", "deletions": "0", "changes": "26"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/api/v1/api.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/storeTypes.scala", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/application_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/completed_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/limit_app_list_json_expectation.json", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/minDate_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/minEndDate_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/stage_with_speculation_summary_expectation.json", "additions": "507", "deletions": "0", "changes": "507"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/spark-events/application_1628109047826_1317105", "additions": "52", "deletions": "0", "changes": "52"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/deploy/history/HistoryServerSuite.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 1, 4]}, {"file": {"name": "core/src/test/scala/org/apache/spark/status/AppStatusListenerSuite.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/status/AppStatusStoreSuite.scala", "additions": "56", "deletions": "1", "changes": "57"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/ui/StagePageSuite.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "dev/.rat-excludes", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 2]}]}
{"author": "dominikgehl", "sha": "5fc4307084a3cfe321d7e6fdb2e576bce8dab427", "commit_date": "2021/07/22 14:04:36", "commit_message": "expose localtimestamp in pyspark.sql.functions", "title": "[SPARK-36259] Expose localtimestamp in pyspark.sql.functions", "body": "\r\n\r\n### What changes were proposed in this pull request?\r\nExposing localtimestamp in pyspark.sql.functions\r\n\r\n\r\n### Why are the changes needed?\r\nWas previously only available in scala\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nnew localtimestamp in pyspark.sql.functions\r\n\r\n\r\n### How was this patch tested?\r\ntest added inline\r\n", "failed_tests": [], "files": [{"file": {"name": "python/docs/source/reference/pyspark.sql.rst", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [2, 3, 4]}, {"file": {"name": "python/pyspark/sql/functions.py", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 4, 7]}, {"file": {"name": "python/pyspark/sql/functions.pyi", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_types.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/sql/types.py", "additions": "22", "deletions": "3", "changes": "25"}, "updated": [0, 0, 1]}]}
{"author": "wangyum", "sha": "65df34649227d1f04065cc76a49778770b433536", "commit_date": "2021/07/21 14:51:57", "commit_message": "Deduplicate the right side of left semi/anti join", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.jdbc.OracleIntegrationSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/DeduplicateRightSideOfLeftSemiAntiJoin.scala", "additions": "56", "deletions": "0", "changes": "56"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [2, 2, 13]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/Statistics.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/DeduplicateRightSideOfLeftSemiAntiJoinSuite.scala", "additions": "98", "deletions": "0", "changes": "98"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AQEOptimizer.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/QueryStageExec.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q10.sf100/explain.txt", "additions": "119", "deletions": "103", "changes": "222"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q10.sf100/simplified.txt", "additions": "63", "deletions": "63", "changes": "126"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q10.sf100/explain.txt", "additions": "220", "deletions": "204", "changes": "424"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q10.sf100/simplified.txt", "additions": "48", "deletions": "48", "changes": "96"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a.sf100/explain.txt", "additions": "614", "deletions": "646", "changes": "1260"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a.sf100/simplified.txt", "additions": "143", "deletions": "185", "changes": "328"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b.sf100/explain.txt", "additions": "559", "deletions": "576", "changes": "1135"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b.sf100/simplified.txt", "additions": "151", "deletions": "184", "changes": "335"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q35.sf100/explain.txt", "additions": "230", "deletions": "214", "changes": "444"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q35.sf100/simplified.txt", "additions": "51", "deletions": "51", "changes": "102"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q69.sf100/explain.txt", "additions": "221", "deletions": "183", "changes": "404"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q69.sf100/simplified.txt", "additions": "56", "deletions": "60", "changes": "116"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q95.sf100/explain.txt", "additions": "134", "deletions": "92", "changes": "226"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q95.sf100/simplified.txt", "additions": "42", "deletions": "36", "changes": "78"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q10a.sf100/explain.txt", "additions": "199", "deletions": "183", "changes": "382"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q10a.sf100/simplified.txt", "additions": "42", "deletions": "42", "changes": "84"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14.sf100/explain.txt", "additions": "559", "deletions": "576", "changes": "1135"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14.sf100/simplified.txt", "additions": "151", "deletions": "184", "changes": "335"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a.sf100/explain.txt", "additions": "742", "deletions": "774", "changes": "1516"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a.sf100/simplified.txt", "additions": "158", "deletions": "200", "changes": "358"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35.sf100/explain.txt", "additions": "230", "deletions": "214", "changes": "444"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35.sf100/simplified.txt", "additions": "51", "deletions": "51", "changes": "102"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35a.sf100/explain.txt", "additions": "219", "deletions": "203", "changes": "422"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35a.sf100/simplified.txt", "additions": "49", "deletions": "49", "changes": "98"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [1, 5, 17]}]}
{"author": "wangyum", "sha": "eb71b8ae8de1fb737eea170e920c24127bcc2b95", "commit_date": "2021/07/17 15:56:31", "commit_message": "Remove the aggregation from left semi/anti join if the same aggregation has already been done on left side", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RemoveRedundantAggregates.scala", "additions": "6", "deletions": "2", "changes": "8"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/DistinctAttributesVisitor.scala", "additions": "100", "deletions": "0", "changes": "100"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/LogicalPlan.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/LogicalPlanDistinctAttributes.scala", "additions": "33", "deletions": "0", "changes": "33"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/RemoveRedundantAggregatesSuite.scala", "additions": "110", "deletions": "11", "changes": "121"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/plans/logical/DistinctAttributesVisitorSuite.scala", "additions": "112", "deletions": "0", "changes": "112"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a.sf100/explain.txt", "additions": "460", "deletions": "481", "changes": "941"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a.sf100/simplified.txt", "additions": "113", "deletions": "118", "changes": "231"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a/explain.txt", "additions": "229", "deletions": "245", "changes": "474"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a/simplified.txt", "additions": "62", "deletions": "64", "changes": "126"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b.sf100/explain.txt", "additions": "393", "deletions": "414", "changes": "807"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b.sf100/simplified.txt", "additions": "106", "deletions": "111", "changes": "217"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b/explain.txt", "additions": "213", "deletions": "229", "changes": "442"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b/simplified.txt", "additions": "62", "deletions": "64", "changes": "126"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q38.sf100/explain.txt", "additions": "143", "deletions": "190", "changes": "333"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q38.sf100/simplified.txt", "additions": "112", "deletions": "125", "changes": "237"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q38/explain.txt", "additions": "79", "deletions": "106", "changes": "185"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q38/simplified.txt", "additions": "60", "deletions": "63", "changes": "123"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q87.sf100/explain.txt", "additions": "143", "deletions": "190", "changes": "333"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q87.sf100/simplified.txt", "additions": "112", "deletions": "125", "changes": "237"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q87/explain.txt", "additions": "79", "deletions": "106", "changes": "185"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q87/simplified.txt", "additions": "60", "deletions": "63", "changes": "123"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14.sf100/explain.txt", "additions": "393", "deletions": "414", "changes": "807"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14.sf100/simplified.txt", "additions": "106", "deletions": "111", "changes": "217"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14/explain.txt", "additions": "213", "deletions": "229", "changes": "442"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14/simplified.txt", "additions": "62", "deletions": "64", "changes": "126"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a.sf100/explain.txt", "additions": "602", "deletions": "623", "changes": "1225"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a.sf100/simplified.txt", "additions": "128", "deletions": "133", "changes": "261"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a/explain.txt", "additions": "279", "deletions": "295", "changes": "574"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a/simplified.txt", "additions": "62", "deletions": "64", "changes": "126"}, "updated": [0, 0, 3]}]}
{"author": "wangyum", "sha": "6339da03b69ac2d0dad37b6aee5d356a84dd92e2", "commit_date": "2021/07/15 08:26:12", "commit_message": "Eliminate join base uniqueness", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.optimizer.AggregateOptimizeSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala", "additions": "21", "deletions": "2", "changes": "23"}, "updated": [0, 1, 7]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/AggregateOptimizeSuite.scala", "additions": "38", "deletions": "9", "changes": "47"}, "updated": [0, 0, 2]}]}
{"author": "wangyum", "sha": "824ba80a0a71bb8c5079c45ec5de6bc9ae198699", "commit_date": "2021/07/10 12:43:25", "commit_message": "First commit", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.JoinSuite", "org.apache.spark.sql.streaming.StreamingOuterJoinSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.streaming.StreamingLeftSemiJoinSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/BroadcastJoinOuterJoinStreamSide.scala", "additions": "48", "deletions": "0", "changes": "48"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 7]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 2, 9]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/BroadcastJoinOuterJoinStreamSideSuite.scala", "additions": "108", "deletions": "0", "changes": "108"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 1, 6]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a.sf100/explain.txt", "additions": "634", "deletions": "533", "changes": "1167"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a.sf100/simplified.txt", "additions": "97", "deletions": "70", "changes": "167"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b.sf100/explain.txt", "additions": "569", "deletions": "468", "changes": "1037"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b.sf100/simplified.txt", "additions": "90", "deletions": "63", "changes": "153"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23a/explain.txt", "additions": "275", "deletions": "245", "changes": "520"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23a/simplified.txt", "additions": "75", "deletions": "67", "changes": "142"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23b/explain.txt", "additions": "364", "deletions": "319", "changes": "683"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23b/simplified.txt", "additions": "93", "deletions": "81", "changes": "174"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q40/explain.txt", "additions": "123", "deletions": "108", "changes": "231"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q40/simplified.txt", "additions": "17", "deletions": "13", "changes": "30"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q72/explain.txt", "additions": "65", "deletions": "50", "changes": "115"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q72/simplified.txt", "additions": "13", "deletions": "9", "changes": "22"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q80/explain.txt", "additions": "462", "deletions": "417", "changes": "879"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q80/simplified.txt", "additions": "65", "deletions": "53", "changes": "118"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14.sf100/explain.txt", "additions": "569", "deletions": "468", "changes": "1037"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14.sf100/simplified.txt", "additions": "90", "deletions": "63", "changes": "153"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a.sf100/explain.txt", "additions": "778", "deletions": "677", "changes": "1455"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a.sf100/simplified.txt", "additions": "114", "deletions": "87", "changes": "201"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q72/explain.txt", "additions": "65", "deletions": "50", "changes": "115"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q72/simplified.txt", "additions": "13", "deletions": "9", "changes": "22"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q75/explain.txt", "additions": "614", "deletions": "455", "changes": "1069"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q75/simplified.txt", "additions": "108", "deletions": "66", "changes": "174"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q78/explain.txt", "additions": "320", "deletions": "275", "changes": "595"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q78/simplified.txt", "additions": "51", "deletions": "39", "changes": "90"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q80a/explain.txt", "additions": "532", "deletions": "487", "changes": "1019"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q80a/simplified.txt", "additions": "73", "deletions": "61", "changes": "134"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala", "additions": "21", "deletions": "1", "changes": "22"}, "updated": [0, 3, 8]}]}
{"author": "wangyum", "sha": "f2d4d3ae0dd405f4bf62813bd849678f459776d6", "commit_date": "2020/03/26 04:56:44", "commit_message": "Repartition by dynamic partition columns before insert table", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala", "additions": "5", "deletions": "3", "changes": "8"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "13", "deletions": "0", "changes": "13"}, "updated": [1, 8, 47]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala", "additions": "101", "deletions": "1", "changes": "102"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/sources/RepartitionWritingDataSourceSuite.scala", "additions": "230", "deletions": "0", "changes": "230"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveSessionStateBuilder.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 4]}]}
{"author": "beliefer", "sha": "86ea00d27e9979d6057a39ac7c0711131a6b6c26", "commit_date": "2021/07/27 07:37:12", "commit_message": "Refactor first set of 20 query parsing errors to use error classes", "title": "", "body": "", "failed_tests": ["org.apache.spark.SparkThrowableSuite", "org.apache.spark.sql.execution.datasources.PruneFileSourcePartitionsSuite"], "files": [{"file": {"name": "core/src/main/resources/error/error-classes.json", "additions": "52", "deletions": "0", "changes": "52"}, "updated": [0, 2, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala", "additions": "0", "deletions": "2", "changes": "2"}, "updated": [0, 2, 22]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala", "additions": "22", "deletions": "29", "changes": "51"}, "updated": [0, 2, 7]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisTest.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala", "additions": "8", "deletions": "3", "changes": "11"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/ErrorParserSuite.scala", "additions": "26", "deletions": "9", "changes": "35"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala", "additions": "15", "deletions": "8", "changes": "23"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/SparkScriptTransformationSuite.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/ShowPartitionsParserSuite.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/TruncateTableParserSuite.scala", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [0, 0, 0]}]}
{"author": "beliefer", "sha": "b4f298e9a97ad4d3afd1b099cba1887efbcf734d", "commit_date": "2021/03/16 03:25:10", "commit_message": "Support the utils for transform number format", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/NumberUtils.scala", "additions": "190", "deletions": "0", "changes": "190"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/NumberUtilsSuite.scala", "additions": "273", "deletions": "0", "changes": "273"}, "updated": [0, 0, 0]}]}
{"author": "SaurabhChawla100", "sha": "4fab369b87791935e2c8739e69766ddc4a0c31d9", "commit_date": "2021/07/06 15:17:24", "commit_message": "refactor the code", "title": "[SPARK-36027][SQL] Add the code change to pushdown filter in case of typedFilter", "body": "### What changes were proposed in this pull request?\r\nIn case of Filter having child as TypedFilter, Pushdown of Filters does not take place\r\n\r\nscala> def testUdfFunction(r: String): Boolean = {\r\n | r.equals(\"hello\")\r\n | }\r\ntestUdfFunction: (r: String)Boolean\r\n\r\nval df= spark.read.parquet(\"/testDir/testParquetSize/Parquetgzip/\")\r\ndf: org.apache.spark.sql.DataFrame = [_1: string, _2: string ... 1 more field]\r\n\r\nBefore Fix \r\n```\r\ndf.filter(x => testUdfFunction(x.getAs(\"_1\"))).filter(\"_2<='id103855'\").queryExecution.executedPlan\r\n \r\nFilter (isnotnull(_2#1) AND (_2#1 <= id103855))\r\n+- *(1) Filter $line20.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$Lambda$3184/1455948476@5ce4af92.apply\r\n +- *(1) ColumnarToRow\r\n +- FileScan parquet [_1#0,_2#1,_3#2] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/testDir/testParquetSize/Parquetgzip], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<_1:string,_2:string,_3:string>\r\n \r\ndf.filter(x => testUdfFunction(x.getAs(\"_1\"))).filter(\"_2<='id103855'\").queryExecution.optimizedPlan\r\n\r\nFilter (isnotnull(_2#1) AND (_2#1 <= id103855))\r\n+- TypedFilter $line22.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$Lambda$3191/320569017@37a2806c, interface org.apache.spark.sql.Row, [StructField(_1,StringType,true), StructField(_2,StringType,true), StructField(_3,StringType,true)], createexternalrow(_1#0.toString, _2#1.toString, _3#2.toString, StructField(_1,StringType,true), StructField(_2,StringType,true), StructField(_3,StringType,true))\r\n +- Relation[_1#0,_2#1,_3#2] parquet\r\n\r\n```\r\n\r\nAfter fix\r\n```\r\ndf.filter(x => testUdfFunction(x.getAs(\"_1\"))).filter(\"_2<='id103855'\").queryExecution.executedPlan\r\n\r\n*(1) Filter $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$Lambda$2966/426572525@4155e85d.apply\r\n+- *(1) Filter (isnotnull(_2#1) AND (_2#1 <= id103855))\r\n   +- *(1) ColumnarToRow\r\n      +- FileScan parquet [_1#0,_2#1,_3#2] Batched: true, DataFilters: [isnotnull(_2#1), (_2#1 <= id103855)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/testDir/testParquetSize/Parquetgzip], PartitionFilters: [], PushedFilters: [IsNotNull(_2), LessThanOrEqual(_2,id103855)], ReadSchema: struct<_1:string,_2:string,_3:string>\r\n\r\ndf.filter(x => testUdfFunction(x.getAs(\"_1\"))).filter(\"_2<='id103855'\").queryExecution.optimizedPlan\r\n\r\nTypedFilter $line18.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$Lambda$3445/1423649465@62065a8c, interface org.apache.spark.sql.Row, [StructField(_1,StringType,true), StructField(_2,StringType,true), StructField(_3,StringType,true)], createexternalrow(_1#0.toString, _2#1.toString, _3#2.toString, StructField(_1,StringType,true), StructField(_2,StringType,true), StructField(_3,StringType,true))\r\n+- Filter (isnotnull(_2#1) AND (_2#1 <= id103855))\r\n   +- Relation [_1#0,_2#1,_3#2] parquet\r\n```\r\n\r\n### Why are the changes needed?\r\nTill now when Filter is having child as TypedFilter, the filter is not pushed down, There is a need to add this code change for pushing down the filter.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n\r\n### How was this patch tested?\r\nAdded the unit test and tested on spark-shell", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [1, 5, 13]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/FilterPushdownSuite.scala", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [0, 0, 0]}]}
{"author": "peter-toth", "sha": "7786c0ccb8d7b716955de0a1e59acc822f0cb00c", "commit_date": "2021/07/09 11:57:23", "commit_message": "[SPARK-36073][SQL] SubExpr elimination should include common child exprs of conditional expressions", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala", "additions": "79", "deletions": "39", "changes": "118"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/SubexpressionEliminationSuite.scala", "additions": "28", "deletions": "0", "changes": "28"}, "updated": [0, 3, 7]}]}
{"author": "peter-toth", "sha": "d86d2c48a3e6244fc091ae09cb9377ade98f66b0", "commit_date": "2021/04/21 17:40:19", "commit_message": "[SPARK-34079][SQL] Merging non-correlated scalar subqueries to multi-column scalar subqueries for better reuse", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueries.scala", "additions": "416", "deletions": "0", "changes": "416"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala", "additions": "32", "deletions": "1", "changes": "33"}, "updated": [2, 8, 16]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreePatterns.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [1, 3, 3]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueriesSuite.scala", "additions": "282", "deletions": "0", "changes": "282"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap.java", "additions": "2", "deletions": "7", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "8", "deletions": "3", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/PlanAdaptiveSubqueries.scala", "additions": "18", "deletions": "6", "changes": "24"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/ReuseAdaptiveSubquery.scala", "additions": "5", "deletions": "3", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala", "additions": "2", "deletions": "8", "changes": "10"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "0", "deletions": "9", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "21", "deletions": "0", "changes": "21"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/reuse/ReuseExchangeAndSubquery.scala", "additions": "10", "deletions": "3", "changes": "13"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala", "additions": "18", "deletions": "2", "changes": "20"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9.sf100/explain.txt", "additions": "183", "deletions": "598", "changes": "781"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9.sf100/simplified.txt", "additions": "65", "deletions": "170", "changes": "235"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9/explain.txt", "additions": "183", "deletions": "598", "changes": "781"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9/simplified.txt", "additions": "65", "deletions": "170", "changes": "235"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala", "additions": "72", "deletions": "0", "changes": "72"}, "updated": [1, 3, 10]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala", "additions": "37", "deletions": "0", "changes": "37"}, "updated": [1, 2, 4]}]}
{"author": "Peng-Lei", "sha": "2076413f2caea4ce49116e1e93fcee87d8df9121", "commit_date": "2021/06/29 11:23:43", "commit_message": "Add support YearMonthIntervalType for width_bucket", "title": "[SPARK-35926][SQL] Add support YearMonthIntervalType for width_bucket", "body": "### What changes were proposed in this pull request?\r\n1. The function `width_bucket` is introduced from [SPARK-21117](https://issues.apache.org/jira/browse/SPARK-21117)\r\n2. The YearMonthIntervalType is just store the value with Int.\r\n3. Modify the `inputType` to allow the width_bucket to support the YearMonthIntervalType\r\n\r\n### Why are the changes needed?\r\n[35926](https://issues.apache.org/jira/browse/SPARK-35926)\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes. The user can use `width_bucket` with Period type.\r\n\r\n\r\n### How was this patch tested?\r\nAdd ut test\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/mathExpressions.scala", "additions": "30", "deletions": "7", "changes": "37"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/MathExpressionsSuite.scala", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/MathFunctionsSuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}]}
{"author": "Peng-Lei", "sha": "67d3622b9f836b39da812259ad5d284d5e62c233", "commit_date": "2021/07/14 05:59:17", "commit_message": "Keep consistent with the namespace naming rule", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.SQLQuerySuite", "org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/CatalogManager.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 7, 16]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 1, 2]}]}
{"author": "Peng-Lei", "sha": "759dd0a179e3a38e6bc8c3b692ac82df0c7ef8bb", "commit_date": "2021/07/01 13:42:53", "commit_message": "Add the show catalogs feature", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.SQLKeywordSuite"], "files": [{"file": {"name": "docs/sql-ref-ansi-compliance.md", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 5]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveCatalogs.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [1, 3, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [1, 5, 15]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/statements.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 3, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2Commands.scala", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [1, 3, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/CatalogManager.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [1, 3, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ShowCatalogsExec.scala", "additions": "47", "deletions": "0", "changes": "47"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala", "additions": "21", "deletions": "0", "changes": "21"}, "updated": [1, 1, 1]}]}
{"author": "maropu", "sha": "3be38824fec054d1556c7e347def0b54413e563d", "commit_date": "2021/03/09 14:23:38", "commit_message": "MapType supports comparable/orderable semantics", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeMapData.java", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala", "additions": "0", "deletions": "16", "changes": "16"}, "updated": [0, 2, 7]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "27", "deletions": "0", "changes": "27"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ordering.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeFloatingNumbers.scala", "additions": "22", "deletions": "4", "changes": "26"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeMaps.scala", "additions": "192", "deletions": "0", "changes": "192"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "8", "deletions": "2", "changes": "10"}, "updated": [1, 7, 9]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/ArrayBasedMapData.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/MapData.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/TypeUtils.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/types/ArrayType.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/types/MapType.scala", "additions": "28", "deletions": "0", "changes": "28"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala", "additions": "15", "deletions": "25", "changes": "40"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/ExpressionTypeCheckingSuite.scala", "additions": "2", "deletions": "17", "changes": "19"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/PredicateSuite.scala", "additions": "0", "deletions": "8", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeFloatingPointNumbersSuite.scala", "additions": "63", "deletions": "1", "changes": "64"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeMapsSuite.scala", "additions": "108", "deletions": "0", "changes": "108"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/ComplexDataSuite.scala", "additions": "22", "deletions": "16", "changes": "38"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "7", "deletions": "32", "changes": "39"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala", "additions": "33", "deletions": "0", "changes": "33"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/comparable-map-explain.sql", "additions": "34", "deletions": "0", "changes": "34"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/comparable-map.sql", "additions": "182", "deletions": "0", "changes": "182"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/comparable-map-explain.sql.out", "additions": "130", "deletions": "0", "changes": "130"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/comparable-map.sql.out", "additions": "986", "deletions": "0", "changes": "986"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/pivot.sql.out", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/udf/udf-pivot.sql.out", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameAggregateSuite.scala", "additions": "8", "deletions": "10", "changes": "18"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameFunctionsSuite.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSetOperationsSuite.scala", "additions": "0", "deletions": "19", "changes": "19"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 7]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/sources/BucketedWriteSuite.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 0, 0]}]}
{"author": "zhengruifeng", "sha": "a9888ba9458d012ab71644bc0808e8123955a824", "commit_date": "2021/07/12 06:18:19", "commit_message": "init\n\ninit\n\ninit", "title": "[SPARK-36087][SQL][WIP] An Impl of skew key detection and data inflation optimization", "body": "### What changes were proposed in this pull request?\r\n1, introduce `ShuffleExecAccumulator` in `ShuffleExchangeExec` to support arbitrary statistics;\r\n\r\n2, impl a key sampling `ShuffleExecAccumulator` to detect skew keys and show debug info on SparkUI;\r\n\r\n3, in `OptimizeSkewedJoin`, estimate the joined size of each partition based on the sampled keys, and split a partition if it is not split yet and its estimated joined size is too larger.\r\n\r\n\r\n### Why are the changes needed?\r\n1, make it easy to add a new statistics which can be used in AQE rules;\r\n2, showing skew info on sparkUI is usefully;\r\n3, spliting partitions based on joined size can resolve data inflation;\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes, new features are added\r\n\r\n\r\n### How was this patch tested?\r\nadded testsuites\r\n", "failed_tests": ["org.apache.spark.sql.TPCDSModifiedPlanStabilityWithStatsSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "36", "deletions": "0", "changes": "36"}, "updated": [0, 8, 16]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeDataInflation.scala", "additions": "125", "deletions": "0", "changes": "125"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeLocalShuffleReader.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "58", "deletions": "15", "changes": "73"}, "updated": [0, 2, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "7", "deletions": "2", "changes": "9"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala", "additions": "79", "deletions": "1", "changes": "80"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExecAccumulator.scala", "additions": "552", "deletions": "0", "changes": "552"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/ShuffleExecAccumulatorSuite.scala", "additions": "155", "deletions": "0", "changes": "155"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "137", "deletions": "1", "changes": "138"}, "updated": [0, 8, 16]}]}
{"author": "jerqi", "sha": "8e2f4fb0b48cb96cc7b6a774269022f2bd2a3882", "commit_date": "2021/08/05 08:33:30", "commit_message": "[SPARK-36223][SQL][TEST] Cover 3 kinds of join in the TPCDSQueryTestSuite", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/TPCDSQueryTestSuite.scala", "additions": "40", "deletions": "4", "changes": "44"}, "updated": [0, 0, 0]}]}
{"author": "Swinky", "sha": "1e8eb5165a7abb1df04f3dc1408e47c9648fd4db", "commit_date": "2021/06/27 18:35:04", "commit_message": "use existing exprId for subquery", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/dynamicpruning/PlanDynamicPruningFilters.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DynamicPartitionPruningSuite.scala", "additions": "24", "deletions": "1", "changes": "25"}, "updated": [0, 1, 4]}]}
{"author": "shipra-a", "sha": "a7e97ddf3c46641d9c8c298d09c4c4e4ac6024ae", "commit_date": "2021/06/09 01:35:41", "commit_message": "https://github.com/apache/spark/pull/28804/commits\n\nCo-authored-by: Karuppayya Rajendran <karuppayya1990@gmail.com>", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "42", "deletions": "0", "changes": "42"}, "updated": [0, 3, 14]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap.java", "additions": "13", "deletions": "0", "changes": "13"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala", "additions": "206", "deletions": "49", "changes": "255"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashMapGenerator.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/WholeStageCodegenSuite.scala", "additions": "37", "deletions": "0", "changes": "37"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/metric/SQLMetricsSuite.scala", "additions": "45", "deletions": "41", "changes": "86"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/AggregationQuerySuite.scala", "additions": "34", "deletions": "27", "changes": "61"}, "updated": [0, 0, 0]}]}
{"author": "dchvn", "sha": "ea82aafb0460bb99a3f5d4d470ba98a9621dc406", "commit_date": "2021/10/04 04:04:19", "commit_message": "[SPARK-36302]: Refactor thirteenth set of 20 query execution errors to use error classes", "title": "[SPARK-36302][SQL] Refactor thirteenth set of 20 query execution errors to use error classes", "body": "\r\n\r\n<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->Adds error classes to some of the exceptions in QueryExecutionErrors.\r\n\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->Improves auditing for developers and adds useful fields for users (error class and SQLSTATE).\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->Fills in missing error class and SQLSTATE fields.\r\n\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->Existing tests\r\n", "failed_tests": [], "files": [{"file": {"name": "core/src/main/resources/error/error-classes.json", "additions": "65", "deletions": "0", "changes": "65"}, "updated": [1, 1, 5]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala", "additions": "59", "deletions": "50", "changes": "109"}, "updated": [1, 1, 4]}]}
{"author": "dchvn", "sha": "110c5c0faf38d4c6524f3c6f78df2fd2117d2172", "commit_date": "2021/09/16 07:54:52", "commit_message": "[SPARK-36711] Support multi-index in new syntax", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/accessors.py", "additions": "36", "deletions": "18", "changes": "54"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "16", "deletions": "8", "changes": "24"}, "updated": [1, 3, 11]}, {"file": {"name": "python/pyspark/pandas/groupby.py", "additions": "15", "deletions": "8", "changes": "23"}, "updated": [0, 1, 10]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "26", "deletions": "0", "changes": "26"}, "updated": [0, 1, 6]}, {"file": {"name": "python/pyspark/pandas/typedef/typehints.py", "additions": "159", "deletions": "87", "changes": "246"}, "updated": [1, 2, 2]}]}
{"author": "kazuyukitanimura", "sha": "b3f68642191f6e9dcfebc6a2c1076f9cbc076aca", "commit_date": "2021/08/31 06:50:05", "commit_message": "[SPARK-36665][SQL] Add more Not operator simplifications\n\n ### What changes were proposed in this pull request?\nThis PR proposes to add more Not operator simplifications in `BooleanSimplification` by applying the following rules\n  - Not(null) == null\n    - e.g. IsNull(Not(...)) can be IsNull(...)\n  - (Not(a) = b) == (a = Not(b))\n    - e.g. Not(...) = true can be (...) = false\n  - (a != b) == (a = Not(b))\n    - e.g. (...) != true can be (...) = false\n\n ### Why are the changes needed?\nThe following query does not push down the filter in the current implementation\n```\nSELECT * FROM t WHERE (not boolean_col) <=> null\n```\nalthough the following equivalent query pushes down the filter as expected.\n```\nSELECT * FROM t WHERE boolean_col <=> null\n```\nThat is because the first query creates `IsNull(Not(boolean_col))` in the current implementation, which should be able to get simplified further to `IsNull(boolean_col)`\nThis PR helps optimizing such cases.\n\n ### Does this PR introduce _any_ user-facing change?\nNo\n\n ### How was this patch tested?\nAdded unit tests\n```\nbuild/sbt \"testOnly *BooleanSimplificationSuite  -- -z SPARK-36665\"\n```", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 2, 11]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/expressions.scala", "additions": "80", "deletions": "0", "changes": "80"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/rules/RuleIdCollection.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NotPropagationSuite.scala", "additions": "176", "deletions": "0", "changes": "176"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NullDownPropagationSuite.scala", "additions": "59", "deletions": "0", "changes": "59"}, "updated": [0, 0, 0]}]}
{"author": "Kimahriman", "sha": "6bb6ecbafc04cab37d04c7fe728d3a8770a1c298", "commit_date": "2021/09/26 22:31:50", "commit_message": "Ignore types when unioning by name", "title": "[SPARK-36918][SQL] Ignore types when comparing structs for unionByName", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nRather than using `DataType.sameType` for comparing structs when unioning by name, only compare the names and orders of the fields, since all we are doing is determining if we need to recreate the struct in a different order. After ResolveUnion is done, the normal union will handle if the types are incompatible or not.\r\n\r\nAdditionally, adds a check to the recursive struct handling as well, so we don't have to recreate a nested struct if only one of its parents or siblings are different.\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nPerformance improvement, especially with nested nullable structs, which get wrapped in an If(IsNull()). Unioning three or more structs can explode the plan due to the multiple structs created extracting values from If(IsNull()) values when the projections are collapsed.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nNo, just a performance improvement.\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nNew unit test for the helper method, and existing tests still pass.\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveUnion.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/types/DataType.scala", "additions": "27", "deletions": "0", "changes": "27"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/types/DataTypeSuite.scala", "additions": "64", "deletions": "0", "changes": "64"}, "updated": [0, 0, 1]}]}
{"author": "Kimahriman", "sha": "38a411dfd0471b2fe182e8c74b8310d5bb31f11c", "commit_date": "2021/07/22 13:02:02", "commit_message": "Track conditionally evaluated expressions to resolve as subexpressions for cases they are already being evaluated", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala", "additions": "113", "deletions": "76", "changes": "189"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [2, 6, 18]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/SubexpressionEliminationSuite.scala", "additions": "55", "deletions": "13", "changes": "68"}, "updated": [0, 0, 4]}]}
{"author": "dgd-contributor", "sha": "50c7f34bb3a7ffdeed059c390cc14dd63791e2d2", "commit_date": "2021/09/21 01:51:17", "commit_message": "[SPARK-36293][SQL] Refactor fourth set of 20 query execution errors to use error classes", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/resources/error/error-classes.json", "additions": "51", "deletions": "0", "changes": "51"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/AnalysisException.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala", "additions": "28", "deletions": "27", "changes": "55"}, "updated": [1, 2, 4]}]}
{"author": "naveensrinivasan", "sha": "b3496f90ed1990a097391eb89be5063e5d76b473", "commit_date": "2021/10/05 01:49:59", "commit_message": "Update dependabot.yml\n\nFixed the typo :face_palm:", "title": "[SPARK-36916] Enable Dependabot for improving security posture of the dependencies", "body": "\r\n### What changes were proposed in this pull request?\r\nEnable dependabot to get security updates and if needed version updates on dependencies \r\n\r\n### Why are the changes needed?\r\n\r\nhttps://docs.github.com/en/code-security/supply-chain-security/keeping-your-dependencies-updated-automatically\r\n\r\nHaving knowledge about vulnerabilities of the dependencies helps the project owners decide on their dependencies security posture to make decisions.\r\n\r\nIf the project decides to get updates only on security updates and not on any version updates then setting these options would not open any PR 's `open-pull-requests-limit: 0`\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNO\r\nThis option has to be enabled in the security section of the project.\r\nhttps://docs.github.com/en/code-security/supply-chain-security/managing-vulnerabilities-in-your-projects-dependencies/configuring-dependabot-security-updates#managing-dependabot-security-updates-for-your-repositories\r\n### How was this patch tested?\r\nN/A\r\n", "failed_tests": [], "files": [{"file": {"name": ".github/dependabot.yml", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [0, 0, 0]}]}
{"author": "xinrong-databricks", "sha": "033c2c8378700675750261b31f5c1f8f7f3c4e8d", "commit_date": "2021/10/04 23:33:11", "commit_message": "_parse_datatype_string", "title": "[SPARK-36910][PYTHON] Inline type hints for python/pyspark/sql/types.py", "body": "### What changes were proposed in this pull request?\r\nInline type hints for python/pyspark/sql/types.py\r\n\r\n### Why are the changes needed?\r\nCurrent stub files cannot support type checking for the function body. Inline type hints can type check the function body.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo.\r\n\r\n### How was this patch tested?\r\nExisting tests.\r\n", "failed_tests": ["pyspark.ml.tests.test_algorithms", "pyspark.pandas.tests.data_type_ops.test_base", "pyspark.pandas.tests.indexes.test_base"], "files": [{"file": {"name": "python/pyspark/pandas/frame.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 3, 10]}, {"file": {"name": "python/pyspark/sql/types.py", "additions": "233", "deletions": "164", "changes": "397"}, "updated": [0, 0, 3]}, {"file": {"name": "python/pyspark/sql/types.pyi", "additions": "0", "deletions": "210", "changes": "210"}, "updated": [0, 0, 1]}]}
{"author": "xinrong-databricks", "sha": "58ac86e39b59dd4001c7b5c65b3621e11d7c8441", "commit_date": "2021/10/04 22:30:37", "commit_message": "inline window.py", "title": "[SPARK-36927][PYTHON] Inline type hints for python/pyspark/sql/window.py", "body": "### What changes were proposed in this pull request?\r\nInline type hints for python/pyspark/sql/window.py\r\n\r\n### Why are the changes needed?\r\nCurrently, stub files are used for type hints. However, statements within functions cannot be type-checked.\r\nThe PR is proposed to inline type hints for python/pyspark/sql/window.py to type check statements within functions.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo.\r\n\r\n\r\n### How was this patch tested?\r\nExisting tests.\r\n", "failed_tests": ["pyspark.ml.tests.test_algorithms", "pyspark.pandas.tests.data_type_ops.test_base", "pyspark.pandas.tests.indexes.test_base"], "files": [{"file": {"name": "python/pyspark/sql/window.py", "additions": "27", "deletions": "21", "changes": "48"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/window.pyi", "additions": "0", "deletions": "41", "changes": "41"}, "updated": [0, 0, 0]}]}
{"author": "xinrong-databricks", "sha": "a89afca4b66d5412d2327180f96a90686903eb8c", "commit_date": "2021/09/24 23:29:35", "commit_message": "prototype", "title": "", "body": "", "failed_tests": ["pyspark.ml.tests.test_algorithms", "pyspark.pandas.tests.data_type_ops.test_base", "pyspark.pandas.tests.indexes.test_base"], "files": [{"file": {"name": "python/pyspark/__init__.pyi", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/functions.py", "additions": "624", "deletions": "419", "changes": "1043"}, "updated": [0, 2, 4]}, {"file": {"name": "python/pyspark/sql/functions.pyi", "additions": "0", "deletions": "377", "changes": "377"}, "updated": [0, 2, 4]}]}
{"author": "huaxingao", "sha": "b9fa9cae607547bc7a9ec7d7490ac55309b6f060", "commit_date": "2021/10/04 05:07:54", "commit_message": "fix JavadocGenerationFailure", "title": "[SPARK-36913][SQL] Implement createIndex and IndexExists in DS V2 JDBC (MySQL dialect)", "body": "\r\n\r\n### What changes were proposed in this pull request?\r\nImplementing `createIndex`/`IndexExists` in DS V2 JDBC\r\n\r\n\r\n### Why are the changes needed?\r\nThis is a subtask of the V2 Index support. I am implementing index support for DS V2 JDBC so we can have a POC and an end to end testing. This PR implements `createIndex` and `IndexExists`. Next PR will implement `listIndexes` and `dropIndex`. I intentionally make the PR small so it's easier to review.\r\n\r\nIndex is not supported by h2 database and create/drop index are not standard SQL syntax. This PR only implements `createIndex` and `IndexExists` in `MySQL` dialect.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes, `createIndex`/`IndexExist` in DS V2 JDBC\r\n\r\n\r\n### How was this patch tested?\r\nnew test\r\n", "failed_tests": [], "files": [{"file": {"name": "external/docker-integration-tests/src/test/scala/org/apache/spark/sql/jdbc/v2/MySQLIntegrationSuite.scala", "additions": "33", "deletions": "0", "changes": "33"}, "updated": [0, 0, 0]}, {"file": {"name": "external/docker-integration-tests/src/test/scala/org/apache/spark/sql/jdbc/v2/V2JDBCTest.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/index/SupportsIndex.java", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/AlreadyExistException.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils.scala", "additions": "41", "deletions": "0", "changes": "41"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCTable.scala", "additions": "52", "deletions": "2", "changes": "54"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala", "additions": "39", "deletions": "1", "changes": "40"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/jdbc/MySQLDialect.scala", "additions": "65", "deletions": "3", "changes": "68"}, "updated": [0, 0, 0]}]}
{"author": "huaxingao", "sha": "bc71de3c94645763bf16b765b9274bbd46041f36", "commit_date": "2021/10/03 16:27:04", "commit_message": "address comments", "title": "[SPARK-36895][SQL] Add Create Index syntax support", "body": "\r\n### What changes were proposed in this pull request?\r\nThis is the 2nd PR for DSv2 index support.\r\n\r\nThis PR adds the following:\r\n\r\n- create index syntax support in parser and analyzer\r\n- `CreateIndex` logic node\r\n- `CreateIndexExec` physical node\r\n\r\n`CreateIndex` is not implemented yet in this PR. Calling `CreateIndex` will throw `SQLFeatureNotSupportedException`, and the parsed index information such as `IndexName` `indexType` `columns` and index properties will be included in the error message for now for testing purpose.\r\n\r\n### Why are the changes needed?\r\nTo support index in DSv2\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes, the create table syntax as the following:\r\n\r\n```\r\nCREATE [index_type] INDEX [index_name] ON [TABLE] table_name (column_index_property_list)[OPTIONS indexPropertyList]\r\n\r\n    column_index_property_list: column_name [OPTIONS(indexPropertyList)]  [ ,  . . . ]\r\n    indexPropertyList: index_property_name [= index_property_value] [ ,  . . . ]\r\n```\r\n\r\n### How was this patch tested?\r\nadd a UT", "failed_tests": ["org.apache.spark.sql.catalyst.SQLKeywordSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4", "additions": "33", "deletions": "22", "changes": "55"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala", "additions": "54", "deletions": "25", "changes": "79"}, "updated": [0, 3, 9]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2Commands.scala", "additions": "16", "deletions": "1", "changes": "17"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/CreateIndexExec.scala", "additions": "56", "deletions": "0", "changes": "56"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 3, 4]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCTableCatalogSuite.scala", "additions": "42", "deletions": "1", "changes": "43"}, "updated": [0, 0, 0]}]}
{"author": "huaxingao", "sha": "cc7414cf3f6537143d4a0296503cebc3b93ee00d", "commit_date": "2021/09/29 17:45:00", "commit_message": "fix test failure", "title": "[SPARK-36868][SQL] Migrate CreateFunctionStatement to v2 command framework", "body": "### What changes were proposed in this pull request?\r\nMigrate `CreateFunctionStatement` to v2 command framework\r\n\r\n### Why are the changes needed?\r\nMigrate to the standard V2 framework\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n\r\n### How was this patch tested?\r\nExisting tests\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala", "additions": "18", "deletions": "8", "changes": "26"}, "updated": [2, 3, 9]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/statements.scala", "additions": "1", "deletions": "12", "changes": "13"}, "updated": [1, 2, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2Commands.scala", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [2, 2, 3]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala", "additions": "14", "deletions": "13", "changes": "27"}, "updated": [2, 2, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala", "additions": "35", "deletions": "25", "changes": "60"}, "updated": [0, 0, 1]}]}
{"author": "huaxingao", "sha": "6ba0a21aca82aaa854cc3645d2923fed904e1a87", "commit_date": "2021/09/28 01:20:09", "commit_message": "Migrate CreateViewStatement to v2 command", "title": "[SPARK-36871][SQL][WIP] Migrate CreateViewStatement to v2 command", "body": "\r\n\r\n### What changes were proposed in this pull request?\r\nMigrate `CreateViewStatement`  to v2 command framework\r\n\r\n### Why are the changes needed?\r\nMigrate to the standard V2 framework\r\n\r\n\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nno\r\n\r\n\r\n### How was this patch tested?\r\nexisting tests", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala", "additions": "5", "deletions": "3", "changes": "8"}, "updated": [2, 2, 9]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/statements.scala", "additions": "1", "deletions": "18", "changes": "19"}, "updated": [1, 2, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2Commands.scala", "additions": "21", "deletions": "1", "changes": "22"}, "updated": [1, 1, 3]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala", "additions": "8", "deletions": "8", "changes": "16"}, "updated": [1, 1, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 0, 2]}]}
{"author": "huaxingao", "sha": "df6ca868b8b27dc74d4ec198dd04b3aee2c59d69", "commit_date": "2021/08/04 15:50:20", "commit_message": "[SPARK-34952][SQL] Aggregate (Min/Max/Count) push down for Parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [1, 3, 17]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetUtils.scala", "additions": "226", "deletions": "0", "changes": "226"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetPartitionReaderFactory.scala", "additions": "101", "deletions": "22", "changes": "123"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetScan.scala", "additions": "31", "deletions": "3", "changes": "34"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetScanBuilder.scala", "additions": "94", "deletions": "6", "changes": "100"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/FileScanSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetAggregatePushDownSuite.scala", "additions": "518", "deletions": "0", "changes": "518"}, "updated": [0, 0, 0]}]}
{"author": "huaxingao", "sha": "9e42d67de52fa000bed3f21e40527360934a56f7", "commit_date": "2021/09/19 22:36:37", "commit_message": "Migrate CreateTableStatement to v2 command framework", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 8]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveCatalogs.scala", "additions": "5", "deletions": "9", "changes": "14"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala", "additions": "7", "deletions": "3", "changes": "10"}, "updated": [0, 2, 8]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/statements.scala", "additions": "0", "deletions": "19", "changes": "19"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2Commands.scala", "additions": "23", "deletions": "4", "changes": "27"}, "updated": [0, 0, 5]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/CatalogV2Util.scala", "additions": "2", "deletions": "7", "changes": "9"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala", "additions": "8", "deletions": "12", "changes": "20"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/rules.scala", "additions": "46", "deletions": "0", "changes": "46"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala", "additions": "6", "deletions": "2", "changes": "8"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala", "additions": "9", "deletions": "6", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/PlanResolutionSuite.scala", "additions": "26", "deletions": "19", "changes": "45"}, "updated": [0, 0, 2]}]}
{"author": "daugraph", "sha": "5a5348aaee0b0004406cdb2fec436590559d2caf", "commit_date": "2021/09/23 06:21:11", "commit_message": "[SPARK-36804][YARN] Using the verbose parameter in yarn mode would cause application submission failure\n\n### What changes were proposed in this pull request?\nSupport --verbose option in YARN mode\n\n### Why are the changes needed?\nIf we submit the spark application with the --verbose parameter in yarn mode, we will get the following exception:\n```bash\nException in thread \"main\" java.lang.IllegalArgumentException: Unknown/unsupported param List(--verbose)Exception in thread \"main\"\n```\nSparkSubmit invoke YarnClusterApplication with --verbose arguments, however ClientArguments used by YarnClusterApplication don't support that argument by now.\nas a result, IllegalArgumentException is thrown in ClientArguments. I think we can support --verbose in YARN mode to keep consistency with other module\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nAfter apply this patch, application will submit successful", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/ClientArguments.scala", "additions": "13", "deletions": "1", "changes": "14"}, "updated": [0, 0, 0]}]}
{"author": "thejdeep", "sha": "6c754cf48f3e90b9c05a729c830505dcc2bfff0f", "commit_date": "2021/10/04 22:24:00", "commit_message": "[SPARK-36834][SHUFFLE] Add support for namespacing log lines emitted by ESS\n\n ### What changes were proposed in this pull request?\n Added a config `spark.yarn.shuffle.service.logs.namespace` which can be used to add a namespace suffix to log lines emitted by the External Shuffle Service.\n\n ### Why are the changes needed?\n Since many instances of ESS can be running on the same NM, it would be easier to distinguish between them.\n\n ### Does this PR introduce _any_ user-facing change?\n No\n\n ### How was this patch tested?\n N/A", "title": "[SPARK-36834][SHUFFLE] Add support for namespacing log lines emitted by external shuffle service", "body": "### What changes were proposed in this pull request?\r\nAdded a config `spark.yarn.shuffle.service.logs.namespace` which can be used to add a namespace suffix to log lines emitted by the External Shuffle Service.\r\n\r\n### Why are the changes needed?\r\nSince many instances of ESS can be running on the same NM, it would be easier to distinguish between them.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\nN/A", "failed_tests": [], "files": [{"file": {"name": "common/network-yarn/src/main/java/org/apache/spark/network/yarn/YarnShuffleService.java", "additions": "18", "deletions": "1", "changes": "19"}, "updated": [0, 0, 1]}, {"file": {"name": "docs/running-on-yarn.md", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 2, 2]}]}
{"author": "ueshin", "sha": "794fc0d142f257daa19dfe2a6e4a2cd21f26f3d7", "commit_date": "2021/10/04 20:18:57", "commit_message": "Fix.", "title": "[SPARK-36884][PYTHON] Inline type hints for pyspark.sql.session", "body": "### What changes were proposed in this pull request?\r\n\r\nInline type hints from `python/pyspark/sql/session.pyi` to `python/pyspark/sql/session.py`.\r\n\r\n### Why are the changes needed?\r\n\r\nCurrently, there is type hint stub files `python/pyspark/sql/session.pyi` to show the expected types for functions, but we can also take advantage of static type checking within the functions by inlining the type hints.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nNo.\r\n\r\n### How was this patch tested?\r\n\r\nExisting test.", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/__init__.pyi", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/session.py", "additions": "204", "deletions": "68", "changes": "272"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/sql/session.pyi", "additions": "0", "deletions": "131", "changes": "131"}, "updated": [0, 0, 0]}]}
{"author": "pralabhkumar", "sha": "c1622689096bfe953d32234a31155d775496f635", "commit_date": "2021/09/13 13:01:51", "commit_message": "Spark to Pandas via Array for ArrayType(Timestamp)", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/sql/pandas/conversion.py", "additions": "26", "deletions": "8", "changes": "34"}, "updated": [0, 2, 2]}, {"file": {"name": "python/pyspark/sql/pandas/serializers.py", "additions": "7", "deletions": "1", "changes": "8"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/pandas/types.py", "additions": "82", "deletions": "21", "changes": "103"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_arrow.py", "additions": "35", "deletions": "12", "changes": "47"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_pandas_cogrouped_map.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/tests/test_pandas_grouped_map.py", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/tests/test_pandas_udf_grouped_agg.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/tests/test_pandas_udf_scalar.py", "additions": "41", "deletions": "7", "changes": "48"}, "updated": [0, 0, 0]}]}
{"author": "sunchao", "sha": "49fcbdcc44399bbd21d2f9581ce9bba341d74bea", "commit_date": "2021/06/09 19:26:29", "commit_message": "wip", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite", "org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/physical/partitioning.scala", "additions": "196", "deletions": "29", "changes": "225"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/DistributionSuite.scala", "additions": "0", "deletions": "38", "changes": "38"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/ShuffleSpecSuite.scala", "additions": "413", "deletions": "0", "changes": "413"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AQEUtils.scala", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/bucketing/DisableUnnecessaryBucketedScan.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "80", "deletions": "31", "changes": "111"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ValidateRequirements.scala", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/joins/ShuffledJoin.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/objects.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinExec.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q17.sf100/explain.txt", "additions": "155", "deletions": "170", "changes": "325"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q17.sf100/simplified.txt", "additions": "74", "deletions": "81", "changes": "155"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q25.sf100/explain.txt", "additions": "155", "deletions": "170", "changes": "325"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q25.sf100/simplified.txt", "additions": "74", "deletions": "81", "changes": "155"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q29.sf100/explain.txt", "additions": "173", "deletions": "188", "changes": "361"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q29.sf100/simplified.txt", "additions": "75", "deletions": "82", "changes": "157"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q47.sf100/explain.txt", "additions": "110", "deletions": "125", "changes": "235"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q47.sf100/simplified.txt", "additions": "80", "deletions": "89", "changes": "169"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q57.sf100/explain.txt", "additions": "110", "deletions": "125", "changes": "235"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q57.sf100/simplified.txt", "additions": "80", "deletions": "89", "changes": "169"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q72.sf100/explain.txt", "additions": "197", "deletions": "207", "changes": "404"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q72.sf100/simplified.txt", "additions": "105", "deletions": "111", "changes": "216"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q24.sf100/explain.txt", "additions": "61", "deletions": "43", "changes": "104"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q24.sf100/simplified.txt", "additions": "36", "deletions": "33", "changes": "69"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q47.sf100/explain.txt", "additions": "110", "deletions": "125", "changes": "235"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q47.sf100/simplified.txt", "additions": "80", "deletions": "89", "changes": "169"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q51a.sf100/explain.txt", "additions": "249", "deletions": "259", "changes": "508"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q51a.sf100/simplified.txt", "additions": "100", "deletions": "106", "changes": "206"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q57.sf100/explain.txt", "additions": "110", "deletions": "125", "changes": "235"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q57.sf100/simplified.txt", "additions": "80", "deletions": "89", "changes": "169"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q64/explain.txt", "additions": "414", "deletions": "424", "changes": "838"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q64/simplified.txt", "additions": "259", "deletions": "265", "changes": "524"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q72.sf100/explain.txt", "additions": "197", "deletions": "207", "changes": "404"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q72.sf100/simplified.txt", "additions": "105", "deletions": "111", "changes": "216"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/PlannerSuite.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/exchange/EnsureRequirementsSuite.scala", "additions": "460", "deletions": "4", "changes": "464"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/sources/BucketedReadSuite.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingJoinSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}]}
{"author": "ChenMichael", "sha": "5c03e4d7e5174430e3f8b2b4c9786a4c2dd32f66", "commit_date": "2021/10/01 22:15:34", "commit_message": "[SPARK-36911] - Track amount of time spent in creating query stages, reOptimization, and generating explainString in AQE and then add it to metrics + log times.", "title": "[SPARK-36911][SQL] - Add SQLMetric for AQE Overhead", "body": "\u2026reOptimization, and generating explainString in AQE and then add it to metrics + log times.\r\n\r\n<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nAdd metrics for durations of \"reOptimize\", \"generate explainString\" and \"createQueryStages\" to AdaptiveSparkPlanExec metrics to make it easier to see overhead of AQE for a query.\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nThe changes make it easier to get a sense of the overhead of AQE on a given query.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nNew SQLMetrics are added\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nAdded tests", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "58", "deletions": "3", "changes": "61"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "21", "deletions": "0", "changes": "21"}, "updated": [1, 1, 4]}]}
{"author": "LuciferYang", "sha": "498126083d2f2f42163b12a234ea53858bff0a8e", "commit_date": "2021/09/30 08:03:34", "commit_message": "add commnet", "title": "[SPARK-36796][BUILD][CORE][SQL] Pass all `sql/core` and dependent modules UTs use Maven with JDK 17", "body": "### What changes were proposed in this pull request?\r\nIn order to pass the UTs related to sql/core and dependent modules, this pr mainly does the following change:\r\n\r\n- Add a new property named `extraJavaTestArgs` to `pom.xml`, It includes all `--add-opens` configurations required for UTs with Java 17 and It also include `-XX:+IgnoreUnrecognizedVMOptions` to compatible with Java 8.\r\n- Add a new Util named `JavaModuleUtils`, It is used to supplement `--add-opens` configurations to `spark.driver.extraJavaOptions` and `spark.executor.extraJavaOptions` for test cases using `local` and `local=cluster` modes\r\n- Increase `-Xmx4g` to `-Xmx5g` for `scalatest-maven-plugin` because `SPARK-36464: size returns correct positive number even with over 2GB data` will oom with JDK 17\r\n- `DateTimeFormatterHelper`: the formatting keyword 'B' is disabled for compatibility with Java 8 behavior, where 'B' represents `Pattern letters to output a day period` in Java 17\r\n- Independent verification the result of `postgreSQL/text.sql` because `select format_string('%0$s', 'Hello')` has different behavior between Java 8 and Java 17,  but it seems that using java 17 has expected behavior of PostgreSQL(`PostgreSQL throw ERROR: format specifies argument 0, but arguments are numbered from 1`)\r\n- Replace `UseCompressedOops` with `UseCompressedClassPointers` in `WholeStageCodegenSparkSubmitSuite` because it seems that 'UseCompressedOops' no longer affects the behavior of Array types,  but the behavior of `UseCompressedClassPointers` is expected.\r\n\r\n### Why are the changes needed?\r\nPass Spark UTs with JDK 17\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n`select format_string('%0$s', 'Hello')` will throw exception because Java 17 corrects a wrong behavior about the starting index of format specifications.\r\n\r\n\r\n### How was this patch tested?\r\n- Pass the Jenkins or GitHub Action\r\n- Manual test `mvn clean install -pl sql/core -am` with Java 17, all tests passed \r\n", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/SparkContext.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 2, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/JavaModuleUtils.scala", "additions": "80", "deletions": "0", "changes": "80"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/deploy/SparkSubmitTestUtils.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 1, 1]}, {"file": {"name": "core/src/test/scala/org/apache/spark/launcher/LauncherBackendSuite.scala", "additions": "10", "deletions": "3", "changes": "13"}, "updated": [0, 0, 0]}, {"file": {"name": "pom.xml", "additions": "23", "deletions": "2", "changes": "25"}, "updated": [0, 9, 27]}, {"file": {"name": "project/SparkBuild.scala", "additions": "13", "deletions": "1", "changes": "14"}, "updated": [0, 1, 6]}, {"file": {"name": "sql/catalyst/pom.xml", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeFormatterHelper.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/pom.xml", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/postgreSQL/text.sql-jdk17.out", "additions": "360", "deletions": "0", "changes": "360"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SQLQueryTestSuite.scala", "additions": "16", "deletions": "2", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/WholeStageCodegenSparkSubmitSuite.scala", "additions": "14", "deletions": "3", "changes": "17"}, "updated": [0, 1, 1]}]}
{"author": "Reamer", "sha": "c72b5ce70953eca86f9ab7212bbde6d3d38200c0", "commit_date": "2021/09/29 16:44:45", "commit_message": "Use default WebsocketPingInterval of the library, which is 30 seconds", "title": "[SPARK-36890][K8S] Use default WebsocketPingInterval for Kubernetes watches", "body": "\r\n### What changes were proposed in this pull request?\r\nThis pull request removes the configuration that disables the WebsocketPingInterval. The default value of the [fabric8io/kubernetes-client](https://github.com/fabric8io/kubernetes-client/blob/master/README.md) library is 30 seconds.\r\n\r\n\r\n### Why are the changes needed?\r\nWith a periodical websocket ping, the tunnel timeout in loadbalancers should work as expected.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\nIt is not possible for me to write a test for this particular infrastructure. I have checked the setting locally with my infrastructure with a small JUnit test.\r\n", "failed_tests": [], "files": [{"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/SparkKubernetesClientFactory.scala", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}]}
{"author": "AngersZhuuuu", "sha": "0bff56fc35b2a226af12097de3576c95b3bef640", "commit_date": "2021/10/02 10:01:27", "commit_message": "Update HiveTableScanExec.scala", "title": "[SPARK-36876][SQL] Support Dynamic Partition pruning for HiveTableScanExec", "body": "### What changes were proposed in this pull request?\r\nCurrent code just support dynamic partition pruning for DSV1 and DSV2, here we support HiveTableScan\r\n\r\n\r\n### Why are the changes needed?\r\nOptimize Hive Table Scan dynamic partition pruning \r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNO\r\n\r\n### How was this patch tested?\r\nAdded UT\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/dynamicpruning/CleanupDynamicPruningFilters.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/dynamicpruning/PartitionPruning.scala", "additions": "9", "deletions": "2", "changes": "11"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DynamicPartitionPruningSuite.scala", "additions": "56", "deletions": "43", "changes": "99"}, "updated": [0, 2, 5]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/HiveTableScanExec.scala", "additions": "50", "deletions": "7", "changes": "57"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/DynamicPartitionPruningHiveScanSuite.scala", "additions": "88", "deletions": "0", "changes": "88"}, "updated": [0, 0, 0]}]}
{"author": "AngersZhuuuu", "sha": "26ad1ca4da8d50b0c6f40c0cc1609d6c23231c39", "commit_date": "2021/08/18 08:31:54", "commit_message": "[SPARK-36540][YARN]YARN-CLIENT mode should check Shutdown message when AMEndpoint disconencted", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "docs/running-on-yarn.md", "additions": "13", "deletions": "0", "changes": "13"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala", "additions": "13", "deletions": "2", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/config.scala", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/yarn/src/main/scala/org/apache/spark/scheduler/cluster/YarnClientSchedulerBackend.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/yarn/src/main/scala/org/apache/spark/scheduler/cluster/YarnSchedulerBackend.scala", "additions": "11", "deletions": "2", "changes": "13"}, "updated": [0, 0, 0]}]}
{"author": "zero323", "sha": "2cd905c704fae6436be339ba9cb52c1b898fd6aa", "commit_date": "2021/10/02 09:25:19", "commit_message": "Separate date / time types", "title": "[SPARK-36894][PYTHON] Synchronize RDD.toDF annotations with SparkSession.createDataFrame variants.", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nThis pull request synchronizes `RDD.toDF` annotations with `SparkSession.createDataFrame` variants.\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\n- Adds support for providing `str` schema.\r\n- Add supports for converting `RDDs` of \"atomic\" values, if schema is provided.\r\n\r\nAdditionally it introduces a `TypeVar` representing supported \"atomic\" values. This was done to avoid issue with manual data tests, where the following\r\n\r\n```python\r\nsc.parallelize([1]).toDF(schema=IntegerType())\r\n```\r\n\r\nresults in \r\n\r\n```\r\nerror: No overload variant of \"toDF\" of \"RDD\" matches argument type \"IntegerType\"  [call-overload]\r\nnote: Possible overload variants:\r\nnote:     def toDF(self, schema: Union[List[str], Tuple[str, ...], None] = ..., sampleRatio: Optional[float] = ...) -> DataFrame\r\nnote:     def toDF(self, schema: Union[StructType, str, None] = ...) -> DataFrame\r\n```\r\n\r\nwhen `Union` type is used (this problem doesn't surface when non-self bound is used).\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\nType checker only.\r\n\r\nPlease note, that these annotations serve primarily to support documentation, as checks on `self` types are still very limited.\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\n\r\nExisting tests and manual data tests.", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/rdd.pyi", "additions": "11", "deletions": "4", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/_typing.pyi", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "ahshahid", "sha": "a45b25e190f220b5152192d9e1105aea0707e705", "commit_date": "2020/11/23 19:25:09", "commit_message": "pull changes to forked master (#7)\n\n* [SPARK-33045][SQL][FOLLOWUP] Fix build failure with Scala 2.13\r\n\r\n### What changes were proposed in this pull request?\r\n\r\nExplicitly convert `scala.collection.mutable.Buffer` to `Seq`. In Scala 2.13 `Seq` is an alias of `scala.collection.immutable.Seq` instead of `scala.collection.Seq`.\r\n\r\n### Why are the changes needed?\r\n\r\nWithout the change build with Scala 2.13 fails with the following:\r\n```\r\n[error] /home/runner/work/spark/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala:1417:41: type mismatch;\r\n[error]  found   : scala.collection.mutable.Buffer[org.apache.spark.unsafe.types.UTF8String]\r\n[error]  required: Seq[org.apache.spark.unsafe.types.UTF8String]\r\n[error]                 case null => LikeAll(e, patterns)\r\n[error]                                         ^\r\n[error] /home/runner/work/spark/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala:1418:41: type mismatch;\r\n[error]  found   : scala.collection.mutable.Buffer[org.apache.spark.unsafe.types.UTF8String]\r\n[error]  required: Seq[org.apache.spark.unsafe.types.UTF8String]\r\n[error]                 case _ => NotLikeAll(e, patterns)\r\n[error]                                         ^\r\n```\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nNo\r\n\r\n### How was this patch tested?\r\n\r\nN/A\r\n\r\nCloses #30431 from sunchao/SPARK-33045-followup.\r\n\r\nAuthored-by: Chao Sun <sunchao@apple.com>\r\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\r\n\r\n* [MINOR] Structured Streaming statistics page indent fix\r\n\r\n### What changes were proposed in this pull request?\r\nStructured Streaming statistics page code contains an indentation issue. This PR fixes it.\r\n\r\n### Why are the changes needed?\r\nIndent fix.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo.\r\n\r\n### How was this patch tested?\r\nExisting unit tests.\r\n\r\nCloses #30434 from gaborgsomogyi/STAT-INDENT-FIX.\r\n\r\nAuthored-by: Gabor Somogyi <gabor.g.somogyi@gmail.com>\r\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\r\n\r\n* [MINOR][DOCS] Document 'without' value for HADOOP_VERSION in pip installation\r\n\r\n### What changes were proposed in this pull request?\r\n\r\nI believe it's self-descriptive.\r\n\r\n### Why are the changes needed?\r\n\r\nTo document supported features.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nYes, the docs are updated. It's master only.\r\n\r\n### How was this patch tested?\r\n\r\nManually built the docs via `cd python/docs` and `make clean html`:\r\n\r\n![Screen Shot 2020-11-20 at 10 59 07 AM](https://user-images.githubusercontent.com/6477701/99748225-7ad9b280-2b1f-11eb-86fd-165012b1bb7c.png)\r\n\r\nCloses #30436 from HyukjinKwon/minor-doc-fix.\r\n\r\nAuthored-by: HyukjinKwon <gurwls223@apache.org>\r\nSigned-off-by: HyukjinKwon <gurwls223@apache.org>\r\n\r\n* [SPARK-32919][SHUFFLE][TEST-MAVEN][TEST-HADOOP2.7] Driver side changes for coordinating push based shuffle by selecting external shuffle services for merging partitions\r\n\r\n### What changes were proposed in this pull request?\r\nDriver side changes for coordinating push based shuffle by selecting external shuffle services for merging partitions.\r\n\r\nThis PR includes changes related to `ShuffleMapStage` preparation which is selection of merger locations and initializing them as part of `ShuffleDependency`.\r\n\r\nCurrently this code is not used as some of the changes would come subsequently as part of https://issues.apache.org/jira/browse/SPARK-32917 (shuffle blocks push as part of `ShuffleMapTask`), https://issues.apache.org/jira/browse/SPARK-32918 (support for finalize API) and https://issues.apache.org/jira/browse/SPARK-32920 (finalization of push/merge phase). This is why the tests here are also partial, once these above mentioned changes are raised as PR we will have enough tests for DAGScheduler piece of code as well.\r\n\r\n### Why are the changes needed?\r\nAdded a new API in `SchedulerBackend` to get merger locations for push based shuffle. This is currently implemented for Yarn and other cluster managers can have separate implementations which is why a new API is introduced.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes, user facing config to enable push based shuffle is introduced\r\n\r\n### How was this patch tested?\r\nAdded unit tests partially and some of the changes in DAGScheduler depends on future changes, DAGScheduler tests will be added along with those changes.\r\n\r\nLead-authored-by: Venkata krishnan Sowrirajan vsowrirajanlinkedin.com\r\nCo-authored-by: Min Shen mshenlinkedin.com\r\n\r\nCloses #30164 from venkata91/upstream-SPARK-32919.\r\n\r\nLead-authored-by: Venkata krishnan Sowrirajan <vsowrirajan@linkedin.com>\r\nCo-authored-by: Min Shen <mshen@linkedin.com>\r\nSigned-off-by: Mridul Muralidharan <mridul<at>gmail.com>\r\n\r\n* [SPARK-33441][BUILD][FOLLOWUP] Make unused-imports check for SBT specific\r\n\r\n### What changes were proposed in this pull request?\r\nMove \"unused-imports\" check config to `SparkBuild.scala` and make it SBT specific.\r\n\r\n### Why are the changes needed?\r\nMake unused-imports check for SBT specific.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\nPass the Jenkins or GitHub Action\r\n\r\nCloses #30441 from LuciferYang/SPARK-33441-FOLLOWUP.\r\n\r\nAuthored-by: yangjie01 <yangjie01@baidu.com>\r\nSigned-off-by: HyukjinKwon <gurwls223@apache.org>\r\n\r\n* [SPARK-32512][SQL][TESTS][FOLLOWUP] Remove duplicate tests for ALTER TABLE .. PARTITIONS from DataSourceV2SQLSuite\r\n\r\n### What changes were proposed in this pull request?\r\nRemove tests from `DataSourceV2SQLSuite` that were copied to `AlterTablePartitionV2SQLSuite` by https://github.com/apache/spark/pull/29339.\r\n\r\n### Why are the changes needed?\r\n- To reduce tests execution time\r\n- To improve test maintenance\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\nBy running the modified tests:\r\n```\r\n$ build/sbt \"test:testOnly *DataSourceV2SQLSuite\"\r\n$ build/sbt \"test:testOnly *AlterTablePartitionV2SQLSuite\"\r\n```\r\n\r\nCloses #30444 from MaxGekk/dedup-tests-AlterTablePartitionV2SQLSuite.\r\n\r\nAuthored-by: Max Gekk <max.gekk@gmail.com>\r\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\r\n\r\n* [SPARK-33422][DOC] Fix the correct display of left menu item\r\n\r\n### What changes were proposed in this pull request?\r\nLimit the height of the menu area on the left to display vertical scroll bar\r\n\r\n### Why are the changes needed?\r\n\r\nThe bottom menu item cannot be displayed when the left menu tree is long\r\n\r\n### Does this PR introduce any user-facing change?\r\n\r\nYes, if the menu item shows more, you'll see it by pulling down the vertical scroll bar\r\n\r\nbefore:\r\n![image](https://user-images.githubusercontent.com/28332082/98805115-16995d80-2452-11eb-933a-3b72c14bea78.png)\r\n\r\nafter:\r\n![image](https://user-images.githubusercontent.com/28332082/98805418-7e4fa880-2452-11eb-9a9b-8d265078297c.png)\r\n\r\n### How was this patch tested?\r\nNA\r\n\r\nCloses #30335 from liucht-inspur/master.\r\n\r\nAuthored-by: liucht <liucht@inspur.com>\r\nSigned-off-by: HyukjinKwon <gurwls223@apache.org>\r\n\r\n* [SPARK-33468][SQL] ParseUrl  in ANSI mode should fail if input string is not a valid url\r\n\r\n### What changes were proposed in this pull request?\r\n\r\nWith `ParseUrl`, instead of return null we throw exception if input string is not a vaild url.\r\n\r\n### Why are the changes needed?\r\n\r\nFor ANSI mode.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nYes, user will get exception if `set spark.sql.ansi.enabled=true`.\r\n\r\n### How was this patch tested?\r\n\r\nAdd test.\r\n\r\nCloses #30399 from ulysses-you/SPARK-33468.\r\n\r\nLead-authored-by: ulysses <youxiduo@weidian.com>\r\nCo-authored-by: ulysses-you <youxiduo@weidian.com>\r\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\r\n\r\n* [SPARK-28704][SQL][TEST] Add back Skiped HiveExternalCatalogVersionsSuite in HiveSparkSubmitSuite at JDK9+\r\n\r\n### What changes were proposed in this pull request?\r\nWe skip test HiveExternalCatalogVersionsSuite when testing with JAVA_9 or later because our previous version does not support JAVA_9 or later. We now add it back since we have a version supports JAVA_9 or later.\r\n\r\n### Why are the changes needed?\r\n\r\nTo recover test coverage.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\nCheck CI logs.\r\n\r\nCloses #30428 from AngersZhuuuu/SPARK-28704.\r\n\r\nAuthored-by: angerszhu <angers.zhu@gmail.com>\r\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\r\n\r\n* [SPARK-33466][ML][PYTHON] Imputer support mode(most_frequent) strategy\r\n\r\n### What changes were proposed in this pull request?\r\nimpl a new strategy `mode`: replace missing using the most frequent value along each column.\r\n\r\n### Why are the changes needed?\r\nit is highly scalable, and had been a function in [sklearn.impute.SimpleImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer) for a long time.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes, a new strategy is added\r\n\r\n### How was this patch tested?\r\nupdated testsuites\r\n\r\nCloses #30397 from zhengruifeng/imputer_max_freq.\r\n\r\nLead-authored-by: Ruifeng Zheng <ruifengz@foxmail.com>\r\nCo-authored-by: zhengruifeng <ruifengz@foxmail.com>\r\nSigned-off-by: Sean Owen <srowen@gmail.com>\r\n\r\n* [MINOR][TESTS][DOCS] Use fully-qualified class name in docker integration test\r\n\r\n### What changes were proposed in this pull request?\r\nchange\r\n```\r\n./build/sbt -Pdocker-integration-tests \"testOnly *xxxIntegrationSuite\"\r\n```\r\nto\r\n```\r\n./build/sbt -Pdocker-integration-tests \"testOnly org.apache.spark.sql.jdbc.xxxIntegrationSuite\"\r\n```\r\n\r\n### Why are the changes needed?\r\nWe only want to start v1 ```xxxIntegrationSuite```, not the newly added```v2.xxxIntegrationSuite```.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\nManually checked\r\n\r\nCloses #30448 from huaxingao/dockertest.\r\n\r\nAuthored-by: Huaxin Gao <huaxing@us.ibm.com>\r\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\r\n\r\n* [SPARK-33492][SQL] DSv2: Append/Overwrite/ReplaceTable should invalidate cache\r\n\r\n### What changes were proposed in this pull request?\r\n\r\nThis adds changes in the following places:\r\n- logic to also refresh caches referencing the target table in v2 `AppendDataExec`, `OverwriteByExpressionExec`, `OverwritePartitionsDynamicExec`, as well as their v1 fallbacks `AppendDataExecV1` and `OverwriteByExpressionExecV1`.\r\n- logic to invalidate caches referencing the target table in v2 `ReplaceTableAsSelectExec` and its atomic version `AtomicReplaceTableAsSelectExec`. These are only supported in v2 at the moment though.\r\n\r\nIn addition to the above, in order to test the v1 write fallback behavior, I extended `InMemoryTableWithV1Fallback` to also support batch reads.\r\n\r\n### Why are the changes needed?\r\n\r\nCurrently in DataSource v2 we don't refresh or invalidate caches referencing the target table when the table content is changed by operations such as append, overwrite, or replace table. This is different from DataSource v1, and could potentially cause data correctness issue if the staled caches are queried later.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nYes. Now When a data source v2 is cached (either directly or indirectly), all the relevant caches will be refreshed or invalidated if the table is replaced.\r\n\r\n### How was this patch tested?\r\n\r\nAdded unit tests for the new code path.\r\n\r\nCloses #30429 from sunchao/SPARK-33492.\r\n\r\nAuthored-by: Chao Sun <sunchao@apple.com>\r\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\r\n\r\n* [SPARK-32670][SQL] Group exception messages in Catalyst Analyzer in one file\r\n\r\n### What changes were proposed in this pull request?\r\n\r\nGroup all messages of `AnalysisExcpetions` created and thrown directly in org.apache.spark.sql.catalyst.analysis.Analyzer in one file.\r\n* Create a new object: `org.apache.spark.sql.CatalystErrors` with many exception-creating functions.\r\n* When the `Analyzer` wants to create and throw a new `AnalysisException`, call functions of `CatalystErrors`\r\n\r\n### Why are the changes needed?\r\n\r\nThis is the sample PR that groups exception messages together in several files. It will largely help with standardization of error messages and its maintenance.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nNo. Error messages remain unchanged.\r\n\r\n### How was this patch tested?\r\n\r\nNo new tests - pass all original tests to make sure it doesn't break any existing behavior.\r\n\r\n### Naming of exception functions\r\n\r\nAll function names ended with `Error`.\r\n* For specific errors like `groupingIDMismatch` and `groupingColInvalid`, directly use them as name, just like `groupingIDMismatchError` and `groupingColInvalidError`.\r\n* For generic errors like `dataTypeMismatch`,\r\n  * if confident with the context, prefix and condition can be added, like `pivotValDataTypeMismatchError`\r\n  * if not sure about the context, add a `For` suffix of the specific component that this exception is related to, like `dataTypeMismatchForDeserializerError`\r\n\r\nCloses #29497 from anchovYu/32670.\r\n\r\nLead-authored-by: anchovYu <aureole@sjtu.edu.cn>\r\nCo-authored-by: anchovYu <xyyu15@gmail.com>\r\nSigned-off-by: HyukjinKwon <gurwls223@apache.org>\r\n\r\n* [SPARK-33223][SS][FOLLOWUP] Clarify the meaning of \"number of rows dropped by watermark\" in SS UI page\r\n\r\n### What changes were proposed in this pull request?\r\n\r\nThis PR fixes the representation to clarify the meaning of \"number of rows dropped by watermark\" in SS UI page.\r\n\r\n### Why are the changes needed?\r\n\r\n`Aggregated Number Of State Rows Dropped By Watermark` says that the dropped rows are from the state, whereas they're not. We say \"evicted from the state\" for the case, which is \"normal\" to emit outputs and reduce memory usage of the state.\r\n\r\nThe metric actually represents the number of \"input\" rows dropped by watermark, and the meaning of \"input\" is relative to the \"stateful operator\". That's a bit confusing as we normally think \"input\" as \"input from source\" whereas it's not.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nYes, UI element & tooltip change.\r\n\r\n### How was this patch tested?\r\n\r\nOnly text change in UI, so we know how thing will be changed intuitively.\r\n\r\nCloses #30439 from HeartSaVioR/SPARK-33223-FOLLOWUP.\r\n\r\nAuthored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>\r\nSigned-off-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>\r\n\r\n* [SPARK-33505][SQL][TESTS] Fix adding new partitions by INSERT INTO `InMemoryPartitionTable`\r\n\r\n### What changes were proposed in this pull request?\r\n1. Add a hook method to `addPartitionKey()` of `InMemoryTable` which is called per every row.\r\n2. Override `addPartitionKey()` in `InMemoryPartitionTable`, and add partition key every time when new row is inserted to the table.\r\n\r\n### Why are the changes needed?\r\nTo be able to write unified tests for datasources V1 and V2. Currently, INSERT INTO a V1 table creates partitions but the same doesn't work for the custom catalog `InMemoryPartitionTableCatalog` used in DSv2 tests.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\nBy running the affected test suite `DataSourceV2SQLSuite`.\r\n\r\nCloses #30449 from MaxGekk/insert-into-InMemoryPartitionTable.\r\n\r\nAuthored-by: Max Gekk <max.gekk@gmail.com>\r\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\r\n\r\n* [SPARK-32381][CORE][FOLLOWUP][TEST-HADOOP2.7] Don't remove SerializableFileStatus and SerializableBlockLocation for Hadoop 2.7\r\n\r\n### What changes were proposed in this pull request?\r\n\r\nRevert the change in #29959 and don't remove `SerializableFileStatus` and `SerializableBlockLocation`.\r\n\r\n### Why are the changes needed?\r\n\r\nIn Hadoop 2.7 `FileStatus` and `BlockLocation` are not serializable, so we still need the two wrapper classes.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nNo\r\n\r\n### How was this patch tested?\r\n\r\nN/A\r\n\r\nCloses #30447 from sunchao/SPARK-32381-followup.\r\n\r\nAuthored-by: Chao Sun <sunchao@apple.com>\r\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\r\n\r\n* Revert \"[SPARK-28704][SQL][TEST] Add back Skiped HiveExternalCatalogVersionsSuite in HiveSparkSubmitSuite at JDK9+\"\r\n\r\nThis reverts commit 47326ac1c6a296a84af76d832061741740ae9f12.\r\n\r\n* [SPARK-33463][SQL] Keep Job Id during incremental collect in Spark Thrift Server\r\n\r\n### What changes were proposed in this pull request?\r\n\r\nWhen enabling **spark.sql.thriftServer.incrementalCollect** Job Ids get lost and tracing queries in Spark Thrift Server ends up being too complicated.\r\n\r\n### Why are the changes needed?\r\n\r\nBecause it will make easier tracing Spark Thrift Server queries.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nNo\r\n\r\n### How was this patch tested?\r\n\r\nThe current tests are enough. No need of more tests.\r\n\r\nCloses #30390 from gumartinm/master.\r\n\r\nAuthored-by: Gustavo Martin Morcuende <gu.martinm@gmail.com>\r\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\r\n\r\n* [SPARK-28704][SQL][TEST] Add back Skiped HiveExternalCatalogVersionsSuite in HiveSparkSubmitSuite at JDK9+\r\n\r\n### What changes were proposed in this pull request?\r\nWe skip test HiveExternalCatalogVersionsSuite when testing with JAVA_9 or later because our previous version does not support JAVA_9 or later. We now add it back since we have a version supports JAVA_9 or later.\r\n\r\n### Why are the changes needed?\r\n\r\nTo recover test coverage.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\nCheck CI logs.\r\n\r\nCloses #30451 from AngersZhuuuu/SPARK-28704.\r\n\r\nAuthored-by: angerszhu <angers.zhu@gmail.com>\r\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\r\n\r\n* [SPARK-31962][SQL] Provide modifiedAfter and modifiedBefore options when filtering from a batch-based file data source\r\n\r\n### What changes were proposed in this pull request?\r\n\r\nTwo new options, _modifiiedBefore_  and _modifiedAfter_, is provided expecting a value in 'YYYY-MM-DDTHH:mm:ss' format.  _PartioningAwareFileIndex_ considers these options during the process of checking for files, just before considering applied _PathFilters_ such as `pathGlobFilter.`  In order to filter file results, a new PathFilter class was derived for this purpose.  General house-keeping around classes extending PathFilter was performed for neatness.  It became apparent support was needed to handle multiple potential path filters.  Logic was introduced for this purpose and the associated tests written.\r\n\r\n### Why are the changes needed?\r\n\r\nWhen loading files from a data source, there can often times be thousands of file within a respective file path.  In many cases I've seen, we want to start loading from a folder path and ideally be able to begin loading files having modification dates past a certain point.  This would mean out of thousands of potential files, only the ones with modification dates greater than the specified timestamp would be considered.  This saves a ton of time automatically and reduces significant complexity managing this in code.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nThis PR introduces an option that can be used with batch-based Spark file data sources.  A documentation update was made to reflect an example and usage of the new data source option.\r\n\r\n**Example Usages**\r\n_Load all CSV files modified after date:_\r\n`spark.read.format(\"csv\").option(\"modifiedAfter\",\"2020-06-15T05:00:00\").load()`\r\n\r\n_Load all CSV files modified before date:_\r\n`spark.read.format(\"csv\").option(\"modifiedBefore\",\"2020-06-15T05:00:00\").load()`\r\n\r\n_Load all CSV files modified between two dates:_\r\n`spark.read.format(\"csv\").option(\"modifiedAfter\",\"2019-01-15T05:00:00\").option(\"modifiedBefore\",\"2020-06-15T05:00:00\").load()\r\n`\r\n\r\n### How was this patch tested?\r\n\r\nA handful of unit tests were added to support the positive, negative, and edge case code paths.\r\n\r\nIt's also live in a handful of our Databricks dev environments.  (quoted from cchighman)\r\n\r\nCloses #30411 from HeartSaVioR/SPARK-31962.\r\n\r\nLead-authored-by: CC Highman <christopher.highman@microsoft.com>\r\nCo-authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>\r\nSigned-off-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>\r\n\r\n* [SPARK-33469][SQL] Add current_timezone function\r\n\r\n### What changes were proposed in this pull request?\r\n\r\nAdd a `CurrentTimeZone` function and replace the value at `Optimizer` side.\r\n\r\n### Why are the changes needed?\r\n\r\nLet user get current timezone easily. Then user can call\r\n```\r\nSELECT current_timezone()\r\n```\r\n\r\nPresto: https://prestodb.io/docs/current/functions/datetime.html\r\nSQL Server: https://docs.microsoft.com/en-us/sql/t-sql/functions/current-timezone-transact-sql?view=sql-server-ver15\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nYes, a new function.\r\n\r\n### How was this patch tested?\r\n\r\nAdd test.\r\n\r\nCloses #30400 from ulysses-you/SPARK-33469.\r\n\r\nLead-authored-by: ulysses <youxiduo@weidian.com>\r\nCo-authored-by: ulysses-you <youxiduo@weidian.com>\r\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\r\n\r\n* [SPARK-33512][BUILD] Upgrade test libraries\r\n\r\n### What changes were proposed in this pull request?\r\n\r\nThis PR aims to update the test libraries.\r\n- ScalaTest: 3.2.0 -> 3.2.3\r\n- JUnit: 4.12 -> 4.13.1\r\n- Mockito: 3.1.0 -> 3.4.6\r\n- JMock: 2.8.4 -> 2.12.0\r\n- maven-surefire-plugin: 3.0.0-M3 -> 3.0.0-M5\r\n- scala-maven-plugin: 4.3.0 -> 4.4.0\r\n\r\n### Why are the changes needed?\r\n\r\nThis will make the test frameworks up-to-date for Apache Spark 3.1.0.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nNo.\r\n\r\n### How was this patch tested?\r\n\r\nPass the CIs.\r\n\r\nCloses #30456 from dongjoon-hyun/SPARK-33512.\r\n\r\nAuthored-by: Dongjoon Hyun <dongjoon@apache.org>\r\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\r\n\r\n* [MINOR][INFRA] Suppress warning in check-license\r\n\r\n### What changes were proposed in this pull request?\r\nThis PR aims to suppress the warning `File exists` in check-license\r\n\r\n### Why are the changes needed?\r\n\r\n**BEFORE**\r\n```\r\n% dev/check-license\r\nAttempting to fetch rat\r\nRAT checks passed.\r\n\r\n% dev/check-license\r\nmkdir: target: File exists\r\nRAT checks passed.\r\n```\r\n\r\n**AFTER**\r\n```\r\n% dev/check-license\r\nAttempting to fetch rat\r\nRAT checks passed.\r\n\r\n% dev/check-license\r\nRAT checks passed.\r\n```\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo.\r\n\r\n### How was this patch tested?\r\nManually do dev/check-license twice.\r\n\r\nCloses #30460 from williamhyun/checklicense.\r\n\r\nAuthored-by: William Hyun <williamhyun3@gmail.com>\r\nSigned-off-by: HyukjinKwon <gurwls223@apache.org>\r\n\r\n* [SPARK-33427][SQL][FOLLOWUP] Put key and value into IdentityHashMap sequantially\r\n\r\n### What changes were proposed in this pull request?\r\n\r\nThis follow-up fixes an issue when inserting key/value pairs into `IdentityHashMap` in `SubExprEvaluationRuntime`.\r\n\r\n### Why are the changes needed?\r\n\r\nThe last commits to #30341 follows review comment to use `IdentityHashMap`. Because we leverage `IdentityHashMap` to compare keys in reference, we should not convert expression pairs to Scala map before inserting. Scala map compares keys by equality so we will loss keys with different references.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nNo\r\n\r\n### How was this patch tested?\r\n\r\nRun benchmark to verify.\r\n\r\nCloses #30459 from viirya/SPARK-33427-map.\r\n\r\nAuthored-by: Liang-Chi Hsieh <viirya@gmail.com>\r\nSigned-off-by: HyukjinKwon <gurwls223@apache.org>\r\n\r\n* [SPARK-33143][PYTHON] Add configurable timeout to python server and client\r\n\r\n### What changes were proposed in this pull request?\r\nSpark creates local server to serialize several type of data for python. The python code tries to connect to the server, immediately after it's created but there are several system calls in between (this may change in each Spark version):\r\n* getaddrinfo\r\n* socket\r\n* settimeout\r\n* connect\r\n\r\nUnder some circumstances in heavy user environments these calls can be super slow (more than 15 seconds). These issues must be analyzed one-by-one but since these are system calls the underlying OS and/or DNS servers must be debugged and fixed. This is not trivial task and at the same time data processing must work somehow. In this PR I'm only intended to add a configuration possibility to increase the mentioned timeouts in order to be able to provide temporary workaround. The rootcause analysis is ongoing but I think this can vary in each case.\r\n\r\nBecause the server part doesn't contain huge amount of log entries to with one can measure time, I've added some.\r\n\r\n### Why are the changes needed?\r\nProvide workaround when localhost python server connection timeout appears.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes, new configuration added.\r\n\r\n### How was this patch tested?\r\nExisting unit tests + manual test.\r\n```\r\n#Compile Spark\r\n\r\necho \"spark.io.encryption.enabled true\" >> conf/spark-defaults.conf\r\necho \"spark.python.authenticate.socketTimeout 10\" >> conf/spark-defaults.conf\r\n\r\n$ ./bin/pyspark\r\nPython 3.8.5 (default, Jul 21 2020, 10:48:26)\r\n[Clang 11.0.3 (clang-1103.0.32.62)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n20/11/20 10:17:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\nSetting default log level to \"WARN\".\r\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\r\n20/11/20 10:17:03 WARN SparkEnv: I/O encryption enabled without RPC encryption: keys will be visible on the wire.\r\nWelcome to\r\n      ____              __\r\n     / __/__  ___ _____/ /__\r\n    _\\ \\/ _ \\/ _ `/ __/  '_/\r\n   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.1.0-SNAPSHOT\r\n      /_/\r\n\r\nUsing Python version 3.8.5 (default, Jul 21 2020 10:48:26)\r\nSpark context Web UI available at http://192.168.0.189:4040\r\nSpark context available as 'sc' (master = local[*], app id = local-1605863824276).\r\nSparkSession available as 'spark'.\r\n>>> sc.setLogLevel(\"TRACE\")\r\n>>> sc.parallelize([0, 2, 3, 4, 6], 5).glom().collect()\r\n20/11/20 10:17:09 TRACE PythonParallelizeServer: Creating listening socket\r\n20/11/20 10:17:09 TRACE PythonParallelizeServer: Setting timeout to 10 sec\r\n20/11/20 10:17:09 TRACE PythonParallelizeServer: Waiting for connection on port 59726\r\n20/11/20 10:17:09 TRACE PythonParallelizeServer: Connection accepted from address /127.0.0.1:59727\r\n20/11/20 10:17:09 TRACE PythonParallelizeServer: Client authenticated\r\n20/11/20 10:17:09 TRACE PythonParallelizeServer: Closing server\r\n...\r\n20/11/20 10:17:10 TRACE SocketFuncServer: Creating listening socket\r\n20/11/20 10:17:10 TRACE SocketFuncServer: Setting timeout to 10 sec\r\n20/11/20 10:17:10 TRACE SocketFuncServer: Waiting for connection on port 59735\r\n20/11/20 10:17:10 TRACE SocketFuncServer: Connection accepted from address /127.0.0.1:59736\r\n20/11/20 10:17:10 TRACE SocketFuncServer: Client authenticated\r\n20/11/20 10:17:10 TRACE SocketFuncServer: Closing server\r\n[[0], [2], [3], [4], [6]]\r\n>>>\r\n```\r\n\r\nCloses #30389 from gaborgsomogyi/SPARK-33143.\r\n\r\nLead-authored-by: Gabor Somogyi <gabor.g.somogyi@gmail.com>\r\nCo-authored-by: Hyukjin Kwon <gurwls223@gmail.com>\r\nCo-authored-by: HyukjinKwon <gurwls223@apache.org>\r\nSigned-off-by: HyukjinKwon <gurwls223@apache.org>\r\n\r\n* [SPARK-33510][BUILD] Update SBT to 1.4.4\r\n\r\n### What changes were proposed in this pull request?\r\nThis PR aims to update SBT from 1.4.2 to 1.4.4.\r\n\r\n### Why are the changes needed?\r\n\r\nThis will bring the latest bug fixes.\r\n- https://github.com/sbt/sbt/releases/tag/v1.4.3\r\n- https://github.com/sbt/sbt/releases/tag/v1.4.4\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo.\r\n\r\n### How was this patch tested?\r\nPass the CIs.\r\n\r\nCloses #30453 from williamhyun/sbt143.\r\n\r\nAuthored-by: William Hyun <williamhyun3@gmail.com>\r\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\r\n\r\n* Revert \"[SPARK-32481][CORE][SQL] Support truncate table to move data to trash\"\r\n\r\n### What changes were proposed in this pull request?\r\n\r\nThis reverts commit 065f17386d1851d732b4c1badf1ce2e14d0de338, which is not part of any released version. That is, this is an unreleased feature\r\n\r\n### Why are the changes needed?\r\n\r\nI like the concept of Trash, but I think this PR might just resolve a very specific issue by introducing a mechanism without a proper design doc. This could make the usage more complex.\r\n\r\nI think we need to consider the big picture. Trash directory is an important concept. If we decide to introduce it, we should consider all the code paths of Spark SQL that could delete the data, instead of Truncate only. We also need to consider what is the current behavior if the underlying file system does not provide the API `Trash.moveToAppropriateTrash`. Is the exception good? How about the performance when users are using the object store instead of HDFS? Will it impact the GDPR compliance?\r\n\r\nIn sum, I think we should not merge the PR https://github.com/apache/spark/pull/29552 without the design doc and implementation plan. That is why I reverted it before the code freeze of Spark 3.1\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nReverted the original commit\r\n\r\n### How was this patch tested?\r\nThe existing tests.\r\n\r\nCloses #30463 from gatorsmile/revertSpark-32481.\r\n\r\nAuthored-by: Xiao Li <gatorsmile@gmail.com>\r\nSigned-off-by: HyukjinKwon <gurwls223@apache.org>\r\n\r\n* [SPARK-33515][SQL] Improve exception messages while handling UnresolvedTable\r\n\r\n### What changes were proposed in this pull request?\r\n\r\nThis PR proposes to improve the exception messages while `UnresolvedTable` is handled based on this suggestion: https://github.com/apache/spark/pull/30321#discussion_r521127001.\r\n\r\nCurrently, when an identifier is resolved to a view when a table is expected, the following exception message is displayed (e.g., for `COMMENT ON TABLE`):\r\n```\r\nv is a temp view not table.\r\n```\r\nAfter this PR, the message will be:\r\n```\r\nv is a temp view. 'COMMENT ON TABLE' expects a table.\r\n```\r\n\r\nAlso, if an identifier is not resolved, the following exception message is currently used:\r\n```\r\nTable not found: t\r\n```\r\nAfter this PR, the message will be:\r\n```\r\nTable not found for 'COMMENT ON TABLE': t\r\n```\r\n\r\n### Why are the changes needed?\r\n\r\nTo improve the exception message.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nYes, the exception message will be changed as described above.\r\n\r\n### How was this patch tested?\r\n\r\nUpdated existing tests.\r\n\r\nCloses #30461 from imback82/unresolved_table_message.\r\n\r\nAuthored-by: Terry Kim <yuminkim@gmail.com>\r\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\r\n\r\n* [SPARK-33511][SQL] Respect case sensitivity while resolving V2 partition specs\r\n\r\n### What changes were proposed in this pull request?\r\n1. Pre-process partition specs in `ResolvePartitionSpec`, and convert partition names according to the partition schema and the SQL config `spark.sql.caseSensitive`. In the PR, I propose to invoke `normalizePartitionSpec` for that. The function is used in DSv1 commands, so, the behavior will be similar to DSv1.\r\n2. Move `normalizePartitionSpec()` from `sql/core/.../datasources/PartitioningUtils` to `sql/catalyst/.../util/PartitioningUtils` to use it in Catalyst's rule `ResolvePartitionSpec`\r\n\r\n### Why are the changes needed?\r\nDSv1 commands like `ALTER TABLE .. ADD PARTITION` and `ALTER TABLE .. DROP PARTITION` respect the SQL config `spark.sql.caseSensitive` while resolving partition specs. For example:\r\n```sql\r\nspark-sql> CREATE TABLE tbl1 (id bigint, data string) USING parquet PARTITIONED BY (id);\r\nspark-sql> ALTER TABLE tbl1 ADD PARTITION (ID=1);\r\nspark-sql> SHOW PARTITIONS tbl1;\r\nid=1\r\n```\r\nThe same command fails on V2 Table catalog with error:\r\n```\r\nAnalysisException: Partition key ID not exists\r\n```\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes. After the changes, partition spec resolution works as for DSv1 (without the exception showed above).\r\n\r\n### How was this patch tested?\r\nBy running `AlterTablePartitionV2SQLSuite`.\r\n\r\nCloses #30454 from MaxGekk/partition-spec-case-sensitivity.\r\n\r\nAuthored-by: Max Gekk <max.gekk@gmail.com>\r\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\r\n\r\n* [SPARK-33278][SQL][FOLLOWUP] Improve OptimizeWindowFunctions to avoid transfer first to nth_value\r\n\r\n### What changes were proposed in this pull request?\r\nhttps://github.com/apache/spark/pull/30178 provided `OptimizeWindowFunctions` used to transfer `first` to `nth_value`.\r\nIf the window frame is `UNBOUNDED PRECEDING AND CURRENT ROW` or `UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING`, `nth_value` has better performance than `first`.\r\nBut the `OptimizeWindowFunctions` need to exclude other window frame.\r\n\r\n### Why are the changes needed?\r\n Improve `OptimizeWindowFunctions` to avoid transfer `first` to `nth_value` if the specified window frame isn't `UNBOUNDED PRECEDING AND CURRENT ROW` or `UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING`.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n'No'.\r\n\r\n### How was this patch tested?\r\nJenkins test.\r\n\r\nCloses #30419 from beliefer/SPARK-33278_followup.\r\n\r\nLead-authored-by: gengjiaan <gengjiaan@360.cn>\r\nCo-authored-by: beliefer <beliefer@163.com>\r\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\r\n\r\nCo-authored-by: Chao Sun <sunchao@apple.com>\r\nCo-authored-by: Gabor Somogyi <gabor.g.somogyi@gmail.com>\r\nCo-authored-by: HyukjinKwon <gurwls223@apache.org>\r\nCo-authored-by: Venkata krishnan Sowrirajan <vsowrirajan@linkedin.com>\r\nCo-authored-by: Min Shen <mshen@linkedin.com>\r\nCo-authored-by: yangjie01 <yangjie01@baidu.com>\r\nCo-authored-by: Max Gekk <max.gekk@gmail.com>\r\nCo-authored-by: liucht <liucht@inspur.com>\r\nCo-authored-by: ulysses <youxiduo@weidian.com>\r\nCo-authored-by: angerszhu <angers.zhu@gmail.com>\r\nCo-authored-by: Ruifeng Zheng <ruifengz@foxmail.com>\r\nCo-authored-by: Huaxin Gao <huaxing@us.ibm.com>\r\nCo-authored-by: anchovYu <aureole@sjtu.edu.cn>\r\nCo-authored-by: anchovYu <xyyu15@gmail.com>\r\nCo-authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>\r\nCo-authored-by: Dongjoon Hyun <dongjoon@apache.org>\r\nCo-authored-by: Gustavo Martin Morcuende <gu.martinm@gmail.com>\r\nCo-authored-by: CC Highman <christopher.highman@microsoft.com>\r\nCo-authored-by: William Hyun <williamhyun3@gmail.com>\r\nCo-authored-by: Liang-Chi Hsieh <viirya@gmail.com>\r\nCo-authored-by: Hyukjin Kwon <gurwls223@gmail.com>\r\nCo-authored-by: Xiao Li <gatorsmile@gmail.com>\r\nCo-authored-by: Terry Kim <yuminkim@gmail.com>\r\nCo-authored-by: gengjiaan <gengjiaan@360.cn>\r\nCo-authored-by: beliefer <beliefer@163.com>", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.jdbc.MariaDBKrbIntegrationSuite", "org.apache.spark.sql.TPCHPlanStabilitySuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala-2.12/org/apache/spark/sql/catalyst/expressions/ConstraintSetImplicit.scala", "additions": "36", "deletions": "0", "changes": "36"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala-2.12/org/apache/spark/sql/catalyst/expressions/ExpressionMap.scala", "additions": "65", "deletions": "0", "changes": "65"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala-2.13/org/apache/spark/sql/catalyst/expressions/ConstraintSetImplicit.scala", "additions": "36", "deletions": "0", "changes": "36"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala-2.13/org/apache/spark/sql/catalyst/expressions/ExpressionMap.scala", "additions": "57", "deletions": "0", "changes": "57"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ConstraintSet.scala", "additions": "1515", "deletions": "0", "changes": "1515"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ExpressionSet.scala", "additions": "52", "deletions": "18", "changes": "70"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/predicates.scala", "additions": "12", "deletions": "0", "changes": "12"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "184", "deletions": "17", "changes": "201"}, "updated": [1, 5, 12]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/LogicalPlan.scala", "additions": "7", "deletions": "1", "changes": "8"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/QueryPlanConstraints.scala", "additions": "18", "deletions": "10", "changes": "28"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala", "additions": "33", "deletions": "24", "changes": "57"}, "updated": [0, 3, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [1, 10, 27]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/InferFiltersFromConstraintsSuite.scala", "additions": "10", "deletions": "2", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/plans/ConstraintPropagationSuite.scala", "additions": "39", "deletions": "13", "changes": "52"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/plans/OptimizedConstraintPropagationSuite.scala", "additions": "1947", "deletions": "0", "changes": "1947"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/plans/PlanTest.scala", "additions": "6", "deletions": "5", "changes": "11"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/plans/SQLHelper.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q75.sf100/SPARK-33152_explain.txt", "additions": "752", "deletions": "0", "changes": "752"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q75.sf100/SPARK-33152_simplified.txt", "additions": "237", "deletions": "0", "changes": "237"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q75/SPARK-33152_explain.txt", "additions": "647", "deletions": "0", "changes": "647"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q75/SPARK-33152_simplified.txt", "additions": "180", "deletions": "0", "changes": "180"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q75.sf100/SPARK-33152_explain.txt", "additions": "750", "deletions": "0", "changes": "750"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q75.sf100/SPARK-33152_simplified.txt", "additions": "238", "deletions": "0", "changes": "238"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q75/SPARK-33152_explain.txt", "additions": "750", "deletions": "0", "changes": "750"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q75/SPARK-33152_simplified.txt", "additions": "238", "deletions": "0", "changes": "238"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpch-plan-stability/q1/SPARK-33152_explain.txt", "additions": "56", "deletions": "0", "changes": "56"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpch-plan-stability/q1/SPARK-33152_simplified.txt", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpch-plan-stability/q19/SPARK-33152_explain.txt", "additions": "79", "deletions": "0", "changes": "79"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpch-plan-stability/q19/SPARK-33152_simplified.txt", "additions": "20", "deletions": "0", "changes": "20"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpch-plan-stability/q9/SPARK-33152_explain.txt", "additions": "221", "deletions": "0", "changes": "221"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpch-plan-stability/q9/SPARK-33152_simplified.txt", "additions": "60", "deletions": "0", "changes": "60"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/PlanStabilitySuite.scala", "additions": "56", "deletions": "14", "changes": "70"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/internal/ExecutorSideSQLConfSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/test/SQLTestUtils.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/HiveSerDeSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "venkata91", "sha": "13ad530ca6ca574336f15192fab584fe6400cf24", "commit_date": "2021/09/02 04:53:49", "commit_message": "SPARK-33701: Adaptive shuffle merge finalization", "title": "", "body": "", "failed_tests": ["org.apache.spark.shuffle.ShuffleBlockPusherSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/Dependency.scala", "additions": "30", "deletions": "7", "changes": "37"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/MapOutputTracker.scala", "additions": "68", "deletions": "0", "changes": "68"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/SparkEnv.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 1, 6]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala", "additions": "238", "deletions": "64", "changes": "302"}, "updated": [0, 0, 4]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerEvent.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/StageInfo.scala", "additions": "14", "deletions": "2", "changes": "16"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessage.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [1, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/shuffle/ShuffleBlockPusher.scala", "additions": "30", "deletions": "0", "changes": "30"}, "updated": [0, 0, 7]}, {"file": {"name": "core/src/main/scala/org/apache/spark/shuffle/ShuffleWriteProcessor.scala", "additions": "13", "deletions": "0", "changes": "13"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/ThreadUtils.scala", "additions": "9", "deletions": "1", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/MapOutputTrackerSuite.scala", "additions": "28", "deletions": "0", "changes": "28"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/DAGSchedulerSuite.scala", "additions": "284", "deletions": "3", "changes": "287"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/test/scala/org/apache/spark/shuffle/ShuffleBlockPusherSuite.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 0, 5]}]}
{"author": "Peng-Lei", "sha": "920b59511c48e74edf2bcb6bad9a790170a7b296", "commit_date": "2021/09/23 07:12:40", "commit_message": "add draft", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "R/pkg/R/utils.R", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/resources/error/error-classes.json", "additions": "73", "deletions": "0", "changes": "73"}, "updated": [0, 2, 4]}, {"file": {"name": "core/src/main/scala/org/apache/spark/SparkException.scala", "additions": "40", "deletions": "2", "changes": "42"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala", "additions": "74", "deletions": "78", "changes": "152"}, "updated": [0, 2, 4]}]}
{"author": "Peng-Lei", "sha": "1fbe12a3142a7c075fdeef692423d61c14bdecdb", "commit_date": "2021/09/24 07:02:49", "commit_message": "Simplify code", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "docs/sql-ref-ansi-compliance.md", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/CatalogManager.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala", "additions": "13", "deletions": "0", "changes": "13"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/command/SetCatalogCommand.scala", "additions": "33", "deletions": "0", "changes": "33"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/DDLParserSuite.scala", "additions": "12", "deletions": "0", "changes": "12"}, "updated": [0, 0, 0]}]}
{"author": "dohongdayi", "sha": "e578d1c9491c615308f92302cd9300a81cc1b203", "commit_date": "2021/10/01 06:02:37", "commit_message": "Merge branch 'apache:master' into use_branch", "title": "[SPARK-36849][SQL] Migrate UseStatement to v2 command framework", "body": "What changes were proposed in this pull request?\r\nMigrate UseStatement to v2 command framework\r\n\r\nWhy are the changes needed?\r\nMigrate to the standard V2 framework\r\n\r\nDoes this PR introduce any user-facing change?\r\nno\r\n\r\nHow was this patch tested?\r\nexisting tests", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveCatalogs.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [1, 3, 9]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/statements.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2Commands.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [1, 2, 3]}]}
{"author": "itholic", "sha": "5c9b1682a528daffde759726369051adee1ab723", "commit_date": "2021/10/01 05:49:06", "commit_message": "Handling None", "title": "[SPARK-36438][PYTHON] Support list-like Python objects for Series comparison", "body": "### What changes were proposed in this pull request?\r\n\r\nThis PR proposes to implement `Series` comparison with list-like Python objects.\r\n\r\nCurrently `Series` doesn't support the comparison to list-like Python objects such as `list`, `tuple`, `dict`, `set`.\r\n\r\n**Before**\r\n\r\n```python\r\n>>> psser\r\n0    1\r\n1    2\r\n2    3\r\ndtype: int64\r\n\r\n>>> psser == [3, 2, 1]\r\nTraceback (most recent call last):\r\n...\r\nTypeError: The operation can not be applied to list.\r\n...\r\n```\r\n\r\n**After**\r\n\r\n```python\r\n>>> psser\r\n0    1\r\n1    2\r\n2    3\r\ndtype: int64\r\n\r\n>>> psser == [3, 2, 1]\r\n0    False\r\n1     True\r\n2    False\r\ndtype: bool\r\n```\r\n\r\nThis was originally proposed in https://github.com/databricks/koalas/pull/2022, and all reviews in origin PR has been resolved.\r\n\r\n### Why are the changes needed?\r\n\r\nTo follow pandas' behavior.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nYes, the `Series` comparison with list-like Python objects now possible.\r\n\r\n\r\n### How was this patch tested?\r\n\r\nUnittests\r\n", "failed_tests": ["pyspark.pandas.tests.data_type_ops.test_categorical_ops"], "files": [{"file": {"name": "python/pyspark/pandas/base.py", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 2, 3]}, {"file": {"name": "python/pyspark/pandas/data_type_ops/base.py", "additions": "83", "deletions": "4", "changes": "87"}, "updated": [0, 0, 3]}, {"file": {"name": "python/pyspark/pandas/series.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 3, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames.py", "additions": "37", "deletions": "0", "changes": "37"}, "updated": [0, 2, 5]}, {"file": {"name": "python/pyspark/pandas/tests/test_series.py", "additions": "36", "deletions": "0", "changes": "36"}, "updated": [0, 3, 8]}]}
{"author": "xkrogen", "sha": "92e296b245f5ff0f446f46f1888ddfdc900a2044", "commit_date": "2021/06/25 13:53:57", "commit_message": "[SPARK-35672][CORE][YARN] Pass user classpath entries to executors using config instead of command line\n\n### What changes were proposed in this pull request?\nRefactor the logic for constructing the user classpath from `yarn.ApplicationMaster` into `yarn.Client` so that it can be leveraged on the executor side as well, instead of having the driver construct it and pass it to the executor via command-line arguments. A new method, `getUserClassPath`, is added to `CoarseGrainedExecutorBackend` which defaults to `Nil` (consistent with the existing behavior where non-YARN resource managers do not configure the user classpath). `YarnCoarseGrainedExecutorBackend` overrides this to construct the user classpath from the existing `APP_JAR` and `SECONDARY_JARS` configs.\n\n### Why are the changes needed?\nUser-provided JARs are made available to executors using a custom classloader, so they do not appear on the standard Java classpath. Instead, they are passed as a list to the executor which then creates a classloader out of the URLs. Currently in the case of YARN, this list of JARs is crafted by the Driver (in `ExecutorRunnable`), which then passes the information to the executors (`CoarseGrainedExecutorBackend`) by specifying each JAR on the executor command line as `--user-class-path /path/to/myjar.jar`. This can cause extremely long argument lists when there are many JARs, which can cause the OS argument length to be exceeded, typically manifesting as the error message:\n\n> /bin/bash: Argument list too long\n\nA [Google search](https://www.google.com/search?q=spark%20%22%2Fbin%2Fbash%3A%20argument%20list%20too%20long%22&oq=spark%20%22%2Fbin%2Fbash%3A%20argument%20list%20too%20long%22) indicates that this is not a theoretical problem and afflicts real users, including ours. Passing this list using the configurations instead resolves this issue.\n\n### Does this PR introduce _any_ user-facing change?\nNo, except for fixing the bug, allowing for larger JAR lists to be passed successfully. Configuration of JARs is identical to before.\n\n### How was this patch tested?\nNew unit tests were added in `YarnClusterSuite`. Also, we have been running a similar fix internally for 4 months with great success.\n\nCloses #32810 from xkrogen/xkrogen-SPARK-35672-classpath-scalable.\n\nAuthored-by: Erik Krogen <xkrogen@apache.org>\nSigned-off-by: Thomas Graves <tgraves@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala", "additions": "6", "deletions": "11", "changes": "17"}, "updated": [1, 2, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/executor/Executor.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 1, 3]}, {"file": {"name": "core/src/test/scala/org/apache/spark/executor/CoarseGrainedExecutorBackendSuite.scala", "additions": "8", "deletions": "9", "changes": "17"}, "updated": [1, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesExecutorBackend.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala", "additions": "3", "deletions": "6", "changes": "9"}, "updated": [1, 1, 1]}, {"file": {"name": "resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala", "additions": "89", "deletions": "4", "changes": "93"}, "updated": [1, 2, 2]}, {"file": {"name": "resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnable.scala", "additions": "0", "deletions": "12", "changes": "12"}, "updated": [1, 1, 1]}, {"file": {"name": "resource-managers/yarn/src/main/scala/org/apache/spark/executor/YarnCoarseGrainedExecutorBackend.scala", "additions": "5", "deletions": "3", "changes": "8"}, "updated": [1, 1, 1]}, {"file": {"name": "resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/ClientSuite.scala", "additions": "74", "deletions": "0", "changes": "74"}, "updated": [1, 1, 1]}, {"file": {"name": "resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/YarnClusterSuite.scala", "additions": "68", "deletions": "9", "changes": "77"}, "updated": [1, 1, 1]}]}
{"author": "tanelk", "sha": "56fbf155f6d95afc2c6f6a1eac280ba5dc285668", "commit_date": "2020/09/17 19:46:53", "commit_message": "Bitwise operations are commutative", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/First.scala", "additions": "0", "deletions": "3", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Last.scala", "additions": "0", "deletions": "3", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/collect.scala", "additions": "0", "deletions": "4", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 3, 9]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/EliminateDistinctSuite.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/FilterPushdownSuite.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/UnwrapCastInBinaryComparisonSuite.scala", "additions": "3", "deletions": "4", "changes": "7"}, "updated": [1, 2, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala", "additions": "20", "deletions": "9", "changes": "29"}, "updated": [0, 0, 3]}]}
{"author": "mkaravel", "sha": "36cf3c2f5c0edad8663bf4261d2a117cf7aab9df", "commit_date": "2021/09/30 06:26:37", "commit_message": "[WIP] Add lpad and rpad functions for binary strings", "title": "[WIP][SQL] Add lpad and rpad functions for binary strings", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\n", "failed_tests": ["pyspark.tests.test_worker"], "files": [{"file": {"name": "common/unsafe/src/main/java/org/apache/spark/unsafe/types/ByteArray.java", "additions": "78", "deletions": "0", "changes": "78"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala", "additions": "64", "deletions": "15", "changes": "79"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/functions.scala", "additions": "22", "deletions": "0", "changes": "22"}, "updated": [0, 2, 5]}]}
{"author": "mkaravel", "sha": "9339974fbfc9effded8c26cbdf47cfb55d197545", "commit_date": "2021/09/21 06:46:15", "commit_message": "[SPARK-38611][SQL] Add SQL functions for the BINARY data type for AND, OR, XOR, NOT", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "common/unsafe/src/main/java/org/apache/spark/unsafe/types/ByteArray.java", "additions": "91", "deletions": "0", "changes": "91"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala", "additions": "170", "deletions": "0", "changes": "170"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/functions.scala", "additions": "41", "deletions": "0", "changes": "41"}, "updated": [1, 3, 5]}, {"file": {"name": "sql/core/src/test/resources/sql-functions/sql-expression-schema.md", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/string-functions.sql", "additions": "39", "deletions": "1", "changes": "40"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/string-functions.sql.out", "additions": "241", "deletions": "1", "changes": "242"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/string-functions.sql.out", "additions": "241", "deletions": "1", "changes": "242"}, "updated": [0, 0, 0]}]}
{"author": "JkSelf", "sha": "da23eec29c2b06b058993b3835f35dceef963739", "commit_date": "2021/09/30 01:51:56", "commit_message": "Make the shuffle hash join factor configurable", "title": "[SPARK-36898][SQL] Make the shuffle hash join factor configurable", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\nCurrently the shuffle hash join factor is 3x, which cannot be changed. This PR make the factor can be configurable by adding a new config \"spark.sql.shuffleHashJoinFactor\".\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\n\r\n### Why are the changes needed?\r\nMake the choice of shuffle hash join more flexible\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nAdd new config\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\n\r\n### How was this patch tested?\r\nExisting tests.\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [1, 2, 8]}]}
{"author": "imback82", "sha": "94fa7d13d3fdd0305e4de50a6af919dbdc4d37c0", "commit_date": "2021/09/06 21:03:46", "commit_message": "initial commit", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.execution.command.ShowTablesSuite", "org.apache.spark.sql.hive.ListTablesSuite", "org.apache.spark.sql.hive.execution.command.DropTableSuite", "org.apache.spark.sql.SQLContextSuite", "org.apache.spark.sql.execution.GlobalTempViewSuite", "org.apache.spark.sql.execution.command.v1.ShowTablesSuite", "org.apache.spark.sql.execution.QueryExecutionSuite", "org.apache.spark.sql.execution.command.v1.DropTableSuite", "org.apache.spark.sql.execution.command.v2.DropTableSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 2, 19]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/KeepLegacyOutputs.scala", "additions": "42", "deletions": "0", "changes": "42"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 1, 12]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ShowTablesExec.scala", "additions": "9", "deletions": "2", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2SessionCatalog.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/QueryExecutionSuite.scala", "additions": "3", "deletions": "7", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/ShowTablesSuiteBase.scala", "additions": "7", "deletions": "1", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/v1/ShowTablesSuite.scala", "additions": "26", "deletions": "2", "changes": "28"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/command/ShowTablesSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "sleep1661", "sha": "1e3db06a929ff756fcfb356801c1b3a4354ddc1b", "commit_date": "2021/08/31 09:46:51", "commit_message": "[SPARK-36575][CORE] Should ignore task finished event if its task set is gone in TaskSchedulerImpl.handleSuccessfulTask", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala", "additions": "7", "deletions": "1", "changes": "8"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/TaskSchedulerImplSuite.scala", "additions": "86", "deletions": "1", "changes": "87"}, "updated": [0, 0, 0]}]}
{"author": "PengleiShi", "sha": "a85e0b15e9e46050d045c7f387009a1561a8dddf", "commit_date": "2021/09/29 07:26:55", "commit_message": "[SPARK-33887][SQL] Allow insert overwrite same table with static partition if using dynamic partition overwrite mode", "title": "[SPARK-33887][SQL] Allow insert overwrite same table with static partition if using dynamic partition overwrite mode", "body": "\r\n<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\nFor dynamic partition overwrite, we do not delete partition directories ahead, We write to staging directories and move to final partition directories after writing job is done. So it is ok to have outputPath try to overwrite inputpath. In view of this, i think inserting static partition in dynamic partition overwrite mode is also supported, i adjust the judging conditions of `dynamicPartitionOverwrite` to allow this.\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\n\r\n### Why are the changes needed?\r\nIn some cases, we need overwrite static partition using itself.\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\n\r\n### How was this patch tested?\r\nUT\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/sources/InsertSuite.scala", "additions": "43", "deletions": "0", "changes": "43"}, "updated": [0, 0, 0]}]}
{"author": "zhengruifeng", "sha": "07588a06181667d101c91cedc0b6fa61f4c45100", "commit_date": "2021/07/23 07:10:07", "commit_message": "nit\n\nnit", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/GeneralSkewSuite.scala", "additions": "419", "deletions": "0", "changes": "419"}, "updated": [0, 0, 0]}]}
{"author": "zhengruifeng", "sha": "c1d3e48827caab6b616a2c6c4fb8bed4ab9888a0", "commit_date": "2021/08/16 07:41:07", "commit_message": "init", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "19", "deletions": "0", "changes": "19"}, "updated": [0, 3, 17]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "375", "deletions": "138", "changes": "513"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/BaseAggregateExec.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/SortAggregateExec.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowExec.scala", "additions": "14", "deletions": "7", "changes": "21"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/WholeStageCodegenSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "227", "deletions": "0", "changes": "227"}, "updated": [0, 3, 18]}]}
{"author": "grantjenks", "sha": "ad978fe0bd6a390224a89826778257d49da2851b", "commit_date": "2021/09/23 21:15:44", "commit_message": "Add docs about using Shiv for packaging (similar to PEX)\n\nThe Shiv packaging tool works similarly to PEX and can be used to distribute\nPython with its dependencies in an executable. These changes mention Shiv and\ndemonstrate it's use similar to the PEX project.", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/docs/source/user_guide/python_packaging.rst", "additions": "34", "deletions": "11", "changes": "45"}, "updated": [0, 0, 0]}]}
{"author": "Swinky", "sha": "f61f68355ad06fad9ae893b97e84d3d97fe562ac", "commit_date": "2021/09/21 11:02:04", "commit_message": "remove extra DPP filters", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/dynamicpruning/PartitionPruning.scala", "additions": "49", "deletions": "7", "changes": "56"}, "updated": [0, 2, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DynamicPartitionPruningSuite.scala", "additions": "89", "deletions": "0", "changes": "89"}, "updated": [0, 2, 3]}]}
{"author": "ivoson", "sha": "a33d6ef8952240c44ca2f23e802f350627eb8a89", "commit_date": "2021/09/03 02:17:20", "commit_message": "Expose execution id to QueryExecutionListener", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/util/QueryExecutionListener.scala", "additions": "45", "deletions": "10", "changes": "55"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/util/DataFrameCallbackSuite.scala", "additions": "38", "deletions": "0", "changes": "38"}, "updated": [0, 0, 0]}]}
{"author": "xkrogen", "sha": "bfb98833ddd60db80cc6b122861baed6639da4a6", "commit_date": "2021/10/26 22:16:21", "commit_message": "remove major version > 3 check", "title": "[SPARK-37121] [HIVE][TEST] Fix Python version detection bug in TestUtils used by HiveExternalCatalogVersionsSuite", "body": "### What changes were proposed in this pull request?\r\nFix a bug in `TestUtils.isPythonVersionAtLeast38` to allow for `HiveExternalCatalogVersionsSuite` to test against Spark 2.x releases in environments with Python <= 3.7.\r\n\r\n### Why are the changes needed?\r\nThe logic in `TestUtils.isPythonVersionAtLeast38` was added in #30044 to prevent Spark 2.4 from being run in an environment where the Python3 version installed was >= Python 3.8, which is not compatible with Spark 2.4. However, this method always returns true, so only Spark 3.x versions will ever be included in the version set for `HiveExternalCatalogVersionsSuite`, regardless of the system-installed version of Python.\r\n\r\nThe problem is here:\r\nhttps://github.com/apache/spark/blob/951efb80856e2a92ba3690886c95643567dae9d0/core/src/main/scala/org/apache/spark/TestUtils.scala#L280-L291\r\nIt's trying to evaluate the version of Python using a `ProcessLogger`, but the logger accepts a `String => Unit` function, i.e., it does not make use of the return value in any way (since it's meant for logging). So the result of the `startsWith` checks are thrown away, and `attempt.isSuccess && attempt.get == 0` will always be true as long as your system has a `python3` binary (of any version).\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo, test changes only.\r\n\r\n### How was this patch tested?\r\nConfirmed by checking that `HiveExternalCatalogVersionsSuite` downloads binary distros for Spark 2.x lines as well as 3.x when I symlink my `python3` to Python 3.7, and only downloads distros for the 3.x lines when I symlink my `python3` to Python 3.9.\r\n\r\n```bash\r\nbrew link --force python@3.7\r\n# run HiveExternalCatalogVersionsSuite and validate that 2.x and 3.x tests get executed\r\nbrew unlink python@3.7\r\nbrew link --force python@3.9\r\n# run HiveExternalCatalogVersionsSuite and validate that only 3.x tests get executed\r\n```", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/TestUtils.scala", "additions": "9", "deletions": "9", "changes": "18"}, "updated": [0, 0, 0]}]}
{"author": "huaxingao", "sha": "1293ae026dcdc05065c13b6e1a8496a7e2e8082c", "commit_date": "2021/10/18 19:07:12", "commit_message": "fix comment", "title": "[SPARK-36647][SQL][TESTS] Push down Aggregate (Min/Max/Count) for Parquet if filter is on partition col", "body": "### What changes were proposed in this pull request?\r\nI just realized that with the changes in https://github.com/apache/spark/pull/33650, the restriction for not pushing down Min/Max/Count for partition filter was already removed. This PR just added test to make sure Min/Max/Count in parquet are pushed down if filter is on partition col.\r\n\r\n\r\n### Why are the changes needed?\r\nTo complete the work for Aggregate (Min/Max/Count) push down for Parquet\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n\r\n### How was this patch tested?\r\nnew test\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetScanBuilder.scala", "additions": "3", "deletions": "4", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetAggregatePushDownSuite.scala", "additions": "30", "deletions": "10", "changes": "40"}, "updated": [0, 1, 1]}]}
{"author": "huaxingao", "sha": "08f57cd602bd7ebe0a04c9ba2def748d8e534ece", "commit_date": "2021/10/25 18:00:18", "commit_message": "simplify push down limit logging", "title": "[SPARK-37020][SQL] DS V2 LIMIT push down", "body": "\r\n\r\n### What changes were proposed in this pull request?\r\nPush down limit to data source for better performance\r\n\r\n\r\n### Why are the changes needed?\r\nFor LIMIT, e.g. `SELECT * FROM table LIMIT 10`, Spark retrieves all the data from table and then returns 10 rows. If we can push LIMIT to data source side, the data transferred to Spark will be dramatically reduced.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes. new interface `SupportsPushDownLimit`\r\n\r\n\r\n### How was this patch tested?\r\nnew test\r\n", "failed_tests": [], "files": [{"file": {"name": "docs/sql-data-sources-jdbc.md", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/read/ScanBuilder.java", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/read/SupportsPushDownLimit.java", "additions": "36", "deletions": "0", "changes": "36"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD.scala", "additions": "11", "deletions": "4", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownUtils.scala", "additions": "18", "deletions": "1", "changes": "19"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala", "additions": "37", "deletions": "5", "changes": "42"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCScan.scala", "additions": "14", "deletions": "3", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/jdbc/DerbyDialect.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala", "additions": "12", "deletions": "0", "changes": "12"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/jdbc/MsSqlServerDialect.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/jdbc/TeradataDialect.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala", "additions": "52", "deletions": "2", "changes": "54"}, "updated": [0, 0, 1]}]}
{"author": "huaxingao", "sha": "09e4e9a369dd8edd324af967e88cf83c9e32d252", "commit_date": "2021/10/19 23:45:39", "commit_message": "add REPEATABLE in sql-ref-ansi-compliance.md", "title": "[SPARK-37038][SQL][WIP] DSV2 Sample Push Down", "body": "\r\n### What changes were proposed in this pull request?\r\nPush down Sample to data source for better performance. If Sample is pushed down, it will be removed from logical plan so it will not be applied at Spark any more.\r\n\r\nCurrent Plan without Sample push down:\r\n\r\n```\r\n== Parsed Logical Plan ==\r\n'Project [*]\r\n+- 'Sample 0.0, 0.8, false, 157\r\n   +- 'UnresolvedRelation [postgresql, new_table], [], false\r\n\r\n== Analyzed Logical Plan ==\r\ncol1: int, col2: int\r\nProject [col1#163, col2#164]\r\n+- Sample 0.0, 0.8, false, 157\r\n   +- SubqueryAlias postgresql.new_table\r\n      +- RelationV2[col1#163, col2#164] new_table\r\n\r\n== Optimized Logical Plan ==\r\nSample 0.0, 0.8, false, 157\r\n+- RelationV2[col1#163, col2#164] new_table\r\n\r\n== Physical Plan ==\r\n*(1) Sample 0.0, 0.8, false, 157\r\n+- *(1) Scan org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCScan$$anon$1@6dde4769 [col1#163,col2#164] PushedAggregates: [], PushedFilters: [], PushedGroupby: [], PushedLimit: [], PushedSample: TABLESAMPLE  0.0 0.8 false 157, ReadSchema: struct<col1:int,col2:int>\r\n\r\n```\r\nafter Sample push down:\r\n\r\n```\r\n== Parsed Logical Plan ==\r\n'Project [*]\r\n+- 'Sample 0.0, 0.8, false, 187\r\n   +- 'UnresolvedRelation [postgresql, new_table], [], false\r\n\r\n== Analyzed Logical Plan ==\r\ncol1: int, col2: int\r\nProject [col1#163, col2#164]\r\n+- Sample 0.0, 0.8, false, 187\r\n   +- SubqueryAlias postgresql.new_table\r\n      +- RelationV2[col1#163, col2#164] new_table\r\n\r\n== Optimized Logical Plan ==\r\nRelationV2[col1#163, col2#164] new_table\r\n\r\n== Physical Plan ==\r\n*(1) Scan org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCScan$$anon$1@65b57543 [col1#163,col2#164] PushedAggregates: [], PushedFilters: [], PushedGroupby: [], PushedLimit: [], PushedSample: TABLESAMPLE  0.0 0.8 false 187, ReadSchema: struct<col1:int,col2:int>\r\n```\r\n\r\nThe new interface is implemented using JDBC for POC and end to end test. `TABLESAMPLE` is not supported by all the databases. `TABLESAMPLE` has been implemented using postgresql in this PR.\r\n\r\n### Why are the changes needed?\r\nReduce IO and improve performance.\r\nFor SAMPLE, e.g. `SELECT * FROM t TABLESAMPLE (1 PERCENT)`, Spark retrieves all the data from table and then return 1% rows. It will dramatically reduce the transferred data size and improve performance if we can push Sample to data source side.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes. new interface `SupportsPushDownTableSample`\r\n\r\n\r\n### How was this patch tested?\r\nNew test", "failed_tests": ["org.apache.spark.ml.source.image.ImageFileFormatSuite"], "files": [{"file": {"name": "docs/sql-data-sources-jdbc.md", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "docs/sql-ref-ansi-compliance.md", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 4]}, {"file": {"name": "external/docker-integration-tests/src/test/scala/org/apache/spark/sql/jdbc/v2/PostgresIntegrationSuite.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "external/docker-integration-tests/src/test/scala/org/apache/spark/sql/jdbc/v2/V2JDBCTest.scala", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 2, 2]}, {"file": {"name": "sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 3, 6]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/expressions/Expressions.java", "additions": "22", "deletions": "0", "changes": "22"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/expressions/TableSample.java", "additions": "55", "deletions": "0", "changes": "55"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/read/SupportsPushDownTableSample.java", "additions": "42", "deletions": "0", "changes": "42"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 5, 12]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/expressions/expressions.scala", "additions": "20", "deletions": "0", "changes": "20"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala", "additions": "10", "deletions": "1", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD.scala", "additions": "16", "deletions": "4", "changes": "20"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation.scala", "additions": "5", "deletions": "2", "changes": "7"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala", "additions": "7", "deletions": "2", "changes": "9"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownUtils.scala", "additions": "18", "deletions": "2", "changes": "20"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala", "additions": "29", "deletions": "3", "changes": "32"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCScan.scala", "additions": "20", "deletions": "3", "changes": "23"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 2, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/jdbc/PostgresDialect.scala", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}]}
{"author": "huaxingao", "sha": "e146c179c8c7ebd8e5065401659f506caf4512e8", "commit_date": "2021/09/19 22:36:37", "commit_message": "Migrate CreateTableStatement to v2 command framework", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 8]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveCatalogs.scala", "additions": "0", "deletions": "11", "changes": "11"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala", "additions": "7", "deletions": "3", "changes": "10"}, "updated": [0, 2, 8]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/statements.scala", "additions": "0", "deletions": "19", "changes": "19"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2Commands.scala", "additions": "26", "deletions": "4", "changes": "30"}, "updated": [0, 0, 5]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/CatalogV2Util.scala", "additions": "2", "deletions": "7", "changes": "9"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala", "additions": "8", "deletions": "13", "changes": "21"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/rules.scala", "additions": "46", "deletions": "0", "changes": "46"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala", "additions": "6", "deletions": "2", "changes": "8"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala", "additions": "9", "deletions": "6", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/PlanResolutionSuite.scala", "additions": "26", "deletions": "19", "changes": "45"}, "updated": [0, 0, 2]}]}
{"author": "c21", "sha": "9b8b9ef7efa3ab055edced6d039cc98867f3483f", "commit_date": "2021/10/26 23:36:54", "commit_message": "Address all comments", "title": "[SPARK-34960][SQL] Aggregate push down for ORC", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nThis PR is to add aggregate push down feature for ORC data source v2 reader.\r\n\r\nAt a high level, the PR does:\r\n\r\n* The supported aggregate expression is MIN/MAX/COUNT same as [Parquet aggregate push down](https://github.com/apache/spark/pull/33639).\r\n* BooleanType, ByteType, ShortType, IntegerType, LongType, FloatType, DoubleType, DateType are allowed in MIN/MAXX aggregate push down. All other columns types are not allowed in MIN/MAX aggregate push down.\r\n* All columns types are supported in COUNT aggregate push down.\r\n* Nested column's sub-fields are disallowed in aggregate push down.\r\n* If the file does not have valid statistics, Spark will throw exception and fail query.\r\n* If aggregate has filter or group-by column, aggregate will not be pushed down.\r\n\r\nAt code level, the PR does:\r\n* `OrcScanBuilder`: `pushAggregation()` checks whether the aggregation can be pushed down. The most checking logic is shared between Parquet and ORC, extracted into `AggregatePushDownUtils.getSchemaForPushedAggregation()`. `OrcScanBuilder` will create a `OrcScan` with aggregation and aggregation data schema.\r\n* `OrcScan`: `createReaderFactory` creates a ORC reader factory with aggregation and schema. Similar change with `ParquetScan`.\r\n* `OrcPartitionReaderFactory`: `buildReaderWithAggregates` creates a ORC reader with aggregate push down (i.e. read ORC file footer to process columns statistics, instead of reading actual data in the file). `buildColumnarReaderWithAggregates` creates a columnar ORC reader similarly. Both delegate the real work to read footer in `OrcUtils.createAggInternalRowFromFooter`.\r\n* `OrcUtils.createAggInternalRowFromFooter`: reads ORC file footer to process columns statistics (real heavy lift happens here). Similar to `ParquetUtils.createAggInternalRowFromFooter`. Leverage utility method such as `OrcFooterReader.readStatistics`.\r\n* `OrcFooterReader`: `readStatistics` reads the ORC `ColumnStatistics[]` into Spark `OrcColumnStatistics`. The transformation is needed here, because ORC `ColumnStatistics[]` stores all columns statistics in a flatten array style, and hard to process. Spark `OrcColumnStatistics` stores the statistics in nested tree structure (e.g. like `StructType`). This is used by `OrcUtils.createAggInternalRowFromFooter`\r\n* `OrcColumnStatistics`: the easy-to-manipulate structure for ORC `ColumnStatistics`. This is used by `OrcFooterReader.readStatistics`.\r\n\r\n\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nTo improve the performance of query with aggregate.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nYes. A user-facing config `spark.sql.orc.aggregatePushdown` is added to control enabling/disabling the aggregate push down for ORC. By default the feature is disabled.\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nAdded unit test in `FileSourceAggregatePushDownSuite.scala`. Refactored all unit tests in https://github.com/apache/spark/pull/33639, and it now works for both Parquet and ORC.", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [3, 7, 13]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/types/StructType.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnStatistics.java", "additions": "80", "deletions": "0", "changes": "80"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcFooterReader.java", "additions": "67", "deletions": "0", "changes": "67"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/AggregatePushDownUtils.scala", "additions": "142", "deletions": "0", "changes": "142"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcDeserializer.scala", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcUtils.scala", "additions": "119", "deletions": "3", "changes": "122"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetUtils.scala", "additions": "0", "deletions": "41", "changes": "41"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/orc/OrcPartitionReaderFactory.scala", "additions": "76", "deletions": "17", "changes": "93"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/orc/OrcScan.scala", "additions": "38", "deletions": "7", "changes": "45"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/orc/OrcScanBuilder.scala", "additions": "38", "deletions": "5", "changes": "43"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetPartitionReaderFactory.scala", "additions": "8", "deletions": "6", "changes": "14"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetScan.scala", "additions": "2", "deletions": "8", "changes": "10"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetScanBuilder.scala", "additions": "18", "deletions": "76", "changes": "94"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/FileScanSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/FileSourceAggregatePushDownSuite.scala", "additions": "187", "deletions": "135", "changes": "322"}, "updated": [0, 0, 0]}]}
{"author": "ueshin", "sha": "44da384c661c31d27462cd96634b0cfb3cc83ba7", "commit_date": "2021/10/25 20:02:48", "commit_message": "Remove unnecessary 'F401' comments.", "title": "[SPARK-37011][PYTHON] Remove unnecessary 'noqa: F401' comments", "body": "### What changes were proposed in this pull request?\r\n\r\nRemove unnecessary 'noqa: F401' comments.\r\n\r\n### Why are the changes needed?\r\n\r\nNow that `flake8` in Jenkins was upgraded (#34384), we can remove unnecessary 'noqa: F401' comments.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nNo.\r\n\r\n### How was this patch tested?\r\n\r\nExisting tests.", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/_typing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 1]}]}
{"author": "LuciferYang", "sha": "c79c0ff46d8356d17b0384cde4ec9ce0de6d722e", "commit_date": "2021/08/04 03:27:21", "commit_message": "add a new method to avoid file truncate", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java", "additions": "1", "deletions": "4", "changes": "5"}, "updated": [1, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/storage/DiskBlockObjectWriter.scala", "additions": "21", "deletions": "0", "changes": "21"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala", "additions": "1", "deletions": "6", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala", "additions": "1", "deletions": "6", "changes": "7"}, "updated": [1, 2, 3]}]}
{"author": "AngersZhuuuu", "sha": "a668f504d82340229f6f1cc34e6bf79096cd18a1", "commit_date": "2021/10/26 13:08:34", "commit_message": "Update HiveShim.scala", "title": "[SPARK-37115][SQL] HiveClientImpl should use shim to wrap all hive client calls", "body": "### What changes were proposed in this pull request?\r\nIn this pr we use `shim` to wrap all hive client api to make it easier.\r\n\r\n\r\n### Why are the changes needed?\r\nUse `shim` to wrap all hive client api  to make it easier.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n\r\n### How was this patch tested?\r\nExisted UT\r\n", "failed_tests": ["org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.HiveExternalSessionCatalogSuite", "org.apache.spark.sql.hive.execution.command.AlterTableRenamePartitionSuite"], "files": [{"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala", "additions": "31", "deletions": "30", "changes": "61"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveShim.scala", "additions": "171", "deletions": "1", "changes": "172"}, "updated": [1, 1, 1]}]}
{"author": "AngersZhuuuu", "sha": "d03a64cf0b4e5e303c35acabab84315b9247b09b", "commit_date": "2021/10/26 14:17:48", "commit_message": "follow comment", "title": "[SPARK-37082][SQL] Implements histogram_numeric aggregation function which supports partial aggregation.", "body": "### What changes were proposed in this pull request?\r\nThis PR implements aggregation function `histogram_numeric`. Function `histogram_numeric` returns an approximate histogram of a numerical column using a user-specified number of bins. For example, the histogram of column `col` when split to 3 bins.\r\n\r\nSyntax:\r\n#### an approximate histogram of a numerical column using a user-specified number of bins. \r\nhistogram_numebric(col, nBins)\r\n\r\n###### Returns an approximate histogram of a column `col` into 3 bins.\r\nSELECT histogram_numebric(col, 3) FROM table\r\n\r\n##### Returns an approximate histogram of a column `col` into 5 bins.\r\nSELECT histogram_numebric(col, 5) FROM table\r\n\r\n### Why are the changes needed?\r\n\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo change from user side\r\n\r\n### How was this patch tested?\r\nAdded UT\r\n", "failed_tests": ["org.apache.spark.sql.hive.execution.HiveUDAFSuite", "org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite", "org.apache.spark.sql.ExpressionsSchemaSuite", "org.apache.spark.sql.expressions.ExpressionInfoSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/util/NumericHistogram.java", "additions": "288", "deletions": "0", "changes": "288"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/SessionCatalog.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/HistogramNumeric.scala", "additions": "207", "deletions": "0", "changes": "207"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/aggregate/HistogramNumericSuite.scala", "additions": "166", "deletions": "0", "changes": "166"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-functions/sql-expression-schema.md", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/group-by.sql", "additions": "12", "deletions": "0", "changes": "12"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/group-by.sql.out", "additions": "19", "deletions": "1", "changes": "20"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveSessionCatalog.scala", "additions": "1", "deletions": "3", "changes": "4"}, "updated": [0, 1, 2]}]}
{"author": "AngersZhuuuu", "sha": "3af7e148c3b97e408531228bbc89e83a9938a5e9", "commit_date": "2021/10/22 15:45:45", "commit_message": "update", "title": "[SPARK-36975][SQL] Correct the hive client calls\u2018s metrics in HiveClientImpl", "body": "### What changes were proposed in this pull request?\r\nCurrently we collect hive client calls metrics from `withHiveState`, it's not correct that in one call `withHiveState`, there may be many hive client calls. we need to split such behaviors.\r\n\r\nIn this pr we add a parameter to API `withHiveState` to collect the correct hive client calls when call `withHiveState` and and add some direct metrics  collect method in some uncertain calls `HiveCatalogMetrics.incrementHiveClientCalls`\r\n\r\n\r\n### Why are the changes needed?\r\nMake hive client calls metrics more accrate\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n\r\n### How was this patch tested?\r\nExisted UT\r\n", "failed_tests": ["org.apache.spark.sql.hive.execution.command.ShowPartitionsSuite", "org.apache.spark.sql.hive.execution.command.AlterTableRenamePartitionSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.command.DropTableSuite", "org.apache.spark.sql.hive.execution.command.AlterTableDropPartitionSuite", "org.apache.spark.sql.hive.execution.command.AlterTableAddPartitionSuite"], "files": [{"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClient.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala", "additions": "65", "deletions": "26", "changes": "91"}, "updated": [1, 2, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/PartitionedTablePerfStatsSuite.scala", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/client/VersionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/command/AlterTableDropPartitionSuite.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/command/AlterTableRenamePartitionSuite.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}]}
{"author": "AngersZhuuuu", "sha": "700eae7bc1f11efcaf8fd8062773bd1875fbd5ed", "commit_date": "2021/10/22 06:27:54", "commit_message": "fix UT in avro module", "title": "[SPARK-37066][SQL] Improve error message to show file path when failed to read next file", "body": "### What changes were proposed in this pull request?\r\nWhen use ORC vectorized reader,  if `OrcColumnarBatchReader` throw `ArrayIndexOutofBoundsException`, it only show error message of index, won't show which file is being reading\r\n```\r\n\r\n21/10/19 18:12:58 ERROR Executor: Exception in task 34.1 in stage 14.0 (TID 257)\r\njava.lang.ArrayIndexOutOfBoundsException: 1024\r\n\tat org.apache.orc.impl.TreeReaderFactory$TreeReader.nextVector(TreeReaderFactory.java:292)\r\n\tat org.apache.orc.impl.TreeReaderFactory$StringDictionaryTreeReader.nextVector(TreeReaderFactory.java:1820)\r\n\tat org.apache.orc.impl.TreeReaderFactory$StringTreeReader.nextVector(TreeReaderFactory.java:1517)\r\n\tat org.apache.orc.impl.ConvertTreeReaderFactory$DateFromStringGroupTreeReader.nextVector(ConvertTreeReaderFactory.java:1802)\r\n\tat org.apache.orc.impl.TreeReaderFactory$StructTreeReader.nextBatch(TreeReaderFactory.java:2059)\r\n\tat org.apache.orc.impl.RecordReaderImpl.nextBatch(RecordReaderImpl.java:1324)\r\n\tat org.apache.spark.sql.execution.datasources.orc.OrcColumnarBatchReader.nextBatch(OrcColumnarBatchReader.java:196)\r\n\tat org.apache.spark.sql.execution.datasources.orc.OrcColumnarBatchReader.nextKeyValue(OrcColumnarBatchReader.java:99)\r\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:37)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:173)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:503)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n```\r\n\r\nIt caused difficult for SRE/Admin to find out which file have problem. \r\n\r\nIn this pr, we add a catch in `FileScanRDD` & `FilePartitionReader` to log an error message to show the file path when failed read data.\r\n\r\n\r\n\r\n### Why are the changes needed?\r\nImprove error message for orc vectorized reader\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n\r\n### How was this patch tested?\r\n\r\nExisted UT\r\n\r\nAfter this patch, error message in our PROD\r\n```\r\n21/10/22 16:01:39 WARN TaskSetManager: Lost task 312.0 in stage 1.0 (TID 55) (ip-10-130-15-162.idata-server.shopee.io executor 30): org.apache.spark.sql.execution.QueryExecutionException: Encountered error while reading Parquet file hdfs://path/to/file/part-00001-2d12874d-36b5-4091-9202-3dc0c81c2d9b-c000.snappy.orc. Details:\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:193)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:503)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.ArrayIndexOutOfBoundsException: 1024\r\n\tat org.apache.orc.impl.TreeReaderFactory$TreeReader.nextVector(TreeReaderFactory.java:292)\r\n\tat org.apache.orc.impl.TreeReaderFactory$StringDictionaryTreeReader.nextVector(TreeReaderFactory.java:1820)\r\n\tat org.apache.orc.impl.TreeReaderFactory$StringTreeReader.nextVector(TreeReaderFactory.java:1517)\r\n\tat org.apache.orc.impl.ConvertTreeReaderFactory$DateFromStringGroupTreeReader.nextVector(ConvertTreeReaderFactory.java:1802)\r\n\tat org.apache.orc.impl.TreeReaderFactory$StructTreeReader.nextBatch(TreeReaderFactory.java:2059)\r\n\tat org.apache.orc.impl.RecordReaderImpl.nextBatch(RecordReaderImpl.java:1324)\r\n\tat org.apache.spark.sql.execution.datasources.orc.OrcColumnarBatchReader.nextBatch(OrcColumnarBatchReader.java:196)\r\n\tat org.apache.spark.sql.execution.datasources.orc.OrcColumnarBatchReader.nextKeyValue(OrcColumnarBatchReader.java:99)\r\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:37)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:173)\r\n\t... 19 more\r\n```", "failed_tests": [], "files": [{"file": {"name": "external/avro/src/test/scala/org/apache/spark/sql/avro/AvroLogicalTypeSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "external/avro/src/test/scala/org/apache/spark/sql/avro/AvroSuite.scala", "additions": "9", "deletions": "3", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala", "additions": "6", "deletions": "4", "changes": "10"}, "updated": [0, 2, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileScanRDD.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/FilePartitionReader.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategySuite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/csv/CSVSuite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRebaseDatetimeSuite.scala", "additions": "9", "deletions": "2", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "3", "deletions": "4", "changes": "7"}, "updated": [0, 0, 1]}]}
{"author": "AngersZhuuuu", "sha": "beefd1fb4d0ccc51f1a250a9d727302f3891574c", "commit_date": "2021/10/22 09:33:24", "commit_message": "[SPARK-37097][YARN] yarn-cluster mode don't need to retry when AM container exit code 0 but application failed.", "title": "[SPARK-37097][YARN] yarn-cluster mode don't need to retry when AM container exit code 0 but application failed.", "body": "### What changes were proposed in this pull request?\r\n\r\nSome yarn-cluster application meet such exception.\r\n```\r\n21/10/20 03:31:55 ERROR Client: Application diagnostics message: Application application_1632999510150_2163647 failed 1 times (global limit =8; local limit is =1) due to AM Container for appattempt_1632999510150_2163647_000001 exited with  exitCode: 0\r\nFailing this attempt.Diagnostics: For more detailed output, check the application tracking page: http://ip-xx-xx-xx-xx.idata-server.shopee.io:8088/cluster/app/application_1632999510150_2163647 Then click on links to logs of each attempt.\r\n. Failing the application.\r\nException in thread \"main\" org.apache.spark.SparkException: Application application_1632999510150_2163647 finished with failed status\r\n```\r\n\r\nIt's caused by below situation:\r\n1. yarn-cluster mode application usr code finished, AM shutdown hook triggered\r\n2. AM call unregister from RM but timeout, since AM shutdown hook have try catch, won't throw exception, so AM container exit with code 0(application user code running success).\r\n3. Since RM lose connection with AM, then treat this container as failed final status.\r\n4. Then client side got application report as final status failed but am container exit code 0. client treat it as failed, then retry.\r\n\r\n\r\nit's a unnecessary retry. we can avoid it.\r\n\r\n\r\n### Why are the changes needed?\r\nAvoid unnecessary retry\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n\r\n### How was this patch tested?\r\n\r\nManual tested", "failed_tests": [], "files": [{"file": {"name": "resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 1]}]}
{"author": "AngersZhuuuu", "sha": "76894256f1b490c8bb46ab8a97c3a16878a0d896", "commit_date": "2021/10/21 04:15:47", "commit_message": "Update", "title": "[SPARK-37035][SQL] Improve error message when use parquet vectorize reader", "body": "### What changes were proposed in this pull request?\r\nWhen we use parquet, found vectorized read won't show error message about failed read parquet file path. This makes it difficult to find out that the file is faulty.\r\n\r\nNone-vectorized parquet reader \r\n```\r\nException: Encounter error while reading parquet files. One possible cause: Parquet column cannot be converted in the corresponding files. Details:\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:193)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file hdfs://R2/projects/data_notificationmart/dwd_traceid_sent_civ_first_di/tz_type=local/grass_region=TW/grass_date=2021-10-13/noti_type=AR/part-00013-22bdd509-4469-47f7-a37e-50fddd4266a7-c000.zstd.parquet\r\n\tat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:251)\r\n\tat org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:207)\r\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:181)\r\n\t... 15 more\r\n```\r\n\r\n\r\nVectorize parquet reader\r\n```\r\n21/10/15 18:01:54 WARN TaskSetManager: Lost task 1881.0 in stage 16.0 (TID 10380, ip-10-130-169-140.idata-server.shopee.io, executor 168): TaskKilled (Stage cancelled)\r\n: An error occurred while calling o362.showString.\r\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 963 in stage 17.0 failed 4 times, most recent failure: Lost task 963.3 in stage 17.0 (TID 10351, ip-10-130-75-201.idata-server.shopee.io, executor 99): java.lang.UnsupportedOperationException: org.apache.parquet.column.values.dictionary.PlainValuesDictionary$PlainIntegerDictionary\r\n\tat org.apache.parquet.column.Dictionary.decodeToLong(Dictionary.java:49)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetDictionary.decodeToLong(ParquetDictionary.java:36)\r\n\tat org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.getLong(OnHeapColumnVector.java:364)\r\n\tat org.apache.spark.sql.execution.vectorized.MutableColumnarRow.getLong(MutableColumnarRow.java:120)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.writeFields_0_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anonfun$doExecute$2$$anonfun$apply$2.apply(DataSourceScanExec.scala:351)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anonfun$doExecute$2$$anonfun$apply$2.apply(DataSourceScanExec.scala:349)\r\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\r\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n```\r\n\r\nThis is caused in vectorized reader, it use `Dictionary` don't have file information,\r\n```\r\npublic abstract class Dictionary {\r\n    private final Encoding encoding;\r\n\r\n    public Dictionary(Encoding encoding) {\r\n        this.encoding = encoding;\r\n    }\r\n\r\n    public Encoding getEncoding() {\r\n        return this.encoding;\r\n    }\r\n\r\n    public abstract int getMaxId();\r\n\r\n    public Binary decodeToBinary(int id) {\r\n        throw new UnsupportedOperationException(this.getClass().getName());\r\n    }\r\n\r\n    public int decodeToInt(int id) {\r\n        throw new UnsupportedOperationException(this.getClass().getName());\r\n    }\r\n\r\n    public long decodeToLong(int id) {\r\n        throw new UnsupportedOperationException(this.getClass().getName());\r\n    }\r\n\r\n    public float decodeToFloat(int id) {\r\n        throw new UnsupportedOperationException(this.getClass().getName());\r\n    }\r\n\r\n    public double decodeToDouble(int id) {\r\n        throw new UnsupportedOperationException(this.getClass().getName());\r\n    }\r\n\r\n    public boolean decodeToBoolean(int id) {\r\n        throw new UnsupportedOperationException(this.getClass().getName());\r\n    }\r\n}\r\n```\r\n and spark have wrapper a `ParquetDictionary` so we can do this in `ParquetDictionary`, to add file information in error message.\r\n\r\n### Why are the changes needed?\r\nImprove error message to help make sure which file have problem.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nUser can know which file failed to read when use parquet vectorized  reader\r\n\r\n\r\n### How was this patch tested?\r\nWIP\r\n", "failed_tests": ["org.apache.spark.sql.execution.datasources.parquet.ParquetVectorizedSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 2, 5]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetDictionary.java", "additions": "54", "deletions": "22", "changes": "76"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase.java", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader.java", "additions": "5", "deletions": "2", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader.java", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetVectorizedSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}]}
{"author": "AngersZhuuuu", "sha": "719dda2df069d16c09bfa8db8c32e1855b22f4fe", "commit_date": "2021/10/19 07:32:01", "commit_message": "update", "title": "[WIP][SPARK-37053][CORE] Add metrics to SparkHistoryServer", "body": "### What changes were proposed in this pull request?\r\nAdd metrics system to history server.\r\n\r\n### Why are the changes needed?\r\nAdd metrics to History Server\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nUser can use history server monitor data\r\n\r\n### How was this patch tested?\r\nAdded UT\r\n", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/deploy/history/ApplicationCache.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/history/ApplicationHistoryProvider.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala", "additions": "19", "deletions": "11", "changes": "30"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala", "additions": "13", "deletions": "0", "changes": "13"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/history/HistoryServerSource.scala", "additions": "85", "deletions": "0", "changes": "85"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/metrics/MetricsConfig.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/deploy/history/HistoryServerSuite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/scala/org/apache/spark/metrics/MetricsConfigSuite.scala", "additions": "31", "deletions": "2", "changes": "33"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/metrics/MetricsSystemSuite.scala", "additions": "13", "deletions": "3", "changes": "16"}, "updated": [0, 0, 0]}]}
{"author": "AngersZhuuuu", "sha": "9d0ea390f0b1b944ddf7b674b0ba553226daaedc", "commit_date": "2021/10/09 02:20:35", "commit_message": "update", "title": "[SPARK-36957][SQL] Add Aggregate function Product to SQL function", "body": "### What changes were proposed in this pull request?\r\nAdd Aggregate function Product to SQL function\r\n\r\n\r\n### Why are the changes needed?\r\nWe have support Aggregate function in DSL and  DataFrame API, we need to support it in SQL query too.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nUser can use aggregate function `product` in SQL\r\n\r\n\r\n### How was this patch tested?\r\nUT\r\n", "failed_tests": ["org.apache.spark.sql.ExpressionsSchemaSuite", "org.apache.spark.sql.SQLQuerySuite", "org.apache.spark.sql.expressions.ExpressionInfoSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Product.scala", "additions": "10", "deletions": "3", "changes": "13"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-functions/sql-expression-schema.md", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/group-by.sql", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/group-by.sql.out", "additions": "12", "deletions": "1", "changes": "13"}, "updated": [0, 0, 0]}]}
{"author": "AngersZhuuuu", "sha": "632d7255c4ddb1aeb367ed6c0123121348975e7e", "commit_date": "2021/08/25 03:12:01", "commit_message": "[SPARK-36579][SQL] Make spark source stagingDir can use user defined", "title": "", "body": "", "failed_tests": ["org.apache.spark.rdd.PairRDDFunctionsSuite", "org.apache.spark.internal.io.FileCommitProtocolInstantiationSuite", "org.apache.spark.scheduler.OutputCommitCoordinatorSuite", "org.apache.spark.internal.io.cloud.CommitterBindingSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/internal/io/FileCommitProtocol.scala", "additions": "104", "deletions": "0", "changes": "104"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "13", "deletions": "0", "changes": "13"}, "updated": [0, 2, 18]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala", "additions": "1", "deletions": "10", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SQLHadoopMapReduceCommitProtocol.scala", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/sources/InsertSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/sources/StagingInsertSuite.scala", "additions": "70", "deletions": "0", "changes": "70"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/SaveAsHiveFile.scala", "additions": "6", "deletions": "89", "changes": "95"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/InsertSuite.scala", "additions": "8", "deletions": "8", "changes": "16"}, "updated": [0, 0, 0]}]}
{"author": "AngersZhuuuu", "sha": "c2f606f0a6a3816f5d46ade0466912be0dfb4996", "commit_date": "2021/08/24 08:14:26", "commit_message": "[36562][SQL] Add new NewSQLHadoopMapReduceCommitProtocol resolve conflict", "title": "", "body": "", "failed_tests": ["org.apache.spark.ui.UISeleniumSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [1, 2, 6]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/io/FileCommitProtocol.scala", "additions": "114", "deletions": "11", "changes": "125"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala", "additions": "12", "deletions": "7", "changes": "19"}, "updated": [0, 0, 0]}, {"file": {"name": "hadoop-cloud/src/hadoop-3/main/scala/org/apache/spark/internal/io/cloud/PathOutputCommitProtocol.scala", "additions": "2", "deletions": "5", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala", "additions": "6", "deletions": "3", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SQLHadoopMapReduceCommitProtocol.scala", "additions": "9", "deletions": "1", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SQLPathHadoopMapReduceCommitProtocol.scala", "additions": "59", "deletions": "0", "changes": "59"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SQLPathOutputCommitter.scala", "additions": "501", "deletions": "0", "changes": "501"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/sources/InsertSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/sources/StagingInsertSuite.scala", "additions": "139", "deletions": "0", "changes": "139"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/SaveAsHiveFile.scala", "additions": "6", "deletions": "89", "changes": "95"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/InsertSuite.scala", "additions": "13", "deletions": "8", "changes": "21"}, "updated": [0, 0, 0]}]}
{"author": "ulysses-you", "sha": "de0d183b72cf022cac2da1e860d2bd3f64e06d82", "commit_date": "2021/10/18 12:04:30", "commit_message": "Cancel all running job after AQE plan finished", "title": "[SPARK-37043][SQL] Cancel all running job after AQE plan finished", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nCancel running job after AQE plan finished, so this PR add a `runningStages` in `AdaptiveExecutionContext` to record the running stages.\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nWe see stage was still running after AQE plan finished. This is because the plan which contains a join with one empty side has been converted to `LocalTableScanExec` during `AQEOptimizer`, but the other side of this join is still running (shuffle map stage).\r\n\r\nIt's no meaning to keep running the stage, so It's better to cancel the running stage after AQE plan finished in case wasting the task resource.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nno\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nadd test.", "failed_tests": ["org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SubquerySuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "30", "deletions": "1", "changes": "31"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "41", "deletions": "1", "changes": "42"}, "updated": [0, 0, 2]}]}
{"author": "vmalakhin", "sha": "a1f8e92ee6171fe2c7eadb24443e55386cb8ee56", "commit_date": "2021/10/26 13:02:12", "commit_message": "SPARK-37102: Hadoop Cloud: removed redundant exclusions", "title": "[SPARK-37102][BUILD] Removed redundant exclusions in `hadoop-cloud` module", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\nRedundant exclusions were removed for hadoop-cloud module so the build output contains required dependency for hadoop-azure artifact (ackson-mapper-asl).\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\n\r\n### Why are the changes needed?\r\nCurrently Hadoop ABFS connector (for Azure Data Lake Storage Gen2) is broken due to missing dependency. So required dependencies for hadoop-azure artifact should be included into distribution output if hadoop-cloud module enabled.\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\n\r\n### How was this patch tested?\r\nUnfortunately Microsoft does not provide support for Data Lake Storage Gen2 within azurite emulator - so the change was tested manually and the diff was checked to see if anything else was picked up for build outputs (before and after the change). So the only change is inclusion of jackson-mapper-asl-1.9.13.jar.\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->\r\n", "failed_tests": ["org.apache.spark.sql.hive.execution.HashAggregationQuerySuite", "org.apache.spark.SparkFunSuite"], "files": [{"file": {"name": "hadoop-cloud/pom.xml", "additions": "0", "deletions": "32", "changes": "32"}, "updated": [0, 0, 0]}]}
{"author": "eejbyfeldt", "sha": "1e9edeae385fff3ecac30e8caee0564b1f8d3ef5", "commit_date": "2021/10/26 14:08:22", "commit_message": "Also update other places.", "title": "[SPARK-37071][CORE] Make OpenHashMap serialize without reference tracking", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\nChange the anonymous functions in OpenHashMap to member methods. This avoid having a member which captures the OpenHashMap object in its closure. This fixes so that OpenHashMap instances can be serialized with Kryo with reference tracking turned off.\r\n\r\nI am not sure why the original implementation had the anonymous function members in the first place. But if it was implemented that way for performance reason another possible fix is just to mark the `grow` and `move` members as transient.\r\n\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\n\r\n### Why are the changes needed?\r\nUser might want to turn off referenceTracking in kryo since it has performance benefits, but currently this will unnecessary and unexpectedly prevent them from using some features of spark that uses OpenHashMap internally.\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo.\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\n\r\n### How was this patch tested?\r\nExisting tests and a new test in the `KryoSerializerSuite`.\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->\r\n\r\n", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/util/collection/OpenHashMap.scala", "additions": "2", "deletions": "7", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/collection/PrimitiveKeyOpenHashMap.scala", "additions": "2", "deletions": "7", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/serializer/KryoSerializerSuite.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "graphx/src/main/scala/org/apache/spark/graphx/util/collection/GraphXPrimitiveKeyOpenHashMap.scala", "additions": "2", "deletions": "7", "changes": "9"}, "updated": [0, 0, 0]}]}
{"author": "imback82", "sha": "a8820abd45be0c10a7296acc00a66aa54020df60", "commit_date": "2021/10/25 16:35:36", "commit_message": "add back with TODO", "title": "[SPARK-37031][SQL][TESTS] Unify v1 and v2 DESCRIBE NAMESPACE tests", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\n1. Move `DESCRIBE DATABASE/NAMESPACE` parsing tests to `DescribeNamespaceParserSuite`.\r\n2. Put common `DESCRIBE NAMESPACE` tests into one trait `org.apache.spark.sql.execution.command. DescribeNamespaceSuiteBase`, and put datasource specific tests to the `v1.DescribeNamespaceSuite` and `v2.DescribeNamespaceSuite`.\r\n\r\nThe changes follow the approach of #30287.\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n1. The unification will allow to run common `DESCRIBE NAMESPACE` tests for both DSv1/Hive DSv1 and DSv2\r\n2. We can detect missing features and differences between DSv1 and DSv2 implementations.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nNo\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nExisting unit tests and new tests.", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala", "additions": "0", "deletions": "9", "changes": "9"}, "updated": [0, 2, 5]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala", "additions": "0", "deletions": "39", "changes": "39"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/DDLSuite.scala", "additions": "1", "deletions": "16", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/DescribeNamespaceParserSuite.scala", "additions": "39", "deletions": "0", "changes": "39"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/DescribeNamespaceSuiteBase.scala", "additions": "50", "deletions": "0", "changes": "50"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/v1/DescribeNamespaceSuite.scala", "additions": "59", "deletions": "0", "changes": "59"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/v2/DescribeNamespaceSuite.scala", "additions": "70", "deletions": "0", "changes": "70"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/command/DescribeNamespaceSuite.scala", "additions": "26", "deletions": "0", "changes": "26"}, "updated": [0, 0, 0]}]}
{"author": "HeartSaVioR", "sha": "0aec07a885d32b3222f7ae410bf3a809f55c35ac", "commit_date": "2021/10/21 03:57:16", "commit_message": "fix UT", "title": "[SPARK-37062][SS] Introduce a new data source for providing consistent set of rows per microbatch", "body": "### What changes were proposed in this pull request?\r\n\r\nThis PR proposes to introduce a new data source having short name as \"rate-micro-batch\", which produces similar input rows as \"rate\" (increment long values with timestamps), but ensures that each micro-batch has a \"predictable\" set of input rows.\r\n\r\n\"rate-micro-batch\" data source receives a config to specify the number of rows per micro-batch, which defines the set of input rows for further micro-batches. For example, if the number of rows per micro-batch is set to 1000, the first batch would have 1000 rows having value range as `0~999`, the second batch would have 1000 rows having value range as `1000~1999`, and so on. This characteristic brings different use cases compared to rate data source, as we can't predict the input rows for rate data source like this.\r\n\r\nFor generated time (timestamp column), the data source applies the same mechanism to make the value of column be predictable. `startTimestamp` option defines the starting value of generated time, and `advanceMillisPerBatch` option defines how much time the generated time should advance per micro-batch. All input rows in the same micro-batch will have same timestamp.\r\n\r\nThis source supports the following options:\r\n\r\n* `rowsPerBatch` (e.g. 100): How many rows should be generated per micro-batch.\r\n* `numPartitions` (e.g. 10, default: Spark's default parallelism): The partition number for the generated rows.\r\n* `startTimestamp` (e.g. 1000, default: 0): starting value of generated time\r\n* `advanceMillisPerBatch` (e.g. 1000, default: 1000): the amount of time being advanced in generated time on each micro-batch.\r\n\r\n### Why are the changes needed?\r\n\r\nThe \"rate\" data source has been known to be used as a benchmark for streaming query.\r\n\r\nWhile this helps to put the query to the limit (how many rows the query could process per second), the rate data source doesn't provide consistent rows per batch into stream, which leads two environments be hard to compare with.\r\n\r\nFor example, in many cases, you may want to compare the metrics in the batches between test environments (like running same streaming query with different options). These metrics are strongly affected if the distribution of input rows in batches are changing, especially a micro-batch has been lagged (in any reason) and rate data source produces more input rows to the next batch.\r\n\r\nAlso, when you test against streaming aggregation, you may want the data source produces the same set of input rows per batch (deterministic), so that you can plan how these input rows will be aggregated and how state rows will be evicted, and craft the test query based on the plan.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nYes, end users can leverage a new data source in micro-batch mode of streaming query to test/benchmark.\r\n\r\n### How was this patch tested?\r\n\r\nNew UTs, and manually tested via below query in spark-shell:\r\n\r\n```\r\nspark.readStream.format(\"rate-micro-batch\").option(\"rowsPerBatch\", 20).option(\"numPartitions\", 3).load().writeStream.format(\"console\").start()\r\n```\r\n", "failed_tests": ["org.apache.spark.sql.execution.streaming.sources.RatePerEpochProviderSuite"], "files": [{"file": {"name": "docs/structured-streaming-programming-guide.md", "additions": "13", "deletions": "0", "changes": "13"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/resources/META-INF/services/org.apache.spark.sql.sources.DataSourceRegister", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/RatePerMicroBatchProvider.scala", "additions": "127", "deletions": "0", "changes": "127"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/RatePerMicroBatchStream.scala", "additions": "155", "deletions": "0", "changes": "155"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/sources/RatePerMicroBatchProviderSuite.scala", "additions": "141", "deletions": "0", "changes": "141"}, "updated": [0, 0, 0]}]}
{"author": "itholic", "sha": "cc7adca3d04f11a03f2902a46b0604acb96cd310", "commit_date": "2021/10/26 03:55:00", "commit_message": "[SPARK-37036][PYTHON] Add util function to raise advice warning for pandas API on Spark.", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/accessors.py", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 2, 6]}, {"file": {"name": "python/pyspark/pandas/base.py", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 2, 5]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "40", "deletions": "0", "changes": "40"}, "updated": [0, 4, 15]}, {"file": {"name": "python/pyspark/pandas/generic.py", "additions": "13", "deletions": "0", "changes": "13"}, "updated": [0, 3, 4]}, {"file": {"name": "python/pyspark/pandas/groupby.py", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 2, 8]}, {"file": {"name": "python/pyspark/pandas/indexes/base.py", "additions": "12", "deletions": "0", "changes": "12"}, "updated": [0, 2, 6]}, {"file": {"name": "python/pyspark/pandas/namespace.py", "additions": "21", "deletions": "0", "changes": "21"}, "updated": [0, 3, 8]}, {"file": {"name": "python/pyspark/pandas/series.py", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [1, 4, 10]}, {"file": {"name": "python/pyspark/pandas/utils.py", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [0, 2, 3]}]}
{"author": "dchvn", "sha": "4684e6c2313f46e5ecf2a76e900f743c3f235520", "commit_date": "2021/10/18 10:02:31", "commit_message": "[SPARK-37042][PYTHON] Inline type hints for kinesis.py and listener.py in python/pyspark/streaming", "title": "[SPARK-37042][PYTHON] Inline type hints for kinesis.py and listener.py in python/pyspark/streaming", "body": "\r\n### What changes were proposed in this pull request?\r\nInline type hints for kinesis.py and listener.py in python/pyspark/streaming\r\n\r\n### Why are the changes needed?\r\nWe can take advantage of static type checking within the functions by inlining the type hints.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\nExisting tests", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/streaming/kinesis.py", "additions": "46", "deletions": "15", "changes": "61"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/streaming/kinesis.pyi", "additions": "0", "deletions": "49", "changes": "49"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/streaming/listener.py", "additions": "13", "deletions": "10", "changes": "23"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/streaming/listener.pyi", "additions": "0", "deletions": "35", "changes": "35"}, "updated": [0, 0, 0]}]}
{"author": "dchvn", "sha": "2e2283de4b77fd407935ff074e0f465b73cbc7c4", "commit_date": "2021/10/26 02:56:59", "commit_message": "fix", "title": "[SPARK-37014][PYTHON] Inline type hints for python/pyspark/streaming/context.py", "body": "### What changes were proposed in this pull request?\r\nInline type hints for python/pyspark/streaming/context.py from Inline type hints for python/pyspark/streaming/context.pyi.\r\n\r\n### Why are the changes needed?\r\nCurrently, there is type hint stub files python/pyspark/streaming/context.pyi to show the expected types for functions, but we can also take advantage of static type checking within the functions by inlining the type hints.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\nExisting test.", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/streaming/context.py", "additions": "104", "deletions": "59", "changes": "163"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/streaming/context.pyi", "additions": "0", "deletions": "75", "changes": "75"}, "updated": [0, 0, 0]}]}
{"author": "dchvn", "sha": "1eadd750b1bb8e24cc658076d858ca2a718cade2", "commit_date": "2021/10/21 07:03:25", "commit_message": "[SPARK-37083] Inline type hints for python/pyspark/accumulators.py", "title": "[SPARK-37083][PYTHON] Inline type hints for python/pyspark/accumulators.py", "body": "### What changes were proposed in this pull request?\r\nInline type hints for python/pyspark/accumulators.py\r\n\r\n### Why are the changes needed?\r\nWe can take advantage of static type checking within the functions by inlining the type hints.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\nExisting tests", "failed_tests": ["pyspark.pandas.tests.data_type_ops.test_base", "pyspark.pandas.tests.indexes.test_base"], "files": [{"file": {"name": "python/pyspark/accumulators.py", "additions": "54", "deletions": "32", "changes": "86"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/accumulators.pyi", "additions": "0", "deletions": "73", "changes": "73"}, "updated": [0, 0, 0]}]}
{"author": "dchvn", "sha": "45e0406c842f9787c2a376a2431677865301817c", "commit_date": "2021/10/11 03:20:53", "commit_message": "[SPARK-36969][PYTHON] Inline type hints for python/pyspark/context.py", "title": "[SPARK-36969][PYTHON] Inline type hints for SparkContext", "body": "### What changes were proposed in this pull request?\r\nInline type hints for ```SparkContext```\r\n\r\n### Why are the changes needed?\r\nInline type hints for ```SparkContext```\r\n\r\nAfter this PR, we can remove multiple \r\n```python\r\n# type: ignore[attr-defined]\r\n```\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\nexist test\r\n", "failed_tests": ["pyspark.pandas.tests.data_type_ops.test_base", "pyspark.pandas.tests.indexes.test_base"], "files": [{"file": {"name": "python/pyspark/conf.py", "additions": "44", "deletions": "21", "changes": "65"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/conf.pyi", "additions": "0", "deletions": "44", "changes": "44"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/context.py", "additions": "293", "deletions": "149", "changes": "442"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/context.pyi", "additions": "0", "deletions": "194", "changes": "194"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/files.py", "additions": "14", "deletions": "8", "changes": "22"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/files.pyi", "additions": "0", "deletions": "24", "changes": "24"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/resource/requests.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/avro/functions.py", "additions": "6", "deletions": "5", "changes": "11"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/column.py", "additions": "11", "deletions": "9", "changes": "20"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/context.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/dataframe.py", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 2, 3]}, {"file": {"name": "python/pyspark/sql/functions.py", "additions": "245", "deletions": "171", "changes": "416"}, "updated": [0, 3, 6]}, {"file": {"name": "python/pyspark/sql/observation.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/pandas/conversion.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 3]}, {"file": {"name": "python/pyspark/sql/readwriter.py", "additions": "14", "deletions": "8", "changes": "22"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/session.py", "additions": "26", "deletions": "16", "changes": "42"}, "updated": [0, 1, 3]}, {"file": {"name": "python/pyspark/sql/streaming.py", "additions": "12", "deletions": "6", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/types.py", "additions": "7", "deletions": "4", "changes": "11"}, "updated": [0, 0, 3]}, {"file": {"name": "python/pyspark/sql/udf.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/utils.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/sql/window.py", "additions": "12", "deletions": "4", "changes": "16"}, "updated": [0, 1, 1]}]}
{"author": "dchvn", "sha": "2aac8edf824525b498f3b6efd45b1e00b075e570", "commit_date": "2021/10/18 03:45:41", "commit_message": "[SPARK-37015][PYTHON] Inline type hints for python/pyspark/streaming/dstream.py", "title": "[SPARK-37015][PYTHON] Inline type hints for python/pyspark/streaming/dstream.py", "body": "### What changes were proposed in this pull request?\r\nInline type hints for python/pyspark/streaming/dstream.py\r\n\r\n### Why are the changes needed?\r\nWe can take advantage of static type checking within the functions by inlining the type hints.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\nExisting tests", "failed_tests": ["pyspark.streaming.tests.test_dstream"], "files": [{"file": {"name": "python/pyspark/streaming/dstream.py", "additions": "356", "deletions": "117", "changes": "473"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/streaming/dstream.pyi", "additions": "0", "deletions": "216", "changes": "216"}, "updated": [0, 0, 0]}]}
{"author": "dchvn", "sha": "fdd0296ea57717a8131cbf41c054d44dcba0ff45", "commit_date": "2021/10/18 07:19:50", "commit_message": "[SPARK-36302] Not enough functions in this issues", "title": "[SPARK-36302][SQL] Refactor thirteenth set of 20 query execution errors to use error classes", "body": "\r\n\r\n<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->Adds error classes to some of the exceptions in QueryExecutionErrors.\r\n\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->Improves auditing for developers and adds useful fields for users (error class and SQLSTATE).\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->Fills in missing error class and SQLSTATE fields.\r\n\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->Existing tests\r\n", "failed_tests": [], "files": [{"file": {"name": "core/src/main/resources/error/error-classes.json", "additions": "59", "deletions": "0", "changes": "59"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala", "additions": "73", "deletions": "55", "changes": "128"}, "updated": [0, 2, 6]}]}
{"author": "dchvn", "sha": "9e00238130f1a4c04dfcd6e3a68e07d1fd58d558", "commit_date": "2021/10/18 04:14:13", "commit_message": "remove unused error classes", "title": "[SPARK-36296][SQL] Refactor seventh set of 20 in QueryExecutionErrors to use error classes", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\nRefactor some exceptions in QueryExecutionErrors to use error classes.\r\n```\r\nmissingJdbcTableNameAndQueryError\r\nemptyOptionError\r\ninvalidJdbcTxnIsolationLevelError\r\ncannotGetJdbcTypeError\r\nunrecognizedSqlTypeError\r\nunsupportedJdbcTypeError\r\nunsupportedArrayElementTypeBasedOnBinaryError\r\nnestedArraysUnsupportedError\r\ncannotTranslateNonNullValueForFieldError\r\ninvalidJdbcNumPartitionsError\r\ntransactionUnsupportedByJdbcServerError\r\ndataTypeUnsupportedYetError\r\nunsupportedOperationForDataTypeError\r\ninputFilterNotFullyConvertibleError\r\ncannotReadFooterForFileError\r\ncannotReadFooterForFileError\r\nfoundDuplicateFieldInCaseInsensitiveModeError\r\nfailedToMergeIncompatibleSchemasError\r\nddlUnsupportedTemporarilyError\r\noperatingOnCanonicalizationPlanError\r\n```\r\n\r\n\r\n### Why are the changes needed?\r\nThere are currently ~350 exceptions in this file; so this PR only focuses on the seventh set of 20.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n\r\n### How was this patch tested?\r\nExisting UT Testcase\r\n", "failed_tests": [], "files": [{"file": {"name": "core/src/main/resources/error/README.md", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/resources/error/error-classes.json", "additions": "58", "deletions": "1", "changes": "59"}, "updated": [0, 1, 5]}, {"file": {"name": "core/src/main/scala/org/apache/spark/SparkException.scala", "additions": "18", "deletions": "4", "changes": "22"}, "updated": [0, 0, 5]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala", "additions": "38", "deletions": "36", "changes": "74"}, "updated": [0, 2, 6]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 4, 4]}]}
{"author": "dchvn", "sha": "b228cae60fc16774caa04435a9aac0f6cdb2bab1", "commit_date": "2021/10/07 02:29:41", "commit_message": "[SPARK-36304]: change error-class's name", "title": "[SPARK-36304][SQL] Refactor fifteenth set of 20 query execution errors to use error classes", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->Adds error classes to some of the exceptions in QueryExecutionErrors.\r\n\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->Improves auditing for developers and adds useful fields for users (error class and SQLSTATE).\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->Fills in missing error class and SQLSTATE fields.\r\n\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->Existing tests\r\n", "failed_tests": [], "files": [{"file": {"name": "core/src/main/resources/error/error-classes.json", "additions": "71", "deletions": "0", "changes": "71"}, "updated": [0, 1, 5]}, {"file": {"name": "core/src/main/scala/org/apache/spark/SparkException.scala", "additions": "13", "deletions": "0", "changes": "13"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala", "additions": "69", "deletions": "39", "changes": "108"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/RowTest.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/encoders/EncoderErrorMessageSuite.scala", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/util/ArrowUtilsSuite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/sources/RateStreamProviderSuite.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/sources/TextSocketStreamSuite.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 1, 1]}]}
{"author": "dchvn", "sha": "0870b3a8e7e35f0e7f675ac6c22ff168560f2494", "commit_date": "2021/10/05 10:55:33", "commit_message": "fix error", "title": "[SPARK-36292][SQL] Refactor third set of 20 in QueryExecutionErrors to use error classes", "body": "### What changes were proposed in this pull request?\r\nRefactor some exceptions in QueryExecutionErrors to use error classes.\r\n```\r\ncannotGenerateCodeForUnsupportedTypeError\r\ncannotInterpolateClassIntoCodeBlockError\r\ncustomCollectionClsNotResolvedError\r\nclassUnsupportedByMapObjectsError\r\nnullAsMapKeyNotAllowedError\r\nmethodNotDeclaredError\r\nconstructorNotFoundError\r\nprimaryConstructorNotFoundError\r\nunsupportedNaturalJoinTypeError\r\nnotExpectedUnresolvedEncoderError\r\nunsupportedEncoderError\r\nnotOverrideExpectedMethodsError\r\nfailToConvertValueToJsonError\r\nunexpectedOperatorInCorrelatedSubquery\r\nunreachableError\r\nunsupportedRoundingMode\r\nresolveCannotHandleNestedSchema\r\ninputExternalRowCannotBeNullError\r\nfieldCannotBeNullMsg\r\nfieldCannotBeNullError\r\n```\r\n\r\n\r\n### Why are the changes needed?\r\nThere are currently ~350 exceptions in this file; so this PR only focuses on the third set of 20.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n\r\n### How was this patch tested?\r\nExisted UT Testcase\r\n", "failed_tests": ["org.apache.spark.scheduler.BasicSchedulerIntegrationSuite", "org.apache.spark.SparkThrowableSuite"], "files": [{"file": {"name": "core/src/main/resources/error/error-classes.json", "additions": "77", "deletions": "4", "changes": "81"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 5]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala", "additions": "41", "deletions": "25", "changes": "66"}, "updated": [0, 1, 4]}]}
{"author": "dchvn", "sha": "c3b32bdd7d2a79bf4d90ea795fcbd95f86f2fb41", "commit_date": "2021/10/20 13:51:02", "commit_message": "Inline status.py", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/status.py", "additions": "12", "deletions": "6", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/status.pyi", "additions": "0", "deletions": "42", "changes": "42"}, "updated": [0, 0, 0]}]}
{"author": "dchvn", "sha": "d2b819d2e1d6e229aaad5804c5e0417ba157bcf9", "commit_date": "2021/08/23 15:12:18", "commit_message": "[SPARK-36396] Implement_DataFrame.cov", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/frame.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "181", "deletions": "0", "changes": "181"}, "updated": [0, 0, 13]}, {"file": {"name": "python/pyspark/pandas/missing/frame.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "59", "deletions": "1", "changes": "60"}, "updated": [0, 0, 6]}]}
{"author": "dchvn", "sha": "2cd0af1b5c2f0e54669398eab53e4254fe3239ce", "commit_date": "2021/08/25 13:02:40", "commit_message": "[SPARK-36402][PYTHON] Implement series.combine", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/series.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 9]}, {"file": {"name": "python/pyspark/pandas/missing/series.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/series.py", "additions": "168", "deletions": "0", "changes": "168"}, "updated": [0, 0, 10]}, {"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames.py", "additions": "38", "deletions": "0", "changes": "38"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_series.py", "additions": "61", "deletions": "0", "changes": "61"}, "updated": [0, 1, 4]}]}
{"author": "dchvn", "sha": "9be6ab38e2a71c36049c25d7754273f52de46769", "commit_date": "2021/09/21 01:51:17", "commit_message": "[SPARK-36293][SQL] Refactor fourth set of 20 query execution errors", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/resources/error/error-classes.json", "additions": "60", "deletions": "0", "changes": "60"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/AnalysisException.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala", "additions": "73", "deletions": "27", "changes": "100"}, "updated": [1, 2, 4]}]}
{"author": "wangyum", "sha": "affe2f64457c67c133e45cfd6de372649d10e2cb", "commit_date": "2021/10/22 04:58:08", "commit_message": "Remove javax.annotation-api", "title": "[SPARK-37090][BUILD] Upgrade libthrift to avoid security vulnerabilities", "body": "### What changes were proposed in this pull request?\r\n\r\nThis pr backport HIVE-21498 to upgrade libthrift to 0.13.0.\r\n\r\n### Why are the changes needed?\r\n\r\nTo addresses CVEs:\r\n\r\nComponent Name | Component Version Name | Vulnerability | Fixed version\r\n-- | -- | -- | --\r\nApache Thrift | 0.11.0-4. | [CVE-2019-0205](https://nvd.nist.gov/vuln/detail/CVE-2019-0205) | 0.13.0\r\nApache Thrift | 0.11.0-4. | CVE-2019-0210 | 0.13.0\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nNo.\r\n\r\n### How was this patch tested?\r\n\r\nExisting test.", "failed_tests": [], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-2.7-hive-2.3", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 3, 14]}, {"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 3, 15]}, {"file": {"name": "pom.xml", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [3, 9, 29]}, {"file": {"name": "sql/hive-thriftserver/src/main/java/org/apache/hive/service/auth/TSetIpAddressProcessor.java", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}]}
{"author": "zero323", "sha": "846cdc0e200d9ab8cc009b0d3c39122116f9d6fd", "commit_date": "2021/10/12 19:55:32", "commit_message": "Run mypy tests against ml, sql, streaming and core examples", "title": "[SPARK-36997][PYTHON][TESTS] Run mypy tests against ml, sql, streaming and core examples", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\nThis PR:\r\n\r\n- Adds `mypy_examples_test` and `mypy_annotation_test` and refactors `mypy_test` in `def/lint-python` to enable testing PySpark examples with mypy.\r\n- Adjusts examples for `ml`, `sql`, `streaming` and core to address detected type checking issues.\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\nThe goal of this PR is to improve test coverage of type hints.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\nIn general, no. \r\n\r\nThe only change, directly visible to the end user, are small adjustments to the example scripts.\r\n\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\n\r\nExisting tests with additions listed above.\r\n", "failed_tests": [], "files": [{"file": {"name": "dev/lint-python", "additions": "44", "deletions": "16", "changes": "60"}, "updated": [0, 0, 1]}, {"file": {"name": "examples/src/main/python/__init__.py", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/python/als.py", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/python/avro_inputformat.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/python/ml/__init__,py", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/python/ml/chi_square_test_example.py", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/python/ml/correlation_example.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/python/mllib/__init__.py", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/python/parquet_inputformat.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/python/sort.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/python/sql/__init__.py", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/python/sql/streaming/__init__,py", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/python/sql/streaming/structured_network_wordcount_windowed.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/python/streaming/__init__.py", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/python/streaming/network_wordjoinsentiments.py", "additions": "10", "deletions": "2", "changes": "12"}, "updated": [0, 0, 0]}]}
{"author": "zero323", "sha": "69a772faf3069e566cb1b8069f49e0e922bd6043", "commit_date": "2021/10/24 11:12:57", "commit_message": "Make DStream covariant", "title": "[SPARK-37104][PYTHON] Make RDD and DStream covariant", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\nThis PR changes changes `RDD[~T}` and `DStream[~T]` to `RDD[+T]` and `DStream[+T]` respectively.\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\nTo improve usability of the current annotations and simplify further development of type hints.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\nType hints only.\r\n\r\nUsers will be able to use both subclasses of `RDD` / `DStream` in certain contexts, without explicit annotations or casts.\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->\r\n\r\nExisting tests and not released data tests (SPARK-36989).\r\n", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/rdd.pyi", "additions": "53", "deletions": "52", "changes": "105"}, "updated": [1, 1, 1]}, {"file": {"name": "python/pyspark/streaming/dstream.pyi", "additions": "29", "deletions": "28", "changes": "57"}, "updated": [0, 0, 0]}]}
{"author": "zero323", "sha": "34559c901b93221f2866030248fcd1f3dfa82e0d", "commit_date": "2021/10/16 08:35:07", "commit_message": "Reposition type: ingores or replace with cast", "title": "[WIP][SPARK-37022][PYTHON] Use black as a formatter for  PySpark", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\nThis PR applies `black` (21.5b2) formatting to the whole `python/pyspark` source tree.\r\n\r\nAdditionally, the following changes were made:\r\n\r\n- Disabled E501 (line too long) in pycodestyle config \u2012 black allows line to exceed `line-length` in cases of inline comments.  There are 15 cases like this, all listed below\r\n    ```\r\n    pycodestyle checks failed:\r\n    ./python/pyspark/sql/catalog.py:349:101: E501 line too long (103 > 100 characters)\r\n    ./python/pyspark/sql/session.py:652:101: E501 line too long (106 > 100 characters)\r\n    ./python/pyspark/sql/utils.py:50:101: E501 line too long (108 > 100 characters)\r\n    ./python/pyspark/sql/streaming.py:1063:101: E501 line too long (128 > 100 characters)\r\n    ./python/pyspark/sql/streaming.py:1071:101: E501 line too long (112 > 100 characters)\r\n    ./python/pyspark/sql/streaming.py:1080:101: E501 line too long (124 > 100 characters)\r\n    ./python/pyspark/sql/streaming.py:1259:101: E501 line too long (134 > 100 characters)\r\n    ./python/pyspark/sql/pandas/conversion.py:136:101: E501 line too long (106 > 100 characters)\r\n    ./python/pyspark/ml/param/_shared_params_code_gen.py:111:101: E501 line too long (103 > 100 characters)\r\n    ./python/pyspark/ml/param/_shared_params_code_gen.py:136:101: E501 line too long (105 > 100 characters)\r\n    ./python/pyspark/ml/param/_shared_params_code_gen.py:163:101: E501 line too long (101 > 100 characters)\r\n    ./python/pyspark/ml/param/_shared_params_code_gen.py:233:101: E501 line too long (101 > 100 characters)\r\n    ./python/pyspark/ml/param/_shared_params_code_gen.py:265:101: E501 line too long (101 > 100 characters)\r\n    ./python/pyspark/tests/test_readwrite.py:235:101: E501 line too long (114 > 100 characters)\r\n    ./python/pyspark/tests/test_readwrite.py:336:101: E501 line too long (114 > 100 characters)\r\n    ```\r\n- After reformatting, minor typing changes were made:\r\n   - Realign certain `type: ignore` comments with ignored code.\r\n   -  Apply explicit `casts` to  ` @unittest.skipIf` messages.  The following\r\n      ```python\r\n      @unittest.skipIf(\r\n          not have_pandas or not have_pyarrow, pandas_requirement_message or pyarrow_requirement_message\r\n      )  # type: ignore[arg-type]\r\n      ```\r\n      replaced with\r\n              \r\n      ```python\r\n      @unittest.skipIf(\r\n          not have_pandas or not have_pyarrow, \r\n          cast(str, pandas_requirement_message or pyarrow_requirement_message),\r\n      )\r\n      ```\r\n\r\n\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\nConsistency and reduced maintenance overhead.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\nYes.\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\n\r\nExisting liners and tests.\r\n", "failed_tests": [], "files": [{"file": {"name": "dev/lint-python", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "dev/reformat-python", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "dev/tox.ini", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyproject.toml", "additions": "22", "deletions": "0", "changes": "22"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/__init__.py", "additions": "32", "deletions": "9", "changes": "41"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/_globals.py", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/accumulators.py", "additions": "12", "deletions": "8", "changes": "20"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/accumulators.pyi", "additions": "1", "deletions": "3", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/broadcast.py", "additions": "9", "deletions": "11", "changes": "20"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/conf.py", "additions": "5", "deletions": "3", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/context.py", "additions": "222", "deletions": "87", "changes": "309"}, "updated": [1, 1, 1]}, {"file": {"name": "python/pyspark/daemon.py", "additions": "7", "deletions": "6", "changes": "13"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/files.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/find_spark_home.py", "additions": "10", "deletions": "5", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/install.py", "additions": "21", "deletions": "17", "changes": "38"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/java_gateway.py", "additions": "14", "deletions": "17", "changes": "31"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/join.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/__init__.py", "additions": "44", "deletions": "8", "changes": "52"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/_typing.pyi", "additions": "1", "deletions": "3", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/base.py", "additions": "22", "deletions": "12", "changes": "34"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/base.pyi", "additions": "2", "deletions": "6", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/classification.py", "additions": "901", "deletions": "358", "changes": "1259"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/classification.pyi", "additions": "31", "deletions": "36", "changes": "67"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/clustering.py", "additions": "380", "deletions": "155", "changes": "535"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/clustering.pyi", "additions": "12", "deletions": "16", "changes": "28"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/common.py", "additions": "15", "deletions": "14", "changes": "29"}, "updated": [1, 1, 1]}, {"file": {"name": "python/pyspark/ml/evaluation.py", "additions": "278", "deletions": "116", "changes": "394"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/evaluation.pyi", "additions": "15", "deletions": "21", "changes": "36"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/feature.py", "additions": "1123", "deletions": "510", "changes": "1633"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/ml/feature.pyi", "additions": "85", "deletions": "143", "changes": "228"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/fpm.py", "additions": "98", "deletions": "51", "changes": "149"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/fpm.pyi", "additions": "5", "deletions": "7", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/functions.py", "additions": "11", "deletions": "11", "changes": "22"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/functions.pyi", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/image.py", "additions": "19", "deletions": "15", "changes": "34"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/linalg/__init__.py", "additions": "142", "deletions": "98", "changes": "240"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/linalg/__init__.pyi", "additions": "4", "deletions": "12", "changes": "16"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/param/__init__.py", "additions": "21", "deletions": "11", "changes": "32"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/param/_shared_params_code_gen.py", "additions": "170", "deletions": "72", "changes": "242"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/param/shared.py", "additions": "197", "deletions": "41", "changes": "238"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/pipeline.py", "additions": "33", "deletions": "19", "changes": "52"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/pipeline.pyi", "additions": "1", "deletions": "3", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/recommendation.py", "additions": "149", "deletions": "57", "changes": "206"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/recommendation.pyi", "additions": "5", "deletions": "11", "changes": "16"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/regression.py", "additions": "716", "deletions": "240", "changes": "956"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/regression.pyi", "additions": "18", "deletions": "26", "changes": "44"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/stat.py", "additions": "22", "deletions": "13", "changes": "35"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/stat.pyi", "additions": "2", "deletions": "6", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/tests/test_algorithms.py", "additions": "153", "deletions": "92", "changes": "245"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/tests/test_base.py", "additions": "16", "deletions": "10", "changes": "26"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/tests/test_evaluation.py", "additions": "14", "deletions": "6", "changes": "20"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/tests/test_feature.py", "additions": "172", "deletions": "82", "changes": "254"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/tests/test_image.py", "additions": "19", "deletions": "13", "changes": "32"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/tests/test_linalg.py", "additions": "76", "deletions": "69", "changes": "145"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/tests/test_param.py", "additions": "88", "deletions": "41", "changes": "129"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/tests/test_persistence.py", "additions": "136", "deletions": "78", "changes": "214"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/tests/test_pipeline.py", "additions": "6", "deletions": "4", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/tests/test_stat.py", "additions": "9", "deletions": "7", "changes": "16"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/tests/test_training_summary.py", "additions": "97", "deletions": "55", "changes": "152"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/tests/test_tuning.py", "additions": "355", "deletions": "295", "changes": "650"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/ml/tests/test_util.py", "additions": "22", "deletions": "21", "changes": "43"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/tests/test_wrapper.py", "additions": "21", "deletions": "12", "changes": "33"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/tree.py", "additions": "150", "deletions": "79", "changes": "229"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/tree.pyi", "additions": "1", "deletions": "3", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/tuning.py", "additions": "298", "deletions": "177", "changes": "475"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/ml/tuning.pyi", "additions": "5", "deletions": "7", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/ml/util.py", "additions": "56", "deletions": "39", "changes": "95"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/wrapper.py", "additions": "10", "deletions": "3", "changes": "13"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/wrapper.pyi", "additions": "1", "deletions": "3", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/mllib/__init__.py", "additions": "14", "deletions": "3", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/mllib/classification.py", "additions": "162", "deletions": "64", "changes": "226"}, "updated": [1, 1, 1]}, {"file": {"name": "python/pyspark/mllib/clustering.py", "additions": "155", "deletions": "75", "changes": "230"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/mllib/clustering.pyi", "additions": "5", "deletions": "15", "changes": "20"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/mllib/common.py", "additions": "19", "deletions": "17", "changes": "36"}, "updated": [1, 1, 1]}, {"file": {"name": "python/pyspark/mllib/evaluation.py", "additions": "69", "deletions": "56", "changes": "125"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/mllib/evaluation.pyi", "additions": "1", "deletions": "3", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/mllib/feature.py", "additions": "88", "deletions": "49", "changes": "137"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/mllib/feature.pyi", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/mllib/fpm.py", "additions": "9", "deletions": "9", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/mllib/fpm.pyi", "additions": "1", "deletions": "3", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/mllib/linalg/__init__.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/mllib/linalg/__init__.pyi", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/mllib/linalg/distributed.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/mllib/linalg/distributed.pyi", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/mllib/random.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/mllib/recommendation.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/mllib/recommendation.pyi", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/mllib/regression.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/mllib/regression.pyi", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/mllib/stat/KernelDensity.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/mllib/stat/__init__.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/mllib/stat/_statistics.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/mllib/stat/_statistics.pyi", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/mllib/stat/distribution.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/mllib/tests/test_algorithms.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/mllib/tests/test_feature.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/mllib/tests/test_linalg.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/mllib/tests/test_stat.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/mllib/tests/test_streaming_algorithms.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/mllib/tests/test_util.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/mllib/tree.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/mllib/tree.pyi", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/mllib/util.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/base.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [2, 2, 5]}, {"file": {"name": "python/pyspark/pandas/config.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [1, 2, 3]}, {"file": {"name": "python/pyspark/pandas/data_type_ops/base.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [1, 1, 3]}, {"file": {"name": "python/pyspark/pandas/data_type_ops/categorical_ops.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [2, 5, 15]}, {"file": {"name": "python/pyspark/pandas/generic.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [1, 2, 3]}, {"file": {"name": "python/pyspark/pandas/groupby.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [1, 5, 9]}, {"file": {"name": "python/pyspark/pandas/indexes/base.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 2, 6]}, {"file": {"name": "python/pyspark/pandas/namespace.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [2, 4, 8]}, {"file": {"name": "python/pyspark/pandas/plot/matplotlib.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/series.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [2, 3, 9]}, {"file": {"name": "python/pyspark/pandas/sql_processor.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [1, 2, 3]}, {"file": {"name": "python/pyspark/pandas/utils.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [1, 2, 3]}, {"file": {"name": "python/pyspark/profiler.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/profiler.pyi", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/rdd.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [1, 1, 1]}, {"file": {"name": "python/pyspark/rdd.pyi", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/rddsampler.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/resource/__init__.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/resource/profile.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/resource/requests.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/resource/tests/test_resources.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/serializers.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/shell.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/shuffle.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/__init__.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/avro/__init__.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/avro/functions.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/catalog.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 2, 3]}, {"file": {"name": "python/pyspark/sql/column.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/sql/conf.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/context.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/dataframe.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [1, 3, 6]}, {"file": {"name": "python/pyspark/sql/functions.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [2, 6, 10]}, {"file": {"name": "python/pyspark/sql/group.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [1, 1, 1]}, {"file": {"name": "python/pyspark/sql/observation.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 3, 3]}, {"file": {"name": "python/pyspark/sql/pandas/_typing/__init__.pyi", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/pandas/_typing/protocols/frame.pyi", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/sql/pandas/_typing/protocols/series.pyi", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/pandas/conversion.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 1, 4]}, {"file": {"name": "python/pyspark/sql/pandas/functions.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/pandas/functions.pyi", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/pandas/group_ops.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/sql/pandas/map_ops.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/sql/pandas/serializers.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/sql/pandas/typehints.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/sql/pandas/types.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 1, 3]}, {"file": {"name": "python/pyspark/sql/pandas/utils.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/sql/readwriter.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [1, 1, 1]}, {"file": {"name": "python/pyspark/sql/session.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [1, 2, 4]}, {"file": {"name": "python/pyspark/sql/streaming.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [1, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_arrow.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_catalog.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/tests/test_column.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_conf.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/tests/test_context.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/tests/test_dataframe.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [1, 1, 2]}, {"file": {"name": "python/pyspark/sql/tests/test_datasources.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/tests/test_functions.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/sql/tests/test_group.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/tests/test_pandas_cogrouped_map.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/tests/test_pandas_grouped_map.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/tests/test_pandas_map.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/tests/test_pandas_udf.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_pandas_udf_grouped_agg.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/tests/test_pandas_udf_scalar.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/tests/test_pandas_udf_typehints.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/tests/test_pandas_udf_window.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/tests/test_readwriter.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/tests/test_serde.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/tests/test_session.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/tests/test_streaming.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_types.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_udf.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_utils.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [1, 3, 3]}, {"file": {"name": "python/pyspark/sql/types.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [1, 1, 4]}, {"file": {"name": "python/pyspark/sql/udf.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/utils.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [2, 3, 4]}, {"file": {"name": "python/pyspark/sql/window.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 2, 2]}, {"file": {"name": "python/pyspark/statcounter.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/status.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/storagelevel.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/streaming/__init__.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/streaming/context.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/streaming/context.pyi", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/streaming/dstream.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/streaming/dstream.pyi", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/streaming/kinesis.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/streaming/listener.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/streaming/tests/test_context.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/streaming/tests/test_dstream.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/streaming/tests/test_kinesis.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/streaming/tests/test_listener.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/streaming/util.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/taskcontext.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/testing/mllibutils.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/testing/mlutils.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/testing/pandasutils.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/testing/sqlutils.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/testing/streamingutils.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/testing/utils.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/tests/test_appsubmit.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/tests/test_broadcast.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/tests/test_conf.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/tests/test_context.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/tests/test_daemon.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/tests/test_install_spark.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/tests/test_join.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/tests/test_pin_thread.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/tests/test_profiler.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/tests/test_rdd.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/tests/test_rddbarrier.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/tests/test_readwrite.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/tests/test_serializers.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/tests/test_shuffle.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/tests/test_taskcontext.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/tests/test_util.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/tests/test_worker.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/traceback_utils.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/util.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/worker.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}]}
{"author": "zero323", "sha": "a8401332952fb9f6d3c199b1d688d69c98b6545f", "commit_date": "2021/10/22 20:32:03", "commit_message": "Unindent lines", "title": "[WIP][SPARK-37085][PYTHON][SQL] Add list/tuple overloads to array, struct, create_map, map_concat", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\nThis PR adds overloads to the following `pyspark.sql.functions`:\r\n\r\n- `array`\r\n- `struct`\r\n- `create_map`\r\n- `map_concat`\r\n\r\nto support calls with a single `list` or `tuple` argument, i.e.\r\n\r\n```python\r\narray([\"foo\", \"bar\"])\r\n```\r\n\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\nThese calls are supported by the current implementation, but don't type check.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\nType checker only, as described above.\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->\r\n\r\nExisting tests and manual tests (to be added in SPARK-36989)", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/sql/functions.py", "additions": "64", "deletions": "8", "changes": "72"}, "updated": [2, 7, 13]}]}
{"author": "zero323", "sha": "f7dcc0271cc8d4975b68ef9c7d54165a6be0ae8d", "commit_date": "2021/09/29 22:23:01", "commit_message": "Support str schema in RDD.toDF annotations", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/rdd.pyi", "additions": "11", "deletions": "4", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/_typing.pyi", "additions": "12", "deletions": "0", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/context.py", "additions": "16", "deletions": "22", "changes": "38"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/session.py", "additions": "6", "deletions": "13", "changes": "19"}, "updated": [0, 0, 2]}]}
{"author": "sunchao", "sha": "96fe294dbb6f05f109d5b92802e44b88cf0dcb08", "commit_date": "2021/10/13 01:13:29", "commit_message": "rebase", "title": "[SPARK-36935][SQL] Extend ParquetSchemaConverter to compute Parquet repetition & definition level", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\nThis PR includes the following:\r\n1. adds a new class `ParquetColumn`, which is a wrapper on a Spark `DataType` with additional Parquet column information, including its max repetition level & definition level, path from the root schema, column descriptor if the node is a leaf, the children nodes is it is a non-leaf, etc. This is needed to support complex type in the vectorized path, where we need to assemble a column vector of complex type using these information.\r\n2. extends `ParquetSchemaConverter` to convert from a Parquet `MessageType` to a `ParquetColumn`, mostly by piggy-backing on the existing logic. A new method `converParquetColumn` is added for this purpose, and the existing `convert` is changed to simply by calling the former.\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\nIn order to support complex type for the vectorized Parquet reader (see SPARK-34863 for more info), we'll need to capture Parquet specific information such as max repetition/definition level for Spark's `DataType`, so that we can assemble primitive column vectors into ones with complex type (e.g., struct, map, array).\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\nNo.\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\n\r\nExtended the test cases in `ParquetSchemaSuite`", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/java/org/apache/parquet/io/ColumnIOUtil.java", "additions": "40", "deletions": "0", "changes": "40"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase.java", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetColumn.scala", "additions": "68", "deletions": "0", "changes": "68"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "9", "deletions": "2", "changes": "11"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "146", "deletions": "41", "changes": "187"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "1202", "deletions": "39", "changes": "1241"}, "updated": [0, 0, 1]}]}
{"author": "sunchao", "sha": "b5a1d077b486e4f6df8ad762854999df897451d4", "commit_date": "2021/06/09 19:26:29", "commit_message": "wip", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/physical/partitioning.scala", "additions": "196", "deletions": "29", "changes": "225"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/DistributionSuite.scala", "additions": "0", "deletions": "38", "changes": "38"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/ShuffleSpecSuite.scala", "additions": "413", "deletions": "0", "changes": "413"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AQEUtils.scala", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/bucketing/DisableUnnecessaryBucketedScan.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "80", "deletions": "31", "changes": "111"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ValidateRequirements.scala", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/joins/ShuffledJoin.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/objects.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinExec.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q17.sf100/explain.txt", "additions": "155", "deletions": "170", "changes": "325"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q17.sf100/simplified.txt", "additions": "74", "deletions": "81", "changes": "155"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q25.sf100/explain.txt", "additions": "155", "deletions": "170", "changes": "325"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q25.sf100/simplified.txt", "additions": "74", "deletions": "81", "changes": "155"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q29.sf100/explain.txt", "additions": "173", "deletions": "188", "changes": "361"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q29.sf100/simplified.txt", "additions": "75", "deletions": "82", "changes": "157"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q47.sf100/explain.txt", "additions": "110", "deletions": "125", "changes": "235"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q47.sf100/simplified.txt", "additions": "80", "deletions": "89", "changes": "169"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q57.sf100/explain.txt", "additions": "110", "deletions": "125", "changes": "235"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q57.sf100/simplified.txt", "additions": "80", "deletions": "89", "changes": "169"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q72.sf100/explain.txt", "additions": "197", "deletions": "207", "changes": "404"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q72.sf100/simplified.txt", "additions": "105", "deletions": "111", "changes": "216"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q24.sf100/explain.txt", "additions": "61", "deletions": "43", "changes": "104"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q24.sf100/simplified.txt", "additions": "36", "deletions": "33", "changes": "69"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q47.sf100/explain.txt", "additions": "110", "deletions": "125", "changes": "235"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q47.sf100/simplified.txt", "additions": "80", "deletions": "89", "changes": "169"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q51a.sf100/explain.txt", "additions": "249", "deletions": "259", "changes": "508"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q51a.sf100/simplified.txt", "additions": "100", "deletions": "106", "changes": "206"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q57.sf100/explain.txt", "additions": "110", "deletions": "125", "changes": "235"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q57.sf100/simplified.txt", "additions": "80", "deletions": "89", "changes": "169"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q64/explain.txt", "additions": "414", "deletions": "424", "changes": "838"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q64/simplified.txt", "additions": "259", "deletions": "265", "changes": "524"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q72.sf100/explain.txt", "additions": "197", "deletions": "207", "changes": "404"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q72.sf100/simplified.txt", "additions": "105", "deletions": "111", "changes": "216"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/PlannerSuite.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/exchange/EnsureRequirementsSuite.scala", "additions": "460", "deletions": "4", "changes": "464"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/sources/BucketedReadSuite.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingJoinSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}]}
{"author": "weixiuli", "sha": "28c25b677779b373dc28018f3e209c77b2f69344", "commit_date": "2021/02/08 03:25:13", "commit_message": "[SPARK-37028][UI] Add a 'kill' executor link in the Web UI.", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/executorspage-template.html", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/executorspage.js", "additions": "35", "deletions": "1", "changes": "36"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/utils.js", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/ui/JettyUtils.scala", "additions": "14", "deletions": "2", "changes": "16"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/ui/SparkUI.scala", "additions": "9", "deletions": "1", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/ui/exec/ExecutorsTab.scala", "additions": "24", "deletions": "1", "changes": "25"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/ui/jobs/JobsTab.scala", "additions": "0", "deletions": "4", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala", "additions": "0", "deletions": "4", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/ui/UISeleniumSuite.scala", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [0, 1, 2]}]}
{"author": "zhengruifeng", "sha": "3c518652d74e50bcb85024dd5e5e75dc63121ce7", "commit_date": "2021/10/25 02:31:02", "commit_message": "move rule to operatorOptimizationBatch", "title": "[SPARK-37099][SQL] Impl a rank-based filter to optimize top-k computation", "body": "### What changes were proposed in this pull request?\r\nadd a new node `RankLimit` to filter out uncessary data in the map side.\r\n\r\n\r\n### Why are the changes needed?\r\n1, reduce the shuffle write;\r\n2, solve skewed-window problem, a practical case was optimized from 2.5h to 26min\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\na new config is added\r\n\r\n\r\n### How was this patch tested?\r\nadded testsuits", "failed_tests": ["org.apache.spark.sql.execution.metric.SQLMetricsSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "76", "deletions": "1", "changes": "77"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/rules/RuleIdCollection.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreePatterns.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [2, 7, 11]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/InsertRankLimitSuite.scala", "additions": "161", "deletions": "0", "changes": "161"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/window/RankLimitExec.scala", "additions": "71", "deletions": "0", "changes": "71"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameWindowFunctionsSuite.scala", "additions": "129", "deletions": "0", "changes": "129"}, "updated": [0, 1, 1]}]}
{"author": "kazuyukitanimura", "sha": "b3f68642191f6e9dcfebc6a2c1076f9cbc076aca", "commit_date": "2021/08/31 06:50:05", "commit_message": "[SPARK-36665][SQL] Add more Not operator simplifications\n\n ### What changes were proposed in this pull request?\nThis PR proposes to add more Not operator simplifications in `BooleanSimplification` by applying the following rules\n  - Not(null) == null\n    - e.g. IsNull(Not(...)) can be IsNull(...)\n  - (Not(a) = b) == (a = Not(b))\n    - e.g. Not(...) = true can be (...) = false\n  - (a != b) == (a = Not(b))\n    - e.g. (...) != true can be (...) = false\n\n ### Why are the changes needed?\nThe following query does not push down the filter in the current implementation\n```\nSELECT * FROM t WHERE (not boolean_col) <=> null\n```\nalthough the following equivalent query pushes down the filter as expected.\n```\nSELECT * FROM t WHERE boolean_col <=> null\n```\nThat is because the first query creates `IsNull(Not(boolean_col))` in the current implementation, which should be able to get simplified further to `IsNull(boolean_col)`\nThis PR helps optimizing such cases.\n\n ### Does this PR introduce _any_ user-facing change?\nNo\n\n ### How was this patch tested?\nAdded unit tests\n```\nbuild/sbt \"testOnly *BooleanSimplificationSuite  -- -z SPARK-36665\"\n```", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 2, 11]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/expressions.scala", "additions": "80", "deletions": "0", "changes": "80"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/rules/RuleIdCollection.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NotPropagationSuite.scala", "additions": "176", "deletions": "0", "changes": "176"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NullDownPropagationSuite.scala", "additions": "59", "deletions": "0", "changes": "59"}, "updated": [0, 0, 0]}]}
{"author": "wankunde", "sha": "8fa9e86308ec6e66929a30c191cacfcafe2073a6", "commit_date": "2021/10/25 08:11:59", "commit_message": "Update code", "title": "[SPARK-36967][CORE] Report accurate shuffle block size if its skewed", "body": "### What changes were proposed in this pull request?\r\n\r\nA shuffle block is considered as skewed and will be accurately recorded in HighlyCompressedMapStatus if its size if larger than this factor multiplying  the median shuffle block size.\r\n\r\nBefore this change \r\n\r\n![map_status_before](https://user-images.githubusercontent.com/3626747/137251903-08a3544c-dc77-4b78-8ae5-93b42a54bd03.png)\r\n\r\nAfter this change\r\n\r\n![map_status_after](https://user-images.githubusercontent.com/3626747/137251871-355db24d-d66b-4702-8766-216db30a39e0.jpg)\r\n\r\n### Why are the changes needed?\r\n\r\nNow map task will report accurate shuffle block size if the block size is greater than \"spark.shuffle.accurateBlockThreshold\"( 100M by default ). But if there are a large number of map tasks and the shuffle block sizes of these tasks are smaller than \"spark.shuffle.accurateBlockThreshold\", there may be unrecognized data skew.\r\n\r\nFor example, there are 10000 map task and 10000 reduce task, and each map task create 50M shuffle blocks for reduce 0, and 10K shuffle blocks for the left reduce tasks, reduce 0 is data skew, but the stat of this plan do not have this information.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\n\r\nUpdate exists UTs\r\n", "failed_tests": ["pyspark.pandas.tests.indexes.test_base", "org.apache.spark.sql.avro.AvroV1Suite", "org.apache.spark.sql.avro.AvroV2Suite", "org.apache.spark.sql.hive.thriftserver.HiveThriftBinaryServerSuite", "org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.streaming.StreamingContextSuite", "org.apache.spark.streaming.CheckpointSuite", "org.apache.spark.streaming.StreamingListenerSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveDDLSuite", "org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite", "org.apache.spark.sql.hive.execution.HiveQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.HiveContextCompatibilitySuite", "org.apache.spark.sql.hive.orc.HiveOrcQuerySuite", "org.apache.spark.sql.hive.execution.command.ShowNamespacesSuite", "org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite", "org.apache.spark.sql.hive.execution.command.ShowTablesSuite", "org.apache.spark.sql.hive.CachedTableSuite", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.HiveCharVarcharTestSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.execution.command.TruncateTableSuite", "org.apache.spark.sql.hive.execution.command.AlterTableRenameSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.execution.command.DropTableSuite", "org.apache.spark.sql.hive.execution.PruningSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SQLQuerySuite", "org.apache.spark.sql.execution.command.v1.AlterTableRenameSuite", "org.apache.spark.sql.execution.arrow.ArrowConvertersSuite", "org.apache.spark.sql.DynamicPartitionPruningV1Suite", "org.apache.spark.sql.ReplaceNullWithFalseInPredicateEndToEndSuite", "org.apache.spark.sql.DeprecatedAPISuite", "org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCTableCatalogSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.TypedImperativeAggregateSuite", "org.apache.spark.sql.execution.datasources.binaryfile.BinaryFileFormatSuite", "org.apache.spark.sql.execution.PersistedViewTestSuite", "org.apache.spark.sql.connector.V1WriteFallbackSessionCatalogSuite", "org.apache.spark.sql.GeneratorFunctionSuite", "org.apache.spark.sql.execution.datasources.csv.CSVLegacyTimeParserSuite", "org.apache.spark.sql.DynamicPartitionPruningV2Suite", "org.apache.spark.sql.DynamicPartitionPruningV1Suite", "org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite", "org.apache.spark.sql.execution.ui.SQLAppStatusListenerSuite", "org.apache.spark.sql.connector.DataSourceV2SQLSuite", "org.apache.spark.sql.JoinSuite", "org.apache.spark.sql.CachedTableSuite", "org.apache.spark.sql.execution.GlobalTempViewTestSuite", "org.apache.spark.sql.UnwrapCastInComparisonEndToEndSuite", "org.apache.spark.sql.test.DataFrameReaderWriterSuite", "org.apache.spark.sql.connector.DataSourceV2DataFrameSessionCatalogSuite", "org.apache.spark.sql.sources.FilteredScanSuite", "org.apache.spark.sql.SubquerySuite", "org.apache.spark.sql.execution.command.v1.ShowNamespacesSuite", "org.apache.spark.sql.execution.command.v1.ShowTablesSuite", "org.apache.spark.sql.execution.RemoveRedundantProjectsSuite", "org.apache.spark.sql.execution.WholeStageCodegenSuite", "org.apache.spark.sql.streaming.test.DataStreamTableAPISuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.DSV2CharVarcharTestSuite", "org.apache.spark.sql.SparkSessionExtensionSuite", "org.apache.spark.sql.DataFrameSelfJoinSuite", "org.apache.spark.sql.ApproximatePercentileQuerySuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.execution.datasources.csv.CSVv1Suite", "org.apache.spark.sql.connector.DataSourceV2SQLSessionCatalogSuite", "org.apache.spark.sql.execution.LocalTempViewTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.execution.command.v2.ShowNamespacesSuite", "org.apache.spark.sql.execution.command.v1.TruncateTableSuite", "org.apache.spark.sql.DataFrameWriterV2Suite", "org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite", "org.apache.spark.sql.FileSourceCharVarcharTestSuite", "org.apache.spark.sql.execution.command.v2.AlterTableRenameSuite", "org.apache.spark.sql.connector.DataSourceV2DataFrameSuite", "org.apache.spark.sql.execution.command.v1.DropTableSuite", "org.apache.spark.sql.execution.joins.ExistenceJoinSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite", "org.apache.spark.sql.execution.datasources.PathFilterSuite", "org.apache.spark.sql.execution.command.v2.ShowTablesSuite", "org.apache.spark.sql.execution.command.v2.DropTableSuite", "org.apache.spark.sql.execution.command.v2.TruncateTableSuite", "org.apache.spark.sql.execution.RemoveRedundantProjectsSuite", "org.apache.spark.sql.ColumnExpressionSuite", "org.apache.spark.sql.execution.datasources.json.JsonV1Suite", "org.apache.spark.sql.connector.DataSourceV2Suite", "org.apache.spark.sql.execution.TakeOrderedAndProjectSuite", "org.apache.spark.sql.execution.datasources.json.JsonV2Suite", "org.apache.spark.sql.execution.datasources.json.JsonLegacyTimeParserSuite", "org.apache.spark.sql.execution.datasources.csv.CSVv2Suite", "org.apache.spark.sql.DataFrameRangeSuite", "org.apache.spark.sql.sources.InsertSuite", "org.apache.spark.sql.DataFrameSetOperationsSuite", "org.apache.spark.sql.execution.joins.OuterJoinSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "21", "deletions": "0", "changes": "21"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/MapStatus.scala", "additions": "29", "deletions": "3", "changes": "32"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/Utils.scala", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [0, 0, 6]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/MapStatusSuite.scala", "additions": "57", "deletions": "0", "changes": "57"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "3", "deletions": "12", "changes": "15"}, "updated": [0, 0, 2]}]}
{"author": "RabbidHY", "sha": "5cfa930d2aac6cb4a20a4aaeeb2256b811892a27", "commit_date": "2021/10/19 09:07:19", "commit_message": "Pull out sorting expressions", "title": "[SPARK-36763][SQL] Pull out complex sorting expressions", "body": "### What changes were proposed in this pull request?\r\n\r\nThis pr pull out complex sorting expressions if it is global sorting to reduce the evaluation of complex expressions. For example:\r\n```sql\r\nSELECT id AS a, id AS b FROM range(10) ORDER BY a - b\r\n```\r\nBefore this pr:\r\n```\r\n== Physical Plan ==\r\nAdaptiveSparkPlan isFinalPlan=false\r\n+- Sort [(a#0L - b#1L) ASC NULLS FIRST], true, 0\r\n   +- Exchange rangepartitioning((a#0L - b#1L) ASC NULLS FIRST, 5), ENSURE_REQUIREMENTS, [id=#12]\r\n      +- Project [id#2L AS a#0L, id#2L AS b#1L]\r\n         +- Range (0, 10, step=1, splits=2)\r\n```\r\nAfter this pr:\r\n```\r\n== Physical Plan ==\r\nAdaptiveSparkPlan isFinalPlan=false\r\n+- Project [a#0L, b#1L]\r\n   +- Sort [_sortingexpression#5L ASC NULLS FIRST], true, 0\r\n      +- Exchange rangepartitioning(_sortingexpression#5L ASC NULLS FIRST, 5), ENSURE_REQUIREMENTS, [id=#16]\r\n         +- Project [id#2L AS a#0L, id#2L AS b#1L, (id#2L - id#2L) AS _sortingexpression#5L]\r\n            +- Range (0, 10, step=1, splits=2)\r\n```\r\n\r\n### Why are the changes needed?\r\n\r\nImprove order performance if the sorting expressions contains complex expressions.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nNo.\r\n\r\n\r\n### How was this patch tested?\r\n\r\nUnit test and benchmark test:\r\n```scala\r\nimport org.apache.spark.benchmark.Benchmark\r\nval numRows = 1024 * 1024 * 10\r\nspark.sql(s\"CREATE TABLE t1 using parquet AS select id AS a, id AS b FROM range(${numRows}L)\")\r\nval benchmark = new Benchmark(\"Benchmark pull out ordering expressions\", numRows, minNumIters = 5)\r\n\r\nSeq(false, true).foreach { pullOutEnabled =>\r\n  val name = s\"Pull out ordering expressions ${if (pullOutEnabled) \"(Enabled)\" else \"(Disabled)\"}\"\r\n  benchmark.addCase(name) { _ =>\r\n    withSQLConf(\"spark.sql.pullOutOrderingExpressions\" -> s\"$pullOutEnabled\") {\r\n      spark.sql(\"SELECT t1.* FROM t1 ORDER BY translate(t1.a, '123', 'abc')\").write.format(\"noop\").mode(\"Overwrite\").save()\r\n    }\r\n  }\r\n}\r\nbenchmark.run()\r\n```\r\n```\r\nJava HotSpot(TM) 64-Bit Server VM 1.8.0_251-b08 on Mac OS X 10.15.7\r\nIntel(R) Core(TM) i9-9980HK CPU @ 2.40GHz\r\nBenchmark pull out ordering expressions:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n------------------------------------------------------------------------------------------------------------------------\r\nPull out ordering expressions (Disabled)           9232           9753         867          1.1         880.4       1.0X\r\nPull out ordering expressions (Enabled)            7084           7462         370          1.5         675.5       1.3X\r\n```", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 7]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/PullOutComplexExpressions.scala", "additions": "34", "deletions": "6", "changes": "40"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/PullOutSortingExpressionsSuite.scala", "additions": "75", "deletions": "0", "changes": "75"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/complexTypesSuite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q79.sf100/explain.txt", "additions": "9", "deletions": "9", "changes": "18"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q79.sf100/simplified.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q79/explain.txt", "additions": "9", "deletions": "9", "changes": "18"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q79/simplified.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q89.sf100/explain.txt", "additions": "9", "deletions": "9", "changes": "18"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q89.sf100/simplified.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q89/explain.txt", "additions": "9", "deletions": "9", "changes": "18"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q89/simplified.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q36.sf100/explain.txt", "additions": "9", "deletions": "9", "changes": "18"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q36.sf100/simplified.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q36/explain.txt", "additions": "9", "deletions": "9", "changes": "18"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q36/simplified.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q47.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q47.sf100/simplified.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q47/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q47/simplified.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q57.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q57.sf100/simplified.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q57/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q57/simplified.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q70.sf100/explain.txt", "additions": "9", "deletions": "9", "changes": "18"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q70.sf100/simplified.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q70/explain.txt", "additions": "9", "deletions": "9", "changes": "18"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q70/simplified.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q79.sf100/explain.txt", "additions": "9", "deletions": "9", "changes": "18"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q79.sf100/simplified.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q79/explain.txt", "additions": "9", "deletions": "9", "changes": "18"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q79/simplified.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q86.sf100/explain.txt", "additions": "9", "deletions": "9", "changes": "18"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q86.sf100/simplified.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q86/explain.txt", "additions": "9", "deletions": "9", "changes": "18"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q86/simplified.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q89.sf100/explain.txt", "additions": "9", "deletions": "9", "changes": "18"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q89.sf100/simplified.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q89/explain.txt", "additions": "9", "deletions": "9", "changes": "18"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q89/simplified.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q36a.sf100/explain.txt", "additions": "9", "deletions": "9", "changes": "18"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q36a.sf100/simplified.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q36a/explain.txt", "additions": "9", "deletions": "9", "changes": "18"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q36a/simplified.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q47.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q47.sf100/simplified.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q47/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q47/simplified.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q57.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q57.sf100/simplified.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q57/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q57/simplified.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q70a.sf100/explain.txt", "additions": "9", "deletions": "9", "changes": "18"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q70a.sf100/simplified.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q70a/explain.txt", "additions": "9", "deletions": "9", "changes": "18"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q70a/simplified.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q86a.sf100/explain.txt", "additions": "9", "deletions": "9", "changes": "18"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q86a.sf100/simplified.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q86a/explain.txt", "additions": "9", "deletions": "9", "changes": "18"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q86a/simplified.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "mkaravel", "sha": "769b245087e157fec2aec7adb874d6de71d15224", "commit_date": "2021/09/21 06:46:15", "commit_message": "[SPARK-38611][SQL] Add SQL functions for the BINARY data type for AND, OR, XOR, NOT", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "common/unsafe/src/main/java/org/apache/spark/unsafe/types/ByteArray.java", "additions": "212", "deletions": "0", "changes": "212"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala", "additions": "233", "deletions": "0", "changes": "233"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/functions.scala", "additions": "96", "deletions": "0", "changes": "96"}, "updated": [1, 3, 5]}, {"file": {"name": "sql/core/src/test/resources/sql-functions/sql-expression-schema.md", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/string-functions.sql", "additions": "93", "deletions": "1", "changes": "94"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/string-functions.sql.out", "additions": "601", "deletions": "1", "changes": "602"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/string-functions.sql.out", "additions": "601", "deletions": "1", "changes": "602"}, "updated": [0, 0, 0]}]}
{"author": "Kimahriman", "sha": "386fc9c25f57c8fddf91537fb9464fd07c933d30", "commit_date": "2021/10/15 11:34:18", "commit_message": "Add codegen support to array transform", "title": "[SPARK-37019][SQL] Add codegen support to array transform", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nThis PR adds codegen support to ArrayTransform. This is my first time playing around with codegen, so definitely looking for any feedback. I ran into several issues along the way which you'll see in some checks I had to add. Specifically:\r\n- I added lambda variable tracking to the codegen context, to make sure a function can't be split out while there is any active lambda variables\r\n- Made sure lambda functions themselves can never be considered for subexpression elimination\r\n- I still have to set the atomic references in the codegen to support any children with CodegenFallback or just that fallback to interpreted in their codegen path\r\n\r\nQuestions I have:\r\n- Does it make sense to support both the traditional ExprCode approach with lambda variables while also setting the atomic reference value for fallback cases? Or should I just use the atomic reference to get the value everywhere even in codegen cases since I have to set it anyway and simplify the code a little bit?\r\n- Are there any other ways code could be split that could break the ExprCode approach?\r\n- Obviously any other corner cases anyone can think of?\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nTo improve performance of transform operations, letting the children be codegen'd and participate in WholeStageCodegen\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nNo, only performance improvements.\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nExisting unit tests, let me know if there's other codegen-specific unit tests I should add.", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.HiveCharVarcharTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.DSV2CharVarcharTestSuite", "org.apache.spark.sql.DataFrameSuite", "org.apache.spark.sql.FileSourceCharVarcharTestSuite", "org.apache.spark.sql.DataFrameFunctionsSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala", "additions": "6", "deletions": "5", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "39", "deletions": "0", "changes": "39"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/higherOrderFunctions.scala", "additions": "118", "deletions": "4", "changes": "122"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [2, 2, 6]}]}
{"author": "crflynn", "sha": "4615139d32a1e329f24af4026c8a894452828ba6", "commit_date": "2021/10/21 12:48:36", "commit_message": "fix typehints docstrings", "title": "[SPARK-18621][PYTHON] make sql type reprs eval-able", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nThese changes update the `__repr__` methods of type classes in `pyspark.sql.types` to print string representations which are `eval`-able. In other words, any instance of a `DataType` will produce a repr which can be passed to `eval()` to create an identical instance.\r\n\r\nSimilar changes previously submitted: https://github.com/apache/spark/pull/25495\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nThis [bug](https://issues.apache.org/jira/browse/SPARK-18621) has been around for a while. The current implementation returns a string representation which is valid in scala rather than python. These changes fix the repr to be valid with python.\r\n\r\nThe [motivation](https://docs.python.org/3/library/functions.html#repr) is \"to return a string that would yield an object with the same value when passed to eval()\".\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\nExample:\r\n\r\nCurrent implementation:\r\n\r\n```python\r\nfrom pyspark.sql.types import *\r\n\r\nstruct = StructType([StructField('f1', StringType(), True)])\r\nrepr(struct)\r\n# StructType(List(StructField(f1,StringType,true)))\r\nnew_struct = eval(repr(struct))\r\n# Traceback (most recent call last):\r\n#   File \"<input>\", line 1, in <module>\r\n#   File \"<string>\", line 1, in <module>\r\n# NameError: name 'List' is not defined\r\n\r\nstruct_field = StructField('f1', StringType(), True)\r\nrepr(struct_field)\r\n# StructField(f1,StringType,true)\r\nnew_struct_field = eval(repr(struct_field))\r\n# Traceback (most recent call last):\r\n#   File \"<input>\", line 1, in <module>\r\n#   File \"<string>\", line 1, in <module>\r\n# NameError: name 'f1' is not defined\r\n```\r\n\r\nWith changes:\r\n\r\n```python\r\nfrom pyspark.sql.types import *\r\n\r\nstruct = StructType([StructField('f1', StringType(), True)])\r\nrepr(struct)\r\n# StructType([StructField('f1', StringType(), True)])\r\nnew_struct = eval(repr(struct))\r\nstruct == new_struct\r\n# True\r\n\r\nstruct_field = StructField('f1', StringType(), True)\r\nrepr(struct_field)\r\n# StructField('f1', StringType(), True)\r\nnew_struct_field = eval(repr(struct_field))\r\nstruct_field == new_struct_field\r\n# True\r\n```\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nThe changes include a test which asserts that an instance of each type is equal to the `eval` of its `repr`, as in the above example.\r\n", "failed_tests": ["pyspark.pandas.spark.utils"], "files": [{"file": {"name": "common/network-yarn/src/main/java/org/apache/spark/network/yarn/YarnShuffleService.java", "additions": "16", "deletions": "2", "changes": "18"}, "updated": [1, 1, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/SparkContext.scala", "additions": "19", "deletions": "0", "changes": "19"}, "updated": [1, 2, 5]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "dev/create-release/release-tag.sh", "additions": "7", "deletions": "6", "changes": "13"}, "updated": [1, 1, 1]}, {"file": {"name": "docs/running-on-yarn.md", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [1, 2, 4]}, {"file": {"name": "launcher/src/main/java/org/apache/spark/launcher/JavaModuleOptions.java", "additions": "47", "deletions": "0", "changes": "47"}, "updated": [1, 1, 1]}, {"file": {"name": "launcher/src/main/java/org/apache/spark/launcher/SparkSubmitCommandBuilder.java", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "pom.xml", "additions": "18", "deletions": "2", "changes": "20"}, "updated": [2, 8, 28]}, {"file": {"name": "project/SparkBuild.scala", "additions": "13", "deletions": "1", "changes": "14"}, "updated": [1, 5, 9]}, {"file": {"name": "python/pyspark/ml/functions.py", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/extensions.py", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/pandas/internal.py", "additions": "38", "deletions": "31", "changes": "69"}, "updated": [0, 2, 4]}, {"file": {"name": "python/pyspark/pandas/spark/utils.py", "additions": "21", "deletions": "8", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 4]}, {"file": {"name": "python/pyspark/pandas/tests/test_series.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 9]}, {"file": {"name": "python/pyspark/pandas/typedef/typehints.py", "additions": "25", "deletions": "25", "changes": "50"}, "updated": [0, 2, 10]}, {"file": {"name": "python/pyspark/sql/dataframe.py", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [2, 5, 8]}, {"file": {"name": "python/pyspark/sql/tests/test_dataframe.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/sql/tests/test_types.py", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/sql/types.py", "additions": "17", "deletions": "17", "changes": "34"}, "updated": [2, 3, 6]}, {"file": {"name": "sql/catalyst/pom.xml", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 5, 6]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/CatalogManager.scala", "additions": "10", "deletions": "6", "changes": "16"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala", "additions": "0", "deletions": "26", "changes": "26"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala", "additions": "34", "deletions": "0", "changes": "34"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/pom.xml", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 5, 10]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala", "additions": "34", "deletions": "5", "changes": "39"}, "updated": [1, 5, 7]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/command/functions.scala", "additions": "0", "deletions": "9", "changes": "9"}, "updated": [1, 2, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/command/views.scala", "additions": "1", "deletions": "20", "changes": "21"}, "updated": [1, 1, 1]}]}
{"author": "karenfeng", "sha": "cb76b8f593b0d676ea8123b360a39588fda88358", "commit_date": "2021/10/21 18:20:23", "commit_message": "some clean up\n\nSigned-off-by: Karen Feng <karen.feng@databricks.com>", "title": "[SPARK-37092][CORE][SQL] Add error class prefix to error message and enforce testing", "body": "### What changes were proposed in this pull request?\r\n\r\nAdds the error class prefix to error messages and checks that error classes are encountered in tests.\r\nIncidentally:\r\n- Cleans up existing error classes that overlap in functionality\r\n- Removes unused error classes\r\n- Adds tests for untested error classes\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\n", "failed_tests": ["org.apache.spark.shuffle.sort.ShuffleExternalSorterSuite", "org.apache.spark.SparkThrowableSuite", "org.apache.spark.sql.catalyst.expressions.LiteralExpressionSuite", "org.apache.spark.deploy.yarn.YarnClusterSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite"], "files": [{"file": {"name": "core/src/main/resources/error/error-classes.json", "additions": "10", "deletions": "20", "changes": "30"}, "updated": [0, 1, 4]}, {"file": {"name": "core/src/main/scala/org/apache/spark/ErrorInfo.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/SparkException.scala", "additions": "7", "deletions": "5", "changes": "12"}, "updated": [0, 0, 3]}, {"file": {"name": "core/src/test/scala/org/apache/spark/SparkThrowableSuite.scala", "additions": "40", "deletions": "5", "changes": "45"}, "updated": [0, 1, 3]}, {"file": {"name": "external/docker-integration-tests/src/test/scala/org/apache/spark/sql/jdbc/OracleIntegrationSuite.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/ArrayData.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala", "additions": "5", "deletions": "4", "changes": "9"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala", "additions": "9", "deletions": "17", "changes": "26"}, "updated": [0, 2, 5]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/LiteralExpressionSuite.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ScalaUDFSuite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/ArrayDataIndexedSeqSuite.scala", "additions": "7", "deletions": "5", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/types/StructTypeSuite.scala", "additions": "19", "deletions": "12", "changes": "31"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/array.sql.out", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/decimalArithmeticOperations.sql.out", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "8", "deletions": "8", "changes": "16"}, "updated": [0, 5, 8]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/map.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/timestamp.sql.out", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/columnresolution-negative.sql.out", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/date.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/datetime-legacy.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/group-by.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 2, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 5, 7]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/join-lateral.sql.out", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/natural-join.sql.out", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/pivot.sql.out", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/postgreSQL/aggregates_part1.sql.out", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/postgreSQL/case.sql.out", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/postgreSQL/float4.sql.out", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/postgreSQL/float8.sql.out", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/postgreSQL/int8.sql.out", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/postgreSQL/join.sql.out", "additions": "8", "deletions": "8", "changes": "16"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/postgreSQL/select_having.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/postgreSQL/select_implicit.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/postgreSQL/union.sql.out", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/query_regex_column.sql.out", "additions": "8", "deletions": "8", "changes": "16"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/subquery/negative-cases/invalid-correlation.sql.out", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/table-aliases.sql.out", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/timestampNTZ/timestamp-ansi.sql.out", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/typeCoercion/native/stringCastAndExpressions.sql.out", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/udf/postgreSQL/udf-aggregates_part1.sql.out", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/udf/postgreSQL/udf-case.sql.out", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/udf/postgreSQL/udf-join.sql.out", "additions": "8", "deletions": "8", "changes": "16"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/udf/postgreSQL/udf-select_having.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/udf/postgreSQL/udf-select_implicit.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/udf/udf-group-by.sql.out", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/udf/udf-pivot.sql.out", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFramePivotSuite.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SQLInsertTestSuite.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/arrow/ArrowConvertersSuite.scala", "additions": "7", "deletions": "3", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/DDLParserSuite.scala", "additions": "6", "deletions": "4", "changes": "10"}, "updated": [0, 3, 5]}, {"file": {"name": "sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/HiveThriftServerErrors.scala", "additions": "1", "deletions": "3", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/ThriftServerWithSparkContextSuite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}]}
{"author": "beliefer", "sha": "89fc1c0ac2d529972193e9feed1f35e8a93da00a", "commit_date": "2021/10/21 08:05:21", "commit_message": "Spark SQL should support create function with Aggregator", "title": "[SPARK-37018][SQL] Spark SQL should support create function with Aggregator", "body": "### What changes were proposed in this pull request?\r\nSpark SQL not supports to create function of `Aggregator` yet and deprecated `UserDefinedAggregateFunction`.\r\nIf we want remove `UserDefinedAggregateFunction`, Spark SQL should provide a new option.\r\nNote: This PR replaces https://github.com/apache/spark/pull/34303.\r\n\r\n\r\n### Why are the changes needed?\r\nWe need to provide a new way to create user defined aggregate function so as remove `UserDefinedAggregateFunction` in future.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes. Users will create user defined aggregate function by implement `Aggregator`.\r\n\r\n\r\n### How was this patch tested?\r\nNew tests.\r\n", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "pyspark.sql.tests.test_pandas_udf_scalar"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/FunctionExpressionBuilder.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala", "additions": "36", "deletions": "2", "changes": "38"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/udaf.sql", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/udaf.sql.out", "additions": "25", "deletions": "1", "changes": "26"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/UDFSuite.scala", "additions": "35", "deletions": "0", "changes": "35"}, "updated": [0, 1, 1]}]}
{"author": "Yikun", "sha": "a157e07884eaa0139bef4b044bec314da075586a", "commit_date": "2021/10/16 02:42:21", "commit_message": "Support arithmetic operations of decimal(nan) series", "title": "[SPARK-36231][PYTHON] Support arithmetic operations of decimal(nan) series", "body": "### What changes were proposed in this pull request?\r\nThis patch has changes as below to follow the pandas behavior:\r\n- **Add nan value process in _non_fractional_astype**: Follow the pandas [to_string](https://github.com/pandas-dev/pandas/blob/0a9f9eed3e3eb7d5fa23cbc588e78b9bef915a89/pandas/core/series.py#L1486) covert method, it should be `\"NaN\"` rather than `str(np.nan)`(`\"nan\"`)\uff0c which is covered by `self.assert_eq(pser.astype(str), psser.astype(str))`.\r\n- **Add null value process in rpow**, which is covered by `def test_rpow(self)`\r\n- **Add index_ops.hasnans in `astype`**, which is covered by `test_astype`.\r\n\r\nThis patch also move `numeric_w_nan_pdf` into `numeric_pdf`, that means all float_nan/decimal_nan separated test case have been cleaned up and merged into numeric test.\r\n\r\n\r\n\r\n### Why are the changes needed?\r\nFollow the pandas behavior\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes, correct the null value result to follow the pandas behavior\r\n\r\n\r\n### How was this patch tested?\r\nut to cover all changes", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/data_type_ops/num_ops.py", "additions": "21", "deletions": "2", "changes": "23"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/pandas/tests/data_type_ops/test_num_ops.py", "additions": "20", "deletions": "35", "changes": "55"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/data_type_ops/testing_utils.py", "additions": "7", "deletions": "43", "changes": "50"}, "updated": [0, 0, 1]}]}
{"author": "Yikun", "sha": "3dfd2ce269c69208a1ea5b867fecf6a830a190ba", "commit_date": "2021/10/11 03:13:07", "commit_message": "Add the ability to specify a scheduler.", "title": "[SPARK-36059][K8S] Support `spark.kubernetes.driver.scheduler.name`", "body": "### What changes were proposed in this pull request?\r\nThis patch adds the support driver for selecting scheduler through schedulerName.\r\n\r\n### Why are the changes needed?\r\nWe have added the scheduler specified ability in executor side, https://github.com/apache/spark/pull/26088. And in some scenarios, users want to specify the driver scheduler to make sure driverPod can be scheduled separately.\r\n\r\nPart of [SPARK-36057](https://issues.apache.org/jira/browse/SPARK-36057) .\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes, add `spark.kubernetes.driver.scheduler.name` conf\r\n\r\n\r\n### How was this patch tested?\r\n- UT\r\n", "failed_tests": [], "files": [{"file": {"name": "docs/running-on-kubernetes.md", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicDriverFeatureStep.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/KubernetesConfSuite.scala", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 0]}]}
{"author": "Yikun", "sha": "4e185c7beb8139b7d8fc5beb25f4171f57a8f542", "commit_date": "2021/10/08 08:59:54", "commit_message": "Add PodGroup", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "21", "deletions": "0", "changes": "21"}, "updated": [0, 0, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/PodGroupFeatureStep.scala", "additions": "95", "deletions": "0", "changes": "95"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/submit/KubernetesClientApplication.scala", "additions": "18", "deletions": "1", "changes": "19"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/submit/KubernetesDriverBuilder.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/batch/PodGroupAllocator.scala", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/batch/PodGroupSelector.scala", "additions": "50", "deletions": "0", "changes": "50"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/batch/resource/PodGroup.scala", "additions": "46", "deletions": "0", "changes": "46"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/batch/resource/PodGroupSpec.scala", "additions": "40", "deletions": "0", "changes": "40"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/batch/resource/PodGroupStatus.scala", "additions": "38", "deletions": "0", "changes": "38"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/scheduler/cluster/k8s/batch/PodGroupFeatureStepSuite.scala", "additions": "50", "deletions": "0", "changes": "50"}, "updated": [0, 0, 0]}]}
{"author": "tanelk", "sha": "0c543b2ca99649c43b2b3d0ac9fd0eb8f51d790c", "commit_date": "2021/10/20 16:08:36", "commit_message": "SPARK-37074", "title": "[SPARK-37074][SQL] Push extra predicates through non-join", "body": "### What changes were proposed in this pull request?\r\nIn the `Optimizer` partially push some predicates through a non-join nodes, that produce new columns: `Aggregate`, `Generate`, `Window`.\r\n\r\n### Why are the changes needed?\r\nPerformance improvements\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\nNew UTs", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/PushExtraPredicateThroughNonJoin.scala", "additions": "78", "deletions": "0", "changes": "78"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/rules/RuleIdCollection.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/FilterPushdownSuite.scala", "additions": "66", "deletions": "1", "changes": "67"}, "updated": [0, 0, 0]}]}
{"author": "tanelk", "sha": "9b21934e51ec393ae350aba20172f96a6fc22fec", "commit_date": "2021/06/03 12:20:24", "commit_message": "Better outputPartitioning for ExpandExec", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/ExpandExec.scala", "additions": "14", "deletions": "5", "changes": "19"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala", "additions": "47", "deletions": "2", "changes": "49"}, "updated": [2, 2, 6]}]}
{"author": "tanelk", "sha": "0d40311401c8bb7ad54bd3e183f46599b413b961", "commit_date": "2020/09/17 19:46:53", "commit_message": "Bitwise operations are commutative", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/First.scala", "additions": "0", "deletions": "3", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Last.scala", "additions": "0", "deletions": "3", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/collect.scala", "additions": "0", "deletions": "4", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 3, 9]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/EliminateDistinctSuite.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/FilterPushdownSuite.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/UnwrapCastInBinaryComparisonSuite.scala", "additions": "3", "deletions": "4", "changes": "7"}, "updated": [1, 2, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala", "additions": "20", "deletions": "9", "changes": "29"}, "updated": [0, 0, 3]}]}
{"author": "tanelk", "sha": "d67a3f0483b23d99e0752278d01acebab73fbc70", "commit_date": "2021/08/12 11:54:40", "commit_message": "Remove literals from grouping expressions when using the DataFrame API", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "4", "deletions": "5", "changes": "9"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [1, 3, 10]}]}
{"author": "Peng-Lei", "sha": "4088c77dfe07b3b9a5449a95dd1d89431b6b2766", "commit_date": "2021/09/23 07:12:40", "commit_message": "add draft", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "R/pkg/R/utils.R", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/resources/error/error-classes.json", "additions": "70", "deletions": "0", "changes": "70"}, "updated": [0, 2, 4]}, {"file": {"name": "core/src/main/scala/org/apache/spark/SparkException.scala", "additions": "32", "deletions": "2", "changes": "34"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala", "additions": "74", "deletions": "78", "changes": "152"}, "updated": [0, 2, 4]}]}
{"author": "thejdeep", "sha": "96b4fdc712ab3938ed20a3b7c266b7b3fabe282b", "commit_date": "2021/10/13 17:51:13", "commit_message": " [SPARK-36998][CORE] Handle concurrent eviction of same application file in SHS\n\n ### What changes were proposed in this pull request?\n To gracefully handle the error thrown when we try to make room for parsing of different applications and they try to evict the same application by deleting the directory path.\n\nAlso, added a test for `deleteStore` in `HistoryServerDiskManagerSuite`\n\n ### Why are the changes needed?\n Otherwise, an IOException is thrown when it cannot find the directory path to exist.\n\n ### Does this PR introduce _any_ user-facing change?\n no\n\n ### How was this patch tested?\n Added a unit test for `deleteStore` but not specifically testing the concurrency fix.", "title": "[SPARK-36998][CORE] Handle concurrent eviction of same application in SHS", "body": " ### What changes were proposed in this pull request?\r\n To gracefully handle the error thrown when we try to make room for parsing of different applications and they try to evict the same application by deleting the directory path.\r\n\r\nAlso, added a test for `deleteStore` in `HistoryServerDiskManagerSuite`\r\n\r\n ### Why are the changes needed?\r\n Otherwise, an NoSuchFileException is thrown when it cannot find the directory path to exist.\r\n\r\n ### Does this PR introduce _any_ user-facing change?\r\n no\r\n\r\n ### How was this patch tested?\r\nAdded a unit test for `deleteStore` but not specifically testing the concurrency fix.", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/deploy/history/HistoryServerDiskManager.scala", "additions": "8", "deletions": "1", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/deploy/history/HistoryServerDiskManagerSuite.scala", "additions": "16", "deletions": "2", "changes": "18"}, "updated": [0, 0, 0]}]}
{"author": "pralabhkumar", "sha": "abb42785318b1e14b7c7a33f28279ca9cef53a32", "commit_date": "2021/09/13 13:01:51", "commit_message": "Spark to Pandas via Array for ArrayType(Timestamp)\n\nResolved warning with respect to GH41555\n\nChanges with respect to lint-python\n\nFix other test case (DataFrameTests, ComplexOpsTest) failure issue\n\nFixed issues which causing ReshapeTest.test_get_dummies_date_datetime to fail\n\nCommented test cases which suppose to fail for ArrayType(TimeStamp)\n\nChanges for ArrayType(Timestamp) from pandas\n\nAdderessed review comments\n\nModified test cases to use array<array<timestamp>> for expecting the failure\n\nRefactored code\n\nSimplified the logic for timestamp conversion\n\nResolve conflicts\n\nFixed lint error\n\nCode refactoring\n\nSimplified the logic for conversion of Array of timestamp", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/sql/pandas/conversion.py", "additions": "27", "deletions": "10", "changes": "37"}, "updated": [0, 2, 2]}, {"file": {"name": "python/pyspark/sql/pandas/serializers.py", "additions": "7", "deletions": "1", "changes": "8"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/pandas/types.py", "additions": "68", "deletions": "15", "changes": "83"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_arrow.py", "additions": "34", "deletions": "11", "changes": "45"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_pandas_cogrouped_map.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/tests/test_pandas_grouped_map.py", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/tests/test_pandas_udf_grouped_agg.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/tests/test_pandas_udf_scalar.py", "additions": "41", "deletions": "7", "changes": "48"}, "updated": [0, 0, 0]}]}
{"author": "senthh", "sha": "305dacf51ce7bf39f93a2e0ba97960afdb3034d1", "commit_date": "2021/10/14 15:39:15", "commit_message": "[SPARK-36996][SQL] fixing \"SQL column nullable setting not retained as part of spark read\" issue", "title": "[WIP][SPARK-36996][SQL] Fixing \"SQL column nullable setting not retained as part of spark.read\" issue", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nChanges included in this PR:\r\n1.  alwaysNullable value as \"False\" in getQueryOutputSchema function of JDBCRDD.scala so all column Nullable value will be fetched from SQL metadata.\r\n2. pass \"nullable\" value as true  if column typeName is \"TIMESTAMP\" ; this changes is included in order to satisfy issue reported in SPARK-19726.\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nWe require this changes in order to fetch exact Nullable value from SQL metadata otherwise all columns be treated as NULLABLE.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nNo\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\n1. Tested in Internal Cluster\r\n2. Used available Test Cases", "failed_tests": ["org.apache.spark.sql.jdbc.v2.OracleIntegrationSuite", "org.apache.spark.sql.jdbc.v2.MsSqlServerIntegrationSuite", "org.apache.spark.sql.jdbc.v2.DB2IntegrationSuite", "org.apache.spark.sql.jdbc.v2.PostgresIntegrationSuite", "org.apache.spark.sql.jdbc.JDBCV2Suite", "org.apache.spark.sql.jdbc.JDBCSuite", "org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCTableCatalogSuite", "org.apache.spark.sql.jdbc.JDBCWriteSuite"], "files": [{"file": {"name": "external/docker-integration-tests/src/test/scala/org/apache/spark/sql/jdbc/v2/V2JDBCTest.scala", "additions": "9", "deletions": "9", "changes": "18"}, "updated": [1, 2, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 3, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCTableCatalogSuite.scala", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCSuite.scala", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCWriteSuite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}]}
{"author": "naveensrinivasan", "sha": "8b027308c2b1aedbed9a0feb9918b575cb638e70", "commit_date": "2021/10/03 17:18:31", "commit_message": "Create dependabot.yml", "title": "[SPARK-36916][INFRA] Enable Dependabot for improving security posture of the dependencies", "body": "\r\n### What changes were proposed in this pull request?\r\nEnable dependabot to get security updates and if needed version updates on dependencies \r\n\r\n### Why are the changes needed?\r\n\r\nhttps://docs.github.com/en/code-security/supply-chain-security/keeping-your-dependencies-updated-automatically\r\n\r\nHaving knowledge about vulnerabilities of the dependencies helps the project owners decide on their dependencies security posture to make decisions.\r\n\r\nIf the project decides to get updates only on security updates and not on any version updates then setting these options would not open any PR 's `open-pull-requests-limit: 0`\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNO\r\nThis option has to be enabled in the security section of the project.\r\nhttps://docs.github.com/en/code-security/supply-chain-security/managing-vulnerabilities-in-your-projects-dependencies/configuring-dependabot-security-updates#managing-dependabot-security-updates-for-your-repositories\r\n### How was this patch tested?\r\nN/A\r\n", "failed_tests": [], "files": [{"file": {"name": ".github/dependabot.yml", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}
{"author": "sleep1661", "sha": "d560882c946eb1326fbfa67b49a26e656b0fb776", "commit_date": "2021/08/31 09:46:51", "commit_message": "[SPARK-36575][CORE] Should ignore task finished event if its task set is gone in TaskSchedulerImpl.handleSuccessfulTask", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala", "additions": "7", "deletions": "1", "changes": "8"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/TaskSchedulerImplSuite.scala", "additions": "85", "deletions": "1", "changes": "86"}, "updated": [0, 0, 0]}]}
{"author": "gaoyajun02", "sha": "f16f2608195838310c0202fcbc0d15d4ac9269c5", "commit_date": "2021/10/14 07:12:51", "commit_message": "fixup", "title": "[SPARK-36964][CORE][YARN] Share cached dnsToSwitchMapping for yarn locality container requests", "body": "### What changes were proposed in this pull request?\r\n\r\nsince SPARK-13704, spark re-implemented RackResolver and created a separate dnsToSwitchMapping instance. Here change to use RackResolver's internal dnsToSwitchMapping to reduce the time taken by YarnAllocator to add container requests.\r\n\r\n### Why are the changes needed?\r\n\r\nIf submits a stage with abundant tasks, rack resolving takes a long time when YarnAllocator add requests with locality preference, which is caused by a large number of loops to execute the rack parsing script, eventually causing ExecutorAllocationManager request total Executors rpc  timeout.\r\n\r\n### How was this patch tested?\r\nUT +  manually testing on a 5w+ node cluster.", "failed_tests": [], "files": [{"file": {"name": "resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/SparkRackResolver.scala", "additions": "19", "deletions": "8", "changes": "27"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocator.scala", "additions": "14", "deletions": "5", "changes": "19"}, "updated": [0, 0, 0]}]}
{"author": "hiboyang", "sha": "c411ff44fbbfd25564aac19d24df4bda52347694", "commit_date": "2021/03/03 00:50:18", "commit_message": "Add spark.shuffle.markFileLostOnExecutorLost to not delete shuffle file on executor lost event", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "assembly/pom.xml", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [1, 1, 1]}, {"file": {"name": "pom.xml", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [3, 10, 25]}, {"file": {"name": "remote-shuffle-service/Dockerfile", "additions": "41", "deletions": "0", "changes": "41"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/NOTICE", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/README.md", "additions": "78", "deletions": "0", "changes": "78"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/kubernetes.yml", "additions": "84", "deletions": "0", "changes": "84"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/pom.xml", "additions": "211", "deletions": "0", "changes": "211"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/RssBuildInfo.java", "additions": "26", "deletions": "0", "changes": "26"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/StreamServer.java", "additions": "381", "deletions": "0", "changes": "381"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/StreamServerConfig.java", "additions": "368", "deletions": "0", "changes": "368"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/clients/BusyStatusSocketClient.java", "additions": "60", "deletions": "0", "changes": "60"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/clients/ClientBase.java", "additions": "349", "deletions": "0", "changes": "349"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/clients/ClientConstants.java", "additions": "22", "deletions": "0", "changes": "22"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/clients/ClientRetryOptions.java", "additions": "42", "deletions": "0", "changes": "42"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/clients/DataBlockSocketReadClient.java", "additions": "379", "deletions": "0", "changes": "379"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/clients/DataBlockSyncWriteClient.java", "additions": "204", "deletions": "0", "changes": "204"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/clients/HeartbeatSocketClient.java", "additions": "61", "deletions": "0", "changes": "61"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/clients/MultiServerAsyncWriteClient.java", "additions": "449", "deletions": "0", "changes": "449"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/clients/MultiServerHeartbeatClient.java", "additions": "143", "deletions": "0", "changes": "143"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/clients/MultiServerReadClient.java", "additions": "22", "deletions": "0", "changes": "22"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/clients/MultiServerSocketReadClient.java", "additions": "201", "deletions": "0", "changes": "201"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/clients/MultiServerSyncWriteClient.java", "additions": "223", "deletions": "0", "changes": "223"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/clients/MultiServerWriteClient.java", "additions": "27", "deletions": "0", "changes": "27"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/clients/NotifyClient.java", "additions": "109", "deletions": "0", "changes": "109"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/clients/PlainShuffleDataSocketReadClient.java", "additions": "48", "deletions": "0", "changes": "48"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/clients/PlainShuffleDataSyncWriteClient.java", "additions": "54", "deletions": "0", "changes": "54"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/clients/PooledShuffleDataSyncWriteClient.java", "additions": "158", "deletions": "0", "changes": "158"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/clients/PooledWriteClientFactory.java", "additions": "343", "deletions": "0", "changes": "343"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/clients/ReadClientDataOptions.java", "additions": "55", "deletions": "0", "changes": "55"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/clients/RegistryClient.java", "additions": "89", "deletions": "0", "changes": "89"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/clients/ReplicatedReadClient.java", "additions": "458", "deletions": "0", "changes": "458"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/clients/ReplicatedWriteClient.java", "additions": "209", "deletions": "0", "changes": "209"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/clients/RetriableSocketReadClient.java", "additions": "122", "deletions": "0", "changes": "122"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/clients/ServerConnectionStringCache.java", "additions": "60", "deletions": "0", "changes": "60"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/clients/ServerConnectionStringResolver.java", "additions": "27", "deletions": "0", "changes": "27"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/clients/ServerIdAwareSocketReadClient.java", "additions": "111", "deletions": "0", "changes": "111"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/clients/ServerIdAwareSyncWriteClient.java", "additions": "160", "deletions": "0", "changes": "160"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/clients/ServerReplicationGroupUtil.java", "additions": "104", "deletions": "0", "changes": "104"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/clients/ShuffleDataReader.java", "additions": "30", "deletions": "0", "changes": "30"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/clients/ShuffleDataSocketReadClient.java", "additions": "124", "deletions": "0", "changes": "124"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/clients/ShuffleDataSyncWriteClient.java", "additions": "49", "deletions": "0", "changes": "49"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/clients/ShuffleDataSyncWriteClientBase.java", "additions": "135", "deletions": "0", "changes": "135"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/clients/ShuffleDataWriter.java", "additions": "43", "deletions": "0", "changes": "43"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/clients/ShuffleWriteConfig.java", "additions": "57", "deletions": "0", "changes": "57"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/clients/SingleServerReadClient.java", "additions": "22", "deletions": "0", "changes": "22"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/clients/SingleServerWriteClient.java", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/clients/TaskDataBlock.java", "additions": "79", "deletions": "0", "changes": "79"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/clients/UnpooledWriteClientFactory.java", "additions": "51", "deletions": "0", "changes": "51"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/clients/WriteClientFactory.java", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/common/AppMapId.java", "additions": "86", "deletions": "0", "changes": "86"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/common/AppShuffleId.java", "additions": "72", "deletions": "0", "changes": "72"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/common/AppShufflePartitionId.java", "additions": "95", "deletions": "0", "changes": "95"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/common/AppTaskAttemptId.java", "additions": "111", "deletions": "0", "changes": "111"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/common/Compression.java", "additions": "70", "deletions": "0", "changes": "70"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/common/DataBlock.java", "additions": "41", "deletions": "0", "changes": "41"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/common/DataBlockHeader.java", "additions": "61", "deletions": "0", "changes": "61"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/common/DownloadServerVerboseInfo.java", "additions": "49", "deletions": "0", "changes": "49"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/common/FilePathAndLength.java", "additions": "58", "deletions": "0", "changes": "58"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/common/FixedLengthInputStream.java", "additions": "120", "deletions": "0", "changes": "120"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/common/LowMemoryListener.java", "additions": "22", "deletions": "0", "changes": "22"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/common/MapTaskCommitStatus.java", "additions": "83", "deletions": "0", "changes": "83"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/common/MapTaskRssInfo.java", "additions": "98", "deletions": "0", "changes": "98"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/common/MemoryMonitor.java", "additions": "100", "deletions": "0", "changes": "100"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/common/PartitionFilePathAndLength.java", "additions": "68", "deletions": "0", "changes": "68"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/common/ServerCandidate.java", "additions": "52", "deletions": "0", "changes": "52"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/common/ServerDetail.java", "additions": "72", "deletions": "0", "changes": "72"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/common/ServerDetailCollection.java", "additions": "93", "deletions": "0", "changes": "93"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/common/ServerList.java", "additions": "77", "deletions": "0", "changes": "77"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/common/ServerReplicationGroup.java", "additions": "40", "deletions": "0", "changes": "40"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/common/ServerRole.java", "additions": "24", "deletions": "0", "changes": "24"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/common/ShuffleMapTaskAttemptId.java", "additions": "71", "deletions": "0", "changes": "71"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/decoders/StreamServerMessageDecoder.java", "additions": "376", "deletions": "0", "changes": "376"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/decoders/StreamServerVersionDecoder.java", "additions": "139", "deletions": "0", "changes": "139"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/exceptions/ExceptionWrapper.java", "additions": "35", "deletions": "0", "changes": "35"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/exceptions/RssAggregateException.java", "additions": "50", "deletions": "0", "changes": "50"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/exceptions/RssDiskSpaceException.java", "additions": "40", "deletions": "0", "changes": "40"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/exceptions/RssDuplicateAppTaskAttemptException.java", "additions": "42", "deletions": "0", "changes": "42"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/exceptions/RssEndOfStreamException.java", "additions": "40", "deletions": "0", "changes": "40"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/exceptions/RssException.java", "additions": "38", "deletions": "0", "changes": "38"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/exceptions/RssFileCorruptedException.java", "additions": "27", "deletions": "0", "changes": "27"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/exceptions/RssFinishUploadException.java", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/exceptions/RssInconsistentReplicaException.java", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/exceptions/RssInvalidDataException.java", "additions": "40", "deletions": "0", "changes": "40"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/exceptions/RssInvalidMapStatusException.java", "additions": "27", "deletions": "0", "changes": "27"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/exceptions/RssInvalidServerIdException.java", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/exceptions/RssInvalidStateException.java", "additions": "41", "deletions": "0", "changes": "41"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/exceptions/RssMaxConnectionsException.java", "additions": "53", "deletions": "0", "changes": "53"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/exceptions/RssMissingShuffleWriteConfigException.java", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/exceptions/RssNetworkException.java", "additions": "40", "deletions": "0", "changes": "40"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/exceptions/RssNoActiveReadClientException.java", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/exceptions/RssNoServerAvailableException.java", "additions": "27", "deletions": "0", "changes": "27"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/exceptions/RssNonRecoverableException.java", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/exceptions/RssOperationQueueFullException.java", "additions": "40", "deletions": "0", "changes": "40"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/exceptions/RssQueueNotReadyException.java", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/exceptions/RssRetryTimeoutException.java", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/exceptions/RssRetryableSparkTaskException.java", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/exceptions/RssServerBusyException.java", "additions": "27", "deletions": "0", "changes": "27"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/exceptions/RssServerDownException.java", "additions": "27", "deletions": "0", "changes": "27"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/exceptions/RssServerResolveException.java", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/exceptions/RssShuffleCorruptedException.java", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/exceptions/RssShuffleDataNotAvailableException.java", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/exceptions/RssShuffleStageNotStartedException.java", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/exceptions/RssStaleTaskAttemptException.java", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/exceptions/RssStreamReadException.java", "additions": "40", "deletions": "0", "changes": "40"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/exceptions/RssTooMuchDataException.java", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/exceptions/RssUberEnvironmentException.java", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/exceptions/RssUnsupportedCompressionException.java", "additions": "41", "deletions": "0", "changes": "41"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/exceptions/RssWriteRecordException.java", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/execution/ExecutorAppState.java", "additions": "63", "deletions": "0", "changes": "63"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/execution/ExecutorShuffleStageState.java", "additions": "336", "deletions": "0", "changes": "336"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/execution/LocalFileStateStore.java", "additions": "253", "deletions": "0", "changes": "253"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/execution/LocalFileStateStoreIterator.java", "additions": "223", "deletions": "0", "changes": "223"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/execution/ShuffleDataWrapper.java", "additions": "57", "deletions": "0", "changes": "57"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/execution/ShuffleExecutor.java", "additions": "762", "deletions": "0", "changes": "762"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/execution/ShufflePartitionWriter.java", "additions": "217", "deletions": "0", "changes": "217"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/execution/StagePersistentInfo.java", "additions": "63", "deletions": "0", "changes": "63"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/execution/StateStore.java", "additions": "42", "deletions": "0", "changes": "42"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/execution/StateStoreLoadResult.java", "additions": "81", "deletions": "0", "changes": "81"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/execution/TaskAttemptCollection.java", "additions": "47", "deletions": "0", "changes": "47"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/execution/TaskAttemptIdAndState.java", "additions": "69", "deletions": "0", "changes": "69"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/handlers/ChannelFutureCloseListener.java", "additions": "37", "deletions": "0", "changes": "37"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/handlers/ChannelIdleCheck.java", "additions": "84", "deletions": "0", "changes": "84"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/handlers/DownloadChannelInboundHandler.java", "additions": "230", "deletions": "0", "changes": "230"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/handlers/DownloadServerHandler.java", "additions": "210", "deletions": "0", "changes": "210"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/handlers/HandlerUtil.java", "additions": "79", "deletions": "0", "changes": "79"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/handlers/HttpChannelInboundHandler.java", "additions": "108", "deletions": "0", "changes": "108"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/handlers/NotifyChannelInboundHandler.java", "additions": "84", "deletions": "0", "changes": "84"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/handlers/NotifyServerHandler.java", "additions": "82", "deletions": "0", "changes": "82"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/handlers/RegistryChannelInboundHandler.java", "additions": "86", "deletions": "0", "changes": "86"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/handlers/RegistryServerHandler.java", "additions": "78", "deletions": "0", "changes": "78"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/handlers/ResponseStatusAndMessage.java", "additions": "46", "deletions": "0", "changes": "46"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/handlers/UploadChannelInboundHandler.java", "additions": "232", "deletions": "0", "changes": "232"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/handlers/UploadChannelManager.java", "additions": "54", "deletions": "0", "changes": "54"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/handlers/UploadServerHandler.java", "additions": "146", "deletions": "0", "changes": "146"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/messages/AppDeletionStateItem.java", "additions": "53", "deletions": "0", "changes": "53"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/messages/BaseMessage.java", "additions": "22", "deletions": "0", "changes": "22"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/messages/ConnectDownloadRequest.java", "additions": "123", "deletions": "0", "changes": "123"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/messages/ConnectDownloadResponse.java", "additions": "108", "deletions": "0", "changes": "108"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/messages/ConnectNotifyRequest.java", "additions": "53", "deletions": "0", "changes": "53"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/messages/ConnectNotifyResponse.java", "additions": "55", "deletions": "0", "changes": "55"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/messages/ConnectRegistryRequest.java", "additions": "55", "deletions": "0", "changes": "55"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/messages/ConnectRegistryResponse.java", "additions": "53", "deletions": "0", "changes": "53"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/messages/ConnectUploadRequest.java", "additions": "71", "deletions": "0", "changes": "71"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/messages/ConnectUploadResponse.java", "additions": "55", "deletions": "0", "changes": "55"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/messages/ControlMessage.java", "additions": "21", "deletions": "0", "changes": "21"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/messages/FinishApplicationAttemptRequestMessage.java", "additions": "62", "deletions": "0", "changes": "62"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/messages/FinishApplicationJobRequestMessage.java", "additions": "103", "deletions": "0", "changes": "103"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/messages/FinishUploadMessage.java", "additions": "77", "deletions": "0", "changes": "77"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/messages/GetBusyStatusRequest.java", "additions": "55", "deletions": "0", "changes": "55"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/messages/GetBusyStatusResponse.java", "additions": "87", "deletions": "0", "changes": "87"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/messages/GetDataAvailabilityRequest.java", "additions": "48", "deletions": "0", "changes": "48"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/messages/GetDataAvailabilityResponse.java", "additions": "82", "deletions": "0", "changes": "82"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/messages/GetServersRequestMessage.java", "additions": "73", "deletions": "0", "changes": "73"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/messages/GetServersResponseMessage.java", "additions": "70", "deletions": "0", "changes": "70"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/messages/HeartbeatMessage.java", "additions": "80", "deletions": "0", "changes": "80"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/messages/MessageConstants.java", "additions": "73", "deletions": "0", "changes": "73"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/messages/RegisterServerRequestMessage.java", "additions": "83", "deletions": "0", "changes": "83"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/messages/RegisterServerResponseMessage.java", "additions": "55", "deletions": "0", "changes": "55"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/messages/SerializableMessage.java", "additions": "22", "deletions": "0", "changes": "22"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/messages/ServerResponseMessage.java", "additions": "37", "deletions": "0", "changes": "37"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/messages/ShuffleDataWrapper.java", "additions": "57", "deletions": "0", "changes": "57"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/messages/ShuffleStageStatus.java", "additions": "92", "deletions": "0", "changes": "92"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/messages/StageCorruptionStateItem.java", "additions": "58", "deletions": "0", "changes": "58"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/messages/StageInfoStateItem.java", "additions": "102", "deletions": "0", "changes": "102"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/messages/StartUploadMessage.java", "additions": "112", "deletions": "0", "changes": "112"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/messages/TaskAttemptCommitStateItem.java", "additions": "108", "deletions": "0", "changes": "108"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/metadata/InMemoryServiceRegistry.java", "additions": "104", "deletions": "0", "changes": "104"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/metadata/ServerSequenceServiceRegistry.java", "additions": "99", "deletions": "0", "changes": "99"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/metadata/ServiceRegistry.java", "additions": "74", "deletions": "0", "changes": "74"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/metadata/ServiceRegistryUtils.java", "additions": "221", "deletions": "0", "changes": "221"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/metadata/ServiceRegistryWrapper.java", "additions": "113", "deletions": "0", "changes": "113"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/metadata/StandaloneServiceRegistryClient.java", "additions": "91", "deletions": "0", "changes": "91"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/metrics/ApplicationJobStatusMetrics.java", "additions": "52", "deletions": "0", "changes": "52"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/metrics/ApplicationJobStatusMetricsKey.java", "additions": "60", "deletions": "0", "changes": "60"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/metrics/ApplicationMetrics.java", "additions": "47", "deletions": "0", "changes": "47"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/metrics/ApplicationMetricsKey.java", "additions": "61", "deletions": "0", "changes": "61"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/metrics/ClientConnectMetrics.java", "additions": "54", "deletions": "0", "changes": "54"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/metrics/ClientConnectMetricsKey.java", "additions": "60", "deletions": "0", "changes": "60"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/metrics/ExceptionMetricGroupContainer.java", "additions": "33", "deletions": "0", "changes": "33"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/metrics/ExceptionMetrics.java", "additions": "52", "deletions": "0", "changes": "52"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/metrics/ExceptionMetricsKey.java", "additions": "60", "deletions": "0", "changes": "60"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/metrics/M3DummyScope.java", "additions": "115", "deletions": "0", "changes": "115"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/metrics/M3DummyScopeBuilder.java", "additions": "60", "deletions": "0", "changes": "60"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/metrics/M3Stats.java", "additions": "148", "deletions": "0", "changes": "148"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/metrics/MetadataClientMetrics.java", "additions": "60", "deletions": "0", "changes": "60"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/metrics/MetadataClientMetricsContainer.java", "additions": "33", "deletions": "0", "changes": "33"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/metrics/MetadataClientMetricsKey.java", "additions": "69", "deletions": "0", "changes": "69"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/metrics/MetricGroup.java", "additions": "35", "deletions": "0", "changes": "35"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/metrics/MetricGroupContainer.java", "additions": "40", "deletions": "0", "changes": "40"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/metrics/NettyServerSideMetricGroupContainer.java", "additions": "36", "deletions": "0", "changes": "36"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/metrics/NettyServerSideMetricsKey.java", "additions": "50", "deletions": "0", "changes": "50"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/metrics/NotifyClientMetrics.java", "additions": "45", "deletions": "0", "changes": "45"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/metrics/NotifyClientMetricsKey.java", "additions": "60", "deletions": "0", "changes": "60"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/metrics/NotifyServerMetricsContainer.java", "additions": "43", "deletions": "0", "changes": "43"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/metrics/ReadClientMetrics.java", "additions": "76", "deletions": "0", "changes": "76"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/metrics/ReadClientMetricsKey.java", "additions": "60", "deletions": "0", "changes": "60"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/metrics/ScheduledMetricCollector.java", "additions": "141", "deletions": "0", "changes": "141"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/metrics/ServerHandlerMetrics.java", "additions": "60", "deletions": "0", "changes": "60"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/metrics/WriteClientMetrics.java", "additions": "80", "deletions": "0", "changes": "80"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/metrics/WriteClientMetricsKey.java", "additions": "58", "deletions": "0", "changes": "58"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/storage/ShuffleFileOutputStream.java", "additions": "102", "deletions": "0", "changes": "102"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/storage/ShuffleFileStorage.java", "additions": "116", "deletions": "0", "changes": "116"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/storage/ShuffleFileUtils.java", "additions": "49", "deletions": "0", "changes": "49"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/storage/ShuffleOutputStream.java", "additions": "43", "deletions": "0", "changes": "43"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/storage/ShuffleStorage.java", "additions": "88", "deletions": "0", "changes": "88"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/tools/FileDescriptorStressTest.java", "additions": "100", "deletions": "0", "changes": "100"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/tools/FsyncPerfTest.java", "additions": "186", "deletions": "0", "changes": "186"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/tools/PartitionFileChecker.java", "additions": "112", "deletions": "0", "changes": "112"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/tools/SerializerBenchmark.java", "additions": "248", "deletions": "0", "changes": "248"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/tools/StreamReadClientVerify.java", "additions": "208", "deletions": "0", "changes": "208"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/tools/StreamServerStressTool.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/tools/StreamServerStressToolLongRun.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/tools/StreamServerStressToolWrite64GB.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/tools/TestUtils.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/util/AsyncSocketCompletionHandler.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/util/AsyncSocketState.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/util/ByteBufUtils.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/util/CountedOutputStream.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/util/ExceptionUtils.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/util/FileUtils.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/util/HttpUtils.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/util/JsonUtils.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/util/LogUtils.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/util/MonitorUtils.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/util/MovingAverageCalculator.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/util/NettyUtils.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/util/NetworkUtils.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/util/ObjectWrapper.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/util/RateCounter.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/util/RetryUtils.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/util/ServerHostAndPort.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/util/SocketAsyncWriteCallback.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/util/SocketUtils.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/util/StreamUtils.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/util/StringUtils.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/util/SystemUtils.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/java/org/apache/spark/remoteshuffle/util/ThreadUtils.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/resources/log4j-remote-shuffle-service.properties", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/scala/org/apache/spark/serializer/RssKryoSerializationStream.scala", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/scala/org/apache/spark/shuffle/MockTaskContext.scala", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/scala/org/apache/spark/shuffle/RssEmptyShuffleReader.scala", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/scala/org/apache/spark/shuffle/RssOpts.scala", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/scala/org/apache/spark/shuffle/RssServerSelectionResult.scala", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/scala/org/apache/spark/shuffle/RssShuffleBlockResolver.scala", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/scala/org/apache/spark/shuffle/RssShuffleHandle.scala", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/scala/org/apache/spark/shuffle/RssShuffleManager.scala", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/scala/org/apache/spark/shuffle/RssShuffleReader.scala", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/scala/org/apache/spark/shuffle/RssShuffleServerHandle.scala", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/scala/org/apache/spark/shuffle/RssShuffleWriter.scala", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/scala/org/apache/spark/shuffle/internal/BlockDownloaderPartitionRangeRecordIterator.scala", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/scala/org/apache/spark/shuffle/internal/BlockDownloaderPartitionRecordIterator.scala", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/scala/org/apache/spark/shuffle/internal/CombinerRecordBufferManager.scala", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/scala/org/apache/spark/shuffle/internal/DefaultRecordBufferManager.scala", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/scala/org/apache/spark/shuffle/internal/EmptyRecordIterator.scala", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/scala/org/apache/spark/shuffle/internal/KryoRecordBufferManager.scala", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/scala/org/apache/spark/shuffle/internal/MapOutputRssInfo.scala", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/scala/org/apache/spark/shuffle/internal/RecordBufferManager.scala", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/scala/org/apache/spark/shuffle/internal/RssSparkListener.scala", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/scala/org/apache/spark/shuffle/internal/RssStressTool.scala", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/scala/org/apache/spark/shuffle/internal/RssUtils.scala", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/scala/org/apache/spark/shuffle/internal/RssWritePerfTool.scala", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/scala/org/apache/spark/shuffle/internal/ShuffleWritePerfTool.scala", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/main/scala/org/apache/spark/shuffle/sort/SortMergeWritePerfTool.scala", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/test/java/org/apache/spark/remoteshuffle/ExampleJavaSuite.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/test/java/org/apache/spark/remoteshuffle/StreamServerCleanupTest.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/test/java/org/apache/spark/remoteshuffle/StreamServerConfigTest.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/test/java/org/apache/spark/remoteshuffle/StreamServerHttpTest.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/test/java/org/apache/spark/remoteshuffle/StreamServerMultiAttemptTest.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/test/java/org/apache/spark/remoteshuffle/StreamServerTest.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/test/java/org/apache/spark/remoteshuffle/StreamServerWritingTooMuchDataTest.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/test/java/org/apache/spark/remoteshuffle/clients/BusyStatusSocketClientTest.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/test/java/org/apache/spark/remoteshuffle/clients/DataBlockSocketReadClientTest.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/test/java/org/apache/spark/remoteshuffle/clients/DataBlockSyncWriteClientTest.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/test/java/org/apache/spark/remoteshuffle/clients/HeartbeatSocketClientTest.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/test/java/org/apache/spark/remoteshuffle/clients/MultiServerAsyncWriteClientTest.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/test/java/org/apache/spark/remoteshuffle/clients/MultiServerHeartbeatClientTest.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/test/java/org/apache/spark/remoteshuffle/clients/MultiServerSocketReadClientTest.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/test/java/org/apache/spark/remoteshuffle/clients/MultiServerSyncWriteClientTest.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/test/java/org/apache/spark/remoteshuffle/clients/NotifyClientTest.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/test/java/org/apache/spark/remoteshuffle/clients/PooledWriteClientFactoryRandomTest.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/test/java/org/apache/spark/remoteshuffle/clients/PooledWriteClientFactoryTest.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/test/java/org/apache/spark/remoteshuffle/clients/RecordSocketReadClientTest.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/test/java/org/apache/spark/remoteshuffle/clients/RegistryClientTest.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/test/java/org/apache/spark/remoteshuffle/clients/ReplicatedReadClientTest.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/test/java/org/apache/spark/remoteshuffle/clients/ReplicatedWriteClientTest.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/test/java/org/apache/spark/remoteshuffle/clients/RetriableSocketReadClientTest.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/test/java/org/apache/spark/remoteshuffle/clients/ServerIdAwareSocketReadClientTest.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/test/java/org/apache/spark/remoteshuffle/clients/ServerIdAwareSyncWriteClientTest.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/test/java/org/apache/spark/remoteshuffle/clients/ServerIdleTimeoutTest.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/test/java/org/apache/spark/remoteshuffle/clients/ServerReplicationGroupUtilTest.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/test/java/org/apache/spark/remoteshuffle/clients/WriteClientEdgeCaseTest.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/test/java/org/apache/spark/remoteshuffle/common/FixedLengthInputStreamTest.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/test/java/org/apache/spark/remoteshuffle/common/MapTaskRssInfoTest.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/test/java/org/apache/spark/remoteshuffle/common/ServerDetailCollectionTest.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/test/java/org/apache/spark/remoteshuffle/execution/LocalFileLocalFileStateStoreIteratorTest.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/test/java/org/apache/spark/remoteshuffle/execution/LocalFileStateStoreStressTest.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/test/java/org/apache/spark/remoteshuffle/execution/LocalFileStateStoreTest.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/test/java/org/apache/spark/remoteshuffle/execution/ShuffleExecutorTest.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/test/java/org/apache/spark/remoteshuffle/metadata/InMemoryServiceRegistryTest.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "remote-shuffle-service/src/test/java/org/apache/spark/remoteshuffle/metadata/ServerSequenceServiceRegistryTest.java", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}]}
{"author": "peter-toth", "sha": "0a97c8b8052e80524e98821078df4c55a28b14e6", "commit_date": "2021/04/21 17:40:19", "commit_message": "[SPARK-34079][SQL] Merging non-correlated scalar subqueries to multi-column scalar subqueries for better reuse", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueries.scala", "additions": "416", "deletions": "0", "changes": "416"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala", "additions": "32", "deletions": "1", "changes": "33"}, "updated": [2, 8, 16]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreePatterns.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [1, 3, 3]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueriesSuite.scala", "additions": "282", "deletions": "0", "changes": "282"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap.java", "additions": "2", "deletions": "7", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "8", "deletions": "3", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/PlanAdaptiveSubqueries.scala", "additions": "18", "deletions": "6", "changes": "24"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/ReuseAdaptiveSubquery.scala", "additions": "5", "deletions": "3", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala", "additions": "2", "deletions": "8", "changes": "10"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "0", "deletions": "9", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "21", "deletions": "0", "changes": "21"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/reuse/ReuseExchangeAndSubquery.scala", "additions": "10", "deletions": "3", "changes": "13"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala", "additions": "18", "deletions": "2", "changes": "20"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9.sf100/explain.txt", "additions": "183", "deletions": "598", "changes": "781"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9.sf100/simplified.txt", "additions": "65", "deletions": "170", "changes": "235"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9/explain.txt", "additions": "183", "deletions": "598", "changes": "781"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9/simplified.txt", "additions": "65", "deletions": "170", "changes": "235"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala", "additions": "72", "deletions": "0", "changes": "72"}, "updated": [1, 3, 10]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala", "additions": "37", "deletions": "0", "changes": "37"}, "updated": [1, 2, 4]}]}
{"author": "Shockang", "sha": "47cc8e949bbe31a6ebceb96a25be91e28a661d5b", "commit_date": "2021/10/07 13:55:45", "commit_message": "Bump ansi-regex from 5.0.0 to 5.0.1 in /dev\n\nBumps [ansi-regex](https://github.com/chalk/ansi-regex) from 5.0.0 to 5.0.1.\n- [Release notes](https://github.com/chalk/ansi-regex/releases)\n- [Commits](https://github.com/chalk/ansi-regex/compare/v5.0.0...v5.0.1)\n\n---\nupdated-dependencies:\n- dependency-name: ansi-regex\n  dependency-type: indirect\n...\n\nSigned-off-by: dependabot[bot] <support@github.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "dev/package-lock.json", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}]}
{"author": "holdenk", "sha": "65d7cc7ffe7513c84f1a66e118cf23d747ad1805", "commit_date": "2021/10/12 19:13:09", "commit_message": "Add the ability to selectively disable watching or polling for pods on Kubernetes for environments where etcd may be under a high load or otherwise not support polling/watching.", "title": "[SPARK-36462][K8S] Add the ability to selectively disable watching or polling", "body": "\r\n### What changes were proposed in this pull request?\r\n\r\nAdd the ability to selectively disable watching or polling\r\n\r\n### Why are the changes needed?\r\n\r\nWatching or polling for pod status on Kubernetes can place additional load on etcd, with a large number of executors and large number of jobs this can have negative impacts and executors register themselves with the driver under normal operations anyways.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nTwo new config flags.\r\n\r\n\r\n### How was this patch tested?\r\n\r\nNew unit tests + manually tested a forked version of this on an internal cluster with both watching and polling disabled.", "failed_tests": [], "files": [{"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/ExecutorPodsPollingSnapshotSource.scala", "additions": "7", "deletions": "4", "changes": "11"}, "updated": [0, 0, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/ExecutorPodsWatchSnapshotSource.scala", "additions": "14", "deletions": "8", "changes": "22"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterManager.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/scheduler/cluster/k8s/ExecutorPodsPollingSnapshotSourceSuite.scala", "additions": "24", "deletions": "9", "changes": "33"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/scheduler/cluster/k8s/ExecutorPodsWatchSnapshotSourceSuite.scala", "additions": "16", "deletions": "4", "changes": "20"}, "updated": [0, 0, 0]}]}
{"author": "guibin", "sha": "f02f0c2b82ffca256fd46f78a4a2b134a44cc6c6", "commit_date": "2021/09/15 20:38:17", "commit_message": "SPARK-36770 Add optimizer to replace UnboundedFollowing with UnboundedPreceding for First and Last", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/SortOrder.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [1, 1, 9]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/WindowFunctionOptimizer.scala", "additions": "116", "deletions": "0", "changes": "116"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/WindowFunctionOptimizerTestSuite.scala", "additions": "209", "deletions": "0", "changes": "209"}, "updated": [0, 0, 0]}]}
{"author": "changvvb", "sha": "c704de0dab7047671ef026813b7bd291f7b55919", "commit_date": "2021/07/29 06:57:24", "commit_message": "Add new exception of base exception used in QueryExecutionErrors", "title": "", "body": "", "failed_tests": ["org.apache.spark.SparkThrowableSuite", "org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite", "org.apache.spark.sql.catalyst.encoders.RowEncoderSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.DatasetSuite"], "files": [{"file": {"name": "core/src/main/resources/error/README.md", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/resources/error/error-classes.json", "additions": "46", "deletions": "2", "changes": "48"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala", "additions": "58", "deletions": "59", "changes": "117"}, "updated": [0, 0, 11]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/encoders/EncoderResolutionSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/array.sql.out", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/array.sql.out", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/postgreSQL/boolean.sql.out", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [0, 0, 0]}]}
{"author": "beobest2", "sha": "e0831472a0e9b3733c06ab83bf2baef5ce965ab2", "commit_date": "2021/10/08 08:50:12", "commit_message": "Implement Index.putmask", "title": "[SPARK-36403][PYTHON] Implement `Index.putmask` ", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\nImplement `Index.putmask`\r\n\r\nThis pull request is based on https://github.com/databricks/koalas/pull/1560\r\n\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\n`putmask` returns a new Index of the values set with the mask.\r\n`putmask` is supported in pandas. PySpark should support that as well.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\nYes. `Index.putmask` can be used.\r\n```python\r\n>>> pidx = pd.Index([\"a\", \"b\", \"c\", \"d\", \"e\"])\r\n>>> psidx = ps.from_pandas(pidx)\r\n>>> psidx.putmask(psidx < \"c\", \"k\").sort_values()\r\nIndex(['c', 'd', 'e', 'k', 'k'], dtype='object')\r\n>>> psidx.putmask(psidx < \"c\", [\"g\", \"h\", \"i\", \"j\", \"k\"]).sort_values()\r\nIndex(['c', 'd', 'e', 'g', 'h'], dtype='object')\r\n>>> psidx.putmask(psidx < \"c\", (\"g\", \"h\", \"i\", \"j\", \"k\")).sort_values()\r\nIndex(['c', 'd', 'e', 'g', 'h'], dtype='object')\r\n>>> psidx.putmask(psidx < \"c\", ps.Index([\"g\", \"h\", \"i\", \"j\", \"k\"])).sort_values()\r\nIndex(['c', 'd', 'e', 'g', 'h'], dtype='object')\r\n>>> psidx.putmask(psidx < \"c\", \"MASKED\").sort_values()\r\nIndex(['MASKED', 'MASKED', 'c', 'd', 'e'], dtype='object')\r\n```\r\n\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\n\r\nUnit tests.\r\n", "failed_tests": [], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/indexing.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [1, 2, 5]}, {"file": {"name": "python/pyspark/pandas/indexes/base.py", "additions": "116", "deletions": "1", "changes": "117"}, "updated": [0, 1, 5]}, {"file": {"name": "python/pyspark/pandas/indexes/multi.py", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [1, 3, 4]}, {"file": {"name": "python/pyspark/pandas/missing/indexes.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 1, 4]}, {"file": {"name": "python/pyspark/pandas/tests/indexes/test_base.py", "additions": "35", "deletions": "0", "changes": "35"}, "updated": [0, 1, 4]}, {"file": {"name": "python/pyspark/pandas/typedef/typehints.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [2, 4, 8]}]}
{"author": "phamhuyhoang97", "sha": "f6232f6828e76c77d827a482a88515241a0e65b9", "commit_date": "2021/10/06 04:52:06", "commit_message": "fix error", "title": "[SPARK-36100][CORE] Grouping exception in core/status", "body": "### What changes were proposed in this pull request?\r\nThis PR group exception messages in core/src/main/scala/org/apache/spark/status\r\n\r\n### Why are the changes needed?\r\nIt will largely help with standardization of error messages and its maintenance.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo. Error messages remain unchanged.\r\n\r\n### How was this patch tested?\r\nNo new tests - pass all original tests to make sure it doesn't break any existing behavior.", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/errors/SparkCoreErrors.scala", "additions": "103", "deletions": "0", "changes": "103"}, "updated": [0, 0, 4]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusStore.scala", "additions": "4", "deletions": "5", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/KVUtils.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/LiveEntity.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/api/v1/ApiRootResource.scala", "additions": "9", "deletions": "8", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/api/v1/OneApplicationResource.scala", "additions": "12", "deletions": "12", "changes": "24"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/api/v1/SimpleDateParam.scala", "additions": "3", "deletions": "9", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/api/v1/StagesResource.scala", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [0, 0, 0]}]}
{"author": "thangnd197", "sha": "976a0b9c382d4f51258004104c63820035d61768", "commit_date": "2021/10/06 01:32:27", "commit_message": "[SPARK-36102][CORE] Grouping exception in core/deploy", "title": "[SPARK-36102][CORE] Grouping exception in core/deploy", "body": "### What changes were proposed in this pull request?\r\nThis PR group exception messages in core/src/main/scala/org/apache/spark/deploy\r\n\r\n### Why are the changes needed?\r\nIt will largely help with standardization of error messages and its maintenance.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo. Error messages remain unchanged.\r\n\r\n### How was this patch tested?\r\nNo new tests - pass all original tests to make sure it doesn't break any existing behavior.", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/FaultToleranceTest.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/RRunner.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/SparkSubmitArguments.scala", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/StandaloneResourceUtils.scala", "additions": "2", "deletions": "5", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/history/ApplicationCache.scala", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/history/EventLogFileWriters.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala", "additions": "7", "deletions": "11", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/master/Master.scala", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionClient.scala", "additions": "13", "deletions": "14", "changes": "27"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolException.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolMessage.scala", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolRequest.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/security/HadoopFSDelegationTokenProvider.scala", "additions": "3", "deletions": "4", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/worker/DriverRunner.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala", "additions": "5", "deletions": "4", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/worker/ui/LogPage.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/errors/SparkCoreErrors.scala", "additions": "202", "deletions": "2", "changes": "204"}, "updated": [0, 0, 4]}]}
{"author": "thangnd197", "sha": "10ce2cc7bfec822341706838a63ac97f2263ec1c", "commit_date": "2021/10/06 01:36:48", "commit_message": "[SPARK-36099][CORE] Grouping exception in core/util", "title": "[SPARK-36099][CORE] Grouping exception in core/util", "body": "### What changes were proposed in this pull request?\r\nThis PR group exception messages in core/src/main/scala/org/apache/spark/util\r\n\r\n### Why are the changes needed?\r\nIt will largely help with standardization of error messages and its maintenance.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo. Error messages remain unchanged.\r\n\r\n### How was this patch tested?\r\nNo new tests - pass all original tests to make sure it doesn't break any existing behavior.", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/errors/SparkCoreErrors.scala", "additions": "152", "deletions": "5", "changes": "157"}, "updated": [0, 0, 4]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala", "additions": "5", "deletions": "8", "changes": "13"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/DependencyUtils.scala", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/KeyLock.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/ListenerBus.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/NextIterator.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/SerializableBuffer.scala", "additions": "5", "deletions": "3", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/ThreadUtils.scala", "additions": "5", "deletions": "4", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/Utils.scala", "additions": "18", "deletions": "30", "changes": "48"}, "updated": [1, 1, 8]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/collection/AppendOnlyMap.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/collection/CompactBuffer.scala", "additions": "5", "deletions": "4", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala", "additions": "6", "deletions": "5", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/collection/ImmutableBitSet.scala", "additions": "8", "deletions": "6", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/collection/MedianHeap.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/collection/OpenHashSet.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/collection/PartitionedPairBuffer.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/collection/PrimitiveVector.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/io/ChunkedByteBuffer.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/logging/DriverLogger.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/random/RandomSampler.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}]}
{"author": "linhongliu-db", "sha": "48ffd8145621c6d5f5634d8978a4dd4b5841a052", "commit_date": "2021/11/10 05:32:28", "commit_message": "test", "title": "[SPARK-37202][SQL] Treat `injectedFunctions` as custom built-in functions", "body": "### What changes were proposed in this pull request?\r\nThis PR fixes an issue that the `injectedFunctions` in `SparkSessionExtensions` can't be\r\nresolved correctly if it's referred by a temporary view. This is because the injected functions\r\nare not `UserDefinedExpression`. So they will be treated as temporary functions but couldn't\r\nbe captured as temporary functions during view creation. As a result, the function resolution\r\nwill fail if it's reffered by a temp view.\r\n\r\nThis PR adds a new concept `customBuiltinFunction`, so that the injected functions will be\r\ntreated as builtin functions intead of tempoary ones. With with way, it's not needed to be\r\ncaptured by temp view anymore.\r\n\r\n### Why are the changes needed?\r\nbug fix\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nAfter this PR, the `SparkSessionExtensions.injectedFunctions` are not temporary functions anymore.\r\n```scala\r\nval extensions = create { extensions =>\r\n  extensions.injectFunction(MyExtensions.myFunction)\r\n}\r\nwithSession(extensions) { session =>\r\n  // return `false` after this PR, and `true` before this PR\r\n  session.sessionState.catalog.isTemporaryFunction(MyExtensions.myFunction._1)\r\n}\r\n```\r\n\r\n\r\n### How was this patch tested?\r\nnewly added test", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala", "additions": "42", "deletions": "0", "changes": "42"}, "updated": [0, 2, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/SessionCatalog.scala", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 2, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/SparkSessionExtensions.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/command/functions.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SparkSessionExtensionSuite.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [0, 1, 1]}]}
{"author": "gengliangwang", "sha": "760f5b08f07f2e45df454b1f9df9ad33c969bb7c", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetVectorUpdaterFactory.java", "additions": "20", "deletions": "1", "changes": "21"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "27", "deletions": "0", "changes": "27"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "1", "changes": "8"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "66", "deletions": "1", "changes": "67"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}
{"author": "HyukjinKwon", "sha": "e6d77301f891090a0f3bb2f6da9320def859a94d", "commit_date": "2021/11/07 09:06:56", "commit_message": "Add pyspark.sql.tests.test_arrow_map into modules", "title": "[SPARK-37228][SQL][PYTHON] Implement DataFrame.mapInArrow in Python", "body": "### What changes were proposed in this pull request?\r\n\r\nThis PR proposes to implement `DataFrame.mapInArrow` that allows users to apply a function with PyArrow record batches such as:\r\n\r\n```python\r\ndef do_something(iterator):\r\n    for arrow_batch in iterator:\r\n        # do something with `pyarrow.RecordBatch` and create new `pyarrow.RecordBatch`.\r\n        # ...\r\n        yield arrow_batch\r\n\r\ndf.mapInArrow(do_something, df.schema).show()\r\n```\r\n\r\nThe general idea is simple. It shares the same codebase of `DataFrame.mapInPandas` except the pandas conversion logic.\r\n\r\nNote that documentation will be done in another PR.\r\n\r\n### Why are the changes needed?\r\n\r\nFor usability and technical problems. Both are elabourated in more details at SPARK-37227.\r\nPlease also see the discussions at https://github.com/apache/spark/pull/26783.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nYes, this PR adds a new API:\r\n\r\n```python\r\nimport pyarrow as pa\r\n\r\ndf = spark.createDataFrame(\r\n    [(1, \"foo\"), (2, None), (3, \"bar\"), (4, \"bar\")], \"a int, b string\")\r\n\r\ndef func(iterator):\r\n    for batch in iterator:\r\n        # `batch` is pyarrow.RecordBatch.\r\n        yield batch\r\n\r\ndf.mapInArrow(func, df.schema).collect()\r\n```\r\n\r\n### How was this patch tested?\r\n\r\nManually tested, and unit tests were added.", "failed_tests": ["org.apache.spark.sql.streaming.StreamingAggregationSuite", "pyspark.sql.tests.test_arrow_map"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "dev/sparktestsupport/modules.py", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/rdd.py", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/rdd.pyi", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 3, 4]}, {"file": {"name": "python/pyspark/sql/pandas/_typing/__init__.pyi", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/pandas/functions.py", "additions": "5", "deletions": "3", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/pandas/functions.pyi", "additions": "21", "deletions": "0", "changes": "21"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/pandas/map_ops.py", "additions": "63", "deletions": "1", "changes": "64"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/sql/pandas/serializers.py", "additions": "46", "deletions": "0", "changes": "46"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/tests/test_arrow_map.py", "additions": "138", "deletions": "0", "changes": "138"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/udf.py", "additions": "7", "deletions": "5", "changes": "12"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/worker.py", "additions": "18", "deletions": "6", "changes": "24"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/DeduplicateRelations.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/pythonLogicalOperators.scala", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/python/MapInBatchExec.scala", "additions": "90", "deletions": "0", "changes": "90"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/python/MapInPandasExec.scala", "additions": "4", "deletions": "62", "changes": "66"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/python/PythonMapInArrowExec.scala", "additions": "41", "deletions": "0", "changes": "41"}, "updated": [0, 0, 0]}]}
{"author": "dchvn", "sha": "3f529f9bdf3349679dc50caa870b59c178be7cae", "commit_date": "2021/11/10 10:07:25", "commit_message": "update", "title": "[SPARK-37236][PYTHON] Inline type hints for KernelDensity.py, test.py in python/pyspark/mllib/stat/", "body": "### What changes were proposed in this pull request?\r\nInline type hints for KernelDensity.py, test.py in python/pyspark/mllib/stat/\r\n### Why are the changes needed?\r\nWe can take advantage of static type checking within the functions by inlining the type hints.\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n### How was this patch tested?\r\nExisting tests", "failed_tests": ["pyspark.mllib.tests.test_algorithms"], "files": [{"file": {"name": "python/pyspark/mllib/common.pyi", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/mllib/stat/KernelDensity.py", "additions": "9", "deletions": "6", "changes": "15"}, "updated": [1, 1, 1]}, {"file": {"name": "python/pyspark/mllib/stat/KernelDensity.pyi", "additions": "0", "deletions": "27", "changes": "27"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/mllib/stat/test.py", "additions": "13", "deletions": "9", "changes": "22"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/mllib/stat/test.pyi", "additions": "0", "deletions": "39", "changes": "39"}, "updated": [0, 0, 0]}]}
{"author": "dchvn", "sha": "b95c8fa0917c41f5cf86312d295e2a46dfa648e3", "commit_date": "2021/11/08 09:33:00", "commit_message": "fix", "title": "[SPARK-37234][PYTHON] Inline type hints for python/pyspark/mllib/stat/_statistics.py", "body": "### What changes were proposed in this pull request?\r\nInline type hints for python/pyspark/mllib/stat/_statistics.py\r\n### Why are the changes needed?\r\nWe can take advantage of static type checking within the functions by inlining the type hints.\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n### How was this patch tested?\r\nExisting tests", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/mllib/linalg/__init__.pyi", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/mllib/stat/_statistics.py", "additions": "64", "deletions": "17", "changes": "81"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/mllib/stat/_statistics.pyi", "additions": "0", "deletions": "69", "changes": "69"}, "updated": [0, 0, 1]}]}
{"author": "dchvn", "sha": "233b5ac02ec6d6e929131e713089cc5a3066aff5", "commit_date": "2021/10/28 11:08:12", "commit_message": "[SPARK-37146][PYTHON] Inline type hints for python/pyspark/__init__.py", "title": "[SPARK-37146][PYTHON] Inline type hints for python/pyspark/__init__.py", "body": "### What changes were proposed in this pull request?\r\nInline type hints for python/pyspark/\\_\\_init\\_\\_.py\r\n\r\n### Why are the changes needed?\r\nWe can take advantage of static type checking within the functions by inlining the type hints.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\nExisting tests", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/__init__.py", "additions": "25", "deletions": "11", "changes": "36"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/__init__.pyi", "additions": "0", "deletions": "77", "changes": "77"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/sql/conf.py", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/sql/context.py", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 2, 2]}, {"file": {"name": "python/pyspark/sql/dataframe.py", "additions": "8", "deletions": "3", "changes": "11"}, "updated": [0, 3, 8]}]}
{"author": "dchvn", "sha": "b14287e4173f92d81f48d39b1a2e201caf176547", "commit_date": "2021/08/23 15:12:18", "commit_message": "[SPARK-36396] Implement_DataFrame.cov", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/frame.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "189", "deletions": "0", "changes": "189"}, "updated": [0, 0, 13]}, {"file": {"name": "python/pyspark/pandas/missing/frame.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "64", "deletions": "1", "changes": "65"}, "updated": [0, 0, 6]}]}
{"author": "dchvn", "sha": "4fd2a317279b64f52be68d517f5e85f7fe402cbc", "commit_date": "2021/10/18 03:45:41", "commit_message": "[SPARK-37015][PYTHON] Inline type hints for python/pyspark/streaming/dstream.py", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/streaming/dstream.py", "additions": "370", "deletions": "119", "changes": "489"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/streaming/dstream.pyi", "additions": "0", "deletions": "216", "changes": "216"}, "updated": [0, 0, 0]}]}
{"author": "dchvn", "sha": "65294a4bb2db9a65fa3a0144f583b554a943fdbe", "commit_date": "2021/10/21 09:52:10", "commit_message": "Inline statcounter.py", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/statcounter.py", "additions": "21", "deletions": "19", "changes": "40"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/statcounter.pyi", "additions": "0", "deletions": "44", "changes": "44"}, "updated": [0, 0, 0]}]}
{"author": "dchvn", "sha": "98aa78b283ede495a6e7aedc6ff882862de93274", "commit_date": "2021/10/21 07:03:25", "commit_message": "[SPARK-37083] Inline type hints for python/pyspark/accumulators.py", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/_typing.pyi", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/accumulators.py", "additions": "53", "deletions": "31", "changes": "84"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/accumulators.pyi", "additions": "0", "deletions": "73", "changes": "73"}, "updated": [0, 0, 0]}]}
{"author": "dchvn", "sha": "c7d3417293c5de7e2dd8378891c766489016f246", "commit_date": "2021/10/22 06:16:45", "commit_message": "[SPARK-37095][PYTHON] Inline type hints for files in python/pyspark/broadcast.py", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/broadcast.py", "additions": "45", "deletions": "21", "changes": "66"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/broadcast.pyi", "additions": "0", "deletions": "48", "changes": "48"}, "updated": [0, 0, 0]}]}
{"author": "dchvn", "sha": "2e2283de4b77fd407935ff074e0f465b73cbc7c4", "commit_date": "2021/10/15 06:06:17", "commit_message": "[SPARK-37014][PYTHON] Inline type hints for python/pyspark/streaming/context.py", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/streaming/context.py", "additions": "104", "deletions": "59", "changes": "163"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/streaming/context.pyi", "additions": "0", "deletions": "75", "changes": "75"}, "updated": [0, 0, 0]}]}
{"author": "srijith-rajamohan", "sha": "4c720bf3be16d762264050abfab541e2a5a826d0", "commit_date": "2021/07/09 03:30:03", "commit_message": "Automatically build and update the site", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": ".github/workflows/build_and_test.yml", "additions": "12", "deletions": "642", "changes": "654"}, "updated": [1, 2, 20]}, {"file": {"name": "docs/img/SparkComponents.png", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/docs/source/getting_started/index.rst", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 4]}, {"file": {"name": "python/docs/source/index.rst", "additions": "29", "deletions": "18", "changes": "47"}, "updated": [0, 0, 2]}, {"file": {"name": "python/docs/source/user_guide/index.rst", "additions": "53", "deletions": "13", "changes": "66"}, "updated": [0, 0, 2]}]}
{"author": "beliefer", "sha": "dc46ba4ffa445bf92e647390889837b953168856", "commit_date": "2021/11/10 09:00:30", "commit_message": "Update code", "title": "[SPARK-37266][SQL] Optimize the analysis for view text of persistent view and fix security vulnerabilities caused by sql tampering", "body": "### What changes were proposed in this pull request?\r\n\r\nThe current implementation of persistent view is create hive table with view text.\r\nThe view text is just a query string, so the hackers may tamper with it through various means.\r\nSuch as, `select * from tab1` tampered with `drop table tab1`.\r\n\r\n\r\n### Why are the changes needed?\r\nFirst, the view text is query string, `parser.parsePlan(viewText)` occurs more overhead than `parser.parseQuery(viewText)`.\r\nSecond, the view text can be tampered by hackers and issue security vulnerabilities.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n'No'. Unless hackers tamper view text, user will not see any change.\r\n\r\n\r\n### How was this patch tested?\r\nNew tests.\r\n", "failed_tests": ["org.apache.spark.ml.source.image.ImageFileFormatSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/SessionCatalog.scala", "additions": "7", "deletions": "2", "changes": "9"}, "updated": [0, 2, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/ParseDriver.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/ParserInterface.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SparkSessionExtensionSuite.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/HiveCommandSuite.scala", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [0, 0, 0]}]}
{"author": "yaooqinn", "sha": "935b62442955447b74d7c2982745a6d464efb495", "commit_date": "2021/11/06 03:45:29", "commit_message": "Merge branch 'master' into SPARK-37215", "title": "[SPARK-37215][SQL] Support Application Timeouts on YARN", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\nSince #YARN-3813/2.9.0/3.0.0, YARN supports ApplicationTimeouts. It helps enforce lifetime application SLAs. Currently, lifetime indicates the overall time spent by an application in YARN. It is calculated from its submit time to finish time, including running time and the waiting time for resource allocation.\r\n\r\nYARN allows admins to set lifetime of an application at leaf-queue. It also allows users to set it programmatically. During application submission, user can set it in  `ApplicationSubmissionContext#setApplicationTimeouts(Map<ApplicationTimeoutType, Long> applicationTimeouts)`.\r\n\r\nSo far, YARN supports for one timeout type - LIFETIME.\r\n\r\nIn this PR, we `setApplicationTimeouts` when the YARN dependency is available, e.g. the default Hadoop 3.3 or user-specified Hadoop 2.9+ when Hadoop is provided at compile phase.\r\n\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\nThis can enforce the application SLAs.\r\nFor example, when the YARN queue is hit its limits of app concurrency or cpu/mem, an app will be pending for a very long time or even get stuck in  `ACCEPTED` state forever and do nothing. \r\n\r\nSometimes, users also may want their app to succeed or timeout/failed with proper time constraints.\r\n\r\nThis is necessary for end-users use spark througth serverless spark platform like apache kyuubi(incubating) to prevent issue like https://github.com/apache/incubator-kyuubi/issues/1039, https://github.com/apache/incubator-kyuubi/issues/278 and so on.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\nyes, we add a new conf but do not change the current behavior\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->\r\n\r\nnew tests added\r\n", "failed_tests": [], "files": [{"file": {"name": "resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/ResourceRequestHelper.scala", "additions": "31", "deletions": "1", "changes": "32"}, "updated": [1, 1, 1]}, {"file": {"name": "resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/config.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [1, 1, 3]}, {"file": {"name": "resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/ClientSuite.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 1, 2]}, {"file": {"name": "resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/ResourceRequestHelperSuite.scala", "additions": "15", "deletions": "2", "changes": "17"}, "updated": [1, 1, 1]}, {"file": {"name": "resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/ResourceRequestTestHelper.scala", "additions": "9", "deletions": "1", "changes": "10"}, "updated": [0, 0, 0]}]}
{"author": "ulysses-you", "sha": "6ab6dc02d9ac7839589bd9bf5ff4c279fdaea61a", "commit_date": "2021/11/02 12:02:29", "commit_message": "Avoid unnecessary sort in FileFormatWriter if it's not dynamic partition", "title": "[SPARK-37194][SQL] Avoid unnecessary sort in FileFormatWriter if it's not dynamic partition", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nPass a new parameter `dynamicPartition` to `FileFormatWriter.write` so that we can distinguish if we need local sort or not.\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nAvoid unnecessary sort in FileFormatWriter if it's not dynamic partition\r\n\r\n`FileFormatWriter.write` will sort the partition and bucket column before writing. I think this code path assumed the input `partitionColumns` are dynamic but actually it's not. It now is used by three code path:\r\n- `FileStreamSink`; it should be always dynamic partition\r\n- `SaveAsHiveFile`; it followed the assuming that `InsertIntoHiveTable` has removed the static partition and `InsertIntoHiveDirCommand` has no partition\r\n- `InsertIntoHadoopFsRelationCommand`; it passed `partitionColumns` into `FileFormatWriter.write` without removing static partition because we need it to generate the partition path in `DynamicPartitionDataWriter`\r\n\r\nIt shows that the unnecessary sort only affected the `InsertIntoHadoopFsRelationCommand` if we write data with static partition.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nno.\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->\r\nIt should not affect the existed behavior, just improve perf. And for perf number, I did a simple benchmak:\r\n\r\n```sql\r\n\r\nCREATE TABLE test (id long) USING PARQUET PARTITIONED BY (d string);\r\n\r\n-- before this PR, it tooks 1.82  seconds\r\n-- after this PR,  it tooks 1.072 seconds\r\nINSERT OVERWRITE TABLE test PARTITION(d='a') SELECT id FROM range(10000000);\r\n```\r\n\r\n", "failed_tests": ["org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite", "org.apache.spark.sql.sources.BucketedWriteWithHiveSupportSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite", "org.apache.spark.sql.sources.BucketedWriteWithoutHiveSupportSuite", "org.apache.spark.sql.test.DataFrameReaderWriterSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormatWriter.scala", "additions": "5", "deletions": "2", "changes": "7"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSink.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/InsertIntoHiveTable.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/SaveAsHiveFile.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 1]}]}
{"author": "ulysses-you", "sha": "f272a89a9a40e8bd2b514cb6c4c6f9bec9b5a38a", "commit_date": "2021/11/09 05:00:12", "commit_message": "Pull out dynamic partition and bucket sort", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.execution.HiveDDLSuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.execution.command.ShowPartitionsSuite", "org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite", "org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite", "org.apache.spark.sql.hive.InsertSuite", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.sources.BucketedWriteWithHiveSupportSuite", "org.apache.spark.sql.hive.execution.PruneHiveTablePartitionsSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.execution.SQLMetricsSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DynamicPartitionPruningHiveScanSuite", "org.apache.spark.sql.hive.DynamicPartitionPruningHiveScanSuite", "org.apache.spark.sql.SQLQuerySuite", "org.apache.spark.sql.execution.datasources.OrcV2AggregatePushDownSuite", "org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite", "org.apache.spark.sql.DynamicPartitionPruningV1Suite", "org.apache.spark.sql.execution.datasources.BasicWriteJobStatsTrackerMetricSuite", "org.apache.spark.sql.execution.datasources.csv.CSVLegacyTimeParserSuite", "org.apache.spark.sql.DynamicPartitionPruningV1Suite", "org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite", "org.apache.spark.sql.sources.BucketedWriteWithoutHiveSupportSuite", "org.apache.spark.sql.StatisticsCollectionSuite", "org.apache.spark.sql.test.DataFrameReaderWriterSuite", "org.apache.spark.sql.execution.datasources.noop.NoopSuite", "org.apache.spark.sql.execution.datasources.csv.CSVv1Suite", "org.apache.spark.sql.execution.metric.SQLMetricsSuite", "org.apache.spark.sql.execution.datasources.ParquetV2AggregatePushDownSuite", "org.apache.spark.sql.execution.datasources.FileSourceStrategySuite", "org.apache.spark.sql.execution.datasources.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.execution.datasources.OrcV1AggregatePushDownSuite", "org.apache.spark.sql.execution.datasources.csv.CSVv2Suite", "org.apache.spark.sql.execution.datasources.ParquetV1AggregatePushDownSuite", "org.apache.spark.sql.execution.RemoveRedundantProjectsSuite", "org.apache.spark.sql.execution.OptimizeMetadataOnlyQuerySuite", "org.apache.spark.sql.execution.RemoveRedundantProjectsSuite", "pyspark.pandas.tests.test_dataframe_spark_io"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SortExec.scala", "additions": "43", "deletions": "30", "changes": "73"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/command/DataWritingCommand.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 1, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 3, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormatWriter.scala", "additions": "22", "deletions": "72", "changes": "94"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/V1Writes.scala", "additions": "161", "deletions": "0", "changes": "161"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSink.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveSessionStateBuilder.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/CreateHiveTableAsSelectCommand.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/InsertIntoHiveTable.scala", "additions": "8", "deletions": "58", "changes": "66"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/SaveAsHiveFile.scala", "additions": "4", "deletions": "7", "changes": "11"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/V1HiveWrites.scala", "additions": "161", "deletions": "0", "changes": "161"}, "updated": [0, 0, 0]}]}
{"author": "LuciferYang", "sha": "a22f1fc8ca21089037509f3f557afb2d30628c24", "commit_date": "2021/11/09 10:30:05", "commit_message": "Merge branch 'upmaster' into fix-ScalaObjectMapper", "title": "[SPARK-37256][SQL] Replace `ScalaObjectMapper` with `ClassTagExtensions` to fix compilation warning", "body": "### What changes were proposed in this pull request?\r\nThere are some compilation warning log like follows:\r\n```\r\n[WARNING] [Warn] /spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/RebaseDateTime.scala:268: [deprecation @ org.apache.spark.sql.catalyst.util.RebaseDateTime.loadRebaseRecords.mapper.$anon | origin=com.fasterxml.jackson.module.scala.ScalaObjectMapper | version=2.12.1] trait ScalaObjectMapper in package scala is deprecated (since 2.12.1): ScalaObjectMapper is deprecated because Manifests are not supported in Scala3 \r\n```\r\n\r\nRefer to the recommendations of `jackson-module-scala`, this PR use `ClassTagExtensions`  instead of `ScalaObjectMapper`  to fix this compilation warning\r\n\r\n\r\n### Why are the changes needed?\r\nFix compilation warning\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n\r\n### How was this patch tested?\r\nPass the Jenkins or GitHub Action\r\n\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/RebaseDateTime.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/RebaseDateTimeSuite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/RocksDBFileManager.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}
{"author": "LuciferYang", "sha": "af98949b53373be24d4d94fb1c0d563013c875a3", "commit_date": "2021/11/01 06:54:56", "commit_message": "use new error framework", "title": "[SPARK-37013][CORE][SQL][FOLLOWUP] Use the new error framework to throw error in `FormatString` ", "body": "### What changes were proposed in this pull request?\r\nThis is a followup of https://github.com/apache/spark/pull/34313. The main change of this pr is  change to use the new error framework to throw error when `attern.contains(\"%0$\")` is true.\r\n\r\n\r\n### Why are the changes needed?\r\nUse the new error framework to throw error\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n\r\n### How was this patch tested?\r\nPass the Jenkins or GitHub Action\r\n\r\n", "failed_tests": [], "files": [{"file": {"name": "core/src/main/resources/error/error-classes.json", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [1, 1, 5]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala", "additions": "5", "deletions": "4", "changes": "9"}, "updated": [0, 3, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 2, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/postgreSQL/text.sql.out", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}]}
{"author": "LuciferYang", "sha": "7ae2b8d224a91e81776fa63d1f7aa691541460fd", "commit_date": "2021/08/04 03:27:21", "commit_message": "add a new method to avoid file truncate", "title": "", "body": "", "failed_tests": ["org.apache.spark.deploy.yarn.YarnClusterSuite"], "files": [{"file": {"name": "core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java", "additions": "1", "deletions": "4", "changes": "5"}, "updated": [1, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/storage/DiskBlockObjectWriter.scala", "additions": "21", "deletions": "0", "changes": "21"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala", "additions": "1", "deletions": "6", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala", "additions": "1", "deletions": "6", "changes": "7"}, "updated": [1, 2, 3]}]}
{"author": "huaxingao", "sha": "2e50a6016499d23beff27c25122673de677c9025", "commit_date": "2021/11/09 23:26:30", "commit_message": "[SPARK-37262][SQL] Not log empty aggregate and group by in JDBCScan", "title": "[SPARK-37262][SQL] Don't log empty aggregate and group by in JDBCScan", "body": "\r\n\r\n### What changes were proposed in this pull request?\r\nCurrently, the empty pushed aggregate and pushed group by are logged in Explain for JDBCScan\r\n```\r\nScan org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCScan$$anon$1@72e75786 [NAME#1,SALARY#2] PushedAggregates: [], PushedFilters: [IsNotNull(SALARY), GreaterThan(SALARY,100.00)], PushedGroupby: [], ReadSchema: struct<NAME:string,SALARY:decimal(20,2)>\r\n```\r\n\r\nAfter the fix, the JDBCSScan will be\r\n```\r\nScan org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCScan$$anon$1@72e75786 [NAME#1,SALARY#2] PushedFilters: [IsNotNull(SALARY), GreaterThan(SALARY,100.00)], ReadSchema: struct<NAME:string,SALARY:decimal(20,2)>\r\n```\r\n\r\n\r\n### Why are the changes needed?\r\naddress this comment https://github.com/apache/spark/pull/34451#discussion_r740220800\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nno\r\n\r\n\r\n### How was this patch tested?\r\nexisting tests\r\n", "failed_tests": ["org.apache.spark.sql.execution.DataSourceV2ScanExecRedactionSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala", "additions": "4", "deletions": "10", "changes": "14"}, "updated": [0, 2, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala", "additions": "9", "deletions": "10", "changes": "19"}, "updated": [0, 2, 2]}]}
{"author": "huaxingao", "sha": "ee982237fcd7be99521b3dd3b3a416bb2574fcf7", "commit_date": "2021/11/09 22:10:14", "commit_message": "use DateTimeUtils.parseTimestampString", "title": "[SPARK-37219][SQL] Add AS OF syntax support", "body": "### What changes were proposed in this pull request?\r\nhttps://docs.databricks.com/delta/quick-start.html#query-an-earlier-version-of-the-table-time-travel\r\n\r\nDelta Lake time travel allows user to query an older snapshot of a Delta table. To query an older version of a table, user needs to specify a version or timestamp in a SELECT statement using AS OF syntax as the follows\r\n\r\n```\r\nSELECT * FROM default.people10m VERSION AS OF 0;\r\n\r\nSELECT * FROM default.people10m TIMESTAMP AS OF '2019-01-29 00:37:58';\r\n```\r\n\r\nThis PR adds the AS OF syntax support in Spark\r\n\r\n\r\n### Why are the changes needed?\r\nTo support time travel in Spark\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes\r\n\r\nnew SQL syntax:\r\n```\r\nSELECT * FROM default.people10m VERSION AS OF 0;\r\n\r\nSELECT * FROM default.people10m TIMESTAMP AS OF '2019-01-29 00:37:58';\r\n```\r\n\r\nnew `TableCatalog` property to set options to query time travel table:\r\n\r\n```\r\n  /**\r\n   * A reserved property to specify the version of the table.\r\n   */\r\n  String PROP_VERSION = \"versionAsOf\";\r\n\r\n  /**\r\n   * A reserved property to specify the timestamp of the table.\r\n   */\r\n  String PROP_TIMESTAMP = \"timestampAsOf\";\r\n\r\n\r\ndf1 = spark.read.format('delta').option(TableCatalog.PROP_TIMESTAMP, '2019-01-01').load('/mnt/delta/people-10m')\r\n\r\ndf2 = spark.read.format('delta').option('TableCatalog.PROP_VERSION, 2).load('/mnt/delta/people-10m')\r\n```\r\n\r\n### How was this patch tested?\r\nnew UT\r\n", "failed_tests": ["org.apache.spark.sql.catalyst.SQLKeywordSuite", "org.apache.spark.sql.jdbc.JDBCV2Suite"], "files": [{"file": {"name": "docs/sql-ref-ansi-compliance.md", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 2, 5]}, {"file": {"name": "sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4", "additions": "12", "deletions": "1", "changes": "13"}, "updated": [0, 2, 7]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/TableCatalog.java", "additions": "40", "deletions": "0", "changes": "40"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala", "additions": "36", "deletions": "1", "changes": "37"}, "updated": [0, 4, 13]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/CatalogV2Util.scala", "additions": "17", "deletions": "2", "changes": "19"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala", "additions": "38", "deletions": "0", "changes": "38"}, "updated": [0, 3, 9]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Utils.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "huaxingao", "sha": "0e655a8dedf278d52a661abf836d2e061412c03e", "commit_date": "2021/11/06 22:21:58", "commit_message": "address comments", "title": "[SPARK-36646][SQL] Push down group by partition column for aggregate", "body": "\r\n\r\n### What changes were proposed in this pull request?\r\nlift the restriction for aggregate push down for parquet and orc if group by is partition col\r\n\r\n\r\n### Why are the changes needed?\r\nto complete the file source aggregate push down work\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nno\r\n\r\n\r\n### How was this patch tested?\r\nnew tests\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/AggregatePushDownUtils.scala", "additions": "46", "deletions": "4", "changes": "50"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcUtils.scala", "additions": "19", "deletions": "8", "changes": "27"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetUtils.scala", "additions": "19", "deletions": "11", "changes": "30"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/orc/OrcPartitionReaderFactory.scala", "additions": "13", "deletions": "6", "changes": "19"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetPartitionReaderFactory.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/FileSourceAggregatePushDownSuite.scala", "additions": "59", "deletions": "0", "changes": "59"}, "updated": [0, 1, 1]}]}
{"author": "zero323", "sha": "7d5b512df75858263f740eaacc6a188670d35c9b", "commit_date": "2021/10/12 19:55:32", "commit_message": "Run mypy tests against ml, sql, streaming and core examples", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "dev/lint-python", "additions": "26", "deletions": "9", "changes": "35"}, "updated": [0, 0, 1]}, {"file": {"name": "examples/src/main/python/__init__.py", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/python/als.py", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/python/avro_inputformat.py", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/python/ml/__init__,py", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/python/ml/chi_square_test_example.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/python/ml/correlation_example.py", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/python/mllib/__init__.py", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/python/parquet_inputformat.py", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/python/sort.py", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/python/sql/__init__.py", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/python/sql/streaming/__init__,py", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/python/sql/streaming/structured_network_wordcount_windowed.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/python/streaming/__init__.py", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/python/streaming/network_wordjoinsentiments.py", "additions": "12", "deletions": "3", "changes": "15"}, "updated": [0, 0, 0]}]}
{"author": "zero323", "sha": "63b30d3e606e6919c6a6ced07b651542300590c6", "commit_date": "2021/10/24 11:06:31", "commit_message": "Make RDD covariant", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/rdd.pyi", "additions": "53", "deletions": "52", "changes": "105"}, "updated": [1, 1, 1]}, {"file": {"name": "python/pyspark/streaming/dstream.pyi", "additions": "29", "deletions": "28", "changes": "57"}, "updated": [0, 0, 0]}]}
{"author": "AngersZhuuuu", "sha": "1873cd3cd9fe8d63af5a51f5d55849f752b25dfe", "commit_date": "2021/11/09 14:37:57", "commit_message": "Update NestedColumnAliasing.scala", "title": "[SPARK-37201][SQL] GeneratorNestedColumnAliasing support Generate with Filter", "body": "### What changes were proposed in this pull request?\r\nIn current ` GeneratorNestedColumnAliasing`, spark only support  push down case with Project as below\r\n```\r\nProject [v1#225, el#226]\r\n   +- Project [struct#220.v1 AS v1#225, el#226, struct#220]\r\n      +- Generate explode(array#221), false, [el#226]\r\n         +- SubqueryAlias spark_catalog.default.table\r\n            +- Relation default.table[struct#220,array#221] parquet\r\n```\r\n\r\nIn this pr we support push dow with Project and Filter as below\r\n```\r\nProject [v1#225, el#226]\r\n +- Project [struct#220.v1 AS v1#225, el#226, struct#220]\r\n    +- Filter ((el#226 = cx1) AND (struct#220.v2 = v3))\r\n      +- Generate explode(array#221), false, [el#226]\r\n         +- SubqueryAlias spark_catalog.default.table\r\n            +- Relation default.table[struct#220,array#221] parquet\r\n```\r\n\r\n### Why are the changes needed?\r\nImprove GeneratorNestedColumnAliasing to support more case\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n\r\n### How was this patch tested?\r\nAdd UT\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NestedColumnAliasing.scala", "additions": "97", "deletions": "76", "changes": "173"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ColumnPruningSuite.scala", "additions": "28", "deletions": "0", "changes": "28"}, "updated": [0, 0, 0]}]}
{"author": "AngersZhuuuu", "sha": "3362038d8803aa6af579c626b67623e11a494055", "commit_date": "2021/11/01 05:13:19", "commit_message": "Update SparkMetadataOperationSuite.scala", "title": "[WIP][SPARK-37173][SQL] SparkGetFunctionOperation return builtin function only once", "body": "### What changes were proposed in this pull request?\r\nAccording to https://github.com/apache/spark/pull/25252/files#r738489764, if we use wild pattern, it will return too much rows.\r\n\r\nIn this pr we return common builtin functions only once\r\n\r\n### Why are the changes needed?\r\nImprove performance\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\nWIP\r\n", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.SparkMetadataOperationSuite"], "files": [{"file": {"name": "sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkGetFunctionsOperation.scala", "additions": "23", "deletions": "1", "changes": "24"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/SparkMetadataOperationSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "AngersZhuuuu", "sha": "b901a972bfd64a2f41183dec287908135b8e4d71", "commit_date": "2021/11/08 13:07:49", "commit_message": "update", "title": "[SPARK-37169][SQL] Fix incorrect  cast value when cast DateType to NumericType", "body": "### What changes were proposed in this pull request?\r\nIn current Spark, when cast DateType to NumericType such as `cast(date'2020-01-01' as long)`, it will return `null` value, in this pr we fix this incorrect result.\r\n\r\n### Why are the changes needed?\r\nFix bug\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nUser can cast DateType to NumericType with correct result.\r\n\r\n### How was this patch tested?\r\nadded UT\r\n", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.AnsiCastSuite", "org.apache.spark.sql.catalyst.expressions.AnsiCastSuite", "org.apache.spark.sql.catalyst.expressions.CastSuite", "org.apache.spark.sql.catalyst.expressions.TryCastSuite", "org.apache.spark.sql.catalyst.expressions.CastSuite", "org.apache.spark.ml.source.image.ImageFileFormatSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala", "additions": "118", "deletions": "18", "changes": "136"}, "updated": [0, 2, 2]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/AnsiCastSuiteBase.scala", "additions": "2", "deletions": "20", "changes": "22"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CastSuite.scala", "additions": "16", "deletions": "16", "changes": "32"}, "updated": [0, 0, 0]}]}
{"author": "AngersZhuuuu", "sha": "0af1bd4e4d7d7c7b68850713bb8e9db3f783118e", "commit_date": "2021/10/18 04:37:40", "commit_message": "[SPARK-37035][SQL] Improve error message when use parquet vectorize reader", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [0, 2, 6]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetDictionary.java", "additions": "57", "deletions": "22", "changes": "79"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase.java", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader.java", "additions": "6", "deletions": "2", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader.java", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetVectorizedSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}]}
{"author": "AngersZhuuuu", "sha": "bbdcb1faca24c1ae897de3f68c6c1e3b1fe97749", "commit_date": "2021/10/22 09:33:24", "commit_message": "[SPARK-37097][YARN] yarn-cluster mode don't need to retry when AM container exit code 0 but application failed.", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala", "additions": "10", "deletions": "1", "changes": "11"}, "updated": [0, 0, 1]}]}
{"author": "Ngone51", "sha": "ce22c0968fe86dbbdee31bceb233071c43d02c8f", "commit_date": "2021/11/10 01:48:43", "commit_message": "Merge branch 'apache:master' into SPARK-35011", "title": "[SPARK-35011][CORE] Fix false active executor in UI that caused by BlockManager reregistration  ", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\nAlso post the event `SparkListenerExecutorRemoved` when removing an executor, which is known by `BlockManagerMaster` but unknown to `SchedulerBackend`.\r\n\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\nIn https://github.com/apache/spark/pull/32114, it reports an issue that `BlockManagerMaster` could register a `BlockManager` from a dead executor due to reregistration mechanism. The side effect is, the executor will be shown on the UI as an active one, though it's already dead indeed.\r\n\r\nIn https://github.com/apache/spark/pull/32114, we tried to avoid such reregistration for a to-be-dead executor. However, I just realized that we can actually leave such reregistration alone since `HeartbeatReceiver.expireDeadHosts` should clean up those `BlockManager`s in the end. The problem is, the corresponding executors in UI can't be cleaned along with the `BlockManager`s cleaning. Because executors in UI can only be cleaned by `SparkListenerExecutorRemoved`, \r\n while `BlockManager`s  cleaning only post `SparkListenerBlockManagerRemoved` (which is ignored by `AppStatusListener`). \r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\nYes, users would see the false active executor be removed in the end.\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->\r\n\r\nPass existing tests.", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 0]}]}
{"author": "venkata91", "sha": "4801b3fb06fbe4c434397aff29c52ed083fb8006", "commit_date": "2021/09/02 04:53:49", "commit_message": "SPARK-33701: Adaptive shuffle merge finalization", "title": "", "body": "", "failed_tests": ["org.apache.spark.shuffle.ShuffleBlockPusherSuite", "org.apache.spark.sql.kafka010.KafkaMicroBatchV1SourceWithAdminSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/Dependency.scala", "additions": "31", "deletions": "4", "changes": "35"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/MapOutputTracker.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/SparkEnv.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "24", "deletions": "0", "changes": "24"}, "updated": [0, 1, 6]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala", "additions": "215", "deletions": "59", "changes": "274"}, "updated": [0, 0, 4]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerEvent.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessage.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [1, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/shuffle/ShuffleBlockPusher.scala", "additions": "33", "deletions": "0", "changes": "33"}, "updated": [0, 0, 7]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/ThreadUtils.scala", "additions": "9", "deletions": "1", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/DAGSchedulerSuite.scala", "additions": "112", "deletions": "3", "changes": "115"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/test/scala/org/apache/spark/shuffle/ShuffleBlockPusherSuite.scala", "additions": "33", "deletions": "0", "changes": "33"}, "updated": [0, 0, 5]}, {"file": {"name": "docs/configuration.md", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [0, 1, 4]}]}
{"author": "wForget", "sha": "63c58d121d4ca41e496b1f83c2c657f765858997", "commit_date": "2021/11/07 12:30:52", "commit_message": "[SPARK-37210] Write to static partition in dynamic write mode, add test case", "title": "[SPARK-37210][SQL] Write to static partition in dynamic write mode", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nWhen using static partition writing, dynamicPartitionOverwrite should also be set to true. See [SPARK-37210](https://issues.apache.org/jira/browse/SPARK-37210) for details\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nAn error occurred while concurrently writing to different static partitions.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nNo\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->\r\nSee [SPARK-37210](https://issues.apache.org/jira/browse/SPARK-37210) for specific test.", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveParquetSuite.scala", "additions": "21", "deletions": "0", "changes": "21"}, "updated": [0, 1, 3]}]}
{"author": "hgs19921112", "sha": "d06f62b0de377f2cf91051d0551166c61c8333c0", "commit_date": "2021/11/05 13:19:17", "commit_message": "remove 'create temporary macro' unit test", "title": "[SPARK-37216][SQL] Add the Hive macro functionality to SparkSQL", "body": "\r\n### What changes were proposed in this pull request?\r\n\r\nAdd the Hive macro functionality to SparkSQL\r\n\r\n### Why are the changes needed?\r\n\r\nSome Hive sql can move to SparkSQL Smoothly\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nSome new DDL like 'create temparory macro ...'\r\n\r\n### How was this patch tested?\r\n\r\nunit test\r\n\r\nAuthored-by: hgs19921112 <haoguangshi@gmail.com>", "failed_tests": ["org.apache.spark.sql.hive.execution.HiveQuerySuite"], "files": [{"file": {"name": "sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 3, 8]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/SessionCatalog.scala", "additions": "23", "deletions": "2", "changes": "25"}, "updated": [0, 2, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Macro.scala", "additions": "88", "deletions": "0", "changes": "88"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala", "additions": "12", "deletions": "0", "changes": "12"}, "updated": [0, 1, 8]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/command/functions.scala", "additions": "72", "deletions": "4", "changes": "76"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/UDFSuite.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/HiveQuerySuite.scala", "additions": "1", "deletions": "4", "changes": "5"}, "updated": [1, 1, 1]}]}
{"author": "ekoifman", "sha": "8a3f27d5d1384bf9b7ed768fe5fd65bd9adf84e6", "commit_date": "2021/11/10 00:09:15", "commit_message": "[SPARK-37193][SQL] fix test", "title": "[SPARK-37193][SQL] DynamicJoinSelection.shouldDemoteBroadcastHashJoin should not apply to outer joins", "body": "\r\n\r\n<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nDynamicJoinSelection.shouldDemoteBroadcastHashJoin will prevent AQE from converting Sort merge join into a broadcast join because SMJ is faster when the side that would be broadcast has a lot of empty partitions.\r\nThis makes sense for inner joins which can short circuit if one side is empty.\r\nFor (left,right) outer join, the streaming side still has to be processed so demoting broadcast join doesn't have the same advantage.\r\n\r\n\r\n\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nYes, it may cause AQE to choose BHJ more often than before with better performance\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->\r\nSpark UTs\r\nAlso empirical evidence", "failed_tests": ["pyspark.pandas.tests.test_ops_on_diff_frames"], "files": [{"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/DynamicJoinSelection.scala", "additions": "13", "deletions": "5", "changes": "18"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}]}
{"author": "peter-toth", "sha": "61f2b34c49c2f7c76f3ebd320b9d2f927f7ea100", "commit_date": "2021/04/21 17:40:19", "commit_message": "[SPARK-34079][SQL] Merging non-correlated scalar subqueries to multi-column scalar subqueries for better reuse", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueries.scala", "additions": "416", "deletions": "0", "changes": "416"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala", "additions": "32", "deletions": "1", "changes": "33"}, "updated": [2, 8, 16]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreePatterns.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [1, 3, 3]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueriesSuite.scala", "additions": "282", "deletions": "0", "changes": "282"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap.java", "additions": "2", "deletions": "7", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "8", "deletions": "3", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/PlanAdaptiveSubqueries.scala", "additions": "18", "deletions": "6", "changes": "24"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/ReuseAdaptiveSubquery.scala", "additions": "5", "deletions": "3", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala", "additions": "2", "deletions": "8", "changes": "10"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "0", "deletions": "9", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "21", "deletions": "0", "changes": "21"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/reuse/ReuseExchangeAndSubquery.scala", "additions": "10", "deletions": "3", "changes": "13"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala", "additions": "18", "deletions": "2", "changes": "20"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9.sf100/explain.txt", "additions": "183", "deletions": "598", "changes": "781"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9.sf100/simplified.txt", "additions": "65", "deletions": "170", "changes": "235"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9/explain.txt", "additions": "183", "deletions": "598", "changes": "781"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9/simplified.txt", "additions": "65", "deletions": "170", "changes": "235"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala", "additions": "36", "deletions": "0", "changes": "36"}, "updated": [1, 2, 4]}]}
{"author": "peter-toth", "sha": "227cad1af875d9c432fae9a7428d23d42cb4392e", "commit_date": "2021/07/09 11:57:23", "commit_message": "[SPARK-36073][SQL] SubExpr elimination should include common child exprs of conditional expressions", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala", "additions": "80", "deletions": "48", "changes": "128"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala", "additions": "12", "deletions": "1", "changes": "13"}, "updated": [1, 4, 11]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/SubexpressionEliminationSuite.scala", "additions": "28", "deletions": "0", "changes": "28"}, "updated": [0, 3, 7]}]}
{"author": "wankunde", "sha": "a1505cafaa964dbf3e808754f70a80877b0ba19a", "commit_date": "2021/10/14 04:24:25", "commit_message": "add spark.shuffle.accurateBlockSkewedFactor parameter to determine whether to report a shuffle block size", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "22", "deletions": "0", "changes": "22"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/MapStatus.scala", "additions": "29", "deletions": "3", "changes": "32"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/Utils.scala", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [0, 1, 7]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/MapStatusSuite.scala", "additions": "62", "deletions": "0", "changes": "62"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "3", "deletions": "12", "changes": "15"}, "updated": [0, 0, 2]}]}
{"author": "xuechendi", "sha": "0623f15dc78cf2cd730ee57b4d014f73667a72e6", "commit_date": "2021/11/01 05:33:56", "commit_message": "Support RowToColumnarExec to write to Arrow\n\nSigned-off-by: Chendi Xue <chendi.xue@intel.com>", "title": "[SPARK-37124][SQL] Support RowToColumnarExec with Arrow format", "body": "### What changes were proposed in this pull request?\r\nThis Jira is aim to support Arrow format in RowToColumnarExec.\r\n\r\n### Why are the changes needed?\r\nCurrent ArrowColumnVector is not fully equivalent to OnHeap/OffHeapColumnVector in spark, so RowToColumnarExec doesn't support write to Arrow format so far.\r\n\r\nsince Arrow API is now being more stable, and using pandas udf will perform much better than python udf.\r\n\r\n### What has been done in this pull request?\r\nI am  proposing to support RowToColumnarExec with Arrow.\r\n\r\nWhat I did in this PR is to add a load api in ArrowColumnVector to load arrowRecordBatch to ArrowColumnVector, then called inside RowToColumnarExec doExecute.\r\n\r\n### How was this patch tested?\r\nUTs are also added to test this new API and RowToColumnarExec with ArrowFormat.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNO\r\n\r\nSigned-off-by: Chendi Xue <chendi.xue@intel.com>", "failed_tests": ["org.apache.spark.sql.hive.client.HivePartitionFilteringSuite", "org.apache.spark.sql.execution.vectorized.ArrowColumnVectorSuite", "org.apache.spark.sql.execution.arrow.ArrowWriterSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/vectorized/ArrowColumnVector.java", "additions": "28", "deletions": "0", "changes": "28"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [1, 9, 16]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/util/ArrowUtils.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/Columnar.scala", "additions": "33", "deletions": "1", "changes": "34"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/arrow/ArrowConverters.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/vectorized/ArrowColumnVectorSessionSuite.scala", "additions": "286", "deletions": "0", "changes": "286"}, "updated": [0, 0, 0]}]}
{"author": "pralabhkumar", "sha": "bdd9a9e296ade1e9a9b40d22f60bef11c68f802f", "commit_date": "2021/10/30 02:20:00", "commit_message": "removed if else logic", "title": "[SPARK-30537][PYTHON], Fix toPandas  wrong dtypes when applied on empty DF when Arrow enabled", "body": "### What changes were proposed in this pull request?\r\ntoPandas will return correct dtype for empty dataframe when arrow enabled\r\n\r\n```\r\nfrom datetime import datetime\r\nspark_df = spark.createDataFrame([(10, \"Emy\", datetime.today() ), (11, \"Bob\", datetime.today())], [\"age\", \"name\", \"date\"])\r\nspark.createDataFrame(spark.sparkContext.emptyRDD(), schema=spark_df.schema).toPandas().dtypes\r\n\r\nage              int64\r\nname            object\r\ndate    datetime64[ns]\r\ndtype: object\r\n\r\nspark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", True)\r\nspark.createDataFrame(spark.sparkContext.emptyRDD(), schema=spark_df.schema).toPandas().dtypes\r\n\r\nage              int64\r\nname            object\r\ndate    datetime64[ns]\r\n\r\ndtype: object\r\n\r\n```\r\n\r\n### Why are the changes needed?\r\nCurrently toPandas for empty dataframe return object as dtype for all the element when arrow is enabled . However things works fine when arrow is disabled . Therefore this PR will make give the correct dtype when arrow is enabled and dataframe is empty  \r\n\r\n\r\n### Does this PR introduce any user-facing change?\r\nYes user will be able to see correct dtype values \r\n\r\n### How was this patch tested?\r\nunit tests", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/sql/pandas/conversion.py", "additions": "10", "deletions": "1", "changes": "11"}, "updated": [0, 1, 4]}, {"file": {"name": "python/pyspark/sql/tests/test_arrow.py", "additions": "26", "deletions": "0", "changes": "26"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/tests/test_dataframe.py", "additions": "51", "deletions": "43", "changes": "94"}, "updated": [0, 0, 1]}]}
{"author": "parthchandra", "sha": "10659d3bc9c1f3b3b9f8c29000f9f75e95281777", "commit_date": "2019/03/06 13:11:58", "commit_message": "[SPARK-26509][SQL] Parquet DELTA_BYTE_ARRAY is not supported in Spark 2.x's Vectorized Reader", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": ".gitignore", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader.java", "additions": "16", "deletions": "5", "changes": "21"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedDeltaBinaryPackedReader.java", "additions": "319", "deletions": "0", "changes": "319"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedDeltaByteArrayReader.java", "additions": "176", "deletions": "0", "changes": "176"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedValuesReader.java", "additions": "33", "deletions": "0", "changes": "33"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetDeltaEncodingIntegerSuite.scala", "additions": "234", "deletions": "0", "changes": "234"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetDeltaEncodingLongSuite.scala", "additions": "233", "deletions": "0", "changes": "233"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetEncodingSuite.scala", "additions": "40", "deletions": "0", "changes": "40"}, "updated": [0, 0, 0]}]}
{"author": "xkrogen", "sha": "e1b0668ceb138c1450604ca46904418a6b042a42", "commit_date": "2021/06/25 13:53:57", "commit_message": "[SPARK-35672][CORE][YARN] Pass user classpath entries to executors using config instead of command line\n\n### What changes were proposed in this pull request?\nRefactor the logic for constructing the user classpath from `yarn.ApplicationMaster` into `yarn.Client` so that it can be leveraged on the executor side as well, instead of having the driver construct it and pass it to the executor via command-line arguments. A new method, `getUserClassPath`, is added to `CoarseGrainedExecutorBackend` which defaults to `Nil` (consistent with the existing behavior where non-YARN resource managers do not configure the user classpath). `YarnCoarseGrainedExecutorBackend` overrides this to construct the user classpath from the existing `APP_JAR` and `SECONDARY_JARS` configs.\n\n### Why are the changes needed?\nUser-provided JARs are made available to executors using a custom classloader, so they do not appear on the standard Java classpath. Instead, they are passed as a list to the executor which then creates a classloader out of the URLs. Currently in the case of YARN, this list of JARs is crafted by the Driver (in `ExecutorRunnable`), which then passes the information to the executors (`CoarseGrainedExecutorBackend`) by specifying each JAR on the executor command line as `--user-class-path /path/to/myjar.jar`. This can cause extremely long argument lists when there are many JARs, which can cause the OS argument length to be exceeded, typically manifesting as the error message:\n\n> /bin/bash: Argument list too long\n\nA [Google search](https://www.google.com/search?q=spark%20%22%2Fbin%2Fbash%3A%20argument%20list%20too%20long%22&oq=spark%20%22%2Fbin%2Fbash%3A%20argument%20list%20too%20long%22) indicates that this is not a theoretical problem and afflicts real users, including ours. Passing this list using the configurations instead resolves this issue.\n\n### Does this PR introduce _any_ user-facing change?\nNo, except for fixing the bug, allowing for larger JAR lists to be passed successfully. Configuration of JARs is identical to before.\n\n### How was this patch tested?\nNew unit tests were added in `YarnClusterSuite`. Also, we have been running a similar fix internally for 4 months with great success.\n\nCloses #32810 from xkrogen/xkrogen-SPARK-35672-classpath-scalable.\n\nAuthored-by: Erik Krogen <xkrogen@apache.org>\nSigned-off-by: Thomas Graves <tgraves@apache.org>", "title": "", "body": "", "failed_tests": ["org.apache.spark.repl.SingletonReplSuite", "org.apache.spark.sql.kafka010.KafkaMicroBatchV2SourceWithAdminSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala", "additions": "6", "deletions": "11", "changes": "17"}, "updated": [1, 2, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/executor/Executor.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 1, 3]}, {"file": {"name": "core/src/test/scala/org/apache/spark/executor/CoarseGrainedExecutorBackendSuite.scala", "additions": "8", "deletions": "9", "changes": "17"}, "updated": [1, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesExecutorBackend.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala", "additions": "3", "deletions": "6", "changes": "9"}, "updated": [1, 1, 1]}, {"file": {"name": "resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala", "additions": "31", "deletions": "3", "changes": "34"}, "updated": [1, 2, 2]}, {"file": {"name": "resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnable.scala", "additions": "0", "deletions": "12", "changes": "12"}, "updated": [1, 1, 1]}, {"file": {"name": "resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/YarnSparkHadoopUtil.scala", "additions": "73", "deletions": "2", "changes": "75"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/yarn/src/main/scala/org/apache/spark/executor/YarnCoarseGrainedExecutorBackend.scala", "additions": "5", "deletions": "3", "changes": "8"}, "updated": [1, 1, 1]}, {"file": {"name": "resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/ClientSuite.scala", "additions": "35", "deletions": "0", "changes": "35"}, "updated": [1, 1, 1]}, {"file": {"name": "resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/YarnClusterSuite.scala", "additions": "68", "deletions": "9", "changes": "77"}, "updated": [1, 1, 1]}, {"file": {"name": "resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/YarnSparkHadoopUtilSuite.scala", "additions": "40", "deletions": "0", "changes": "40"}, "updated": [0, 0, 0]}]}
{"author": "xkrogen", "sha": "7f3f0a457d7f0852698aa32d85d5bcae3bbab422", "commit_date": "2021/09/15 21:57:44", "commit_message": "SPARK-34378 [AVRO] Enhance AvroSerializer validation to allow extra nullable Avro fields", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/TestUtils.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 1]}, {"file": {"name": "external/avro/src/main/scala/org/apache/spark/sql/avro/AvroSerializer.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 3]}, {"file": {"name": "external/avro/src/main/scala/org/apache/spark/sql/avro/AvroUtils.scala", "additions": "10", "deletions": "3", "changes": "13"}, "updated": [0, 0, 3]}, {"file": {"name": "external/avro/src/test/scala/org/apache/spark/sql/avro/AvroSchemaHelperSuite.scala", "additions": "24", "deletions": "1", "changes": "25"}, "updated": [0, 0, 1]}, {"file": {"name": "external/avro/src/test/scala/org/apache/spark/sql/avro/AvroSerdeSuite.scala", "additions": "26", "deletions": "15", "changes": "41"}, "updated": [0, 0, 1]}, {"file": {"name": "external/avro/src/test/scala/org/apache/spark/sql/avro/AvroSuite.scala", "additions": "26", "deletions": "0", "changes": "26"}, "updated": [0, 0, 3]}]}
{"author": "wangyum", "sha": "d2e10279e77f48de681f78d431a61f41d1afc6d2", "commit_date": "2021/11/08 00:38:52", "commit_message": "Support between and", "title": "[SPARK-37226][SQL] Filter push down through window", "body": "### What changes were proposed in this pull request?\r\n\r\nThis pr enhance `PushPredicateThroughNonJoin` to support filter push down through window if window partition is empty. For example:\r\n```scala\r\nspark.sql(\"CREATE TABLE t1 using parquet AS SELECT id AS a, id AS b FROM range(1000)\")\r\nspark.sql(\"SELECT * FROM (SELECT *, ROW_NUMBER() OVER(ORDER BY a) AS rn FROM t1) t WHERE rn > 100 and rn <= 200\").explain(true)\r\n```\r\nAfter this pr:\r\n```\r\n== Optimized Logical Plan ==\r\nFilter ((rn#3 > 100) AND (rn#3 <= 200))\r\n+- Window [row_number() windowspecdefinition(a#5L ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rn#3], [a#5L ASC NULLS FIRST]\r\n   +- GlobalLimit 200\r\n      +- LocalLimit 200\r\n         +- Sort [a#5L ASC NULLS FIRST], true\r\n            +- Relation default.t1[a#5L,b#6L] parquet\r\n```\r\n\r\n### Why are the changes needed?\r\n\r\nImprove query performance.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nNo.\r\n\r\n\r\n### How was this patch tested?\r\n\r\nUnit test and benchmark test:\r\n```scala\r\nimport org.apache.spark.benchmark.Benchmark\r\nval numRows = 1024 * 1024 * 100\r\nspark.sql(s\"CREATE TABLE t1 using parquet AS SELECT id as a, id as b FROM range(${numRows}L)\")\r\nval benchmark = new Benchmark(\"Benchmark filter push down through window\", numRows, minNumIters = 5)\r\n\r\nSeq(1, 1000).foreach { threshold =>\r\n  val name = s\"Filter push down through window ${if (threshold > 1) \"(Enabled)\" else \"(Disabled)\"}\"\r\n  benchmark.addCase(name) { _ =>\r\n    withSQLConf(\"spark.sql.execution.topKSortFallbackThreshold\" -> s\"$threshold\") {\r\n      spark.sql(\"SELECT * FROM (SELECT *, ROW_NUMBER() OVER(ORDER BY a) AS rn FROM t1) t WHERE rn > 100 and rn <= 200\").write.format(\"noop\").mode(\"Overwrite\").save()\r\n    }\r\n  }\r\n}\r\nbenchmark.run()\r\n```\r\nBenchmark result:\r\n```\r\nJava HotSpot(TM) 64-Bit Server VM 1.8.0_251-b08 on Mac OS X 10.15.7\r\nIntel(R) Core(TM) i9-9980HK CPU @ 2.40GHz\r\nBenchmark filter push down through window:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n--------------------------------------------------------------------------------------------------------------------------\r\nFilter push down through window (Disabled)          79219          87062         NaN          1.3         755.5       1.0X\r\nFilter push down through window (Enabled)            5339           5821         425         19.6          50.9      14.8X\r\n\r\n```", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "24", "deletions": "1", "changes": "25"}, "updated": [2, 2, 7]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/FilterPushdownSuite.scala", "additions": "63", "deletions": "0", "changes": "63"}, "updated": [1, 1, 1]}]}
{"author": "wangyum", "sha": "919492ee6095a007077119a11425abd7f08b02f6", "commit_date": "2021/10/28 06:59:44", "commit_message": "nullability", "title": "[SPARK-31809][SQL] Infer IsNotNull from some special equality join keys", "body": "### What changes were proposed in this pull request?\r\n\r\nWe can infer `IsNotNull` from some special equality join keys. For example:\r\n```sql\r\nCREATE TABLE t1(a string, b string, c string) using parquet;\r\nCREATE TABLE t2(a string, b decimal(38, 18), c string) using parquet;\r\nSELECT t1.* FROM t1 JOIN t2 ON coalesce(t1.a, t1.b)=t2.a; -- case 1\r\nSELECT t1.* FROM t1 JOIN t2 ON CAST(t1.a AS DOUBLE)=CAST(t2.b AS DOUBLE); -- case 2\r\n```\r\nThe `coalesce(t1.a, t1.b)` or `CAST(t1.a AS DOUBLE)` may generate a lot of null values, which will lead to skew join.\r\nAfter this pr:\r\n```\r\n== Physical Plan ==\r\n*(5) Project [a#5, b#6, c#7]\r\n+- *(5) SortMergeJoin [coalesce(a#5, b#6)], [a#8], Inner\r\n   :- *(2) Sort [coalesce(a#5, b#6) ASC NULLS FIRST], false, 0\r\n   :  +- Exchange hashpartitioning(coalesce(a#5, b#6), 200), true, [id=#44]\r\n   :     +- *(1) Filter isnotnull(coalesce(a#5, b#6))\r\n   :        +- Scan hive default.t1 [a#5, b#6, c#7], HiveTableRelation `default`.`t1`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [a#5, b#6, c#7], Statistics(sizeInBytes=8.0 EiB)\r\n   +- *(4) Sort [a#8 ASC NULLS FIRST], false, 0\r\n      +- Exchange hashpartitioning(a#8, 200), true, [id=#52]\r\n         +- *(3) Filter isnotnull(a#8)\r\n            +- Scan hive default.t2 [a#8], HiveTableRelation `default`.`t2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [a#8, b#9, c#10], Statistics(sizeInBytes=8.0 EiB)\r\n```\r\n\r\n### Why are the changes needed?\r\n\r\n1. Avoid skew join in some cases.\r\n2. [Hive support this optimization](https://github.com/apache/hive/blob/rel/release-3.1.2/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinAddNotNullRule.java).\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nNo.\r\n\r\n### How was this patch tested?\r\n\r\nUnit test and benchmark test:\r\nCase1:\r\nBefore this PR | After this PR\r\n-- | --\r\n![image](https://issues.apache.org/jira/secure/attachment/13003914/13003914_default.png) | ![image](https://issues.apache.org/jira/secure/attachment/13003913/13003913_infer.png)\r\n\r\nCase2:\r\nBefore this PR | After this PR\r\n-- | --\r\n![image](https://user-images.githubusercontent.com/5399861/128879249-f08c0577-caf7-422f-b25c-f47113cc5793.png) | ![image](https://user-images.githubusercontent.com/5399861/128879432-eff937d2-999b-4ac8-a216-25b40e093b67.png)\r\n", "failed_tests": ["org.apache.spark.sql.JoinSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "35", "deletions": "5", "changes": "40"}, "updated": [0, 0, 5]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/InferFiltersFromConstraintsSuite.scala", "additions": "47", "deletions": "8", "changes": "55"}, "updated": [0, 1, 1]}]}
{"author": "wangyum", "sha": "c6fca9aad5b3e1d819d35441dd30c6e12de04da2", "commit_date": "2021/07/26 15:19:57", "commit_message": "Push down join condition evaluation", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.optimizer.PullOutJoinConditionSuite", "org.apache.spark.ml.source.image.ImageFileFormatSuite", "org.apache.spark.sql.streaming.StreamingOuterJoinSuite", "org.apache.spark.sql.streaming.StreamingInnerJoinSuite", "org.apache.spark.sql.streaming.StreamingLeftSemiJoinSuite", "org.apache.spark.sql.streaming.StreamingFullOuterJoinSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 3, 13]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/PullOutJoinCondition.scala", "additions": "85", "deletions": "0", "changes": "85"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/PullOutJoinConditionSuite.scala", "additions": "97", "deletions": "0", "changes": "97"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameJoinSuite.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/JoinSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala", "additions": "27", "deletions": "2", "changes": "29"}, "updated": [0, 4, 12]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/WholeStageCodegenSuite.scala", "additions": "6", "deletions": "5", "changes": "11"}, "updated": [0, 0, 0]}]}
{"author": "wangyum", "sha": "f1dec162613e1c572641d0ec6246c98c406080a5", "commit_date": "2021/07/17 15:56:31", "commit_message": "Remove the aggregation from left semi/anti join if the same aggregation has already been done on left side", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RemoveRedundantAggregates.scala", "additions": "6", "deletions": "2", "changes": "8"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/DistinctKeyVisitor.scala", "additions": "119", "deletions": "0", "changes": "119"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/LogicalPlan.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/RemoveRedundantAggregatesSuite.scala", "additions": "110", "deletions": "11", "changes": "121"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/plans/logical/DistinctKeyVisitorSuite.scala", "additions": "122", "deletions": "0", "changes": "122"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a.sf100/explain.txt", "additions": "460", "deletions": "481", "changes": "941"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a.sf100/simplified.txt", "additions": "113", "deletions": "118", "changes": "231"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a/explain.txt", "additions": "229", "deletions": "245", "changes": "474"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a/simplified.txt", "additions": "62", "deletions": "64", "changes": "126"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b.sf100/explain.txt", "additions": "393", "deletions": "414", "changes": "807"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b.sf100/simplified.txt", "additions": "106", "deletions": "111", "changes": "217"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b/explain.txt", "additions": "213", "deletions": "229", "changes": "442"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b/simplified.txt", "additions": "62", "deletions": "64", "changes": "126"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q38.sf100/explain.txt", "additions": "143", "deletions": "190", "changes": "333"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q38.sf100/simplified.txt", "additions": "112", "deletions": "125", "changes": "237"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q38/explain.txt", "additions": "79", "deletions": "106", "changes": "185"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q38/simplified.txt", "additions": "60", "deletions": "63", "changes": "123"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q87.sf100/explain.txt", "additions": "143", "deletions": "190", "changes": "333"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q87.sf100/simplified.txt", "additions": "112", "deletions": "125", "changes": "237"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q87/explain.txt", "additions": "79", "deletions": "106", "changes": "185"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q87/simplified.txt", "additions": "60", "deletions": "63", "changes": "123"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14.sf100/explain.txt", "additions": "393", "deletions": "414", "changes": "807"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14.sf100/simplified.txt", "additions": "106", "deletions": "111", "changes": "217"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14/explain.txt", "additions": "213", "deletions": "229", "changes": "442"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14/simplified.txt", "additions": "62", "deletions": "64", "changes": "126"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a.sf100/explain.txt", "additions": "602", "deletions": "623", "changes": "1225"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a.sf100/simplified.txt", "additions": "128", "deletions": "133", "changes": "261"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a/explain.txt", "additions": "279", "deletions": "295", "changes": "574"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a/simplified.txt", "additions": "62", "deletions": "64", "changes": "126"}, "updated": [0, 0, 3]}]}
{"author": "cxzl25", "sha": "7df3c789d31600bf780d82445fbc2375d78ef909", "commit_date": "2021/11/07 18:12:03", "commit_message": "add missing colon", "title": "[SPARK-37217][SQL] The number of dynamic partitions should early check when writing to external tables", "body": "### What changes were proposed in this pull request?\r\nSPARK-29295 introduces a mechanism that writes to external tables is a dynamic partition method, and the data in the target partition will be deleted first.\r\n\r\nAssuming that 1001 partitions are written, the data of 10001 partitions will be deleted first, but because `hive.exec.max.dynamic.partitions` is 1000 by default, loadDynamicPartitions will fail at this time, but the data of 1001 partitions has been deleted.\r\n\r\nSo we can check whether the number of dynamic partitions is greater than `hive.exec.max.dynamic.partitions` before deleting, it should fail quickly at this time.\r\n\r\n### Why are the changes needed?\r\nAvoid data that cannot be recovered when the job fails.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\nadd UT\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/InsertIntoHiveTable.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/SQLQuerySuite.scala", "additions": "28", "deletions": "0", "changes": "28"}, "updated": [0, 0, 1]}]}
{"author": "cxzl25", "sha": "16a6272b4d52ffb73c2c5a60294692479a2774f4", "commit_date": "2021/09/18 17:24:39", "commit_message": "Pass queryExecution name in CLI when only select query.", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkSQLDriver.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "nicolasazrak", "sha": "e76d9e3522e5d697a652f5363ce29d0b7eb38df5", "commit_date": "2021/11/08 12:42:07", "commit_message": "Fix linting errors", "title": "[SPARK-34521][PYTHON][SQL] Fix spark.createDataFrame when using pandas with StringDtype", "body": "\r\n### What changes were proposed in this pull request?\r\n\r\nThis change fixes `SPARK-34521`. It allows creating a spark DataFrame from a pandas DataFrame that is using a `StringDtype` column.\r\n\r\n### Why are the changes needed?\r\n\r\nPandas stores string columns in two different ways: using a numpy `ndarray` or using a custom `StringArray`. The `StringArray` version is used when specifing the `dtype=string`. When that happens, spark cannot serialize the column to arrow. Converting the `Series` before fixes this problem. \r\n\r\nHowever, due to the different ways to handle string columns, doing `spark.createDataFrame(pandas_dataframe).toPandas()` might not equal to `pandas_dataframe`. The column dtype could be different.\r\n\r\nMore info: https://pandas.pydata.org/docs/user_guide/text.html\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nNo, it only fixes a use case that broken.\r\n\r\n### How was this patch tested?\r\n\r\nUsing the `test_createDataFrame_with_string_dtype` test.\r\n", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/sql/pandas/serializers.py", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/tests/test_arrow.py", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [0, 0, 0]}]}
{"author": "tanelk", "sha": "0369168b75341ce7dc562ad8889875732c3f4007", "commit_date": "2021/11/08 05:23:47", "commit_message": "Add allowlist", "title": "[SPARK-30220] Enable using Exists/In subqueries outside of the Filter node", "body": "### What changes were proposed in this pull request?\r\nEnable using Exists/In subqueries in other nodes besides `Filter`: `Aggregate`, `Project` and `Window`.\r\nThis is allready mostly supported, but it was blocked in the `Analyzer`. Only requires a small tweak in the `Optimizer`.\r\n\r\n### Why are the changes needed?\r\nOne of the last open feature parities between PostgreSQL and Spark: [SPARK-30374](https://issues.apache.org/jira/browse/SPARK-30374)\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\nExisting and new UTs\r\n", "failed_tests": ["org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/subquery.scala", "additions": "12", "deletions": "2", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala", "additions": "0", "deletions": "10", "changes": "10"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/subquery/exists-subquery/exists-outside-filter.sql", "additions": "133", "deletions": "0", "changes": "133"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/group-by-filter.sql.out", "additions": "28", "deletions": "68", "changes": "96"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/subquery/exists-subquery/exists-outside-filter.sql.out", "additions": "224", "deletions": "0", "changes": "224"}, "updated": [0, 0, 0]}]}
{"author": "Peng-Lei", "sha": "bf9de6c5841faf74719ef7c732ce5b6b2e7e6001", "commit_date": "2021/11/04 11:45:35", "commit_message": "add cast between YearMonthIntervalType and IntegralType", "title": "[SPARK-36924][SQL] CAST between ANSI intervals and numerics", "body": "### What changes were proposed in this pull request?\r\nAdd cast `YearMonthIntervalType` to `NumericType`\r\nrequirement: `YearMonthIntervalType` just have one unit and `Numeric` is `IntegralType`\r\ncast rule:\r\n1. `YearMonthIntervalType(YEAR)` to `ByteType`: `value / 12`, it may cause overflow\r\n    eg: `cast 'INTERVAL 1 YEAR' to ByteTpe == 1.toByte`\r\n2. `YearMonthIntervalType(YEAR)` to `ShortType`: `value / 12`, it may cause overflow\r\n3. `YearMonthIntervalType(YEAR)` to `IntegerType`: `value / 12`\r\n4. `YearMonthIntervalType(YEAR)` to `LongType`: `value / 12`\r\n5. `YearMonthIntervalType(MONTH)` to `ByteType`: `value`, it may cause overflow\r\n6. `YearMonthIntervalType(MONTH)` to `ShortType`: `value`, it may cause overflow\r\n7. `YearMonthIntervalType(MONTH)` to `IntegerType`: `value`\r\n8. `YearMonthIntervalType(MONTH)` to `LongType`: `value`\r\n\r\nAdd cast `NumericType` to `YearMonthIntervalType`\r\nrequirement: `YearMonthIntervalType` just have one unit and `Numeric` is `IntegralType`\r\ncast rule:\r\n1. `ByteType` to `YearMonthIntervalType(YEAR)`: `value * 12`\r\n    eg: `cast 1.toByte to YearMonthIntervalType(YEAR) == 'INTERVAL 1 YEAR'`\r\n2. `ShortType` to `YearMonthIntervalType(YEAR)`: `value * 12`\r\n3. `IntegerType` to `YearMonthIntervalType(YEAR)`: `value * 12`, it may cause overflow\r\n4. `LongType` to `YearMonthIntervalType(YEAR)`: `value * 12`, it may cause overflow\r\n5. `ByteType` to `YearMonthIntervalType(MONTH)`: `value`\r\n6. `ShortType` to `YearMonthIntervalType(MONTH)`: `value`\r\n7. `IntegerType` to `YearMonthIntervalType(MONTH)`: `value`\r\n8. `LongType` to `YearMonthIntervalType(MONTH)`: `value`, it may cause overflow\r\n\r\n\r\n### Why are the changes needed?\r\nAccording to 2011 Standards\r\n![\u622a\u56fe](https://user-images.githubusercontent.com/41178002/140504037-b86793f0-2c97-49f7-bcbf-bb6864592aa8.PNG)\r\n\r\n7) If TD is an interval and SD is exact numeric, then TD shall contain only a single <primary datetime field>.\r\n8) If TD is exact numeric and SD is an interval, then SD shall contain only a single <primary datetime field>.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes, user can use cast function between YearMonthIntervalType to NumericType\r\n\r\n\r\n### How was this patch tested?\r\nadd ut testcase\r\n", "failed_tests": ["org.apache.spark.sql.catalyst.expressions.CastSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala", "additions": "136", "deletions": "2", "changes": "138"}, "updated": [2, 2, 2]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CastSuite.scala", "additions": "124", "deletions": "0", "changes": "124"}, "updated": [0, 0, 0]}]}
{"author": "Peng-Lei", "sha": "2806dfd662f7bd6c0bc8a6e04075210c2a8b7a10", "commit_date": "2021/11/03 08:56:45", "commit_message": "add draft", "title": "[SPARK-37195][SQL][TESTS] Unify v1 and v2 SHOW TBLPROPERTIES tests", "body": "### What changes were proposed in this pull request?\r\n\r\n1. Move SHOW TBLPROPERTIES parsing tests to `ShowTblPropertiesParserSuite`.\r\n2. Define the class `command.ShowTblPropertiesSuiteBase` that is parent of `v1.ShowTblPropertiesSuiteBase` and `v2.ShowTblPropertiesSuite`.\r\n3. Define the class `v1.ShowTblPropertiesSuiteBase` that is parent of `v1.ShowTblPropertiesSuite` and `hive.execution.command.ShowTblPropertiesSuite`.\r\n4. move testcase from `DDLParserSuite` to `ShowTblPropertiesParserSuite`\r\n5. move testcase from `DataSourceV2SQLSessionCatalogSuite` and `DataSourceV2SQLSuite` to `v2.ShowTblPropertiesSuite`\r\n6. move testcase from `HiveCommandSuite` to `hive.execution.command.ShowTblPropertiesSuite`\r\n\r\nThe changes follow the approach of [#30287](https://github.com/apache/spark/pull/30287) [#34305](https://github.com/apache/spark/pull/34305)\r\n\r\n### Why are the changes needed?\r\n1. The unification will allow to run common `SHOW TBLPROPERTIES` tests for both DSv1/Hive DSv1 and DSv2\r\n2. We can detect missing features and differences between DSv1 and DSv2 implementations.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\nExisting unit tests", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala", "additions": "0", "deletions": "13", "changes": "13"}, "updated": [2, 4, 9]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSessionCatalogSuite.scala", "additions": "1", "deletions": "17", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala", "additions": "0", "deletions": "56", "changes": "56"}, "updated": [2, 4, 8]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/PlanResolutionSuite.scala", "additions": "6", "deletions": "209", "changes": "215"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/PlanResolutionSuiteBase.scala", "additions": "240", "deletions": "0", "changes": "240"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/ShowTblPropertiesParserSuite.scala", "additions": "37", "deletions": "0", "changes": "37"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/ShowTblPropertiesSuiteBase.scala", "additions": "36", "deletions": "0", "changes": "36"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/v1/ShowTblPropertiesPlanResolutionSuite.scala", "additions": "26", "deletions": "0", "changes": "26"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/v1/ShowTblPropertiesSuite.scala", "additions": "41", "deletions": "0", "changes": "41"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/v2/ShowTblPropertiesPlanResolutionSuite.scala", "additions": "51", "deletions": "0", "changes": "51"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/v2/ShowTblPropertiesSuite.scala", "additions": "106", "deletions": "0", "changes": "106"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/HiveCommandSuite.scala", "additions": "0", "deletions": "40", "changes": "40"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/command/ShowTblPropertiesSuite.scala", "additions": "92", "deletions": "0", "changes": "92"}, "updated": [0, 0, 0]}]}
{"author": "HeartSaVioR", "sha": "17151054e1b5c9fb64784f82d3595d22c35239e2", "commit_date": "2021/10/25 08:30:59", "commit_message": "[SPARK-37224][SS] Optimize write path on RocksDB state store provider", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "docs/structured-streaming-programming-guide.md", "additions": "13", "deletions": "0", "changes": "13"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/benchmarks/StateStoreBasicOperationsBenchmark-results.txt", "additions": "183", "deletions": "0", "changes": "183"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/RocksDB.scala", "additions": "63", "deletions": "16", "changes": "79"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/benchmark/StateStoreBasicOperationsBenchmark.scala", "additions": "370", "deletions": "0", "changes": "370"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/RocksDBStateStoreIntegrationSuite.scala", "additions": "38", "deletions": "1", "changes": "39"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/RocksDBSuite.scala", "additions": "60", "deletions": "0", "changes": "60"}, "updated": [0, 0, 0]}]}
{"author": "Yikun", "sha": "c066dd2b4375e1237c66bddb8606b966f2fd9d33", "commit_date": "2021/10/26 07:22:59", "commit_message": "Add PodGroup", "title": "[WIP][SPARK-36061][K8S] Add support for PodGroup ", "body": "### What changes were proposed in this pull request?\r\n`PodGroup` is a group of pods with strong association and is mainly used in batch scheduling, is of a Custom Resource Definition (CRD) type in Kubernetes, PodGroup concept which was approved by  Kuberentes community in [KEP-583 Coscheduling](https://github.com/kubernetes/enhancements/tree/master/keps/sig-scheduling/583-coscheduling).\r\n\r\n![image](https://user-images.githubusercontent.com/1736354/139654361-5e05e42c-2904-4012-8663-00b235b76d09.png)\r\n\r\n\r\nThis patch adds the PodGroup support for Kuberentes:\r\n- Add PodGroup configuration: Introduce configurations to enable PodGroup support: `spark.kubernetes.enablePodGroup`, and also adds two configurations (`spark.kubernetes.podgroup.min.[cpu|memory]`) to helps user specifing min CPU and min Memory for a PodGroup.\r\n- Add Volcano implementaions: if user specify the spark k8s scheduler as `volcano`, will create the PodGroup with minReousrce requirement in Volcano automically,  If available resources in the cluster cannot satisfy the requirement, no pod in the PodGroup will be scheduled.\r\n- Driver/Executor pod would be labeled with `scheduling.k8s.io/group-name` key and value s\"${kubernetesConf.resourceNamePrefix}-podgroup\".\r\n\r\nSuch as, user can use below configuration to request a group of pods with  4 CPU/ 8G Mem as min requirement, the volcano will help user create these pods if the meet the min requirement (4 CPUU, 8G Mem), If available resources in the cluster cannot satisfy the requirement, no pod in the PodGroup will be scheduled.\r\n\r\n```\r\n  --conf spark.kubernetes.driver.scheduler.name=volcano \\\r\n  --conf spark.kubernetes.enablePodGroup=true \\\r\n  --conf spark.kubernetes.podgroup.min.cpu=4 \\\r\n  --conf spark.kubernetes.podgroup.min.memory=8G \\\r\n```\r\n\r\n### Why are the changes needed?\r\nProvide feature to request minimum resources before scheduling jobs.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes, add podgroup related configuration.\r\n\r\n\r\n### How was this patch tested?\r\n- UT\r\n- e2e test:\r\n```shell\r\n# Setup K8S\r\nminikube start --cpus 3 --memory 4096\r\nkubectl create serviceaccount spark\r\nkubectl create clusterrolebinding spark-role --clusterrole=edit --serviceaccount=spark:spark --namespace=spark\r\n# Setup Volcano\r\nkubectl apply -f https://raw.githubusercontent.com/volcano-sh/volcano/master/installer/volcano-development.yaml\r\n# Submit job\r\nbin/spark-submit \\\r\n  --master k8s://https://127.0.0.1:6443 \\\r\n  --deploy-mode cluster \\\r\n  --conf spark.kubernetes.driver.scheduler.name=volcano \\\r\n  --conf spark.kubernetes.enablePodGroup=true \\\r\n  --conf spark.executor.instances=1 \\\r\n  --conf spark.kubernetes.namespace=default \\\r\n  --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\\r\n  --conf spark.kubernetes.container.image=spark:latest \\\r\n  --class org.apache.spark.examples.SparkPi \\\r\n  --name spark-pi \\\r\n  local:///opt/spark/examples/jars/spark-examples_2.12-3.3.0-SNAPSHOT.jar\r\n```", "failed_tests": [], "files": [{"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "19", "deletions": "0", "changes": "19"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesDriverSpec.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/KubernetesFeatureConfigStep.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/PodGroupFeatureStep.scala", "additions": "97", "deletions": "0", "changes": "97"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/submit/KubernetesClientApplication.scala", "additions": "23", "deletions": "1", "changes": "24"}, "updated": [0, 0, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/submit/KubernetesDriverBuilder.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesExecutorBuilder.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/batch/volcano/PodGroup.scala", "additions": "40", "deletions": "0", "changes": "40"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/batch/volcano/PodGroupSpec.scala", "additions": "60", "deletions": "0", "changes": "60"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/batch/volcano/PodGroupStatus.scala", "additions": "42", "deletions": "0", "changes": "42"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/batch/volcano/V1Beta1.scala", "additions": "40", "deletions": "0", "changes": "40"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/DriverCommandFeatureStepSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/PodGroupFeatureStepSuite.scala", "additions": "71", "deletions": "0", "changes": "71"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/submit/ClientSuite.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}]}
{"author": "taosiyuan163", "sha": "87e23902a6ee82e4bc14452fc7a4aa2c13e1d29b", "commit_date": "2021/11/02 05:45:05", "commit_message": "[SPARK-37178][ML] Add Target Encoding to ml.feature", "title": "[SPARK-37178][ML] Add Target Encoding to ml.feature", "body": "JIRA Issue: https://issues.apache.org/jira/browse/SPARK-37178", "failed_tests": [], "files": [{"file": {"name": "mllib/src/main/scala/org/apache/spark/ml/feature/TargetEncoder.scala", "additions": "212", "deletions": "0", "changes": "212"}, "updated": [0, 0, 0]}, {"file": {"name": "mllib/src/test/scala/org/apache/spark/ml/feature/TargetEncoderSuite.scala", "additions": "52", "deletions": "0", "changes": "52"}, "updated": [0, 0, 0]}]}
{"author": "Kimahriman", "sha": "323a1fa445940e1fb35eaef354494edb629596ba", "commit_date": "2021/10/15 11:34:18", "commit_message": "Add codegen support to array transform", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala", "additions": "6", "deletions": "5", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/higherOrderFunctions.scala", "additions": "138", "deletions": "4", "changes": "142"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [2, 2, 6]}]}
{"author": "Kimahriman", "sha": "f4ed2be60b8694e9b8ac29f6b9ac3f69e11999b2", "commit_date": "2021/07/22 13:02:02", "commit_message": "Track conditionally evaluated expressions to resolve as subexpressions for cases they are already being evaluated", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala", "additions": "113", "deletions": "76", "changes": "189"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [2, 6, 18]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/SubexpressionEliminationSuite.scala", "additions": "55", "deletions": "13", "changes": "68"}, "updated": [0, 0, 4]}]}
{"author": "mkaravel", "sha": "bd83888c5cbdd6cb4286ef7b410524367cb9dd8b", "commit_date": "2021/10/28 00:04:32", "commit_message": "LPAD, RPAD: modify non-ANSI behavior to accept string and padding that are different types.\nThe result in this case is a string. The byte sequence input is cast to a UTF8 string before\nperforming the padding.", "title": "[SPARK-37047][SQL][FOLLOWUP] lpad/rpad should work in non-ANSI mode if parameters str and pad are different types", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\nThis is a followup of #34154 and #34370 . Now `lpad`/`rpad` allow the `str` and `pad` parameters to be of different types in non-ANSI mode. The result type in this case is a character string.\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\nThe changes in this PR restore the behavior (in non-ANSI mode) to that prior to #34154 and #34370 when the `lpad` and `rpad` functions take an `str` and `pad` argument and these arguments are of different types.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\nNo. The overloads for the `lpad` and `rpad` functions have not been released yet.\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->\r\n\r\nExisting tests are enough (and have been updated appropriately).\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala", "additions": "91", "deletions": "20", "changes": "111"}, "updated": [1, 3, 3]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/string-functions.sql", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [1, 2, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/string-functions.sql.out", "additions": "8", "deletions": "8", "changes": "16"}, "updated": [1, 2, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/string-functions.sql.out", "additions": "12", "deletions": "16", "changes": "28"}, "updated": [1, 2, 2]}]}
{"author": "mkaravel", "sha": "3e5ca86557ee91bc4bbe56b1bc5606905d500c96", "commit_date": "2021/09/21 06:46:15", "commit_message": "[SPARK-38611][SQL] Add SQL functions for the BINARY data type for AND, OR, XOR, NOT", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "common/unsafe/src/main/java/org/apache/spark/unsafe/types/ByteArray.java", "additions": "161", "deletions": "0", "changes": "161"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala", "additions": "233", "deletions": "0", "changes": "233"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/functions.scala", "additions": "96", "deletions": "0", "changes": "96"}, "updated": [1, 3, 5]}, {"file": {"name": "sql/core/src/test/resources/sql-functions/sql-expression-schema.md", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/string-functions.sql", "additions": "93", "deletions": "1", "changes": "94"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/string-functions.sql.out", "additions": "601", "deletions": "1", "changes": "602"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/string-functions.sql.out", "additions": "601", "deletions": "1", "changes": "602"}, "updated": [0, 0, 0]}]}
{"author": "haodemon", "sha": "b69ddc2f665e2faf3a5f3d65e1c27be3949fe062", "commit_date": "2021/08/07 01:34:37", "commit_message": "[SPARK-27997][K8S] Add support for OAuth Token refresh", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "docs/running-on-kubernetes.md", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 4]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 4]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/SparkKubernetesClientFactory.scala", "additions": "17", "deletions": "8", "changes": "25"}, "updated": [0, 0, 1]}]}
{"author": "shaneknapp", "sha": "9e2a6fc67f70f3d7f37c37c3f589070cabef951f", "commit_date": "2021/10/27 17:06:55", "commit_message": "strace coz why notwq", "title": "[DO NOT MERGE][PYTHON] testing lint-python", "body": "DO NOT MERGE TYVM", "failed_tests": ["org.apache.spark.storage.BlockManagerDecommissionIntegrationSuite"], "files": [{"file": {"name": "dev/lint-python", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [2, 2, 3]}, {"file": {"name": "python/pyspark/pandas/_typing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 1]}]}
{"author": "tnixon", "sha": "e58cbfb9d63a1b83866eae11b105ac1ea8493c74", "commit_date": "2021/10/27 19:45:33", "commit_message": "Update namespace.py\n\nFixed documentation of the escapechar parameter of read_csv function\r\n(docs incorrectly say this is for escaping the delimiter - it is for escaping a quotechar)", "title": "Minor fix to docs for read_csv", "body": "Fixed documentation of the escapechar parameter of read_csv function\r\n(docs incorrectly say this is for escaping the delimiter - it is for escaping a quotechar)\r\n\r\n<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nMinor update to function documentation.\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nCurrently the documentation for the escapechar parameter is incorrect.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nFixes the documentation\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->\r\nNo testing needed", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/namespace.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 8]}]}
{"author": "ueshin", "sha": "44da384c661c31d27462cd96634b0cfb3cc83ba7", "commit_date": "2021/10/25 20:02:48", "commit_message": "Remove unnecessary 'F401' comments.", "title": "[SPARK-37011][PYTHON] Remove unnecessary 'noqa: F401' comments", "body": "### What changes were proposed in this pull request?\r\n\r\nRemove unnecessary 'noqa: F401' comments.\r\n\r\n### Why are the changes needed?\r\n\r\nNow that `flake8` in Jenkins was upgraded (#34384), we can remove unnecessary 'noqa: F401' comments.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nNo.\r\n\r\n### How was this patch tested?\r\n\r\nExisting tests.", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/_typing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 1]}]}
{"author": "AngersZhuuuu", "sha": "3ca6e3d2b94ccee55018b644441cd9c1bd37c3f9", "commit_date": "2021/11/17 02:22:47", "commit_message": "Update sql-migration-guide.md", "title": "[SPARK-37344][SQL][DOC] spark-sql cli will keep `\\` when match `\\;` after spark3", "body": "### What changes were proposed in this pull request?\r\nBefore Spark 3, if we pass a SQL like `select split('Spark SQL', '\\;')`  to spark-sql, after process, it will actually execute  `select split('Spark SQL', ';')`.\r\n\r\nspark-sql with verbose log:\r\n```\r\n[info]   2021-11-16 18:05:34.86 - stdout> spark-sql> select split('dawdawdawd','\\;');\r\n[info]   2021-11-16 18:05:34.875 - stdout> select split('dawdawdawd',';')\r\n```\r\n\r\nBut after Spark 3 It will execute  `select split('Spark SQL', '\\;')`\r\nspark-sql with verbose log:\r\n```\r\n[info]   2021-11-16 18:05:34.86 - stdout> spark-sql> select split('dawdawdawd','\\;');\r\n[info]   2021-11-16 18:05:34.875 - stdout> select split('dawdawdawd','\\;')\r\n```\r\n\r\n\r\nIn this PR we doc this change.\r\n\r\nThis change is caused by hive commit : https://github.com/apache/hive/commit/65a65826a0d351a3d918bdb98595bdd106d37adb#diff-79277c3cfeb5bc38066fbbe2b90dcee5c870100b8b1e106d53ed0d56817bd0ee\r\n\r\nRelated JIRA ID : https://issues.apache.org/jira/browse/HIVE-15297 \r\n\r\n\r\n### Why are the changes needed?\r\nUpdate migration guide\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\n\r\nNot need\r\n\r\n![image](https://user-images.githubusercontent.com/46485123/142100077-a3c9151d-4a8d-4817-874a-c28dd03131ff.png)\r\n", "failed_tests": [], "files": [{"file": {"name": "docs/sql-migration-guide.md", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 3]}]}
{"author": "AngersZhuuuu", "sha": "bdedc32843da4ae3a110aafbca37ce40a1219433", "commit_date": "2021/11/23 08:12:02", "commit_message": "Update HiveClientImpl.scala", "title": "[SPARK-37446][SQL] Use reflection for getWithoutRegisterFns to allow different Hive versions for building", "body": "### What changes were proposed in this pull request?\r\nSince Hive 2.3.9  start have function `getWithoutRegisterFns`, but user may use hive 2.3.8 or lower version.\r\nHere we should use reflection to let user build with hive 2.3.8 or lower version\r\n\r\n\r\n### Why are the changes needed?\r\nSupport build with hive version lower than 2.3.9 since many user will build spark with it 's own hive code and their own jar (they may do some optimize or. other thing in their own code). This pr make it easier to integrate and won't hurt current logic.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nUser can build spark with hive version lower than 2.3.9\r\n\r\n\r\n### How was this patch tested?\r\n\r\nbuild with command \r\n```\r\n./dev/make-distribution.sh --tgz -Pyarn -Phive -Phive-thriftserver -Dhive.version=2.3.8\r\n```\r\n\r\nJars under dist\r\n![image](https://user-images.githubusercontent.com/46485123/143162194-d505b151-f23d-4268-af19-6dfeccea4a74.png)\r\n", "failed_tests": ["org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveDDLSuite", "org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.HiveExternalCatalogVersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.execution.HiveResolutionSuite", "org.apache.spark.sql.hive.execution.HiveQuerySuite", "org.apache.spark.sql.hive.execution.HiveSerDeSuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite", "org.apache.spark.sql.hive.execution.HashAggregationQueryWithControlledFallbackSuite", "org.apache.spark.sql.hive.execution.BigDataBenchmarkSuite", "org.apache.spark.sql.hive.execution.HiveWindowFunctionQueryFileSuite", "org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite", "org.apache.spark.sql.hive.execution.PruningSuite", "org.apache.spark.sql.hive.JavaDataFrameSuite", "org.apache.spark.sql.hive.JavaDataFrameSuite", "org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite", "org.apache.spark.sql.hive.JavaDataFrameSuite", "org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite", "org.apache.spark.sql.hive.client.HadoopVersionInfoSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.command.AlterTableRenamePartitionSuite", "org.apache.spark.sql.hive.HiveSessionStateSuite", "org.apache.spark.sql.hive.HiveVariableSubstitutionSuite", "org.apache.spark.sql.hive.execution.command.DescribeNamespaceSuite", "org.apache.spark.sql.hive.execution.HiveSerDeReadWriteSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveUserDefinedTypeSuite", "org.apache.spark.sql.HiveCharVarcharTestSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.ObjectHashAggregateSuite", "org.apache.spark.sql.hive.execution.command.TruncateTableSuite", "org.apache.spark.sql.hive.execution.command.AlterTableRenameSuite", "org.apache.spark.sql.hive.execution.SQLMetricsSuite", "org.apache.spark.sql.hive.OptimizeHiveMetadataOnlyQuerySuite", "org.apache.spark.sql.hive.security.HiveHadoopDelegationTokenManagerSuite", "org.apache.spark.sql.hive.QueryPartitionSuite", "org.apache.spark.sql.hive.ParquetHiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.command.MsckRepairTableSuite", "org.apache.spark.sql.hive.TestHiveSuite", "org.apache.spark.sql.hive.HiveSQLInsertTestSuite", "org.apache.spark.sql.sources.CommitFailureTestRelationSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.execution.command.ShowTblPropertiesSuite", "org.apache.spark.sql.hive.HiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.execution.HiveScriptTransformationSuite", "org.apache.spark.sql.hive.execution.command.AlterTableRecoverPartitionsSuite", "org.apache.spark.sql.hive.execution.HiveExplainSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.execution.command.DropTableSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.execution.command.AlterTableDropPartitionSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.HiveCharVarcharDDLTestSuite", "org.apache.spark.sql.hive.execution.command.AlterTableAddPartitionSuite", "org.apache.spark.sql.hive.execution.HiveSerDeSuite", "org.apache.spark.sql.hive.execution.HiveUDAFSuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.DynamicPartitionPruningHiveScanSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.UDFSuite", "org.apache.spark.sql.hive.execution.BigDataBenchmarkSuite", "org.apache.spark.sql.hive.execution.HiveCommandSuite", "org.apache.spark.sql.hive.execution.HashUDAQueryWithControlledFallbackSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.client.HivePartitionFilteringSuite", "org.apache.spark.sql.hive.execution.HiveWindowFunctionQueryFileSuite", "org.apache.spark.sql.hive.ErrorPositionSuite", "org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite", "org.apache.spark.sql.hive.execution.HashAggregationQuerySuite", "org.apache.spark.sql.hive.execution.PruningSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite"], "files": [{"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 7]}]}
{"author": "AngersZhuuuu", "sha": "7d8e8b415706370afa561d6af1a6bbcc808dda13", "commit_date": "2021/11/24 06:32:04", "commit_message": "update", "title": "[SPARK-37445][BUILD] Upgrade hadoop profile to hadoop-3.3 since we support hadoop-3.3 as default now", "body": "### What changes were proposed in this pull request?\r\nUpgrade hadoop profile to hadoop-3.3 since we support hadoop-3.3 as default now.\r\n\r\nIn current project, deps's path is still hadoop-3.2, it's not correct.\r\n\r\n### Why are the changes needed?\r\nUpgrade hadoop profile\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\nNot need", "failed_tests": [], "files": [{"file": {"name": ".github/workflows/build_and_test.yml", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [1, 1, 10]}, {"file": {"name": "dev/create-release/release-build.sh", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "dev/deps/spark-deps-hadoop-3.3-hive-2.3", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "dev/run-tests-jenkins.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "dev/run-tests.py", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 1, 1]}, {"file": {"name": "dev/test-dependencies.sh", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 1, 1]}, {"file": {"name": "hadoop-cloud/pom.xml", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "pom.xml", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 22]}, {"file": {"name": "python/pyspark/install.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/dev/dev-run-integration-tests.sh", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/pom.xml", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 4]}, {"file": {"name": "resource-managers/yarn/pom.xml", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "AngersZhuuuu", "sha": "fa685bfe5a33ca4805fa44949911457db6de45a7", "commit_date": "2021/11/24 05:27:10", "commit_message": "Add UI stelp 1", "title": "[WIP][SPARK-32446][CORE] Add percentile distribution REST API & UI of peak memory metrics for all executors ", "body": "### What changes were proposed in this pull request?\r\nThis pr continue the work of https://github.com/apache/spark/pull/29247 since origin author didn't reply for a long time.\r\nWill add as co-author.\r\n\r\n\r\n### Why are the changes needed?\r\n\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\n\r\n### How was this patch tested?\r\n", "failed_tests": [], "files": [{"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/executorspage-template.html", "additions": "16", "deletions": "1", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/executorspage.js", "additions": "307", "deletions": "2", "changes": "309"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/utils.js", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusStore.scala", "additions": "12", "deletions": "0", "changes": "12"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/api/v1/OneApplicationResource.scala", "additions": "19", "deletions": "0", "changes": "19"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/executor_peak_memory_metrics_distributions_expectation.json", "additions": "22", "deletions": "0", "changes": "22"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/deploy/history/HistoryServerSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 2, 5]}, {"file": {"name": "docs/monitoring.md", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 0, 0]}]}
{"author": "AngersZhuuuu", "sha": "a8bdab59977244ef5541c301ef4ee175acb84e83", "commit_date": "2021/11/24 03:29:22", "commit_message": "remove hive 2,3 special CI", "title": "[SPARK-37437][BUILD] Remove unused hive profile and related CI test", "body": "### What changes were proposed in this pull request?\r\nSince we only support hive-2.3, we should remove the unused profile.\r\n\r\n\r\n### Why are the changes needed?\r\nremove unused profile\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\nNot need\r\n", "failed_tests": ["org.apache.spark.scheduler.BasicSchedulerIntegrationSuite"], "files": [{"file": {"name": "dev/create-release/release-build.sh", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "dev/run-tests-jenkins.py", "additions": "0", "deletions": "3", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "dev/run-tests.py", "additions": "1", "deletions": "22", "changes": "23"}, "updated": [0, 1, 1]}, {"file": {"name": "dev/test-dependencies.sh", "additions": "3", "deletions": "5", "changes": "8"}, "updated": [0, 1, 1]}, {"file": {"name": "pom.xml", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 3, 22]}]}
{"author": "AngersZhuuuu", "sha": "06ae18f9a14164485436226f794dba5c6328ba83", "commit_date": "2021/10/18 02:49:51", "commit_message": "[SPARK-37032][SQL] Remove unuseable link in spark-3.2.0's doc", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/deploy/history/ApplicationCache.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/history/ApplicationHistoryProvider.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala", "additions": "19", "deletions": "11", "changes": "30"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/history/HistoryServerSource.scala", "additions": "85", "deletions": "0", "changes": "85"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/metrics/MetricsConfig.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/deploy/history/HistoryServerSuite.scala", "additions": "52", "deletions": "0", "changes": "52"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/scala/org/apache/spark/metrics/MetricsConfigSuite.scala", "additions": "31", "deletions": "2", "changes": "33"}, "updated": [0, 0, 0]}]}
{"author": "AngersZhuuuu", "sha": "c2d4d47c7e5c89dea0ba2eb61e1e7e9c148c6d62", "commit_date": "2021/10/30 08:55:45", "commit_message": "[SPARK-37169][SQL] Fix un-correct  cast value when cast DateType to NumericType", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala", "additions": "118", "deletions": "18", "changes": "136"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/AnsiCastSuiteBase.scala", "additions": "2", "deletions": "20", "changes": "22"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CastSuite.scala", "additions": "16", "deletions": "16", "changes": "32"}, "updated": [0, 0, 0]}]}
{"author": "sathiyapk", "sha": "e7f62614f316756f9989099ec73fc54f5d73a05d", "commit_date": "2021/11/25 00:07:35", "commit_message": "SPARK-37324 Addresses review comments", "title": "[SPARK-37324][SQL] Adds support for decimal rounding mode up, down, half_down", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n\r\n1. Adds support for Decimal RoundingMode.UP, DOWN and HALF_DOWN by letting users to pass the rounding mode as argument to round function.\r\n2. bround function calls round function with the argument \"half_even\"\r\n\r\n\r\n### Why are the changes needed?\r\n\r\nCurrently there's no easier and straight forward way to round decimals with the mode UP, DOWN and HALF_DOWN. People need to use UDF or complex operations to do the same.\r\n\r\nOpening support for the other rounding modes might interest a lot of use cases.\r\n\r\n**SAP Hana Sql ROUND function does it :**\u00a0\r\n\r\n`ROUND(<number> [, <position> [, <rounding_mode>]])`\r\n\r\nREF : https://help.sap.com/viewer/7c78579ce9b14a669c1f3295b0d8ca16/Cloud/en-US/20e6a27575191014bd54a07fd86c585d.html\r\n\r\n\r\n**Sql Server does something similar to this :**\r\n\r\n`ROUND ( numeric_expression , length [ ,function ] )`\r\n\r\nREF :\u00a0https://docs.microsoft.com/en-us/sql/t-sql/functions/round-transact-sql?view=sql-server-ver15\u00a0\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nNow, users can specify the rounding mode while calling round function for round modes up, down, half_down. Calling round function without rounding mode will default to half_up.\r\n\r\n```\r\n> SELECT round(3.145, 2)\r\n3.15\r\n\r\n>SELECT round(3.145, 2, 'down')\r\n3.14\r\n```\r\n\r\n```\r\ndf.withColumn(\"value_rounded\", round('value, 0)\r\n\r\ndf.withColumn(\"value_rounded\", round('value, 0, \"down\")\r\n```\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->\r\nThis patch was tested locally using unit test and git workflow.", "failed_tests": ["pyspark.pandas.groupby", "pyspark.pandas.tests.test_dataframe", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/mathExpressions.scala", "additions": "57", "deletions": "19", "changes": "76"}, "updated": [1, 1, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/expressions.scala", "additions": "16", "deletions": "1", "changes": "17"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/types/Decimal.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/PhysicalAggregationSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/functions.scala", "additions": "14", "deletions": "4", "changes": "18"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/core/src/test/resources/sql-functions/sql-expression-schema.md", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/postgreSQL/numeric.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q2.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q2/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q78.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q78/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-query-results/v1_4/q2.sql.out", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/MathFunctionsSuite.scala", "additions": "75", "deletions": "0", "changes": "75"}, "updated": [0, 0, 1]}]}
{"author": "sathiyapk", "sha": "6500c827c27e761eabec9015fecd1517f933d1c0", "commit_date": "2021/11/22 00:28:04", "commit_message": "SPARK-37433 Adds Unit Tests to verify NoSuchElementException: None.get exception doesn't reproduce", "title": "[SPARK-37433][SQL] Uses TimeZone.getDefault when timeZoneId is None for ZoneAwareExpression", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nCalling `timeZoneId.get` on Option[String] leads to `NoSuchElementException: None.get`. This PR matches the value of Option[String] and uses `TimeZone.getDefault.toZoneId` when zoneId is None, this avoid unexpected exceptions to users. \r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nCalling `.get` on Option variable never been a good idea. We can either use a default value or choose to throw a meaningful exception. In this case, TimeZone.getDefault will be a good fit for a default value.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nNo\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->\r\nTested Locally and via Unit Test.", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/datetimeExpressions.scala", "additions": "5", "deletions": "2", "changes": "7"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/DateExpressionsSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "zhengruifeng", "sha": "7c1e6c600c3999bb9e1fe1637c18c26046f651c8", "commit_date": "2021/10/22 10:36:04", "commit_message": "init\n\nnit", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "64", "deletions": "1", "changes": "65"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [1, 1, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/rules/RuleIdCollection.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreePatterns.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [1, 5, 9]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/InsertRankLimitSuite.scala", "additions": "161", "deletions": "0", "changes": "161"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/window/RankLimitExec.scala", "additions": "93", "deletions": "0", "changes": "93"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameWindowFunctionsSuite.scala", "additions": "129", "deletions": "0", "changes": "129"}, "updated": [0, 1, 1]}]}
{"author": "zhengruifeng", "sha": "d7c66789651aaacb36d86aaa572047377c222a15", "commit_date": "2021/08/16 07:41:07", "commit_message": "init\n\ndrop last str arg\n\nformat sql in ut\n\nsupport both ShuffleQueryStageExec and BroadcastQueryStageExec as leaves\n\nnit\n\nreslove conflicts\n\nnarrow valid operator whitelist\n\nmove agg stringArgs to subclasses\n\nadd doc && resolve conflicts\n\ndel sample node\n\nresolve conflicts\n\nnit", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "19", "deletions": "0", "changes": "19"}, "updated": [0, 3, 17]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "379", "deletions": "144", "changes": "523"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/BaseAggregateExec.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/SortAggregateExec.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowExec.scala", "additions": "14", "deletions": "7", "changes": "21"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/WholeStageCodegenSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "227", "deletions": "0", "changes": "227"}, "updated": [0, 3, 18]}]}
{"author": "sadikovi", "sha": "f59ba6c4b78308470eae52f0b9da870333cd8782", "commit_date": "2021/11/24 21:43:02", "commit_message": "address comments, switch to failOnError", "title": "[SPARK-37326][SQL] Support TimestampNTZ in CSV data source", "body": "### What changes were proposed in this pull request?\r\n\r\nThis PR adds support for TimestampNTZ type in the CSV data source. \r\n\r\nMost of the functionality has already been added, this patch verifies that writes + reads work for TimestampNTZ type and adds schema inference depending on the timestamp value format written. The following applies:\r\n- If there is a mixture of `TIMESTAMP_NTZ` and `TIMESTAMP_LTZ` values, use `TIMESTAMP_LTZ`.\r\n- If there are only `TIMESTAMP_NTZ` values, resolve using the the default timestamp type configured with `spark.sql.timestampType`.\r\n\r\nIn addition, I introduced a new CSV option `timestampNTZFormat` which is similar to `timestampFormat` but it allows to configure read/write pattern for `TIMESTAMP_NTZ` types. It is basically a copy of timestamp pattern but without timezone.\r\n\r\n### Why are the changes needed?\r\n\r\nThe current CSV source could write values as TimestampNTZ into a file but could not preserve this type when reading the file back, this PR fixes the issue.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nPreviously, CSV data source would infer timestamp values as `TimestampType` when reading a CSV file. Now, the data source would infer the timestamp value type based on the format (with or without timezone) and default timestamp type based on `spark.sql.timestampType`.\r\n\r\nA new CSV option `timestampNTZFormat` is added to control the way values are formatted during writes or parsed during reads.\r\n\r\n### How was this patch tested?\r\n\r\nI extended `CSVSuite` with a few unit tests to verify that write-read roundtrip works for `TIMESTAMP_NTZ` and `TIMESTAMP_LTZ` values. \r\n", "failed_tests": ["org.apache.spark.sql.execution.datasources.csv.CSVLegacyTimeParserSuite"], "files": [{"file": {"name": "docs/sql-data-sources-csv.md", "additions": "9", "deletions": "3", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/csv/CSVInferSchema.scala", "additions": "24", "deletions": "0", "changes": "24"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/csv/CSVOptions.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/csv/UnivocityGenerator.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/csv/UnivocityParser.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala", "additions": "24", "deletions": "8", "changes": "32"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/TimestampFormatter.scala", "additions": "29", "deletions": "7", "changes": "36"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala", "additions": "7", "deletions": "1", "changes": "8"}, "updated": [0, 2, 11]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/DateTimeUtilsSuite.scala", "additions": "12", "deletions": "0", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/csv/CSVSuite.scala", "additions": "192", "deletions": "8", "changes": "200"}, "updated": [0, 0, 4]}]}
{"author": "sadikovi", "sha": "55f9e3f14847ee4bd35baf27157e45b40d7c9231", "commit_date": "2021/11/19 04:40:01", "commit_message": "update timestampNTZ/timestamp.sql.out file", "title": "[SPARK-37360][SQL] Support TimestampNTZ in JSON data source", "body": "### What changes were proposed in this pull request?\r\n\r\nThis PR adds support for TimestampNTZ type in the JSON data source. \r\n\r\nMost of the functionality has already been added, this patch verifies that writes + reads work for TimestampNTZ type and adds schema inference depending on the timestamp value format written. The following applies:\r\n- If there is a mixture of `TIMESTAMP_NTZ` and `TIMESTAMP_LTZ` values, use `TIMESTAMP_LTZ`.\r\n- If there are only `TIMESTAMP_NTZ` values, resolve using the the default timestamp type configured with `spark.sql.timestampType`.\r\n\r\nIn addition, I introduced a new JSON option `timestampNTZFormat` which is similar to `timestampFormat` but it allows to configure read/write pattern for `TIMESTAMP_NTZ` types. It is basically a copy of timestamp pattern but without timezone.\r\n\r\n### Why are the changes needed?\r\n\r\nThe PR fixes issues when writing and reading TimestampNTZ to and from JSON.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nPreviously, JSON data source would infer timestamp values as `TimestampType` when reading a JSON file. Now, the data source would infer the timestamp value type based on the format (with or without timezone) and default timestamp type based on `spark.sql.timestampType`.\r\n\r\nA new JSON option `timestampNTZFormat` is added to control the way values are formatted during writes or parsed during reads.\r\n\r\n### How was this patch tested?\r\n\r\nI extended `JsonSuite` with a few unit tests to verify that write-read roundtrip works for `TIMESTAMP_NTZ` and `TIMESTAMP_LTZ` values.\r\n", "failed_tests": [], "files": [{"file": {"name": "docs/sql-data-sources-json.md", "additions": "8", "deletions": "2", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JSONOptions.scala", "additions": "7", "deletions": "2", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JacksonGenerator.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JacksonParser.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JsonInferSchema.scala", "additions": "12", "deletions": "0", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/TimestampFormatter.scala", "additions": "13", "deletions": "2", "changes": "15"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala", "additions": "7", "deletions": "1", "changes": "8"}, "updated": [1, 2, 11]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/DateExpressionsSuite.scala", "additions": "23", "deletions": "1", "changes": "24"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/timestampNTZ/timestamp.sql.out", "additions": "12", "deletions": "8", "changes": "20"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/json/JsonSuite.scala", "additions": "162", "deletions": "0", "changes": "162"}, "updated": [0, 0, 4]}]}
{"author": "c21", "sha": "783f0e60a88120614e54820f6ad3792e3b50cd84", "commit_date": "2021/11/18 02:15:43", "commit_message": "Introduce Z-order expression", "title": "[SPARK-31585][SQL] Introduce Z-order expression", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nThis PR is to introduce a new expression in Spark - `ZOrder`. The motivation is Z-order enables to sort tuples in a way, to allow efficiently data skipping for columnar file format (Parquet and ORC).\r\n\r\nFor query with filter on combination of multiple columns, example:\r\n\r\n```sql\r\nSELECT *\r\nFROM table\r\nWHERE x = 0 OR y = 0\r\n```\r\n\r\nParquet/ORC cannot skip file/row-groups efficiently when reading, even though the table is sorted (locally or globally) on any columns. However when table is Z-order sorted on multiple columns, Parquet/ORC can skip file/row-groups efficiently when reading. We should add the feature in Spark to allow OSS Spark users benefitted in running these queries.\r\n\r\nWith this PR, user can do Z-order sort when writing the table with followed syntax:\r\n\r\n```sql\r\nINSERT INTO t\r\nSELECT ...\r\nFROM ...\r\nSORT BY ZORDER(x, y, ...)\r\n```\r\n\r\nor\r\n\r\n```sql\r\nINSERT INTO t\r\nSELECT ...\r\nFROM ...\r\nORDER BY ZORDER(x, y, ...)\r\n```\r\n\r\nThen when reading the table with filter on `x` and `y`, the performance can be improved by skipping more files and row-groups. More details below for micro benchmark.\r\n\r\nThis PR adds the support for Z-order on integer types (byte, short, int, and long). For other data types such as float and string will be added as followup. Code-gen support for expression will be also added later.\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nTo improve the query performance when filtering on multiple columns. Seeing 1x-6x run-time improvement in micro benchmark below.\r\n\r\n```scala\r\noverride def runBenchmarkSuite(mainArgs: Array[String]): Unit = {\r\n    def prepareTable(dir: File, numRows: Int): Unit = {\r\n      import spark.implicits._\r\n      val df = spark.range(numRows).map(_ => (Random.nextLong, Random.nextLong))\r\n        .toDF(\"x\", \"y\")\r\n      val zorderedDf = df.sort(Column(ZOrder(Seq($\"x\".expr, $\"y\".expr))))\r\n\r\n      saveAsTable(df, dir, \"\")\r\n      saveAsTable(zorderedDf, dir, \"ZOrder\")\r\n    }\r\n\r\n    def saveAsTable(df: DataFrame, dir: File, suffix: String): Unit = {\r\n      val blockSize = org.apache.parquet.hadoop.ParquetWriter.DEFAULT_PAGE_SIZE\r\n      val orcPath = dir.getCanonicalPath + \"/orc\" + suffix\r\n      val parquetPath = dir.getCanonicalPath + \"/parquet\" + suffix\r\n\r\n      df.write.mode(\"overwrite\")\r\n        .option(\"orc.dictionary.key.threshold\", 0.8)\r\n        .option(\"orc.compress.size\", blockSize)\r\n        .option(\"orc.stripe.size\", blockSize).orc(orcPath)\r\n      spark.read.orc(orcPath).createOrReplaceTempView(\"orcTable\" + suffix)\r\n\r\n      df.write.mode(\"overwrite\")\r\n        .option(\"parquet.block.size\", blockSize).parquet(parquetPath)\r\n      spark.read.parquet(parquetPath).createOrReplaceTempView(\"parquetTable\" + suffix)\r\n    }\r\n\r\n    def withTempTable(tableNames: String*)(f: => Unit): Unit = {\r\n      try f finally tableNames.foreach(spark.catalog.dropTempView)\r\n    }\r\n\r\n    runBenchmark(s\"ZOrder\") {\r\n      withTempPath { dir =>\r\n        withTempTable(\"orcTable\", \"parquetTable\", \"orcTableZOrder\", \"parquetTableZOrder\") {\r\n          prepareTable(dir, 1024 * 1024 * 15)\r\n          val benchmark = new Benchmark(\"zorder\", 1024 * 1024 * 15,\r\n            minNumIters = 5, output = output)\r\n\r\n          benchmark.addCase(\"Parquet no sort\") { _ =>\r\n            spark.sql(s\"SELECT * FROM parquetTable WHERE x = 0 OR y = 0\").noop()\r\n          }\r\n\r\n          benchmark.addCase(\"Parquet z-order sort on (x, y)\") { _ =>\r\n            spark.sql(s\"SELECT * FROM parquetTableZOrder WHERE x = 0 OR y = 0\").noop()\r\n          }\r\n\r\n          benchmark.addCase(\"ORC no sort\") { _ =>\r\n            spark.sql(s\"SELECT * FROM orcTable WHERE x = 0 OR y = 0\").noop()\r\n          }\r\n\r\n          benchmark.addCase(\"ORC z-order sort on (x, y)\") { _ =>\r\n            spark.sql(s\"SELECT * FROM orcTableZOrder WHERE x = 0 OR y = 0\").noop()\r\n          }\r\n\r\n          benchmark.run()\r\n        }\r\n      }\r\n    }\r\n  }\r\n```\r\n\r\n* Compare the performance between reading table having no sort, and table having local Z-order sort on `(x, y)`.\r\nSeeing 6x run-time improvement for Parquet, and 1x for ORC:\r\n\r\n```\r\nJava HotSpot(TM) 64-Bit Server VM 1.8.0_181-b13 on Mac OS X 10.16\r\nIntel(R) Core(TM) i9-9980HK CPU @ 2.40GHz\r\nzorder:                                   Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n------------------------------------------------------------------------------------------------------------------------\r\nParquet no sort                                     274            287          11         57.5          17.4       1.0X\r\nParquet z-order sort on (x, y)                       37             41           2        420.2           2.4       7.3X\r\nORC no sort                                         674            754          47         23.3          42.8       0.4X\r\nORC z-order sort on (x, y)                          262            282          11         59.9          16.7       1.0X\r\n```\r\n\r\n* Compare the performance between reading table having no sort, and table having local sort on `(x, y)`.\r\nNo performance improvement as expected.\r\n\r\n```\r\nJava HotSpot(TM) 64-Bit Server VM 1.8.0_181-b13 on Mac OS X 10.16\r\nIntel(R) Core(TM) i9-9980HK CPU @ 2.40GHz\r\nzorder:                                   Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n------------------------------------------------------------------------------------------------------------------------\r\nParquet no sort                                     285            319          29         55.1          18.1       1.0X\r\nParquet sort on (x, y)                              278            290          10         56.5          17.7       1.0X\r\nORC no sort                                         823            842          21         19.1          52.4       0.3X\r\nORC sort on (x, y)                                  748            760          16         21.0          47.6       0.4X\r\n```\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nYes. The added expression can be used by user - `zorder`.\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->\r\nAdded unit test in `ZOrderExpressionSuite.scala`.", "failed_tests": ["org.apache.spark.sql.ExpressionsSchemaSuite", "org.apache.spark.sql.expressions.ExpressionInfoSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ZOrder.scala", "additions": "222", "deletions": "0", "changes": "222"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ZOrderExpressionSuite.scala", "additions": "105", "deletions": "0", "changes": "105"}, "updated": [0, 0, 0]}]}
{"author": "HyukjinKwon", "sha": "8fa16bbaefd4b3106a653cc8a615755c0bf02204", "commit_date": "2021/11/24 23:42:21", "commit_message": "Update build_and_test.yml", "title": "[SPARK-37453][INFRA][SQL][TESTS] Split TPC-DS build in GitHub Actions", "body": "### What changes were proposed in this pull request?\r\n\r\nThis is kind of a followup for https://github.com/apache/spark/pull/33510 and https://github.com/apache/spark/pull/34641. This PR proposes to split TPC-DS build in GitHub Actions. \r\n\r\n### Why are the changes needed?\r\n\r\nRunning these queries easily causes out-of-memory in GitHub Actions machines, and make the build flaky. We should deflake it.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nNo, dev-only.\r\n\r\n### How was this patch tested?\r\n\r\nGitHub Actions in this PR should test it out.\r\n", "failed_tests": ["org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": ".github/workflows/build_and_test.yml", "additions": "19", "deletions": "2", "changes": "21"}, "updated": [1, 1, 9]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/TPCDSQueryTestSuite.scala", "additions": "32", "deletions": "20", "changes": "52"}, "updated": [0, 2, 2]}]}
{"author": "HyukjinKwon", "sha": "0b6765150798799a418d39209b4e5f6d4a16276e", "commit_date": "2021/11/22 06:25:38", "commit_message": "Uses Python's standard string formatter for SQL API in pandas API on Spark", "title": "[SPARK-37436][PYTHON] Uses Python's standard string formatter for SQL API in pandas API on Spark", "body": "### What changes were proposed in this pull request?\r\n\r\nThis PR proposes to use [Python's standard string formatter](https://docs.python.org/3/library/string.html#custom-string-formatting) instead of hacky custom SQL parser for SQL API in pandas API on Spark\r\n\r\n### Why are the changes needed?\r\n\r\nCurrent implementation of parsing is very hacky, and does not work. It is [dependent on Python's internal module](https://github.com/apache/spark/blob/master/python/pyspark/pandas/sql_processor.py#L291), and [Series is being treated as a table](https://github.com/apache/spark/blob/master/python/pyspark/pandas/sql_processor.py#L339-L340), etc.\r\n\r\nWe should have the Python standard string formatter with the standard interface and the standard support.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nYes.\r\n\r\n**Disallowed:**\r\n\r\n1. `Series` as a table\r\n\r\n    ```python\r\n    myser = ps.Series({'a': [1.0, 2.0, 3.0], 'b': [15.0, 30.0, 45.0]})\r\n    ps.sql(\"SELECT * from {myser}\", myser=myser)\r\n    ```\r\n\r\n2. `list` and `range`\r\n\r\n    ```python\r\n    strs = ['a', 'b']\r\n    ps.sql(\"SELECT 'a' IN {strs}\", strs=strs)\r\n    ```\r\n\r\n3. Automatic local/global variable detection:\r\n\r\n    ```python\r\n    strs = ['a', 'b']\r\n    ps.sql(\"SELECT 'a' IN {strs}\")\r\n    ```\r\n\r\n**Allowed:**\r\n\r\n1. `Series` as a column\r\n\r\n    ```python\r\n    mydf = ps.range(10)\r\n    ps.sql(\"SELECT {ser} FROM {mydf}\", ser=mydf.id, mydf=mydf)\r\n    ```\r\n\r\n2. Reference checking (between `Series` and `DataFrame`)\r\n\r\n    ```python\r\n    mydf = ps.range(10)\r\n    ps.sql(\"SELECT {ser} FROM tblA\", ser=mydf.id)\r\n    ```\r\n\r\n    ```\r\n    ValueError: The series in {ser} does not refer any dataframe specified.\r\n    ```\r\n\r\n3. Attribute supports from frame (standard Python support):\r\n\r\n    ```python\r\n    mydf = ps.range(10)\r\n    ps.sql(\"SELECT {tbl.id} FROM {tbl}\", tbl=mydf)\r\n    ```\r\n\r\n### How was this patch tested?\r\n\r\nDoctests were added.", "failed_tests": ["pyspark.pandas.tests.test_sql"], "files": [{"file": {"name": "dev/sparktestsupport/modules.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 2]}, {"file": {"name": "python/docs/source/migration_guide/pyspark_3.2_to_3.3.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/__init__.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/sql_formatter.py", "additions": "273", "deletions": "0", "changes": "273"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/sql_processor.py", "additions": "1", "deletions": "187", "changes": "188"}, "updated": [0, 2, 4]}, {"file": {"name": "python/pyspark/pandas/tests/test_sql.py", "additions": "42", "deletions": "7", "changes": "49"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/usage_logging/__init__.py", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}]}
{"author": "HyukjinKwon", "sha": "5fb45a2c8fbdfcac15a261f255823a163ca75a2f", "commit_date": "2021/11/24 05:30:24", "commit_message": "Update python/pyspark/serializers.py", "title": "[SPARK-32079][PYTHON] Remove namedtuple hack by replacing built-in pickle to cloudpickle", "body": "### What changes were proposed in this pull request?\r\n\r\nThis PR proposes to replace Python's built-in CPickle to CPickle-based cloudpickle (requires Python 3.8+).\r\nFor Python 3.7 and below, it still uses the legacy built-in CPickle for the performance matter.\r\n\r\nI did a bit of benchmark with basic cases, and I have seen no performance penalty (attached one of the benchmarks below).\r\n\r\n### Why are the changes needed?\r\n\r\nTo remove named tuple hack for the issues such as: SPARK-32079,  SPARK-22674 and SPARK-27810.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nNo.\r\n\r\n### How was this patch tested?\r\n\r\n#### Micro benchmark:\r\n\r\n```python\r\nimport time\r\nimport pickle\r\nfrom pyspark import cloudpickle\r\n\r\ndef measure(f):\r\n    start = time.time()\r\n    f()\r\n    end = time.time()\r\n    print(end - start)\r\n\r\ndata = [123, \"abc\", (1, 2, 3), 2.2] * 100000000\r\nmeasure(lambda: pickle.dumps(data))\r\nmeasure(lambda: cloudpickle.dumps(data))\r\nmeasure(lambda: pickle.loads(pickle.dumps(data)))\r\nmeasure(lambda: cloudpickle.loads(cloudpickle.dumps(data)))\r\n```\r\n\r\n```\r\n5.1765618324279785\r\n5.2591071128845215\r\n12.457043886184692\r\n12.1910879611969\r\n```\r\n\r\n```python\r\nimport time\r\nimport random\r\nimport pickle\r\nfrom pyspark import cloudpickle\r\n\r\ndef measure(f):\r\n    start = time.time()\r\n    f()\r\n    end = time.time()\r\n    print(end - start)\r\n\r\nrand_data = []\r\n\r\nfor _ in range(10000000):\r\n    data = [\r\n        random.randint(1, 100),\r\n        str(random.randint(1, 100)),\r\n        (random.randint(1, 100), random.randint(2, 200), random.randint(3, 300)),\r\n        random.random()\r\n    ]\r\n    random.shuffle(data)\r\n    rand_data.append(data)\r\n\r\nmeasure(lambda: pickle.dumps(rand_data))\r\nmeasure(lambda: cloudpickle.dumps(rand_data))\r\nmeasure(lambda: pickle.loads(pickle.dumps(rand_data)))\r\nmeasure(lambda: cloudpickle.loads(cloudpickle.dumps(rand_data)))\r\n```\r\n\r\n```\r\n7.736639976501465\r\n7.8458099365234375\r\n20.306012868881226\r\n17.787282943725586\r\n```\r\n\r\n\r\n#### E2E benchmark:\r\n\r\n```bash\r\n./bin/pyspark --conf spark.python.profile=true\r\n```\r\n\r\n```python\r\nimport time\r\nfrom collections import namedtuple\r\nrdd = sc.parallelize([123] * 30000000)\r\nrdd.count()  # init\r\nstart = time.time()\r\nrdd.map(lambda x: x).count()\r\nprint(time.time() - start)\r\nsc.show_profiles()\r\n```\r\n\r\nBefore:\r\n\r\n2.3216118812561035 (sec)\r\n\r\n```\r\n============================================================\r\nProfile of RDD<id=2>\r\n============================================================\r\n         60264297 function calls (60264265 primitive calls) in 22.309 seconds\r\n\r\n   Ordered by: internal time, cumulative time\r\n\r\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\r\n 30000016   13.127    0.000   29.890    0.000 rdd.py:1291(<genexpr>)\r\n       32    4.559    0.142   34.449    1.077 {built-in method builtins.sum}\r\n 30000000    3.723    0.000    3.723    0.000 <stdin>:1(<lambda>)\r\n    29297    0.699    0.000    0.699    0.000 {built-in method _pickle.loads}\r\n    29313    0.059    0.000    0.874    0.000 serializers.py:151(_read_with_length)\r\n    58610    0.045    0.000    0.045    0.000 {method 'read' of '_io.BufferedReader' objects}\r\n    29313    0.035    0.000    0.057    0.000 serializers.py:567(read_int)\r\n    29313    0.025    0.000    0.899    0.000 serializers.py:135(load_stream)\r\n    29297    0.016    0.000    0.715    0.000 serializers.py:435(loads)\r\n    29313    0.013    0.000    0.013    0.000 {built-in method _struct.unpack}\r\n    29329    0.006    0.000    0.006    0.000 {built-in method builtins.len}\r\n       16    0.000    0.000    0.000    0.000 rdd.py:409(func)\r\n       16    0.000    0.000    0.001    0.000 serializers.py:256(dump_stream)\r\n...\r\n```\r\n\r\nAfter:\r\n\r\n2.279919147491455 (sec)\r\n\r\n```\r\n============================================================\r\nProfile of RDD<id=2>\r\n============================================================\r\n         90264361 function calls (90264329 primitive calls) in 34.573 seconds\r\n\r\n   Ordered by: internal time, cumulative time\r\n\r\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\r\n 30000016   13.204    0.000   29.982    0.000 rdd.py:1291(<genexpr>)\r\n 30000000   12.087    0.000   15.879    0.000 util.py:77(wrapper)\r\n       32    4.588    0.143   34.571    1.080 {built-in method builtins.sum}\r\n 30000000    3.792    0.000    3.792    0.000 <stdin>:1(<lambda>)\r\n    29297    0.694    0.000    0.694    0.000 {built-in method _pickle.loads}\r\n    29313    0.061    0.000    0.873    0.000 serializers.py:157(_read_with_length)\r\n    58610    0.045    0.000    0.045    0.000 {method 'read' of '_io.BufferedReader' objects}\r\n    29313    0.036    0.000    0.059    0.000 serializers.py:585(read_int)\r\n    29313    0.026    0.000    0.900    0.000 serializers.py:141(load_stream)\r\n    29297    0.018    0.000    0.712    0.000 serializers.py:463(loads)\r\n    29313    0.013    0.000    0.013    0.000 {built-in method _struct.unpack}\r\n    29329    0.007    0.000    0.007    0.000 {built-in method builtins.len}\r\n       16    0.000    0.000   34.573    2.161 worker.py:665(process)\r\n       16    0.000    0.000    0.000    0.000 rdd.py:409(func)\r\n       16    0.000    0.000    0.001    0.000 serializers.py:262(dump_stream)\r\n       16    0.000    0.000    0.001    0.000 cloudpickle_fast.py:59(dumps)\r\n...\r\n```\r\n\r\nExisting test cases should cover all test cases.", "failed_tests": ["pyspark.tests.test_serializers"], "files": [{"file": {"name": "python/pyspark/__init__.py", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/__init__.pyi", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/accumulators.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/context.py", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/ml/common.py", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 2, 3]}, {"file": {"name": "python/pyspark/ml/tests/test_linalg.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/mllib/common.py", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 2, 3]}, {"file": {"name": "python/pyspark/mllib/tests/test_algorithms.py", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/mllib/tests/test_linalg.py", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/rdd.py", "additions": "8", "deletions": "8", "changes": "16"}, "updated": [0, 2, 5]}, {"file": {"name": "python/pyspark/serializers.py", "additions": "94", "deletions": "76", "changes": "170"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/shuffle.py", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/dataframe.py", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [1, 2, 9]}, {"file": {"name": "python/pyspark/sql/streaming.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 2, 5]}, {"file": {"name": "python/pyspark/tests/test_rdd.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/tests/test_serializers.py", "additions": "7", "deletions": "9", "changes": "16"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/tests/test_shuffle.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/worker.py", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 2, 2]}]}
{"author": "viirya", "sha": "f31b061da81a90fbc228e531be3f5d23f259377d", "commit_date": "2021/11/24 01:18:55", "commit_message": "Add micro benchmark.", "title": "[SPARK-37369][SQL] Avoid redundant ColumnarToRow transistion on InMemoryTableScan", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\nThis patch proposes to let `InMemoryTableScanExec` produces row output directly, if its parent query plan only accepts rows instead of columnar output.\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\nWe have a rule to insert columnar transition between row-based and columnar query plans. `InMemoryTableScanExec` can produce columnar output. So if its parent plan isn't columnar, the rule adds a `ColumnarToRow` between them, e.g.,\r\n\r\n```\r\n+- Union\r\n:- ColumnarToRow\r\n: +- InMemoryTableScan i#8, j#9\r\n: +- InMemoryRelation i#8, j#9, StorageLevel(disk, memory, deserialized, 1 replicas)\r\n```\r\n\r\nBut `InMemoryTableScanExec` is different because it can convert from cached batch to columnar batch or row.\r\n\r\nFor such case, we ask `InMemoryTableScanExec` to convert cached batch to columnar batch, and then convert to row in the added `ColumnarToRow`, before the parent query.\r\n\r\nSo for such case, we can simply ask `InMemoryTableScanExec` to produce row output instead of a redundant conversion.\r\n\r\n\r\n```\r\n================================================================================================\r\nInt In-memory\r\n================================================================================================\r\n\r\nOpenJDK 64-Bit Server VM 1.8.0_265-b01 on Mac OS X 10.16\r\nIntel(R) Core(TM) i7-9750H CPU @ 2.60GHz\r\nInt In-Memory scan:                         Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative\r\n--------------------------------------------------------------------------------------------------------------------------\r\ncolumnar deserialization + columnar-to-row            228            245          15          4.4         227.7       1.0X\r\nrow-based deserialization                             179            187          10          5.6         179.4       1.3X\r\n```\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\nNo\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->\r\n\r\nExisting tests.\r\n", "failed_tests": ["org.apache.spark.sql.CachedTableSuite", "org.apache.spark.sql.execution.WholeStageCodegenSuite", "org.apache.spark.sql.execution.columnar.CachedBatchSerializerSuite", "org.apache.spark.sql.execution.metric.SQLMetricsSuite", "org.apache.spark.sql.execution.columnar.InMemoryColumnarQuerySuite", "org.apache.spark.sql.execution.debug.DebuggingSuite"], "files": [{"file": {"name": "sql/core/benchmarks/InMemoryColumnarBenchmark-results.txt", "additions": "12", "deletions": "0", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/Columnar.scala", "additions": "11", "deletions": "3", "changes": "14"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/CachedTableSuite.scala", "additions": "22", "deletions": "4", "changes": "26"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSetOperationsSuite.scala", "additions": "3", "deletions": "4", "changes": "7"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/WholeStageCodegenSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/columnar/CachedBatchSerializerSuite.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/columnar/InMemoryColumnarBenchmark.scala", "additions": "66", "deletions": "0", "changes": "66"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/columnar/InMemoryColumnarQuerySuite.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/debug/DebuggingSuite.scala", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/metric/SQLMetricsSuite.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/CachedTableSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "huaxingao", "sha": "97481b3d4449a042f8d41e17d7fb4ca114a198cd", "commit_date": "2021/09/19 22:36:37", "commit_message": "Migrate CreateTableStatement to v2 command framework", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.streaming.StreamingAggregationSuite", "org.apache.spark.sql.jdbc.JDBCSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveCatalogs.scala", "additions": "0", "deletions": "11", "changes": "11"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala", "additions": "8", "deletions": "3", "changes": "11"}, "updated": [0, 2, 8]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/statements.scala", "additions": "0", "deletions": "19", "changes": "19"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2Commands.scala", "additions": "28", "deletions": "6", "changes": "34"}, "updated": [0, 0, 5]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreeNode.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/CatalogV2Util.scala", "additions": "2", "deletions": "7", "changes": "9"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala", "additions": "12", "deletions": "12", "changes": "24"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ReplaceCharWithVarchar.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala", "additions": "16", "deletions": "16", "changes": "32"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/rules.scala", "additions": "5", "deletions": "4", "changes": "9"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/CreateTableExec.scala", "additions": "9", "deletions": "2", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala", "additions": "12", "deletions": "7", "changes": "19"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/PlanResolutionSuite.scala", "additions": "38", "deletions": "26", "changes": "64"}, "updated": [0, 0, 2]}]}
{"author": "MaxGekk", "sha": "c80127cb91d5617aac71dcd7b1930c7b651aaad3", "commit_date": "2021/11/24 20:34:19", "commit_message": "Use the default pattern yyyy-MM-dd", "title": "[SPARK-36861][SQL] Use `yyyy-MM-dd` as the date pattern in partition discovery", "body": "### What changes were proposed in this pull request?\r\nIn the PR, I propose to explicitly set the date pattern to `yyyy-MM-dd` while inferring types of partition values.\r\n\r\n### Why are the changes needed?\r\nThe existing date partition parser is much more tolerant to its input, and can skip some parts of date strings. For example, see SPARK-36861. As a consequence, it can loose some user's info (pieces of partition values).\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo. New behaviour introduced by https://github.com/apache/spark/pull/33709 hasn't released yet.\r\n\r\n### How was this patch tested?\r\nBy running the modified test suite:\r\n```\r\n$ build/sbt \"test:testOnly *ParquetV2PartitionDiscoverySuite\"\r\n```", "failed_tests": ["org.apache.spark.ml.source.image.ImageFileFormatSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningUtils.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetPartitionDiscoverySuite.scala", "additions": "6", "deletions": "5", "changes": "11"}, "updated": [0, 0, 0]}]}
{"author": "Yaohua628", "sha": "e872d1fae7acde9445dfac389e430d92406f9e09", "commit_date": "2021/11/22 00:28:50", "commit_message": "addressed comments", "title": "[SPARK-37273][SQL] Support hidden file metadata columns in Spark SQL", "body": "### What changes were proposed in this pull request?\r\nThis PR proposes a new interface in Spark SQL that allows users to query the metadata of the input files for all file formats. Spark SQL will expose them as **built-in hidden columns** meaning **users can only see them when they explicitly reference them**. Currently, This PR proposes to support the following metadata columns inside of a metadata struct `_metadata`:\r\n\r\n| Name  | Type | Description | Example |\r\n| ------------- | ------------- | ------------- | ------------- |\r\n| _metadata.file_path  | String  | The absolute file path of the input file. | file:/tmp/spark-7f600b30-b3ec-43a8-8cd2-686491654f9b/f0.csv |\r\n| _metadata.file_name  | String  | The name of the input file along with the extension. | f0.csv |\r\n| _metadata.file_size  | Long  | The length of the input file, in bytes. | 628 |\r\n| _metadata.file_modification_time  | Long  | The modification time of the file, in milliseconds. | 1632701945157 |\r\n\r\nThis proposed hidden file metadata interface has the following behaviors:\r\n- **Hidden**: metadata columns are hidden. They will not show up when only selecting data columns or selecting all `(SELECT *)`. In other words, they are not returned unless being explicitly referenced.\r\n- **Not overwrite the data schema**: in the case of name collisions with data columns, data columns will be returned instead of the metadata columns. In other words, metadata columns can not overwrite user data in any case.\r\n\r\n### Why are the changes needed?\r\nTo improve the Spark SQL observability for **all file formats** that still leverage DSV1.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes. \r\n```\r\nspark.read.format(\"csv\")\r\n     .schema(schema)\r\n     .load(\"file:/tmp/*\")\r\n     .select(\"name\", \"age\",\r\n             \"_metadata.file_path\", \"_metadata.file_name\",\r\n             \"_metadata.file_size\", \"_metadata.file_modification_time\")\r\n```\r\nExample return:\r\n| name  | age | file_path | file_name | file_size | file_modification_time |\r\n| ------------- | ------------- | ------------- | ------------- | ------------- | ------------- |\r\n| Debbie | 18 | file:/tmp/f0.csv | f0.csv | 12 | 710112965421 |\r\n| Frank | 24 | file:/tmp/f1.csv | f1.csv | 11 | 787959365553 |\r\n\r\n### How was this patch tested?\r\nAdd new testsuite: FileMetadataColumnsSuite\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 5]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/namedExpressions.scala", "additions": "20", "deletions": "1", "changes": "21"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/LogicalPlan.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Relation.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala", "additions": "36", "deletions": "5", "changes": "41"}, "updated": [0, 2, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/PartitionedFileUtil.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormat.scala", "additions": "23", "deletions": "1", "changes": "24"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileScanRDD.scala", "additions": "151", "deletions": "7", "changes": "158"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/LogicalRelation.scala", "additions": "23", "deletions": "2", "changes": "25"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/FileMetadataColumnsSuite.scala", "additions": "480", "deletions": "0", "changes": "480"}, "updated": [0, 0, 0]}]}
{"author": "cloud-fan", "sha": "504a3d86506f73ad10cc074aab0a9eb2917b0f69", "commit_date": "2021/11/24 15:39:35", "commit_message": "support expressions in time travel timestamp", "title": "[SPARK-37454][SQL] Support expressions in time travel timestamp", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nAllow expressions to be the timestamp of time travel spec, with 3 limitations:\r\n1. the expression must be foldable (can't refer to columns)\r\n2. can not contain subqueries\r\n3. can be casted to timestamp\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nIt's convenient to support expressions as timestamp in time travel, e.g. we can query the table in yesterday by `FROM t TIMESTAMP AS OF current_date() - INTERVAL 1 DAY`, or `FROM t TIMESTAMP AS OF TIMESTAMP'yesterday'`.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nyes, a new feature\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->\r\nnew tests", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [1, 3, 10]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/TableCatalog.java", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala", "additions": "14", "deletions": "4", "changes": "18"}, "updated": [0, 2, 5]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CTESubstitution.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/RelationTimeTravel.scala", "additions": "34", "deletions": "0", "changes": "34"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveHints.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/TimeTravelSpec.scala", "additions": "57", "deletions": "0", "changes": "57"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/unresolved.scala", "additions": "3", "deletions": "9", "changes": "12"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala", "additions": "23", "deletions": "11", "changes": "34"}, "updated": [0, 1, 11]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/CatalogV2Util.scala", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/expressions/expressions.scala", "additions": "0", "deletions": "26", "changes": "26"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala", "additions": "14", "deletions": "1", "changes": "15"}, "updated": [1, 3, 7]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala", "additions": "1", "deletions": "117", "changes": "118"}, "updated": [0, 3, 10]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala", "additions": "47", "deletions": "1", "changes": "48"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Utils.scala", "additions": "6", "deletions": "5", "changes": "11"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2SessionCatalog.scala", "additions": "22", "deletions": "0", "changes": "22"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala", "additions": "20", "deletions": "27", "changes": "47"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/SupportsCatalogOptionsSuite.scala", "additions": "6", "deletions": "5", "changes": "11"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/SQLViewSuite.scala", "additions": "0", "deletions": "7", "changes": "7"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/SQLViewTestSuite.scala", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 2, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/test/TestHive.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}]}
{"author": "sarutak", "sha": "89f8857b31bc6804c57070f7ec9f66013e69c02a", "commit_date": "2021/11/22 13:58:25", "commit_message": "Don't try to store a V1 table which contains TIMESTAMP_NTZ in Hive compatible format.", "title": "[SPARK-37283][SQL][FOLLOWUP] Avoid trying to store a table which contains timestamp_ntz types in Hive compatible format", "body": "### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nThis is PR is to avoid trying to store a table which contains `timestamp_ntz` types in Hive compatible format.\r\nIn the current master, Spark tries to store such a table in Hive compatible format first, but it doesn't support `timestamp_ntz`.\r\nAs a result warning is logged like as follows though it's finally stored in Spark specific format.\r\n```\r\nCREATE TABLE tbl1(a TIMESTAMP_NTZ) USING Parquet\r\n...\r\n21/11/22 21:57:18 WARN HiveExternalCatalog: Could not persist `default`.`tbl1` in a Hive compatible way. Persisting it into Hive metastore in Spark SQL specific format.\r\norg.apache.hadoop.hive.ql.metadata.HiveException: java.lang.IllegalArgumentException: Error: type expected at the position 0 of 'timestamp_ntz' but 'timestamp_ntz' is found.\r\n        at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:869)\r\n        at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:874)\r\n        at org.apache.spark.sql.hive.client.Shim_v0_12.createTable(HiveShim.scala:614)\r\n        at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$createTable$1(HiveClientImpl.scala:554)\r\n        at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n        at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:304)\r\n        at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:235)\r\n        at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:234)\r\n        at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:284)\r\n        at org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:552)\r\n        at org.apache.spark.sql.hive.HiveExternalCatalog.saveTableIntoHive(HiveExternalCatalog.scala:506)\r\n        at org.apache.spark.sql.hive.HiveExternalCatalog.createDataSourceTable(HiveExternalCatalog.scala:404)\r\n        at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$createTable$1(HiveExternalCatalog.scala:273)\r\n...\r\n```\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nTo fix the confusing behavior.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nNo.\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->\r\nModified the test added in #34551", "failed_tests": [], "files": [{"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveExternalCatalog.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/MetastoreDataSourcesSuite.scala", "additions": "5", "deletions": "3", "changes": "8"}, "updated": [0, 1, 1]}]}
{"author": "ueshin", "sha": "9b2b5d389180656d49fc02d2ccdfa2b5e1fe89b2", "commit_date": "2021/11/24 03:18:30", "commit_message": "Rerun tests.", "title": "[SPARK-37443][PYTHON] Provide a profiler for Python/Pandas UDFs", "body": "### What changes were proposed in this pull request?\r\n\r\nProvides a profiler for Python/Pandas UDFs.\r\n\r\nAs each profiler is connected to the `PythonUDF.resultId`, the query plan will show the `resultId`.\r\n\r\nFor example:\r\n\r\n- run PySpark with a config `spark.python.profile=true`:\r\n\r\n```\r\n% ./bin/pyspark --conf spark.python.profile=true\r\n```\r\n\r\n- run some Python UDFs:\r\n\r\n```py\r\n>>> from pyspark.sql.functions import udf\r\n>>> df = spark.range(10)\r\n\r\n>>> @udf(\"long\")\r\n... def add1(x):\r\n...   return x + 1\r\n...\r\n>>> @udf(\"long\")\r\n... def add2(x):\r\n...   return x + 2\r\n...\r\n>>> added = df.select(add1(\"id\"), add2(\"id\"), add1(\"id\"))\r\n\r\n>>> added.show()\r\n+--------+--------+--------+\r\n|add1(id)|add2(id)|add1(id)|\r\n+--------+--------+--------+\r\n...\r\n+--------+--------+--------+\r\n\r\n>>> added.explain()\r\n== Physical Plan ==\r\n*(2) Project [pythonUDF0#27L AS add1(id)#5L, pythonUDF1#28L AS add2(id)#6L, pythonUDF2#29L AS add1(id)#7L]\r\n+- BatchEvalPython [add1(id#0L)#2L, add2(id#0L)#3L, add1(id#0L)#4L], [pythonUDF0#27L, pythonUDF1#28L, pythonUDF2#29L]\r\n   +- *(1) Range (0, 10, step=1, splits=16)\r\n\r\n>>> sc.show_profiles()\r\n============================================================\r\nProfile of UDF<id=2>\r\n============================================================\r\n         20 function calls in 0.000 seconds\r\n\r\n   Ordered by: internal time, cumulative time\r\n\r\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\r\n       10    0.000    0.000    0.000    0.000 <stdin>:1(add1)\r\n       10    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\r\n\r\n\r\n============================================================\r\nProfile of UDF<id=3>\r\n============================================================\r\n         20 function calls in 0.000 seconds\r\n\r\n   Ordered by: internal time, cumulative time\r\n\r\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\r\n       10    0.000    0.000    0.000    0.000 <stdin>:1(add2)\r\n       10    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\r\n\r\n\r\n============================================================\r\nProfile of UDF<id=4>\r\n============================================================\r\n         20 function calls in 0.000 seconds\r\n\r\n   Ordered by: internal time, cumulative time\r\n\r\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\r\n       10    0.000    0.000    0.000    0.000 <stdin>:1(add1)\r\n       10    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\r\n\r\n\r\n```\r\n\r\nThe UDF IDs can be seen at `add1(...)#2L`, `add2(...)#3L`, `add1(...)#4L` of `BatchEvalPython`.\r\n\r\n**Currently registered UDFs are not supported.**\r\n\r\n### Why are the changes needed?\r\n\r\nCurrently a profiler is provided for only `RDD` operations, but providing a profiler for Python/Pandas UDFs would be great.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nYes, the profiler for Python/Pandas UDFs will work and show the results when a Spark conf `spark.python.profile` is set to `true`.\r\n\r\n### How was this patch tested?\r\n\r\nAdded some tests and manually checked the behavior.", "failed_tests": ["org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/sparktestsupport/modules.py", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 2, 2]}, {"file": {"name": "python/pyspark/context.py", "additions": "8", "deletions": "2", "changes": "10"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/context.pyi", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/profiler.py", "additions": "38", "deletions": "7", "changes": "45"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/profiler.pyi", "additions": "14", "deletions": "3", "changes": "17"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_udf_profiler.py", "additions": "109", "deletions": "0", "changes": "109"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/udf.py", "additions": "27", "deletions": "5", "changes": "32"}, "updated": [0, 2, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/PythonUDF.scala", "additions": "19", "deletions": "1", "changes": "20"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/namedExpressions.scala", "additions": "0", "deletions": "10", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/package.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/IntegratedUDFTestUtils.scala", "additions": "31", "deletions": "4", "changes": "35"}, "updated": [0, 0, 0]}]}
{"author": "zero323", "sha": "1de7df88abae24ace27dc0f36b3c743b1ba51c1d", "commit_date": "2021/11/20 20:31:08", "commit_message": "Reorder imports", "title": "[SPARK-37419][PYTHON][ML] Rewrite _shared_params_code_gen.py  to inline type hints for ml/param/shared.py", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\nThis PR modifies `_shared_params_code_gen.py ` to include type hints in generated code.\r\n\r\nAdditionally, it applies minor cleanup:\r\n\r\n- Switch from manual replace to string interpolation.\r\n- Added magic commas for consistent formatting of the `shared` list.\r\n- Dropped unused arguments from `_gen_param_code`\r\n- Added to type hints to `_gen_param_header` and `_gen_param_code` and dropped `_shared_params_code_gen.pyi` stub.\r\n\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\nCore change is part of the bigger effort to migrate hints from stub files to corresponding Python modules\r\n\r\n> Currently, we use stub files for type annotations, which don't support type checks within function bodies. So we inline type hints to support that.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\nNo\r\n\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->\r\n\r\n- Existing tests.\r\n- Manual execution of `_shared_params_code_gen.py ` and  checking for regressions by comparing output with  the current version of `shared.py`\r\n", "failed_tests": ["pyspark.ml.tests.test_algorithms"], "files": [{"file": {"name": "python/pyspark/ml/param/__init__.pyi", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/param/_shared_params_code_gen.py", "additions": "115", "deletions": "49", "changes": "164"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/ml/param/_shared_params_code_gen.pyi", "additions": "0", "deletions": "19", "changes": "19"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/param/shared.py", "additions": "121", "deletions": "105", "changes": "226"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/ml/param/shared.pyi", "additions": "0", "deletions": "192", "changes": "192"}, "updated": [0, 0, 0]}]}
{"author": "zero323", "sha": "7fe190876b80098e20279ee35fa1009aec1a5973", "commit_date": "2021/10/21 09:04:02", "commit_message": "Add list/tuple overloads to array, struct, create_map, map_concat", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/sql/functions.py", "additions": "56", "deletions": "8", "changes": "64"}, "updated": [3, 7, 13]}]}
{"author": "ulysses-you", "sha": "dfd7435d38f36324c73aac82d76e8fca32ae7621", "commit_date": "2021/11/15 12:33:18", "commit_message": "jira", "title": "[SPARK-37287][SQL] Pull out dynamic partition and bucket sort from FileFormatWriter", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\n- Add a new trait `V1Write` to hold some sort infos of v1 write. e.g., partition columns, bucket spec.\r\n- Then let the following writing command extend the `V1Write`, includes both datasource and hive\r\n  - InsertIntoHadoopFsRelationCommand\r\n  - CreateDataSourceTableAsSelectCommand\r\n  - InsertIntoHiveTable\r\n  - CreateHiveTableAsSelectBase\r\n- Add a new rule `V1Writes` to decide if we should add a `Sort` operator based its `V1Write.requiredOrdering`.  This rule should be similar with `V2Writes`.\r\n- So now we can remove the `SortExec` in `FileFormatWriter.write`.\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n`FileFormatWriter.write` now is used by all V1 write which includes datasource and hive table. However it contains a sort which is based on dynamic partition and bucket columns that can not be seen in plan directly.\r\n\r\nV2 write has a better approach that it satisfies the order or even distribution by using rule `V2Writes`.\r\n\r\nV1 write should do the similar thing with V2 write.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nno.\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->\r\nthis is a code refactor, so it should pass CI", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SortExec.scala", "additions": "43", "deletions": "30", "changes": "73"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala", "additions": "13", "deletions": "1", "changes": "14"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormatWriter.scala", "additions": "22", "deletions": "70", "changes": "92"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala", "additions": "7", "deletions": "4", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/V1Writes.scala", "additions": "148", "deletions": "0", "changes": "148"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSink.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/CreateHiveTableAsSelectCommand.scala", "additions": "19", "deletions": "3", "changes": "22"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/InsertIntoHiveTable.scala", "additions": "14", "deletions": "57", "changes": "71"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/SaveAsHiveFile.scala", "additions": "4", "deletions": "7", "changes": "11"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/V1HiveWritesHelper.scala", "additions": "106", "deletions": "0", "changes": "106"}, "updated": [0, 0, 0]}]}
{"author": "ulysses-you", "sha": "e13f1fa7072f7f7c684ea744299c730c70bdda14", "commit_date": "2021/11/25 01:38:27", "commit_message": "Add small partition factor for rebalance partitions", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 1, 16]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewInRebalancePartitions.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [1, 2, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/ShufflePartitionsUtil.scala", "additions": "11", "deletions": "5", "changes": "16"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/ShufflePartitionsUtilSuite.scala", "additions": "32", "deletions": "4", "changes": "36"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 1, 1]}]}
{"author": "ChenMichael", "sha": "59b4f22de52aa3c4ee5ffd49bfbc913c021dcaf0", "commit_date": "2021/11/23 18:07:18", "commit_message": "Add test that will incorrectly plan broadcast hash join even with AQE off. This test currently fails.", "title": "[SPARK-37442][SQL] InMemoryRelation statistics bug causing broadcast join failures with AQE enabled", "body": "\r\n<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\nImmediately materialize underlying rdd cache (using .count) for an InMemoryRelation when `buildBuffers` is called.\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nCurrently, when `CachedRDDBuilder.buildBuffers` is called, `InMemoryRelation.computeStats` will try to read the accumulators to determine what the relation size is. However, the accumulators are not actually accurate until the cachedRDD is executed and finishes. While this has not happened, the accumulators will report a range from 0 bytes to the accumulator value when the cachedRDD finishes. In AQE, join planning can happen during this time and, if it reads the size as 0 bytes, will likely plan a broadcast join mistakenly believing the build side is very small. If the InMemoryRelation is actually very large in size, then this will cause many issues during execution such as job failure due to broadcasting over 8GB.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nYes. Before, cache materialization doesn't happen until the job starts to run. Now, it happens when trying to get the rdd representing an InMemoryRelation.\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->\r\nTests added", "failed_tests": ["org.apache.spark.sql.hive.CachedTableSuite", "org.apache.spark.sql.CachedTableSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryRelation.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/CachedTableSuite.scala", "additions": "28", "deletions": "24", "changes": "52"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "53", "deletions": "0", "changes": "53"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/joins/BroadcastJoinSuite.scala", "additions": "26", "deletions": "0", "changes": "26"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/CachedTableSuite.scala", "additions": "15", "deletions": "12", "changes": "27"}, "updated": [0, 0, 0]}]}
{"author": "imback82", "sha": "6df9efd6f8e375ce72fa601530743fed3fd52c55", "commit_date": "2021/11/21 05:23:12", "commit_message": "address PR comments", "title": "[SPARK-34332][SQL][TEST] Unify v1 and v2 ALTER TABLE .. SET LOCATION tests", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\n1. Move `ALTER NAMESPACE ... SET LOCATION` parsing tests to `AlterNamespaceSetLocationParserSuite`.\r\n2. Put common `ALTER NAMESPACE ... SET LOCATION` tests into one trait `org.apache.spark.sql.execution.command.AlterNamespaceSetLocationSuiteBase`, and put datasource specific tests to the `v1.AlterNamespaceSetLocationSuite` and `v2.AlterNamespaceSetLocationSuite`.\r\n\r\nThe changes follow the approach of #30287.\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n1. The unification will allow to run common `ALTER NAMESPACE ... SET LOCATION` tests for both DSv1/Hive DSv1 and DSv2\r\n2. We can detect missing features and differences between DSv1 and DSv2 implementations.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nNo\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nExisting unit tests and new tests.", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala", "additions": "0", "deletions": "17", "changes": "17"}, "updated": [1, 3, 12]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala", "additions": "0", "deletions": "16", "changes": "16"}, "updated": [1, 6, 14]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/AlterNamespaceSetLocationParserSuite.scala", "additions": "41", "deletions": "0", "changes": "41"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/AlterNamespaceSetLocationSuiteBase.scala", "additions": "74", "deletions": "0", "changes": "74"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/DDLSuite.scala", "additions": "1", "deletions": "24", "changes": "25"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/v1/AlterNamespaceSetLocationSuite.scala", "additions": "60", "deletions": "0", "changes": "60"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/v2/AlterNamespaceSetLocationSuite.scala", "additions": "43", "deletions": "0", "changes": "43"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/command/AlterNamespaceSetLocationSuite.scala", "additions": "41", "deletions": "0", "changes": "41"}, "updated": [0, 0, 0]}]}
{"author": "beliefer", "sha": "99b603f7a5b4a52b44e9ebde94c8b3e526e866c2", "commit_date": "2021/11/23 01:51:50", "commit_message": "Update code", "title": "[SPARK-36180][SQL] Support TimestampNTZ type in Hive", "body": "### What changes were proposed in this pull request?\r\nSpark not supports TimestampNTZ type in Hive.\r\n\r\n```\r\n[info] Caused by: java.lang.IllegalArgumentException: Error: type expected at the position 0 of 'timestamp_ntz:timestamp' but 'timestamp_ntz' is found.[info] Caused by: java.lang.IllegalArgumentException: Error: type expected at the position 0 of 'timestamp_ntz:timestamp' but 'timestamp_ntz' is found.\r\n[info]  at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$TypeInfoParser.expect(TypeInfoUtils.java:372)\r\n[info]  at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$TypeInfoParser.expect(TypeInfoUtils.java:355)\r\n[info]  at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$TypeInfoParser.parseType(TypeInfoUtils.java:416)\r\n[info]  at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$TypeInfoParser.parseTypeInfos(TypeInfoUtils.java:329)\r\n[info]  at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.getTypeInfosFromTypeString(TypeInfoUtils.java:814)\r\n[info]  at org.apache.hadoop.hive.serde2.lazy.LazySerDeParameters.extractColumnInfo(LazySerDeParameters.java:162)[info]  at org.apache.hadoop.hive.serde2.lazy.LazySerDeParameters.<init>(LazySerDeParameters.java:91)\r\n[info]  at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.initialize(LazySimpleSerDe.java:116)\r\n[info]  at org.apache.hadoop.hive.serde2.AbstractSerDe.initialize(AbstractSerDe.java:54)\r\n[info]  at org.apache.hadoop.hive.serde2.SerDeUtils.initializeSerDe(SerDeUtils.java:533)\r\n[info]  at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:453)\r\n[info]  at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:440)\r\n[info]  at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:281)\r\n[info]  at org.apache.hadoop.hive.ql.metadata.Table.checkValidity(Table.java:199)\r\n[info]  at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:842)\r\n...\r\n```\r\n\r\nHive only providers the timestamp type, so Spark should write both timestamp_ltz and timestamp_ntz as Hive' timestamp.\r\nWhen Spark read schema form Hive, We should restore the timestamp_ntz type.\r\n\r\n### Why are the changes needed?\r\nThe hive 2.3.9 does not have 2 timestamp or a type named timestamp_ntz.\r\nFYI, In hive 3.0, the will be a timestamp with local timezone added.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n'No'. timestamp_ntz is new and not public yet\r\n\r\n\r\n### How was this patch tested?\r\nNew test\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/types/DataType.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala", "additions": "8", "deletions": "3", "changes": "11"}, "updated": [0, 0, 7]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/SQLQuerySuite.scala", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [0, 0, 2]}]}
{"author": "beliefer", "sha": "61d9614bbf90a46be0900d23d3c8d565750d7c4c", "commit_date": "2021/10/21 08:05:21", "commit_message": "Spark SQL should support create function with Aggregator", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/FunctionExpressionBuilder.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 2, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala", "additions": "40", "deletions": "3", "changes": "43"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/udaf.sql", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/udaf.sql.out", "additions": "84", "deletions": "1", "changes": "85"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/UDFSuite.scala", "additions": "50", "deletions": "0", "changes": "50"}, "updated": [0, 1, 1]}]}
{"author": "yaooqinn", "sha": "799a761b8a48673508d838a8b0d3247dcebfd032", "commit_date": "2021/11/24 12:14:17", "commit_message": "nit", "title": "[SPARK-37452][SQL] Char and Varchar breaks backward compatibility between v3.1 and v2", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\nWe will store table schema in table properties for the read-side to restore. In Spark 3.1, we add char/varchar support natively. In some commands like `create table`, `alter table` with these types,  the `char(x)` or `varchar(x)` will be stored directly to those properties. If a user uses Spark 2 to read such a table it will fail to parse the schema.\r\n\r\nFYI, https://github.com/apache/spark/blob/branch-2.4/sql/catalyst/src/main/scala/org/apache/spark/sql/types/DataType.scala#L136\r\n\r\nA table can be a newly created one by Spark 3.1 and later or an existing one modified by Spark 3.1 and on.\r\n  \r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\nbackward compatibility\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\nThat's not necessarily user-facing as a bugfix and only related to internal table properties.\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->\r\n\r\nmanully", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.SparkMetadataOperationSuite"], "files": [{"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveExternalCatalog.scala", "additions": "19", "deletions": "5", "changes": "24"}, "updated": [0, 1, 2]}]}
{"author": "peter-toth", "sha": "b53ef475b6349e351e74635c1b1e5d4a923398fb", "commit_date": "2021/11/24 09:08:21", "commit_message": "fix `JDBCRelation.toString()`.", "title": "[SPARK-37259][SQL] Support CTE and TempTable queries with MSSQL JDBC", "body": "### What changes were proposed in this pull request?\r\nCurrently CTE queries from Spark are not supported with MSSQL server via JDBC. This is because MSSQL server doesn't support the nested CTE syntax that Spark builds from the original query (`options.tableOrQuery`) in `JDBCRDD.resolveTable()` and in `JDBCRDD.compute()`.\r\nUnfortunately, it is non-trivial to split an arbitrary query it into \"with\" and \"regular\" clauses in `MsSqlServerDialect`. So instead, I'm proposing a new general JDBC option \"withClause\" that users can use if they have complex queries with CTE:\r\n```\r\nval withClause = \"WITH t AS (SELECT x, y FROM tbl)\"\r\nval query = \"SELECT * FROM t WHERE x > 10\"\r\nval df = spark.read.format(\"jdbc\")\r\n  .option(\"url\", jdbcUrl)\r\n  .option(\"withClause\", withClause)\r\n  .option(\"query\", query)\r\n  .load()\r\n```\r\nThis change also works with MSSQL's temp table syntax:\r\n```\r\nval withClause = \"(SELECT * INTO #TempTable FROM (SELECT * FROM tbl WHERE x > 10) t)\"\r\nval query = \"SELECT * FROM #TempTable\"\r\nval df = spark.read.format(\"jdbc\")\r\n  .option(\"url\", jdbcUrl)\r\n  .option(\"withClause\", withClause)\r\n  .option(\"query\", query)\r\n  .load()\r\n```\r\n\r\n### Why are the changes needed?\r\nTo support CTE queries with MSSQL.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes, CTE queries are supported form now.\r\n\r\n### How was this patch tested?\r\nAdded new integration UTs.\r\n", "failed_tests": ["org.apache.spark.sql.streaming.StreamingAggregationSuite"], "files": [{"file": {"name": "external/docker-integration-tests/src/test/scala/org/apache/spark/sql/jdbc/MsSqlServerIntegrationSuite.scala", "additions": "57", "deletions": "0", "changes": "57"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 1, 8]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCScanBuilder.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 2]}]}
{"author": "peter-toth", "sha": "cc8690edf34297b1f4948f0221963d21eccffa5a", "commit_date": "2021/04/21 17:40:19", "commit_message": "[SPARK-34079][SQL] Merging non-correlated scalar subqueries to multi-column scalar subqueries for better reuse", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueries.scala", "additions": "422", "deletions": "0", "changes": "422"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/QueryPlan.scala", "additions": "20", "deletions": "13", "changes": "33"}, "updated": [1, 3, 7]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala", "additions": "32", "deletions": "1", "changes": "33"}, "updated": [2, 8, 16]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreePatterns.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [1, 3, 3]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueriesSuite.scala", "additions": "308", "deletions": "0", "changes": "308"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap.java", "additions": "2", "deletions": "7", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "8", "deletions": "3", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/PlanAdaptiveSubqueries.scala", "additions": "17", "deletions": "6", "changes": "23"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/ReuseAdaptiveSubquery.scala", "additions": "5", "deletions": "3", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala", "additions": "2", "deletions": "8", "changes": "10"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "0", "deletions": "9", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "21", "deletions": "0", "changes": "21"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/reuse/ReuseExchangeAndSubquery.scala", "additions": "10", "deletions": "3", "changes": "13"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala", "additions": "18", "deletions": "2", "changes": "20"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9.sf100/explain.txt", "additions": "183", "deletions": "598", "changes": "781"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9.sf100/simplified.txt", "additions": "65", "deletions": "170", "changes": "235"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9/explain.txt", "additions": "183", "deletions": "598", "changes": "781"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9/simplified.txt", "additions": "65", "deletions": "170", "changes": "235"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala", "additions": "36", "deletions": "0", "changes": "36"}, "updated": [1, 2, 4]}]}
{"author": "wangyum", "sha": "a9c4ebda972b024901bf53fa1f3c90c1a034e037", "commit_date": "2021/11/06 15:53:36", "commit_message": "SPARK-37226: Filter push down through window", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "28", "deletions": "1", "changes": "29"}, "updated": [1, 1, 6]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/FilterPushdownSuite.scala", "additions": "85", "deletions": "0", "changes": "85"}, "updated": [1, 1, 1]}]}
{"author": "wangyum", "sha": "50484e23c714e06057993757efa20034a261d2bf", "commit_date": "2021/09/24 06:32:59", "commit_message": "Support DPP if there is no selective predicate on the filtering side", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 2, 8]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/dynamicpruning/PartitionPruning.scala", "additions": "11", "deletions": "2", "changes": "13"}, "updated": [0, 2, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q59/explain.txt", "additions": "162", "deletions": "150", "changes": "312"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q59/simplified.txt", "additions": "9", "deletions": "6", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q2/explain.txt", "additions": "119", "deletions": "107", "changes": "226"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q2/simplified.txt", "additions": "9", "deletions": "6", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q50/explain.txt", "additions": "102", "deletions": "95", "changes": "197"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q50/simplified.txt", "additions": "13", "deletions": "11", "changes": "24"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q59/explain.txt", "additions": "136", "deletions": "126", "changes": "262"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q59/simplified.txt", "additions": "8", "deletions": "6", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q72/explain.txt", "additions": "171", "deletions": "164", "changes": "335"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q72/simplified.txt", "additions": "12", "deletions": "10", "changes": "22"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q76/explain.txt", "additions": "110", "deletions": "96", "changes": "206"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q76/simplified.txt", "additions": "15", "deletions": "11", "changes": "26"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q72/explain.txt", "additions": "171", "deletions": "164", "changes": "335"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q72/simplified.txt", "additions": "12", "deletions": "10", "changes": "22"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DynamicPartitionPruningSuite.scala", "additions": "18", "deletions": "2", "changes": "20"}, "updated": [0, 2, 3]}]}
{"author": "wangyum", "sha": "1b1e54ebaa5c16efe6a552313aebf8125de0dc2e", "commit_date": "2020/03/26 04:56:44", "commit_message": "Repartition by dynamic partition columns before insert table", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [1, 8, 47]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala", "additions": "101", "deletions": "1", "changes": "102"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/sources/RepartitionWritingDataSourceSuite.scala", "additions": "230", "deletions": "0", "changes": "230"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveSessionStateBuilder.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 4]}]}
{"author": "LuciferYang", "sha": "f87467b48d7989fdd026d0b337de617b3f4f9e6d", "commit_date": "2021/11/23 02:53:14", "commit_message": "fix SparkBuild.scala", "title": "[SPARK-37434][BUILD] Add a new profile to auto disable unsupported UTs on MacOs using Apple Silicon", "body": "### What changes were proposed in this pull request?\r\nAfter SPARK-37272  and SPARK-37282,  we can manually add\r\n\r\n```\r\n-Dtest.exclude.tags=org.apache.spark.tags.ExtendedLevelDBTest,org.apache.spark.tags.ExtendedRocksDBTest \r\n```\r\nwhen run `mvn test` or `sbt test` to disable unsupported UTs on MacOs using Apple Silicon.\r\n\r\nThis pr add a new profile to re-write `test.default.exclude.tags` property and  activate it automatically when build on MacOs using Apple Silicon to simplify execution commands.\r\n\r\n\r\n### Why are the changes needed?\r\nSimplify test commands on MacOs using Apple Silicon.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo.\r\n\r\n\r\n### How was this patch tested?\r\n\r\n- Pass the Jenkins or GitHub Action\r\n\r\n- Run the tests on M1, there are 3 modules with `Extended{LevelDB,RocksDB}Test` annotation: `core`,`sql`, `yarn`\r\n 1. For maven, we can use follow commands to test the effectiveness of the current pr:\r\n```\r\nmvn clean install -DskipTest -pl ${moduleName} [-Pyarn]\r\nmvn test -pl ${moduleName} [-Pyarn]\r\n```\r\n 2. For sbt, we can use follow commands to test the effectiveness of the current pr:\r\n```\r\nbuild/sbt \"${moduleName}/test\" [-Pyarn]\r\n```\r\n\r\nUsing the this PR, when testing with MacOs using Apple Silicon , Maven and SBT can automatically skip the UTs with `Extended{LevelDB,RocksDB}Test` annotation.\r\n\r\n\r\n\r\n", "failed_tests": [], "files": [{"file": {"name": "pom.xml", "additions": "13", "deletions": "0", "changes": "13"}, "updated": [0, 3, 22]}, {"file": {"name": "project/SparkBuild.scala", "additions": "9", "deletions": "1", "changes": "10"}, "updated": [1, 2, 14]}]}
{"author": "LuciferYang", "sha": "94369012c9da159d4aef2f68fa965a4c9d602e3d", "commit_date": "2021/11/09 10:25:09", "commit_message": "fix ScalaObjectMapper", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/RebaseDateTime.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/RebaseDateTimeSuite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/RocksDBFileManager.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}
{"author": "LuciferYang", "sha": "1eaad948fd77e442b5cc8a3c8df02d9aa98025e3", "commit_date": "2021/08/04 03:27:21", "commit_message": "add a new method to avoid file truncate", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java", "additions": "1", "deletions": "4", "changes": "5"}, "updated": [1, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/storage/DiskBlockObjectWriter.scala", "additions": "21", "deletions": "0", "changes": "21"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala", "additions": "1", "deletions": "6", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala", "additions": "1", "deletions": "6", "changes": "7"}, "updated": [1, 2, 3]}]}
{"author": "srijith-rajamohan", "sha": "4a22e485119c01af67d26ebd85ff39451d99bf54", "commit_date": "2021/07/09 03:30:03", "commit_message": "Automatically build and update the site", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": ".github/workflows/build_and_test.yml", "additions": "12", "deletions": "642", "changes": "654"}, "updated": [1, 2, 20]}, {"file": {"name": "docs/img/SparkComponents.png", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "python/docs/source/getting_started/index.rst", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 4]}, {"file": {"name": "python/docs/source/index.rst", "additions": "30", "deletions": "18", "changes": "48"}, "updated": [0, 0, 2]}, {"file": {"name": "python/docs/source/user_guide/index.rst", "additions": "53", "deletions": "13", "changes": "66"}, "updated": [0, 0, 2]}]}
{"author": "dchvn", "sha": "baae935fc0c0cc47c4397be662ac7d36dfbbe8b0", "commit_date": "2021/11/12 11:00:20", "commit_message": "Dsv2_index_postgres", "title": "[SPARK-37343][SQL] Implement createIndex, IndexExists and dropIndex in JDBC (Postgres dialect)", "body": "### What changes were proposed in this pull request?\r\nImplementing `createIndex`/`IndexExists`/`dropIndex` in DS V2 JDBC for Postgres dialect.\r\n\r\n### Why are the changes needed?\r\nThis is a subtask of the V2 Index support. This PR implements `createIndex`, `IndexExists` and `dropIndex`. After review for some changes in this PR, I will create new PR for `listIndexs`, or add it in this PR.\r\n\r\nThis PR only implements `createIndex`, `IndexExists` and `dropIndex` in Postgres dialect.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes, `createIndex`/`IndexExists`/`dropIndex` in DS V2 JDBC\r\n\r\n\r\n### How was this patch tested?\r\nNew test.\r\n", "failed_tests": ["org.apache.spark.sql.jdbc.v2.PostgresIntegrationSuite"], "files": [{"file": {"name": "external/docker-integration-tests/src/test/scala/org/apache/spark/sql/jdbc/v2/PostgresIntegrationSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "external/docker-integration-tests/src/test/scala/org/apache/spark/sql/jdbc/v2/V2JDBCTest.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 5, 8]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils.scala", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [0, 3, 7]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/jdbc/MySQLDialect.scala", "additions": "1", "deletions": "8", "changes": "9"}, "updated": [0, 3, 6]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/jdbc/PostgresDialect.scala", "additions": "84", "deletions": "2", "changes": "86"}, "updated": [0, 1, 1]}]}
{"author": "dchvn", "sha": "609831c5753f46cefb67f7e1f680ff3621661178", "commit_date": "2021/11/22 11:10:52", "commit_message": "[SPARK-37407] Inline type hints for python/pyspark/ml/functions.py", "title": "[SPARK-37407][PYTHON] Inline type hints for python/pyspark/ml/functions.py", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->Inline type hints for python/pyspark/ml/functions.py\r\n\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->We can take advantage of static type checking within the functions by inlining the type hints.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as a master.\r\nIf no, write 'No'.\r\n-->No\r\n\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->Existing tests\r\n", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/ml/functions.py", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/ml/functions.pyi", "additions": "0", "deletions": "23", "changes": "23"}, "updated": [0, 1, 1]}]}
{"author": "dchvn", "sha": "e68668213b80063a45a60af2174244d55068f222", "commit_date": "2021/11/22 03:58:40", "commit_message": "[SPARK-37421][PYTHON] Inline type hints for python/pyspark/mllib/evaluation.py", "title": "[SPARK-37421][PYTHON] Inline type hints for python/pyspark/mllib/evaluation.py", "body": "### What changes were proposed in this pull request?\r\nInline type hints for evaluation.py in python/pyspark/mllib/\r\n### Why are the changes needed?\r\nWe can take advantage of static type checking within the functions by inlining the type hints.\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n### How was this patch tested?\r\nExisting tests", "failed_tests": ["pyspark.mllib.evaluation"], "files": [{"file": {"name": "python/pyspark/mllib/evaluation.py", "additions": "78", "deletions": "64", "changes": "142"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/mllib/evaluation.pyi", "additions": "0", "deletions": "92", "changes": "92"}, "updated": [0, 1, 1]}]}
{"author": "dchvn", "sha": "145f22bdcb0701f8248d192f1941ee5d0494938e", "commit_date": "2021/11/12 09:39:31", "commit_message": "[SPARK-36902][SQL] Migrate CreateTableAsSelectStatement to v2 command", "title": "[WIP][SPARK-36902][SQL] Migrate CreateTableAsSelectStatement to v2 command", "body": "\r\n\r\n### What changes were proposed in this pull request?\r\nMigrate CreateTableAsSelectStatement to v2 command\r\n\r\n### Why are the changes needed?\r\nMigrate CreateTableAsSelectStatement to v2 command\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\nexisting tests\r\n", "failed_tests": ["org.apache.spark.sql.connector.SupportsCatalogOptionsSuite", "org.apache.spark.sql.connector.DataSourceV2DataFrameSuite", "org.apache.spark.sql.connector.V1WriteFallbackSessionCatalogSuite", "org.apache.spark.sql.connector.DataSourceV2DataFrameSessionCatalogSuite", "org.apache.spark.sql.connector.DataSourceV2SQLSessionCatalogSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [1, 1, 5]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveCatalogs.scala", "additions": "0", "deletions": "12", "changes": "12"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [0, 3, 13]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/statements.scala", "additions": "0", "deletions": "23", "changes": "23"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2Commands.scala", "additions": "31", "deletions": "8", "changes": "39"}, "updated": [0, 2, 9]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/CatalogV2Util.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/CreateTablePartitioningValidationSuite.scala", "additions": "71", "deletions": "23", "changes": "94"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 2, 9]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala", "additions": "29", "deletions": "15", "changes": "44"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriterV2.scala", "additions": "12", "deletions": "10", "changes": "22"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala", "additions": "17", "deletions": "12", "changes": "29"}, "updated": [0, 2, 7]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/rules.scala", "additions": "46", "deletions": "0", "changes": "46"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala", "additions": "9", "deletions": "6", "changes": "15"}, "updated": [0, 3, 9]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/V2CommandsCaseSensitivitySuite.scala", "additions": "21", "deletions": "7", "changes": "28"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/PlanResolutionSuite.scala", "additions": "13", "deletions": "7", "changes": "20"}, "updated": [0, 0, 0]}]}
{"author": "dchvn", "sha": "991fd8cb8bbb7b33120b754672889ab90b77091a", "commit_date": "2021/11/08 05:30:17", "commit_message": "[SPARK-37234][PYTHON] Inline type hints for python/pyspark/mllib/stat/_statistics.py", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/mllib/_typing.pyi", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/mllib/stat/_statistics.py", "additions": "58", "deletions": "18", "changes": "76"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/mllib/stat/_statistics.pyi", "additions": "0", "deletions": "63", "changes": "63"}, "updated": [0, 0, 1]}]}
{"author": "dchvn", "sha": "c1cb255a87f219e28315598c8820f4b1c9cdd765", "commit_date": "2021/10/22 06:16:45", "commit_message": "[SPARK-37095][PYTHON] Inline type hints for files in python/pyspark/broadcast.py", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/broadcast.py", "additions": "56", "deletions": "26", "changes": "82"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/broadcast.pyi", "additions": "0", "deletions": "48", "changes": "48"}, "updated": [0, 0, 0]}]}
{"author": "dchvn", "sha": "3684fc43a2bed3cb74672729bf5a6264fabd55c8", "commit_date": "2021/10/28 11:08:12", "commit_message": "[SPARK-37146][PYTHON] Inline type hints for python/pyspark/__init__.py", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/__init__.py", "additions": "25", "deletions": "8", "changes": "33"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/__init__.pyi", "additions": "0", "deletions": "77", "changes": "77"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/sql/conf.py", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/sql/context.py", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 2, 2]}, {"file": {"name": "python/pyspark/sql/dataframe.py", "additions": "8", "deletions": "3", "changes": "11"}, "updated": [0, 3, 8]}]}
{"author": "dchvn", "sha": "7a70727468e65d1091ad09f89b66b0196e985963", "commit_date": "2021/10/15 06:06:17", "commit_message": "[SPARK-37014][PYTHON] Inline type hints for python/pyspark/streaming/context.py", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/streaming/context.py", "additions": "91", "deletions": "56", "changes": "147"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/streaming/context.pyi", "additions": "0", "deletions": "71", "changes": "71"}, "updated": [0, 0, 0]}]}
{"author": "dchvn", "sha": "d19a72f79a0ede0ee6ed2656264a8b2878871dc3", "commit_date": "2021/10/18 03:45:41", "commit_message": "[SPARK-37015][PYTHON] Inline type hints for python/pyspark/streaming/dstream.py", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/streaming/dstream.py", "additions": "335", "deletions": "111", "changes": "446"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/streaming/dstream.pyi", "additions": "0", "deletions": "208", "changes": "208"}, "updated": [0, 0, 0]}]}
{"author": "dchvn", "sha": "7b3162618aacb4103df48766fb6c59b34632d62c", "commit_date": "2021/10/21 07:03:25", "commit_message": "[SPARK-37083] Inline type hints for python/pyspark/accumulators.py", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/_typing.pyi", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/accumulators.py", "additions": "51", "deletions": "31", "changes": "82"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/accumulators.pyi", "additions": "0", "deletions": "71", "changes": "71"}, "updated": [0, 0, 0]}]}
{"author": "dchvn", "sha": "b14287e4173f92d81f48d39b1a2e201caf176547", "commit_date": "2021/08/23 15:12:18", "commit_message": "[SPARK-36396] Implement_DataFrame.cov", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/frame.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "189", "deletions": "0", "changes": "189"}, "updated": [0, 0, 13]}, {"file": {"name": "python/pyspark/pandas/missing/frame.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "64", "deletions": "1", "changes": "65"}, "updated": [0, 0, 6]}]}
{"author": "yliou", "sha": "573475491216d2d13680a300e8669b993ee73463", "commit_date": "2021/11/16 00:15:08", "commit_message": "SPARK-37340 Display StageIds in Operators for SQL UI", "title": "[SPARK-37340][UI] Display StageIds in Operators for SQL UI", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\nAdd explicit stageId to operator mapping in the Spark UI that is a more general version of https://issues.apache.org/jira/browse/SPARK-30209, where a stageId-> operator mapping is done with the following algorithm.\r\n 1. Read SparkGraph to get every Node's name and respective AccumulatorIDs.\r\n 2. Gets each stage's AccumulatorIDs.\r\n 3. Maps Operators to stages by checking for non-zero intersection of Step 1 and 2's AccumulatorIDs.\r\n 4. Connect SparkGraphNodes to respective StageIDs for rendering in SQL UI.\r\nAs a result, some operators without max metrics values will also have stageIds in the UI. In some cases, there is no operator->StageID mapping made because no stageIds have accumulatorIds that are a part of the Operator's accumulatorIds. URL links at the top to go to the succeeded jobs and completed stages that were executed as a part of the selected query are also provided.\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\n\r\n### Why are the changes needed?\r\nMakes for easier and quicker debugging and navigation.\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes, `Succeeded Jobs:` and `Completed Stages:`listed at the top of the UI, along with `Stages:` in some of the operators.\r\n<img width=\"697\" alt=\"Screen Shot 2021-11-16 at 11 35 51 AM\" src=\"https://user-images.githubusercontent.com/16739760/142054791-8229d142-41cd-4706-a53e-7abb51e5901c.png\">\r\n\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\n\r\n### How was this patch tested?\r\nManual test locally in SQL UI.\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/ui/ExecutionPage.scala", "additions": "15", "deletions": "3", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLAppStatusListener.scala", "additions": "75", "deletions": "1", "changes": "76"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLAppStatusStore.scala", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SparkPlanGraph.scala", "additions": "18", "deletions": "6", "changes": "24"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/ui/SQLAppStatusListenerSuite.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 1]}]}
{"author": "yliou", "sha": "141a16f2aa767648982f1df088ec1e4667beb86d", "commit_date": "2021/11/17 20:07:52", "commit_message": "Spark-37349 add SQL Rest API parsing logic to accomodate metrics coming in with Spark 3", "title": "Spark-37349 add SQL Rest API parsing logic", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\nFollowing up on https://issues.apache.org/jira/browse/SPARK-31440, values like\r\n`\"value\" : \"total (min, med, max (stageId: taskId))\\n177.0 B (59.0 B, 59.0 B, 59.0 B (stage 1.0: task 5))\"` are currently shown from Rest API calls which are not easily digested in its current form.New processing logic of the values is introduced along with the creation of the following class in the SQL Rest API to organize the metric values: \r\n```\r\ncase class Value private[spark] (stageId: Option[String] = None, taskId: Option[String] = None,\r\n                                 amount: Option[String] = None, min: Option[String] = None,\r\n                                 med: Option[String] = None, max: Option[String] = None)\r\n```\r\nWhich after processing, would make the output look like \r\n`{\r\n      \"value\" : {\r\n        \"stageId\" : \"1.0\",\r\n        \"taskId\" : \"5\",\r\n        \"amount\" : \"177.0 B\",\r\n        \"min\" : \"59.0 B\",\r\n        \"med\" : \"59.0 B\",\r\n        \"max\" : \"59.0 B\"\r\n      }`\r\n\r\nCurrently not in the PR but could be added if there is interest is the normalization of metrics for aggregation purposes such as the following:\r\n- The conversion of hour, minute and second time units to milliseconds.\r\n- PB,TB, GB, MB, KB units are converted to Bytes.\r\n- Comma is removed from Comma formatted Long values (e.g: 8389632)\r\n\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nTo organize and process new metric fields in a more user friendly manner.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nYes, see output below which are gathered from `Check Sql Rest Api Endpoints` Unit Test in SqlResourceWithActualMetricsSuite.scala with AQE set to true.\r\nBefore Changes:\r\n[BeforeSpark37349UT.txt](https://github.com/apache/spark/files/7566623/BeforeSpark37349UT.txt)\r\nAfter changes:\r\n[AfterSpark37349UT.txt](https://github.com/apache/spark/files/7566624/AfterSpark37349UT.txt)\r\n\r\nBackward Compatibility:\r\nAPI Changes were made in `sql/core/src/main/scala/org/apache/spark/status/api/v1/sql/api.scala`.\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->\r\nAdded new Unit Test, manual testing locally.", "failed_tests": [], "files": [{"file": {"name": "project/MimaExcludes.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/status/api/v1/sql/SqlResource.scala", "additions": "100", "deletions": "2", "changes": "102"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/status/api/v1/sql/api.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/metric/SQLMetricsTestUtils.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/status/api/v1/sql/SqlResourceSuite.scala", "additions": "16", "deletions": "12", "changes": "28"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/status/api/v1/sql/SqlResourceWithActualMetricsSuite.scala", "additions": "88", "deletions": "4", "changes": "92"}, "updated": [0, 0, 0]}]}
{"author": "thejdeep", "sha": "2ba73d03b07eca48f5371aa93f18d5ac5a09225d", "commit_date": "2021/11/20 00:59:52", "commit_message": "Fix speculation tests", "title": "[SPARK-36038][CORE] Speculation metrics summary at stage level", "body": " ### What changes were proposed in this pull request?\r\nCurrently there are no speculation metrics available for Spark either at application/job/stage level. This PR is to add some basic speculation metrics for a stage when speculation execution is enabled.\r\n\r\nThis is similar to the existing stage level metrics tracking numTotal (total number of speculated tasks), numCompleted (total number of successful speculated tasks), numFailed (total number of failed speculated tasks), numKilled (total number of killed speculated tasks) etc.\r\n\r\nWith this new set of metrics, it helps further understanding speculative execution feature in the context of the application and also helps in further tuning the speculative execution config knobs.\r\n\r\n ### Why are the changes needed?\r\nAdditional metrics for speculative execution.\r\n\r\n ### Does this PR introduce _any_ user-facing change?\r\nYes, Stages Page in SHS UI will have an additional table for speculation metrics, if present.\r\n\r\n ### How was this patch tested?\r\nUnit tests added and also tested on our internal platform.\r\n\r\nAbsence of speculation metrics : \r\n![Screen Shot 2021-11-15 at 10 12 23 AM](https://user-images.githubusercontent.com/1708757/141836847-b0768265-35eb-4963-b83a-8b785318e82d.png)\r\n\r\nPresence of speculation metrics : \r\n![Screen Shot 2021-11-15 at 10 11 52 AM](https://user-images.githubusercontent.com/1708757/141836945-1078bfcb-1c07-44db-9f33-5dbd09ece6f2.png)\r\n\r\n\r\n", "failed_tests": [], "files": [{"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/stagepage.js", "additions": "32", "deletions": "0", "changes": "32"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/stagespage-template.html", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/webui.css", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusListener.scala", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusStore.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/LiveEntity.scala", "additions": "26", "deletions": "0", "changes": "26"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/api/v1/api.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/storeTypes.scala", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/application_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/completed_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/limit_app_list_json_expectation.json", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/minDate_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/minEndDate_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/stage_with_speculation_summary_expectation.json", "additions": "507", "deletions": "0", "changes": "507"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/test/resources/spark-events/application_1628109047826_1317105", "additions": "52", "deletions": "0", "changes": "52"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/test/scala/org/apache/spark/deploy/history/HistoryServerSuite.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 2, 4]}, {"file": {"name": "core/src/test/scala/org/apache/spark/status/AppStatusListenerSuite.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 2, 3]}, {"file": {"name": "core/src/test/scala/org/apache/spark/status/AppStatusStoreSuite.scala", "additions": "48", "deletions": "1", "changes": "49"}, "updated": [0, 2, 3]}, {"file": {"name": "core/src/test/scala/org/apache/spark/ui/StagePageSuite.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 2]}, {"file": {"name": "dev/.rat-excludes", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 2, 3]}]}
{"author": "JoshRosen", "sha": "0c393a586b2a172c0138eeacf7552d8561157af4", "commit_date": "2021/11/23 07:58:06", "commit_message": "Cache LogicalPlan.isStreaming()", "title": "[SPARK-37447][SQL] Cache LogicalPlan.isStreaming() result in a lazy val", "body": "### What changes were proposed in this pull request?\r\n\r\nThis PR adds caching to `LogicalPlan.isStreaming()`: the default implementation's result will now be cached in a `private lazy val`.\r\n\r\n### Why are the changes needed?\r\n\r\nThis improves the performance of the `DeduplicateRelations` analyzer rule.\r\n\r\nThe default implementation of `isStreaming` recursively visits every node in the tree. `DeduplicateRelations.renewDuplicatedRelations` is recursively invoked on every node in the tree and each invocation calls `isStreaming`. This leads to `O(n^2)` invocations of `isStreaming` on leaf nodes.\r\n\r\nCaching `isStreaming` avoids this performance problem.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nNo.\r\n\r\n### How was this patch tested?\r\n\r\nCorrectness should be covered by existing tests.\r\n\r\nThis significantly improved `DeduplicateRelations` performance in local microbenchmarking with large query plans (~20% reduction in that rule's runtime in one of my tests).", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/LogicalPlan.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}]}
{"author": "kazuyukitanimura", "sha": "af97fb3a629d07628105868d73ae3ba9d8e6dc90", "commit_date": "2021/11/23 07:24:29", "commit_message": "address review comments", "title": "[SPARK-35867][SQL] Enable vectorized read for VectorizedPlainValuesReader.readBooleans", "body": "### What changes were proposed in this pull request?\r\nThis PR proposes to enable vectorized read for ` VectorizedPlainValuesReader.readBooleans`. Currently `readBooleans` unpacks encoded boolean values bit by bit such as\r\n```\r\n  public final void readBooleans(int total, WritableColumnVector c, int rowId) {\r\n    // TODO: properly vectorize this\r\n    for (int i = 0; i < total; i++) {\r\n      c.putBoolean(rowId + i, readBoolean());\r\n    }\r\n  }\r\n```\r\nThe main idea is to unpack 8 bits (a byte) at once instead of bit by bit whenever the alignment allows.\r\n\r\n\r\n### Why are the changes needed?\r\nWith this PR, we observed up-to 100% scan performance gain.\r\n#### JDK11 (Main Branch)\r\n```\r\nIntel(R) Xeon(R) Platinum 8272CL CPU @ 2.60GHz\r\nSQL Single BOOLEAN Column Scan:           Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)\r\n-------------------------------------------------------------------------------------------------------------\r\nSQL Parquet Vectorized                              161            189          24         97.6          10.2\r\n\r\nParquet Reader Single BOOLEAN Column Scan:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)\r\n--------------------------------------------------------------------------------------------------------------\r\nParquetReader Vectorized                             140            147           9        112.7           8.9\r\nParquetReader Vectorized -> Row                       85             88           3        184.4           5.4\r\n```\r\n#### JDK11 (This PR)\r\n```\r\nIntel(R) Xeon(R) Platinum 8272CL CPU @ 2.60GHz\r\nSQL Single BOOLEAN Column Scan:           Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)\r\n-------------------------------------------------------------------------------------------------------------\r\nSQL Parquet Vectorized                              124            149          21        127.2           7.9\r\n\r\nParquet Reader Single BOOLEAN Column Scan:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)\r\n--------------------------------------------------------------------------------------------------------------\r\nParquetReader Vectorized                             100            107          13        157.1           6.4\r\nParquetReader Vectorized -> Row                       52             54           3        303.1           3.3\r\n```\r\n\r\n#### JDK8 (Main Branch)\r\n```\r\nIntel(R) Xeon(R) Platinum 8272CL CPU @ 2.60GHz\r\nSQL Single BOOLEAN Column Scan:           Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)\r\n-------------------------------------------------------------------------------------------------------------\r\nSQL Parquet Vectorized                              189            210          20         83.1          12.0\r\n\r\nParquet Reader Single BOOLEAN Column Scan:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)\r\n--------------------------------------------------------------------------------------------------------------\r\nParquetReader Vectorized                             178            179           3         88.5          11.3\r\nParquetReader Vectorized -> Row                      125            126           2        126.1           7.9\r\n```\r\n#### JDK8 (This PR)\r\n```\r\nIntel(R) Xeon(R) CPU E5-2673 v3 @ 2.40GHz\r\nSQL Single BOOLEAN Column Scan:           Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)\r\n-------------------------------------------------------------------------------------------------------------\r\nSQL Parquet Vectorized                              144            167          12        109.2           9.2\r\n\r\nIntel(R) Xeon(R) CPU E5-2673 v3 @ 2.40GHz\r\nParquet Reader Single BOOLEAN Column Scan:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)\r\n--------------------------------------------------------------------------------------------------------------\r\nParquetReader Vectorized                             119            125           8        131.9           7.6\r\nParquetReader Vectorized -> Row                       60             63           2        260.2           3.8\r\n```\r\n\r\nThe benchmarks are run on GitHub Actions. The results are attached in this PR.\r\n```\r\nsql/core/benchmarks/DataSourceReadBenchmark-jdk11-results.txt\r\nsql/core/benchmarks/DataSourceReadBenchmark-results.txt\r\n```\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n\r\n### How was this patch tested?\r\nUpdated unit tests\r\n```\r\nbuild/sbt \"testOnly *ParquetEncodingSuite\"\r\nbuild/sbt \"testOnly *ColumnarBatchSuite\"\r\n```\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/core/benchmarks/DataSourceReadBenchmark-jdk11-results.txt", "additions": "187", "deletions": "169", "changes": "356"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/benchmarks/DataSourceReadBenchmark-results.txt", "additions": "187", "deletions": "169", "changes": "356"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedPlainValuesReader.java", "additions": "38", "deletions": "12", "changes": "50"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OffHeapColumnVector.java", "additions": "12", "deletions": "0", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OnHeapColumnVector.java", "additions": "12", "deletions": "0", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/WritableColumnVector.java", "additions": "35", "deletions": "0", "changes": "35"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/benchmark/DataSourceReadBenchmark.scala", "additions": "14", "deletions": "7", "changes": "21"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetEncodingSuite.scala", "additions": "5", "deletions": "2", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/vectorized/ColumnarBatchSuite.scala", "additions": "93", "deletions": "0", "changes": "93"}, "updated": [0, 0, 2]}]}
{"author": "Yikun", "sha": "eb04493ec04ce7b8bc0d8f7ed2b1451c38e23722", "commit_date": "2021/11/18 12:27:30", "commit_message": "Improve pod labels", "title": "[SPARK-37372][K8S] Removing redundant label addition and refactoring related test case", "body": "### What changes were proposed in this pull request?\r\n1. Remove redundant Pod label addtions in driver and executor :\r\n\r\nhttps://github.com/apache/spark/blob/3d0663c45f900332ff1ea826f45cbf2c79eba3cd/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicDriverFeatureStep.scala#L144-L145\r\n\r\nhttps://github.com/apache/spark/blob/3d0663c45f900332ff1ea826f45cbf2c79eba3cd/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala#L278-L283\r\n\r\n2. Rename DRIVER_LABELS to CUSTOM_DRIVER_LABELS, LABELS  to CUSTOM_LABELS, make their names more clear, and also add a check assert(metadata.getLabels === conf.labels.asJava) to make sure metadata be set correctly.\r\n\r\nRelated: \r\n- https://github.com/apache/spark/pull/34460\r\n- https://github.com/apache/spark/pull/33508\r\n\r\n### Why are the changes needed?\r\nThese labels are already included by conf.labels as preset labels, we don't need do a extra addition.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNO\r\n\r\n\r\n### How was this patch tested?\r\nUT\r\n", "failed_tests": ["org.apache.spark.deploy.k8s.features.BasicExecutorFeatureStepSuite"], "files": [{"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicDriverFeatureStep.scala", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 0, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicDriverFeatureStepSuite.scala", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [0, 0, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala", "additions": "6", "deletions": "7", "changes": "13"}, "updated": [0, 0, 1]}]}
{"author": "Yikun", "sha": "7d7d98dbc5b5fd182f5f8de9ab58b476833f8703", "commit_date": "2021/11/18 02:08:52", "commit_message": "Address nits", "title": "[SPARK-37331][K8S] Add the ability to create resources before driverPod creating", "body": "### What changes were proposed in this pull request?\r\nThis patch adds a new method `getAdditionalPreKubernetesResources` for `KubernetesFeatureConfigStep`. It returns any additional Kubernetes resources that should be added to support this feature and resources would be setup before driver pod \r\ncreating.\r\n\r\nAfter this patch:\r\n- `getAdditionalPreKubernetesResources`: Devs should return resources in here when they want to create resources before pod creating\r\n- `getAdditionalKubernetesResources`: Devs should return resources in here when they can accept the resources create after pod, and spark will also help to refresh owner reference after resources created, that means if any resource is expected to refresh the owner pod reference, it should be added it here, even if it already in the getAdditionalPreKubernetesResources as same.\r\n\r\n### Why are the changes needed?\r\nWe need to setup K8S resources or extension resources before driver pod creating, and then create pod, after the pod created, the owner refernce would be owner to this Pod.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\nUT", "failed_tests": ["org.apache.spark.deploy.k8s.submit.ClientSuite"], "files": [{"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesDriverSpec.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesUtils.scala", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/KubernetesFeatureConfigStep.scala", "additions": "9", "deletions": "1", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/submit/KubernetesClientApplication.scala", "additions": "9", "deletions": "10", "changes": "19"}, "updated": [0, 0, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/submit/KubernetesDriverBuilder.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/DriverCommandFeatureStepSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/submit/ClientSuite.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}]}
{"author": "Yikun", "sha": "3e71f28f873b80a9acebbd71a3faf200282b4211", "commit_date": "2021/11/15 08:11:56", "commit_message": "Add the ability to creating resources before driver pod", "title": "", "body": "", "failed_tests": ["org.apache.spark.deploy.k8s.submit.ClientSuite"], "files": [{"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesDriverSpec.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesExecutorSpec.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesUtils.scala", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/KubernetesFeatureConfigStep.scala", "additions": "9", "deletions": "1", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/submit/KubernetesClientApplication.scala", "additions": "9", "deletions": "10", "changes": "19"}, "updated": [0, 0, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/submit/KubernetesDriverBuilder.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesExecutorBuilder.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/DriverCommandFeatureStepSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/submit/ClientSuite.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/scheduler/cluster/k8s/ExecutorPodsAllocatorSuite.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/scheduler/cluster/k8s/StatefulsetAllocatorSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "Yikun", "sha": "6bf68b1e09fcd95ef69f3f15e720f5238577dfd4", "commit_date": "2021/10/16 02:42:21", "commit_message": "Support arithmetic operations of decimal(nan) series", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/data_type_ops/num_ops.py", "additions": "21", "deletions": "2", "changes": "23"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/pandas/tests/data_type_ops/test_num_ops.py", "additions": "21", "deletions": "35", "changes": "56"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/data_type_ops/testing_utils.py", "additions": "7", "deletions": "43", "changes": "50"}, "updated": [0, 0, 1]}]}
{"author": "wankunde", "sha": "38ec74818db85e69cfa84668e08de8e2f753c523", "commit_date": "2021/11/17 09:44:01", "commit_message": "[SPARK-37355]Avoid Block Manager registrations when Executor is shutting down", "title": "[SPARK-37355][CORE]Avoid Block Manager registrations when Executor is shutting down", "body": "### What changes were proposed in this pull request?\r\n\r\nAvoid BlockManager registrations when executor is shutting down.\r\n\r\n### Why are the changes needed?\r\n\r\nAs describe in https://github.com/apache/spark/pull/34536 , `HeartbeatReceiver.expireDeadHosts` will not clean those `BlockManager` if the executor is killed with reason Executor heartbeat timed out.  Executors could heartbeat timed out because of network issue, or some other reason like SPARK-20977 \r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nNo\r\n\r\n### How was this patch tested?\r\n\r\nExisting tests.\r\n", "failed_tests": ["org.apache.spark.storage.BlockManagerSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/storage/BlockManager.scala", "additions": "9", "deletions": "5", "changes": "14"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/scala/org/apache/spark/storage/BlockManagerSuite.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 1]}]}
{"author": "wankunde", "sha": "2083cc4a130287f62bea4dcd0b565b019a08d5c8", "commit_date": "2021/10/14 04:24:25", "commit_message": "add spark.shuffle.accurateBlockSkewedFactor parameter to determine whether to report a shuffle block size", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "24", "deletions": "0", "changes": "24"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/MapStatus.scala", "additions": "29", "deletions": "3", "changes": "32"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/Utils.scala", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [0, 1, 7]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/MapStatusSuite.scala", "additions": "62", "deletions": "0", "changes": "62"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "3", "deletions": "12", "changes": "15"}, "updated": [0, 0, 2]}]}
{"author": "yangwwei", "sha": "1ce523dbe3d1ebf3c338cbf89fe8b5ac51b01327", "commit_date": "2021/11/19 23:04:10", "commit_message": "[SPARK-37394] Skip registering to ESS if a customized shuffle manager is configured.", "title": "[SPARK-37394][CORE] Skip registering with ESS if a customized shuffle manager is configured", "body": "### What changes were proposed in this pull request?\r\nPropose to skip registering with ESS if a customized shuffle manager (Remote Shuffle Service) is configured. Otherwise, when the dynamic allocation is enabled without an external shuffle service in place, the Spark executor still tries to connect to the external shuffle service which gets to a connection refused exception.\r\n\r\n\r\n### Why are the changes needed?\r\nTo get dynamic allocation works with a 3rd party remote shuffle service.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n\r\n### How was this patch tested?\r\nTest locally\r\n", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/storage/BlockManager.scala", "additions": "11", "deletions": "2", "changes": "13"}, "updated": [0, 0, 1]}]}
{"author": "xuechendi", "sha": "36ec08a84a1e10f810442940182b69716fbb7519", "commit_date": "2021/11/01 05:33:56", "commit_message": "Support RowToColumnarExec to write to Arrow\n\nSigned-off-by: Chendi Xue <chendi.xue@intel.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/vectorized/ArrowColumnVector.java", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [1, 9, 16]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/Columnar.scala", "additions": "23", "deletions": "1", "changes": "24"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/arrow/ArrowConverters.scala", "additions": "168", "deletions": "32", "changes": "200"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/arrow/ArrowConvertersSuite.scala", "additions": "196", "deletions": "1", "changes": "197"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/vectorized/ArrowColumnVectorSessionSuite.scala", "additions": "88", "deletions": "0", "changes": "88"}, "updated": [0, 0, 0]}]}
{"author": "xinrong-databricks", "sha": "b860fe0b73ee61b0df10d3dabf8718bfb253ac83", "commit_date": "2021/11/19 01:05:37", "commit_message": "fix", "title": "[WIP] Support TimedeltaIndex in pandas API on Spark", "body": "\r\n### What changes were proposed in this pull request?\r\nSupport TimedeltaIndex in pandas API on Spark\r\n\r\n### Why are the changes needed?\r\nSince the support of DayTimeIntervalType is in progress, we may add TimedeltaIndex support in pandas API on Spark as well.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\n\r\n\r\n### How was this patch tested?\r\nUnit tests.", "failed_tests": ["pyspark.sql.tests.test_dataframe", "pyspark.pandas.tests.data_type_ops.test_base", "pyspark.pandas.tests.indexes.test_base"], "files": [{"file": {"name": "python/docs/source/getting_started/quickstart_ps.ipynb", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "python/docs/source/reference/pyspark.pandas/indexing.rst", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/__init__.py", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/indexes/__init__.py", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/indexes/timedelta.py", "additions": "42", "deletions": "0", "changes": "42"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/missing/indexes.py", "additions": "20", "deletions": "0", "changes": "20"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/pandas/tests/indexes/test_base.py", "additions": "38", "deletions": "0", "changes": "38"}, "updated": [0, 0, 3]}]}
{"author": "holdenk", "sha": "14022c0b2e3cdb85ac11511e11a5827a920b8070", "commit_date": "2021/11/22 19:31:22", "commit_message": "Check for null", "title": "[WIP][SPARK-37359][K8S] Cleanup the Spark Kubernetes Integration tests", "body": "### What changes were proposed in this pull request?\r\n\r\nDrop remove reason stats check to make the test less flaky & disable Spark R test [hasn't passed for a long time]\r\nTry and avoid PV/PVC delete/create race condition\r\n\r\n### Why are the changes needed?\r\n\r\nOr K8s test suite is broken so people ignore it. This is not good.\r\n\r\nListener bus message is not always delivered and printed\r\nSparkR tests have been broken for a long time and I don't see any interest in fixing them\r\nPV/PVC creation/deletion can have a race condition during integration tests.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nTest only change\r\n\r\n### How was this patch tested?\r\n\r\nWIP (waiting on CI for k8s int).", "failed_tests": [], "files": [{"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/DecommissionSuite.scala", "additions": "1", "deletions": "3", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/PVTestsSuite.scala", "additions": "22", "deletions": "0", "changes": "22"}, "updated": [0, 0, 0]}]}
{"author": "holdenk", "sha": "33d3918edf9ab6ec7430144dfe17f45791bfa087", "commit_date": "2021/11/01 22:11:49", "commit_message": "Maybe we can push to the listener bus", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala", "additions": "66", "deletions": "2", "changes": "68"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/ExecutorData.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/ExecutorInfo.scala", "additions": "15", "deletions": "5", "changes": "20"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/JsonProtocol.scala", "additions": "8", "deletions": "1", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/ExecutorAllocationManagerSuite.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/CoarseGrainedSchedulerBackendSuite.scala", "additions": "19", "deletions": "0", "changes": "19"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/SparkListenerWithClusterSuite.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/dynalloc/ExecutorMonitorSuite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/util/JsonProtocolSuite.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}]}
{"author": "attilapiros", "sha": "f15f9daa0e0c463e4df36264a943a5f0aac83b70", "commit_date": "2021/11/04 15:56:25", "commit_message": "Collect LocalSparkContext worker logs in case of test failure", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/SparkContext.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 4]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/LocalSparkCluster.scala", "additions": "35", "deletions": "1", "changes": "36"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/scala/org/apache/spark/SparkFunSuite.scala", "additions": "13", "deletions": "1", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/deploy/master/MasterSuite.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}]}
{"author": "cxzl25", "sha": "a2996f859e7c0d326c94376afa27842b44bbbf17", "commit_date": "2021/11/05 11:00:21", "commit_message": "Dynamic partitions should fail quickly when writing to external tables to prevent data deletion", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/InsertIntoHiveTable.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/SQLQuerySuite.scala", "additions": "28", "deletions": "0", "changes": "28"}, "updated": [0, 0, 1]}]}
{"author": "sunchao", "sha": "2e7a69359e5d899f97e71770877a4654369e141d", "commit_date": "2021/11/19 17:45:18", "commit_message": "fix tests", "title": "[SPARK-37376][SQL] Introduce a new DataSource V2 interface HasPartitionKey", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\nThis PR introduces a new V2 min-in `HasPartitionKey` which can be used to return the partition value(s) of a `InputPartition`.\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\nAs part of the Storage Partitioned Join work ([SPIP](https://issues.apache.org/jira/browse/SPARK-37166)), we'll need to introduce a way for a V2 `InputPartition` to return its partition values, which can then be used to compare whether both sides of a join operator can be considered \"compatible\" and thus allows Spark to eliminate shuffle. The info will also be used to group input partitions.\r\n\r\nThis will be used later in follow-up PRs.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\nYes, a new V2 mix-in `HasPartitionKey` will be introduced.\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->\r\n\r\nExtended `InMemoryTable` to support this new interface, and added a new unit test to verify the API.\r\n", "failed_tests": ["org.apache.spark.sql.execution.command.v2.AlterTableRenamePartitionSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/read/HasPartitionKey.java", "additions": "52", "deletions": "0", "changes": "52"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryTable.scala", "additions": "14", "deletions": "8", "changes": "22"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala", "additions": "30", "deletions": "0", "changes": "30"}, "updated": [2, 6, 14]}]}
{"author": "sunchao", "sha": "8ff6be1ba3c74352d9dd29cc4c86cbab55329455", "commit_date": "2021/11/03 23:49:30", "commit_message": "wip", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala", "additions": "41", "deletions": "2", "changes": "43"}, "updated": [0, 1, 2]}, {"file": {"name": "resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/config.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 2]}]}
{"author": "sunchao", "sha": "10c3a450af62b0028601c862005a566d623af253", "commit_date": "2021/11/09 00:02:29", "commit_message": "wip", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/vectorized/ColumnVector.java", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [1, 6, 17]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnarBatchReader.java", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetColumnVector.java", "additions": "321", "deletions": "0", "changes": "321"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetReadState.java", "additions": "42", "deletions": "18", "changes": "60"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase.java", "additions": "13", "deletions": "3", "changes": "16"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader.java", "additions": "56", "deletions": "26", "changes": "82"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader.java", "additions": "100", "deletions": "57", "changes": "157"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedRleValuesReader.java", "additions": "338", "deletions": "18", "changes": "356"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OffHeapColumnVector.java", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OnHeapColumnVector.java", "additions": "27", "deletions": "1", "changes": "28"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/WritableColumnVector.java", "additions": "75", "deletions": "7", "changes": "82"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala", "additions": "21", "deletions": "3", "changes": "24"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/FileBasedDataSourceTest.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcTest.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcV1SchemaPruningSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcV2SchemaPruningSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetColumnIndexSuite.scala", "additions": "13", "deletions": "0", "changes": "13"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormatSuite.scala", "additions": "22", "deletions": "0", "changes": "22"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "326", "deletions": "0", "changes": "326"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaPruningSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetTest.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetVectorizedSuite.scala", "additions": "330", "deletions": "0", "changes": "330"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/vectorized/ColumnVectorSuite.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/vectorized/ColumnarBatchSuite.scala", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 2, 3]}]}
{"author": "sunchao", "sha": "200e42a83de4c14dfb2ffbb5fadb3debd5607de9", "commit_date": "2021/06/09 19:26:29", "commit_message": "wip", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/physical/partitioning.scala", "additions": "175", "deletions": "29", "changes": "204"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/DistributionSuite.scala", "additions": "0", "deletions": "38", "changes": "38"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/ShuffleSpecSuite.scala", "additions": "402", "deletions": "0", "changes": "402"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AQEUtils.scala", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/bucketing/DisableUnnecessaryBucketedScan.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "88", "deletions": "31", "changes": "119"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ValidateRequirements.scala", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/joins/ShuffledJoin.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/objects.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinExec.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q17.sf100/explain.txt", "additions": "155", "deletions": "170", "changes": "325"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q17.sf100/simplified.txt", "additions": "74", "deletions": "81", "changes": "155"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q25.sf100/explain.txt", "additions": "155", "deletions": "170", "changes": "325"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q25.sf100/simplified.txt", "additions": "74", "deletions": "81", "changes": "155"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q29.sf100/explain.txt", "additions": "173", "deletions": "188", "changes": "361"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q29.sf100/simplified.txt", "additions": "75", "deletions": "82", "changes": "157"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q47.sf100/explain.txt", "additions": "110", "deletions": "125", "changes": "235"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q47.sf100/simplified.txt", "additions": "80", "deletions": "89", "changes": "169"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q57.sf100/explain.txt", "additions": "110", "deletions": "125", "changes": "235"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q57.sf100/simplified.txt", "additions": "80", "deletions": "89", "changes": "169"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q72.sf100/explain.txt", "additions": "197", "deletions": "207", "changes": "404"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q72.sf100/simplified.txt", "additions": "105", "deletions": "111", "changes": "216"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q24.sf100/explain.txt", "additions": "61", "deletions": "43", "changes": "104"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q24.sf100/simplified.txt", "additions": "36", "deletions": "33", "changes": "69"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q47.sf100/explain.txt", "additions": "110", "deletions": "125", "changes": "235"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q47.sf100/simplified.txt", "additions": "80", "deletions": "89", "changes": "169"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q51a.sf100/explain.txt", "additions": "249", "deletions": "259", "changes": "508"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q51a.sf100/simplified.txt", "additions": "100", "deletions": "106", "changes": "206"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q57.sf100/explain.txt", "additions": "110", "deletions": "125", "changes": "235"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q57.sf100/simplified.txt", "additions": "80", "deletions": "89", "changes": "169"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q64/explain.txt", "additions": "414", "deletions": "424", "changes": "838"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q64/simplified.txt", "additions": "259", "deletions": "265", "changes": "524"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q72.sf100/explain.txt", "additions": "197", "deletions": "207", "changes": "404"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q72.sf100/simplified.txt", "additions": "105", "deletions": "111", "changes": "216"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/PlannerSuite.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/exchange/EnsureRequirementsSuite.scala", "additions": "460", "deletions": "4", "changes": "464"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/sources/BucketedReadSuite.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingJoinSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}]}
{"author": "ByronHsu", "sha": "19bb045079bcf3a6af4c5b74bb913ae046d8eb5b", "commit_date": "2021/11/02 05:44:25", "commit_message": "add inline type in context", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.data_type_ops.test_base", "pyspark.pandas.tests.indexes.test_base", "org.apache.spark.deploy.yarn.YarnClusterSuite", "org.apache.spark.sql.expressions.ExpressionInfoSuite"], "files": [{"file": {"name": "python/pyspark/accumulators.py", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/accumulators.pyi", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/broadcast.py", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/context.py", "additions": "230", "deletions": "163", "changes": "393"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/context.pyi", "additions": "0", "deletions": "194", "changes": "194"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/clustering.py", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/feature.py", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/wrapper.py", "additions": "8", "deletions": "7", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/spark/functions.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/rdd.py", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/rdd.pyi", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/avro/functions.py", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/sql/column.py", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [0, 1, 3]}, {"file": {"name": "python/pyspark/sql/context.py", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/sql/dataframe.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 8]}, {"file": {"name": "python/pyspark/sql/functions.py", "additions": "156", "deletions": "156", "changes": "312"}, "updated": [1, 3, 13]}, {"file": {"name": "python/pyspark/sql/pandas/conversion.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 4]}, {"file": {"name": "python/pyspark/sql/session.py", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 1, 4]}, {"file": {"name": "python/pyspark/sql/types.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 4]}, {"file": {"name": "python/pyspark/sql/udf.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/sql/window.py", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 1, 3]}, {"file": {"name": "python/pyspark/streaming/context.py", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/util.py", "additions": "8", "deletions": "6", "changes": "14"}, "updated": [0, 0, 0]}]}
{"author": "sleep1661", "sha": "d6f2cfde376298c51e28ed9c7df0cb9e3714a4dc", "commit_date": "2021/11/16 12:04:55", "commit_message": "fix up unit test.  Before call busyTask.markTaskDone, should make sure busyTask was running", "title": "[SPARK-37300][CORE] TaskSchedulerImpl should ignore task finished eve\u2026", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n`TaskSchedulerImpl` handle task finished event at `handleSuccessfulTask` and `handleFailedTask` , but in some case the task was already finished state, which we should ignore task finished event.\r\n\r\nCase describe: \r\nwhen a executor finished a task of some stage, the driver will receive a StatusUpdate event to handle it. At the same time the driver found the executor heartbeat timed out, so the dirver also need handle ExecutorLost event simultaneously. There was a race condition issues here, which will make TaskSetManager.successful and TaskSetManager.tasksSuccessful wrong result. \r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n It will cause `TaskSetManager.successful` and `TaskSetManager.tasksSuccessful` wrong result. \r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nNo\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nAdd a new test.", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala", "additions": "8", "deletions": "1", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/TaskSchedulerImplSuite.scala", "additions": "91", "deletions": "1", "changes": "92"}, "updated": [0, 2, 2]}]}
{"author": "jojochuang", "sha": "38d7ac64cf4d6e46ec1671e08c182ef89d3ac84c", "commit_date": "2021/11/17 01:38:35", "commit_message": "Making the code more Scala-idiomatic.", "title": "[SPARK-37329][YARN] File system delegation tokens are leaked", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nExplicitly cancel the delegation token that's not taken care of by YARN.\r\n\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nLeaking file system delegation tokens create burden for the file system components (for example, KMS), and in the worst case, cause performance regression or even making FS inaccessible.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nNo\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->\r\nManually tested on a small cluster, verify the kms delegation tokens are created and cancelled properly.", "failed_tests": ["org.apache.spark.sql.jdbc.v2.DB2IntegrationSuite", "org.apache.spark.sql.jdbc.MariaDBKrbIntegrationSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/deploy/security/HadoopFSDelegationTokenProvider.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}]}
{"author": "jojochuang", "sha": "17921d5018a0efce2f2beacdda6f9e671a003794", "commit_date": "2021/11/16 08:19:05", "commit_message": "fix: pom.xml to reduce vulnerabilities\n\nThe following vulnerabilities are fixed with an upgrade:\n- https://snyk.io/vuln/SNYK-JAVA-COMGOOGLEGUAVA-1015415\n- https://snyk.io/vuln/SNYK-JAVA-COMGOOGLEGUAVA-32236\n- https://snyk.io/vuln/SNYK-JAVA-IONETTY-1082234\n- https://snyk.io/vuln/SNYK-JAVA-IONETTY-1082235\n- https://snyk.io/vuln/SNYK-JAVA-IONETTY-1082236\n- https://snyk.io/vuln/SNYK-JAVA-IONETTY-1082238", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "pom.xml", "additions": "11", "deletions": "11", "changes": "22"}, "updated": [1, 7, 26]}]}
{"author": "jojochuang", "sha": "d83796f4a59037d028db77181bf8a7ce692c663a", "commit_date": "2021/11/16 08:03:04", "commit_message": "fix: external/kinesis-asl-assembly/pom.xml to reduce vulnerabilities\n\nThe following vulnerabilities are fixed with an upgrade:\n- https://snyk.io/vuln/SNYK-JAVA-COMGOOGLEPROTOBUF-173761", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "external/kinesis-asl-assembly/pom.xml", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "jojochuang", "sha": "3efe4d4003c653c2ae6435812a9f8e41c872c4f5", "commit_date": "2021/11/16 07:12:13", "commit_message": "fix: external/docker-integration-tests/pom.xml to reduce vulnerabilities\n\nThe following vulnerabilities are fixed with an upgrade:\n- https://snyk.io/vuln/SNYK-JAVA-COMGOOGLEGUAVA-1015415\n- https://snyk.io/vuln/SNYK-JAVA-COMGOOGLEGUAVA-32236", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "external/docker-integration-tests/pom.xml", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "Liulietong", "sha": "2fd31e8aa154750fabf094948b5ac586f1aa80f0", "commit_date": "2021/11/16 05:06:30", "commit_message": "Method findNewStageTopPlans will exchange itself rather than it's child because ensureRequirements with requiredDistribution may bring extra shuffle if we just return child of exchange.", "title": "[SPARK-37328][SQL] Fix bug that OptimizeSkewedJoin may not work after it was moved from queryStageOptimizerRules to queryStagePreparationRules.", "body": "\r\n### What changes were proposed in this pull request?\r\nFix the issue that OptimizeSkewedJoin may not work.\r\nSince OptimizeSkewedJoin was moved from `queryStageOptimizerRules` to `queryStagePreparationRules,` the position OptimizeSkewedJoin was applied has been moved from `newQueryStage()` to `reOptimize()`. The plan OptimizeSkewedJoin applied on changed from plan of new stage which is about to submit to whole spark plan.\r\nIn the cases where skewedJoin is not last stage, OptimizeSkewedJoin may not work because the number of collected shuffleStages is more than 2.\r\n\r\n\r\n### Why are the changes needed?\r\nBug fix.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n\r\n### How was this patch tested?\r\nNew test.\r\n", "failed_tests": ["org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "49", "deletions": "1", "changes": "50"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "48", "deletions": "0", "changes": "48"}, "updated": [0, 1, 2]}]}
{"author": "parthchandra", "sha": "2574e69577285075f102d7a39ef373ced8bdaaea", "commit_date": "2019/03/06 13:11:58", "commit_message": "[SPARK-26509][SQL] Parquet DELTA_BYTE_ARRAY is not supported in Spark 2.x's Vectorized Reader", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader.java", "additions": "15", "deletions": "4", "changes": "19"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedDeltaBinaryPackedReader.java", "additions": "315", "deletions": "0", "changes": "315"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedDeltaByteArrayReader.java", "additions": "66", "deletions": "0", "changes": "66"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedReaderBase.java", "additions": "156", "deletions": "0", "changes": "156"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedValuesReader.java", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/benchmark/DataSourceReadBenchmark.scala", "additions": "22", "deletions": "0", "changes": "22"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetDeltaEncodingSuite.scala", "additions": "356", "deletions": "0", "changes": "356"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetEncodingSuite.scala", "additions": "58", "deletions": "0", "changes": "58"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRebaseDatetimeSuite.scala", "additions": "169", "deletions": "149", "changes": "318"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetTest.scala", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [0, 0, 1]}]}
{"author": "wForget", "sha": "58fc7ece98447e547b1a06777cf6783bb1dda0ab", "commit_date": "2021/11/05 07:37:32", "commit_message": "[SPARK-37210] Write to static partition in dynamic write mode", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveParquetSuite.scala", "additions": "30", "deletions": "0", "changes": "30"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/test/TestHiveSingleton.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}]}
{"author": "ekoifman", "sha": "8a3f27d5d1384bf9b7ed768fe5fd65bd9adf84e6", "commit_date": "2021/11/02 03:34:50", "commit_message": "[SPARK-37193][SQL] DynamicJoinSelection.shouldDemoteBroadcastHashJoin should not apply to outer joins", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/DynamicJoinSelection.scala", "additions": "13", "deletions": "5", "changes": "18"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 2]}]}
{"author": "venkata91", "sha": "e605a1fcba013b0db030905b3a2af78de72b5753", "commit_date": "2021/09/02 04:53:49", "commit_message": "SPARK-33701: Adaptive shuffle merge finalization", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/Dependency.scala", "additions": "31", "deletions": "4", "changes": "35"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/MapOutputTracker.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/SparkEnv.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "27", "deletions": "0", "changes": "27"}, "updated": [0, 1, 6]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala", "additions": "220", "deletions": "59", "changes": "279"}, "updated": [0, 0, 4]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerEvent.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessage.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [1, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/shuffle/ShuffleBlockPusher.scala", "additions": "33", "deletions": "0", "changes": "33"}, "updated": [0, 0, 7]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/DAGSchedulerSuite.scala", "additions": "112", "deletions": "3", "changes": "115"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/test/scala/org/apache/spark/shuffle/ShuffleBlockPusherSuite.scala", "additions": "33", "deletions": "0", "changes": "33"}, "updated": [0, 0, 5]}, {"file": {"name": "docs/configuration.md", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [0, 1, 4]}]}
{"author": "mcdull-zhang", "sha": "0250f8bc8ae97eb033a59b9a056b16f573e65463", "commit_date": "2021/11/12 07:29:24", "commit_message": "fix ConcurrentModificationException", "title": "[SPARK-37301][CORE] ConcurrentModificationException caused by CollectionAccumulator serialization in the heartbeat thread", "body": "### What changes were proposed in this pull request?\r\n\r\nIn our production environment, you can use the following code to reproduce the problem:\r\n\r\n```scala\r\nval acc = sc.collectionAccumulator[String](\"test_acc\")\r\n    \r\nsc.parallelize(Array(0)).foreach(_ => {\r\n  var i = 0\r\n  var stop = false\r\n  val start = System.currentTimeMillis()\r\n  while (!stop) {\r\n    acc.add(i.toString)\r\n    if (i % 10000 == 0) {\r\n      acc.reset()\r\n      if ((System.currentTimeMillis() - start) / 1000 > 120) {\r\n        stop = true\r\n      }\r\n    }\r\n    i = i + 1\r\n  }\r\n})\r\nsc.stop()\r\n```\r\n\r\nThis code can make the executor fail to send heartbeats, even more than the default 60 times, and then the executor exits.\r\n\r\n```tex\r\n21/11/11 21:00:23 WARN Executor: Issue communicating with driver in heartbeater\r\norg.apache.spark.SparkException: Exception thrown in awaitResult: \r\n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\r\n\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\r\n\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\r\n\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1007)\r\n\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:212)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\r\n\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\r\n\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\r\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\r\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.util.ConcurrentModificationException\r\n\tat java.util.ArrayList.writeObject(ArrayList.java:766)\r\n\tat sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1028)\r\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)\r\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\r\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\r\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\r\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\r\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\r\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\r\n\tat java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378)\r\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)\r\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\r\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\r\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\r\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\r\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\r\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\r\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\r\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\r\n\tat java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378)\r\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)\r\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\r\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\r\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\r\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\r\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\r\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:44)\r\n\tat org.apache.spark.rpc.netty.RequestMessage.serialize(NettyRpcEnv.scala:601)\r\n\tat org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:244)\r\n\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:555)\r\n\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:559)\r\n\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:102)\r\n\t... 12 more\r\n21/11/11 21:00:23 ERROR Executor: Exit as unable to send heartbeats to driver more than 60 times\r\n```\r\n\r\nThe reason is that when the heartbeat thread serializes the Collection Accumulator, the task thread may modify the Collection Accumulator\r\n\r\n\r\n### Why are the changes needed?\r\n\r\nAvoid heartbeat reporting failure, which may cause application failure\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nNo\r\n\r\n\r\n### How was this patch tested?\r\nExisting tests and manual tests\r\n", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/executor/Executor.scala", "additions": "11", "deletions": "5", "changes": "16"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala", "additions": "8", "deletions": "1", "changes": "9"}, "updated": [0, 1, 1]}]}
{"author": "Kimahriman", "sha": "251f04d40bc07042194cb21adb3ef273254ede67", "commit_date": "2021/11/13 21:52:32", "commit_message": "Track conditionally evaluated expressions to resolve as subexpressions for cases they are already being evaluated", "title": "[SPARK-35564][SQL] Support subexpression elimination for conditionally evaluated expressions", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error message, please read the guideline first:\r\n     https://spark.apache.org/error-message-guidelines.html\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nI am proposing to add support for conditionally evaluated expressions during subexpression elimination. Currently, only expressions that will definitely be always at least twice are candidates for subexpression elimination. This PR updates that logic so that expressions that are always evaluated at least once and conditionally evaluated at least once are also candidates for subexpression elimination. This helps optimize a common case during data normalization and cleaning and want to null out values that don't match a certain pattern, where you have something like:\r\n\r\n```\r\ntransformed = F.regexp_replace(F.lower(F.trim('my_column')))\r\ndf.withColumn('normalized_value', F.when(F.length(transformed) > 0, transformed))\r\n```\r\nor\r\n```\r\ndf.withColumn('normalized_value', F.when(transformed.rlike(<some regex>), transformed))\r\n```\r\n\r\nIn these cases, `transformed` will always be fully calculated twice, because it might only be needed once. I am proposing creating a subexpression for `transformed` in this case.\r\n\r\nIn practice I've seen a decrease in runtime and codegen size of 10-30% in our production pipelines that heavily make use of this type of logic.\r\n\r\nThe only potential downside is creating extra subexpressions, and therefore function calls, more than necessary. This should only be an issue for certain edge cases where your conditional overwhelming evaluates to false. And then the only overhead is running your conditional logic potentially in a separate function rather than inlined in the codegen. I added a config to control this behavior if that is actually a real concern to anyone, but I'd be happy to just remove the config.\r\n\r\nI also updated some of the existing logic for common expressions in coalesce and when that are actually better handled by the new logic, since you are only guaranteed to have the first value of a Coalesce evaluated, as well as the first conditional of a CaseWhen expression.\r\n\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nTo increase the performance of conditional expressions.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nNo, just performance improvements.\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nNew and updated UT.\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala", "additions": "114", "deletions": "65", "changes": "179"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [0, 3, 16]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/SubexpressionEliminationSuite.scala", "additions": "55", "deletions": "13", "changes": "68"}, "updated": [1, 1, 1]}]}
{"author": "Kimahriman", "sha": "e1d891d8986f7b95bbc739c949bfec98f208e410", "commit_date": "2021/10/15 11:34:18", "commit_message": "Add codegen support to array transform", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala", "additions": "6", "deletions": "5", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "34", "deletions": "0", "changes": "34"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/higherOrderFunctions.scala", "additions": "405", "deletions": "8", "changes": "413"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [2, 2, 6]}]}
{"author": "aokolnychyi", "sha": "5e95a57812e0a46205f2b6e2993790b520b07067", "commit_date": "2021/10/08 16:10:11", "commit_message": "[SPARK-35801][SQL] Support DELETE statements that require rewriting data", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/SupportsRowLevelOperations.java", "additions": "41", "deletions": "0", "changes": "41"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/DeltaBatchWrite.java", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/DeltaWrite.java", "additions": "30", "deletions": "0", "changes": "30"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/DeltaWriteBuilder.java", "additions": "37", "deletions": "0", "changes": "37"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/DeltaWriter.java", "additions": "62", "deletions": "0", "changes": "62"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/DeltaWriterFactory.java", "additions": "33", "deletions": "0", "changes": "33"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/LogicalWriteInfo.java", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/RowLevelOperation.java", "additions": "91", "deletions": "0", "changes": "91"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/RowLevelOperationBuilder.java", "additions": "34", "deletions": "0", "changes": "34"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/RowLevelOperationInfo.java", "additions": "41", "deletions": "0", "changes": "41"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/SupportsDelta.java", "additions": "39", "deletions": "0", "changes": "39"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/InternalRowProjection.scala", "additions": "91", "deletions": "0", "changes": "91"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 6]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/RewriteDeleteFromTable.scala", "additions": "120", "deletions": "0", "changes": "120"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/RewriteRowLevelCommand.scala", "additions": "131", "deletions": "0", "changes": "131"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/ReplaceNullWithFalseInPredicate.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/SimplifyConditionalsInPredicate.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/planning/patterns.scala", "additions": "95", "deletions": "0", "changes": "95"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2Commands.scala", "additions": "119", "deletions": "7", "changes": "126"}, "updated": [1, 3, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/RowDeltaUtils.scala", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/WriteDeltaProjections.scala", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/write/LogicalWriteInfoImpl.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/write/RowLevelOperationInfoImpl.scala", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/write/RowLevelOperationTable.scala", "additions": "47", "deletions": "0", "changes": "47"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Implicits.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/PullupCorrelatedPredicatesSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala", "additions": "9", "deletions": "3", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala", "additions": "9", "deletions": "1", "changes": "10"}, "updated": [0, 2, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/OptimizeMetadataOnlyDeleteFromTable.scala", "additions": "63", "deletions": "0", "changes": "63"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownUtils.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ReplaceRewrittenRowLevelCommands.scala", "additions": "33", "deletions": "0", "changes": "33"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/RowLevelCommandScanRelationPushDown.scala", "additions": "67", "deletions": "0", "changes": "67"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2Writes.scala", "additions": "38", "deletions": "10", "changes": "48"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2Exec.scala", "additions": "120", "deletions": "8", "changes": "128"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/PlanResolutionSuite.scala", "additions": "9", "deletions": "6", "changes": "15"}, "updated": [0, 0, 0]}]}
{"author": "taosiyuan163", "sha": "070c0eed6bbe77ee22f8e76ed6885772061430bc", "commit_date": "2021/11/01 13:25:39", "commit_message": "commit TargetEncoder", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "mllib/src/main/scala/org/apache/spark/ml/feature/InformationValueEstimator.scala", "additions": "169", "deletions": "0", "changes": "169"}, "updated": [0, 0, 0]}, {"file": {"name": "mllib/src/main/scala/org/apache/spark/ml/feature/WeightOfEvidenceEstimator.scala", "additions": "249", "deletions": "0", "changes": "249"}, "updated": [0, 0, 0]}, {"file": {"name": "mllib/src/test/scala/org/apache/spark/ml/feature/InformationValueEstimatorSuite.scala", "additions": "49", "deletions": "0", "changes": "49"}, "updated": [0, 0, 0]}, {"file": {"name": "mllib/src/test/scala/org/apache/spark/ml/feature/WeightOfEvidenceEstimatorSuite.scala", "additions": "49", "deletions": "0", "changes": "49"}, "updated": [0, 0, 0]}]}
{"author": "dh20", "sha": "7ac35a9d263cc7ffe20b852d21abcc2316791fab", "commit_date": "2021/01/05 07:36:56", "commit_message": "Merge pull request #1 from apache/master\n\n\u540c\u6b65\u4ee3\u7801", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [1, 3, 33]}]}
{"author": "zero323", "sha": "358cc5c4740c0972927a1cf81d58ab92dbb84d28", "commit_date": "2021/11/18 21:55:45", "commit_message": "Add pkgdown POC with basic configuration and install CI dependencies", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": ".github/workflows/build_and_test.yml", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 4, 10]}, {"file": {"name": "R/create-docs.sh", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "R/pkg/.Rbuildignore", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "R/pkg/.gitignore", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "R/pkg/R/DataFrame.R", "additions": "13", "deletions": "18", "changes": "31"}, "updated": [0, 0, 0]}, {"file": {"name": "R/pkg/R/SQLContext.R", "additions": "15", "deletions": "14", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "R/pkg/R/functions.R", "additions": "11", "deletions": "6", "changes": "17"}, "updated": [0, 0, 2]}, {"file": {"name": "R/pkg/R/jobj.R", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "R/pkg/R/schema.R", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "R/pkg/R/utils.R", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "R/pkg/README.md", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "R/pkg/pkgdown/_pkgdown_template.yml", "additions": "311", "deletions": "0", "changes": "311"}, "updated": [0, 0, 0]}, {"file": {"name": "R/pkg/pkgdown/extra.css", "additions": "48", "deletions": "0", "changes": "48"}, "updated": [0, 0, 0]}, {"file": {"name": "R/pkg/vignettes/sparkr-vignettes.Rmd", "additions": "92", "deletions": "83", "changes": "175"}, "updated": [0, 0, 0]}, {"file": {"name": "docs/_plugins/copy_api_dirs.rb", "additions": "2", "deletions": "5", "changes": "7"}, "updated": [0, 0, 0]}]}
{"author": "zero323", "sha": "7fe190876b80098e20279ee35fa1009aec1a5973", "commit_date": "2021/10/21 09:04:02", "commit_message": "Add list/tuple overloads to array, struct, create_map, map_concat", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/sql/functions.py", "additions": "56", "deletions": "8", "changes": "64"}, "updated": [3, 7, 13]}]}
{"author": "cloud-fan", "sha": "b79eaf3786583665c1306501343e11f21255b444", "commit_date": "2021/12/07 12:41:07", "commit_message": "code cleanup", "title": "[SPARK-37392][SQL] Fix the performance bug when inferring constraints for Generate", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nThis is a performance regression since Spark 3.1, caused by https://issues.apache.org/jira/browse/SPARK-32295\r\n\r\nIf you run the query in the JIRA ticket\r\n```\r\nSeq(\r\n  (1, \"x\", \"x\", \"x\", \"x\", \"x\", \"x\", \"x\", \"x\", \"x\", \"x\", \"x\", \"x\", \"x\", \"x\", \"x\", \"x\", \"x\", \"x\", \"x\", \"x\")\r\n).toDF()\r\n  .checkpoint() // or save and reload to truncate lineage\r\n  .createOrReplaceTempView(\"sub\")\r\n\r\nsession.sql(\"\"\"\r\n  SELECT\r\n    *\r\n  FROM\r\n  (\r\n    SELECT\r\n      EXPLODE( ARRAY( * ) ) result\r\n    FROM\r\n    (\r\n      SELECT\r\n        _1 a, _2 b, _3 c, _4 d, _5 e, _6 f, _7 g, _8 h, _9 i, _10 j, _11 k, _12 l, _13 m, _14 n, _15 o, _16 p, _17 q, _18 r, _19 s, _20 t, _21 u\r\n      FROM\r\n        sub\r\n    )\r\n  )\r\n  WHERE\r\n    result != ''\r\n  \"\"\").show() \r\n```\r\nYou will hit OOM. The reason is that:\r\n1. We infer additional predicates with `Generate`. In this case, it's `size(array(cast(_1#21 as string), _2#22, _3#23, ...) > 0`\r\n2. Because of the cast, the `ConstantFolding` rule can't optimize this `size(array(...))`.\r\n3. We end up with a plan containing this part\r\n```\r\n   +- Project [_1#21 AS a#106, _2#22 AS b#107, _3#23 AS c#108, _4#24 AS d#109, _5#25 AS e#110, _6#26 AS f#111, _7#27 AS g#112, _8#28 AS h#113, _9#29 AS i#114, _10#30 AS j#115, _11#31 AS k#116, _12#32 AS l#117, _13#33 AS m#118, _14#34 AS n#119, _15#35 AS o#120, _16#36 AS p#121, _17#37 AS q#122, _18#38 AS r#123, _19#39 AS s#124, _20#40 AS t#125, _21#41 AS u#126]\r\n      +- Filter (size(array(cast(_1#21 as string), _2#22, _3#23, _4#24, _5#25, _6#26, _7#27, _8#28, _9#29, _10#30, _11#31, _12#32, _13#33, _14#34, _15#35, _16#36, _17#37, _18#38, _19#39, _20#40, _21#41), true) > 0)\r\n         +- LogicalRDD [_1#21, _2#22, _3#23, _4#24, _5#25, _6#26, _7#27, _8#28, _9#29, _10#30, _11#31, _12#32, _13#33, _14#34, _15#35, _16#36, _17#37, _18#38, _19#39, _20#40, _21#41] \r\n```\r\nWhen calculating the constraints of the `Project`, we generate around 2^20 expressions, due to this code\r\n```\r\nvar allConstraints = child.constraints\r\nprojectList.foreach {\r\n  case a @ Alias(l: Literal, _) =>\r\n    allConstraints += EqualNullSafe(a.toAttribute, l)\r\n  case a @ Alias(e, _) =>\r\n    // For every alias in `projectList`, replace the reference in constraints by its attribute.\r\n    allConstraints ++= allConstraints.map(_ transform {\r\n      case expr: Expression if expr.semanticEquals(e) =>\r\n        a.toAttribute\r\n    })\r\n    allConstraints += EqualNullSafe(e, a.toAttribute)\r\n  case _ => // Don't change.\r\n} \r\n```\r\n\r\nThere are 3 issues here:\r\n1. We may infer complicated predicates from `Generate`\r\n2. `ConstanFolding` rule is too conservative. At least `Cast` has no side effect with ANSI-off.\r\n3. When calculating constraints, we should have a upper bound to avoid generating too many expressions.\r\n\r\nThis fixes the first 2 issues, and leaves the third one for the future.\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nfix a performance issue\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nno\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->\r\nnew tests, and run the query in JIRA ticket locally.", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "22", "deletions": "17", "changes": "39"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/expressions.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/InferFiltersFromGenerateSuite.scala", "additions": "43", "deletions": "55", "changes": "98"}, "updated": [0, 0, 0]}]}
{"author": "beliefer", "sha": "b9c7d9645c6e30752bc786fbf1519ce0fd44df0d", "commit_date": "2021/12/07 12:11:53", "commit_message": "Update code", "title": "[SPARK-37527][SQL] Translate more standard aggregate functions for pushdown", "body": "### What changes were proposed in this pull request?\r\nCurrently, Spark aggregate pushdown will translate some standard aggregate functions, so that compile these functions to adapt specify database.\r\nAfter this job, users could override `JdbcDialect.compileAggregate` to implement some aggregate functions supported by some database.\r\nBecause some aggregate functions will be converted show below, this PR no need to match them.\r\n\r\n|Input|Parsed|Optimized|\r\n|------|--------------------|----------|\r\n|`Every`| `aggregate.BoolAnd` |`Min`|\r\n|`Any`| `aggregate.BoolOr` |`Max`|\r\n|`Some`| `aggregate.BoolOr` |`Max`|\r\n\r\n### Why are the changes needed?\r\nMake the implement of `*Dialect` could extends the aggregate functions by override `JdbcDialect.compileAggregate`.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes. Users could pushdown more aggregate functions.\r\n\r\n\r\n### How was this patch tested?\r\nExists tests.\r\n", "failed_tests": [], "files": [{"file": {"name": "external/docker-integration-tests/src/test/scala/org/apache/spark/sql/jdbc/v2/PostgresIntegrationSuite.scala", "additions": "111", "deletions": "1", "changes": "112"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/expressions/aggregate/Average.java", "additions": "41", "deletions": "0", "changes": "41"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/expressions/aggregate/Corr.java", "additions": "50", "deletions": "0", "changes": "50"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/expressions/aggregate/CovarPop.java", "additions": "49", "deletions": "0", "changes": "49"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/expressions/aggregate/CovarSamp.java", "additions": "49", "deletions": "0", "changes": "49"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/expressions/aggregate/StddevPop.java", "additions": "42", "deletions": "0", "changes": "42"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/expressions/aggregate/StddevSamp.java", "additions": "42", "deletions": "0", "changes": "42"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/expressions/aggregate/VarPop.java", "additions": "41", "deletions": "0", "changes": "41"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/expressions/aggregate/VarSamp.java", "additions": "41", "deletions": "0", "changes": "41"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala", "additions": "21", "deletions": "2", "changes": "23"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala", "additions": "8", "deletions": "1", "changes": "9"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/jdbc/H2Dialect.scala", "additions": "21", "deletions": "0", "changes": "21"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala", "additions": "18", "deletions": "1", "changes": "19"}, "updated": [0, 2, 9]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/jdbc/PostgresDialect.scala", "additions": "27", "deletions": "0", "changes": "27"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala", "additions": "54", "deletions": "0", "changes": "54"}, "updated": [0, 0, 3]}]}
{"author": "beliefer", "sha": "9596820a473241928b198c1057f4ac662c77dc08", "commit_date": "2021/12/07 10:20:36", "commit_message": "Update code", "title": "[SPARK-37518][SQL] Inject a early scan pushdown rule", "body": "### What changes were proposed in this pull request?\r\nCurrently, Spark supports push down filters, aggregates and limit. All the job is completed by `V2ScanRelationPushDown`.\r\nBut `V2ScanRelationPushDown` have a lot limit.\r\nUsers want apply custom rule for push down after `V2ScanRelationPushDown` failed.\r\n\r\n\r\n### Why are the changes needed?\r\nEasy for users to apply custom pushdown rules.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n'Yes'.\r\nUsers can inject custom early scan pushdown rules.\r\n\r\n\r\n### How was this patch tested?\r\nNew tests.\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/SparkSessionExtensions.scala", "additions": "19", "deletions": "0", "changes": "19"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SparkSessionExtensionSuite.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 3]}]}
{"author": "beliefer", "sha": "3b9d9804c4120d825b2bff8daf6212c4127a2e9e", "commit_date": "2021/12/03 16:11:21", "commit_message": "Update code", "title": "[SPARK-37483][SQL] Support push down top N to JDBC data source V2", "body": "### What changes were proposed in this pull request?\r\nCurrently, Spark supports push down limit to data source.\r\nHowever, in the user's scenario, limit must have the premise of order by. Because limit and order by are more valuable together.\r\n\r\nOn the other hand, push down top N(same as order by ... limit N) outputs the data with basic order to Spark sort, the the sort of Spark may have some performance improvement.\r\n\r\n\r\n### Why are the changes needed?\r\n1. push down top N is very useful for users scenario.\r\n2. push down top N could improves the performance of sort.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n'No'. Just change the physical execute.\r\n\r\n\r\n### How was this patch tested?\r\nNew tests.\r\n", "failed_tests": ["org.apache.spark.ml.source.image.ImageFileFormatSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/read/SupportsPushDownTopN.java", "additions": "37", "deletions": "0", "changes": "37"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala", "additions": "16", "deletions": "4", "changes": "20"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala", "additions": "19", "deletions": "4", "changes": "23"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD.scala", "additions": "20", "deletions": "5", "changes": "25"}, "updated": [1, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation.scala", "additions": "5", "deletions": "2", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownUtils.scala", "additions": "13", "deletions": "2", "changes": "15"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushedDownOperators.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala", "additions": "32", "deletions": "4", "changes": "36"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCScan.scala", "additions": "5", "deletions": "3", "changes": "8"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCScanBuilder.scala", "additions": "16", "deletions": "3", "changes": "19"}, "updated": [1, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/jdbc/DerbyDialect.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [1, 1, 9]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/jdbc/MsSqlServerDialect.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/jdbc/TeradataDialect.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala", "additions": "92", "deletions": "6", "changes": "98"}, "updated": [0, 0, 3]}]}
{"author": "beliefer", "sha": "5347110de30022524ca5249988ee996ecf5467b1", "commit_date": "2021/12/01 12:16:02", "commit_message": "[SPARK-37463][SQL] Read/Write Timestamp ntz to Orc uses UTC timestamp", "title": "[SPARK-37463][SQL] Read/Write Timestamp ntz to Orc uses UTC timestamp", "body": "### What changes were proposed in this pull request?\r\nThis PR used to fix the issue\r\nhttps://github.com/apache/spark/pull/33588#issuecomment-978719988\r\n\r\nThe root cause is Orc write/read timestamp with local timezone in default. The local timezone will be changed.\r\nIf the Orc writer write timestamp with local timezone(e.g. America/Los_Angeles), when the Orc reader reading the timestamp with local timezone(e.g. Europe/Amsterdam), the value of timestamp will be different.\r\n\r\nIf we let the Orc writer write timestamp with UTC timezone, when the Orc reader reading the timestamp with  UTC timezone too, the value of timestamp will be correct.\r\n\r\nThis PR let Orc write/read Timestamp with UTC timezone by call `useUTCTimestamp(true)` for readers or writers.\r\n\r\nThe related Orc source:\r\nhttps://github.com/apache/orc/blob/3f1e57cf1cebe58027c1bd48c09eef4e9717a9e3/java/core/src/java/org/apache/orc/impl/WriterImpl.java#L525\r\n\r\nhttps://github.com/apache/orc/blob/1f68ac0c7f2ae804b374500dcf1b4d7abe30ffeb/java/core/src/java/org/apache/orc/impl/TreeReaderFactory.java#L1184\r\n\r\nAnother problem is Spark 3.3 or newer read the Orc file written by Spark 3.2 or prior. Because the older Spark write timestamp with local timezone, no need to read them with UTC timezone. Otherwise, an incorrect value of timestamp occurs.\r\n\r\n### Why are the changes needed?\r\nFix the bug for Orc timestamp.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nOrc timestamp ntz is a new feature not release yet.\r\n\r\n\r\n### How was this patch tested?\r\nNew tests.\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnarBatchReader.java", "additions": "7", "deletions": "2", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFileFormat.scala", "additions": "7", "deletions": "6", "changes": "13"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcOutputWriter.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcUtils.scala", "additions": "67", "deletions": "1", "changes": "68"}, "updated": [0, 2, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/orc/OrcPartitionReaderFactory.scala", "additions": "11", "deletions": "9", "changes": "20"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcColumnarBatchReaderSuite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcQuerySuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [1, 2, 2]}]}
{"author": "beliefer", "sha": "03a764036674fc0a9934ee1f8b7b54457a2c9176", "commit_date": "2021/12/01 12:43:18", "commit_message": "Update code", "title": "[SPARK-37463][SQL] Read/Write Timestamp ntz from/to Orc uses UTC time zone", "body": "### What changes were proposed in this pull request?\r\n#33588 (comment) show Spark cannot read/write timestamp ntz and ltz correctly. Based on the discussion https://github.com/apache/spark/pull/34712#issuecomment-981402675, we just to fix read/write timestamp ntz to Orc uses UTC timestamp.\r\n\r\nThe root cause is Orc write/read timestamp with local timezone in default. The local timezone will be changed.\r\nIf the Orc writer write timestamp with local timezone(e.g. America/Los_Angeles), when the Orc reader reading the timestamp with other local timezone(e.g. Europe/Amsterdam), the value of timestamp will be different.\r\n\r\nIf we let the Orc writer write timestamp with UTC timezone, when the Orc reader reading the timestamp with UTC timezone too, the value of timestamp will be correct.\r\n\r\nThe related Orc source:\r\nhttps://github.com/apache/orc/blob/3f1e57cf1cebe58027c1bd48c09eef4e9717a9e3/java/core/src/java/org/apache/orc/impl/WriterImpl.java#L525\r\n\r\nhttps://github.com/apache/orc/blob/1f68ac0c7f2ae804b374500dcf1b4d7abe30ffeb/java/core/src/java/org/apache/orc/impl/TreeReaderFactory.java#L1184\r\n\r\n### Why are the changes needed?\r\nFix the bug about read/write timestamp ntz from/to Orc with different times zone.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo. Orc timestamp ntz is a new feature not release yet.\r\n\r\n\r\n### How was this patch tested?\r\nNew tests.\r\n", "failed_tests": ["org.apache.spark.sql.execution.datasources.orc.OrcV1QuerySuite", "org.apache.spark.sql.execution.datasources.orc.OrcV2QuerySuite"], "files": [{"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcAtomicColumnVector.java", "additions": "6", "deletions": "2", "changes": "8"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnVectorUtils.java", "additions": "11", "deletions": "6", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnarBatchReader.java", "additions": "5", "deletions": "2", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcDeserializer.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFileFormat.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcUtils.scala", "additions": "48", "deletions": "3", "changes": "51"}, "updated": [0, 2, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/orc/OrcPartitionReaderFactory.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcColumnarBatchReaderSuite.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcQuerySuite.scala", "additions": "31", "deletions": "6", "changes": "37"}, "updated": [1, 2, 2]}]}
{"author": "beliefer", "sha": "90f058ce8e8bdf7cb6f13329f08b4f7a18b34953", "commit_date": "2021/03/16 03:25:10", "commit_message": "Support the utils for transform number format", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/NumberUtils.scala", "additions": "190", "deletions": "0", "changes": "190"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/NumberUtilsSuite.scala", "additions": "273", "deletions": "0", "changes": "273"}, "updated": [0, 0, 0]}]}
{"author": "HyukjinKwon", "sha": "1392b252362faa88a3a76d38ce260c4e69aa4bd8", "commit_date": "2021/12/02 04:44:53", "commit_message": "Uses Python's standard string formatter for SQL API in PySpark", "title": "[SPARK-37516][PYTHON][SQL] Uses Python's standard string formatter for SQL API in PySpark", "body": "### What changes were proposed in this pull request?\r\n\r\nThis PR proposes to use [Python's standard string formatter](https://docs.python.org/3/library/string.html#custom-string-formatting) in `SparkSession.sql`, see also https://github.com/apache/spark/pull/34677.\r\n\r\n### Why are the changes needed?\r\n\r\nTo improve usability in PySpark. It works together with Python standard string formatter.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nBy default, there is no user-facing change. If `kwargs` is specified, yes.\r\n\r\n1. Attribute supports from frame (standard Python support):\r\n\r\n    ```python\r\n    mydf = spark.range(10)\r\n    spark.sql(\"SELECT {tbl.id}, {tbl[id]} FROM {tbl}\", tbl=mydf)\r\n    ```\r\n\r\n2. Understanding `DataFrame`:\r\n\r\n    ```python\r\n    mydf = spark.range(10)\r\n    spark.sql(\"SELECT * FROM {tbl}\", tbl=mydf)\r\n    ```\r\n\r\n3. Understanding `Column`. (explicit column reference only):\r\n\r\n    ```python\r\n    mydf = spark.range(10)\r\n    spark.sql(\"SELECT {c} FROM {tbl}\", c=col(\"id\"), tbl=mydf)\r\n    ```\r\n\r\n4. Leveraging other Python string format:\r\n\r\n    ```python\r\n    mydf = spark.range(10)\r\n    spark.sql(\r\n        \"SELECT {col} FROM {mydf} WHERE id IN {x}\",\r\n        col=mydf.id, mydf=mydf, x=tuple(range(4)))\r\n    ```\r\n\r\n### How was this patch tested?\r\n\r\nDoctests were added.", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/sql_formatter.py", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/pandas/tests/test_sql.py", "additions": "0", "deletions": "4", "changes": "4"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/session.py", "additions": "84", "deletions": "6", "changes": "90"}, "updated": [1, 1, 10]}, {"file": {"name": "python/pyspark/sql/sql_formatter.py", "additions": "84", "deletions": "0", "changes": "84"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/tests/test_session.py", "additions": "9", "deletions": "1", "changes": "10"}, "updated": [1, 1, 3]}]}
{"author": "nicolasazrak", "sha": "10d4bcf3932da267f1ab38b17723de9f90e008a1", "commit_date": "2021/11/08 01:27:40", "commit_message": "[SPARK-34521][PYTHON] Allow using arrow with a pandas dataframe using a string dtype column", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/sql/pandas/serializers.py", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/tests/test_arrow.py", "additions": "21", "deletions": "0", "changes": "21"}, "updated": [0, 0, 0]}]}
{"author": "AngersZhuuuu", "sha": "cea5be4be8abdb5dd24ac44f2f4758018d030439", "commit_date": "2021/12/07 05:19:43", "commit_message": "Update sql-distributed-sql-engine-spark-sql-cli.md", "title": "[SPARK-37558][DOC] Improve spark sql cli document", "body": "### What changes were proposed in this pull request?\r\nCurrent Spark SQL CLI doc just show simple usage.  In this pr we add a detail doc for spark sql cli.\r\n\r\n\r\n### Why are the changes needed?\r\nMake doc about Spark SQL CLI more clear. help user to use this command line tool.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n\r\n### How was this patch tested?\r\n![image](https://user-images.githubusercontent.com/46485123/144971126-c64a22a6-45fb-4634-918d-b6e4e2b676d2.png)\r\n\r\n\r\n", "failed_tests": [], "files": [{"file": {"name": "docs/index.md", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [1, 1, 2]}, {"file": {"name": "docs/sql-distributed-sql-engine-spark-sql-cli.md", "additions": "185", "deletions": "0", "changes": "185"}, "updated": [0, 0, 0]}, {"file": {"name": "docs/sql-distributed-sql-engine.md", "additions": "3", "deletions": "7", "changes": "10"}, "updated": [0, 0, 0]}]}
{"author": "AngersZhuuuu", "sha": "b2a42892e21ba6cb22214dff0cbec4af1c99cbc2", "commit_date": "2021/11/23 09:58:58", "commit_message": "[SPARK-32446][SHS] Add percentile distribution REST API of peak memory metrics for all executors", "title": "", "body": "", "failed_tests": ["org.apache.spark.ml.source.image.ImageFileFormatSuite"], "files": [{"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/executorspage-template.html", "additions": "16", "deletions": "1", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/executorspage.js", "additions": "309", "deletions": "0", "changes": "309"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/utils.js", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusStore.scala", "additions": "12", "deletions": "0", "changes": "12"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/api/v1/OneApplicationResource.scala", "additions": "19", "deletions": "0", "changes": "19"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/executor_peak_memory_metrics_distributions_expectation.json", "additions": "22", "deletions": "0", "changes": "22"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/deploy/history/HistoryServerSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 2, 5]}, {"file": {"name": "docs/monitoring.md", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 0, 0]}]}
{"author": "AngersZhuuuu", "sha": "7b394013fadb59e4dfe9c1675d281c2815060aba", "commit_date": "2021/11/23 06:56:09", "commit_message": "[SPARK-37445][BUILD] Upgrade hadoop profile to hadoop-3.3 since we support hadoop-3.3 as default now", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": ".github/workflows/build_and_test.yml", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [0, 1, 9]}, {"file": {"name": "dev/create-release/release-build.sh", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "dev/deps/spark-deps-hadoop-2-hive-2.3", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "dev/deps/spark-deps-hadoop-3-hive-2.3", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "dev/run-tests-jenkins.py", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "dev/run-tests.py", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 1, 1]}, {"file": {"name": "dev/test-dependencies.sh", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 1, 2]}, {"file": {"name": "docs/building-spark.md", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 1]}, {"file": {"name": "hadoop-cloud/pom.xml", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "pom.xml", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [0, 3, 22]}, {"file": {"name": "python/docs/source/getting_started/install.rst", "additions": "8", "deletions": "8", "changes": "16"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/install.py", "additions": "25", "deletions": "6", "changes": "31"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/tests/test_install_spark.py", "additions": "38", "deletions": "4", "changes": "42"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/README.md", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/dev/dev-run-integration-tests.sh", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/pom.xml", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 4]}, {"file": {"name": "resource-managers/yarn/pom.xml", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/ClientSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 4]}, {"file": {"name": "sql/core/pom.xml", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 6]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveExternalCatalogVersionsSuite.scala", "additions": "7", "deletions": "5", "changes": "12"}, "updated": [0, 0, 1]}]}
{"author": "dchvn", "sha": "27207aa3bfd7d45da84180177cf1609eb8df6783", "commit_date": "2021/11/30 10:28:31", "commit_message": "remove ignore[index]", "title": "[SPARK-37153][PYTHON] Inline type hints for python/pyspark/profiler.py", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->Inline type hints for python/pyspark/profiler.py\r\n\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->We can take advantage of static type checking within the functions by inlining the type hints.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->No\r\n\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->Existing tests\r\n", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/profiler.py", "additions": "39", "deletions": "27", "changes": "66"}, "updated": [1, 1, 2]}, {"file": {"name": "python/pyspark/profiler.pyi", "additions": "0", "deletions": "65", "changes": "65"}, "updated": [1, 1, 2]}]}
{"author": "dchvn", "sha": "67338d476bd340f436e7ad7cb4020375a7230f0e", "commit_date": "2021/11/12 11:00:20", "commit_message": "Dsv2_index_postgres", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "external/docker-integration-tests/src/test/scala/org/apache/spark/sql/jdbc/v2/MySQLIntegrationSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 4, 8]}, {"file": {"name": "external/docker-integration-tests/src/test/scala/org/apache/spark/sql/jdbc/v2/PostgresIntegrationSuite.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 1, 2]}, {"file": {"name": "external/docker-integration-tests/src/test/scala/org/apache/spark/sql/jdbc/v2/V2JDBCTest.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 5, 8]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils.scala", "additions": "70", "deletions": "1", "changes": "71"}, "updated": [0, 3, 7]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/jdbc/MySQLDialect.scala", "additions": "7", "deletions": "35", "changes": "42"}, "updated": [0, 3, 6]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/jdbc/PostgresDialect.scala", "additions": "59", "deletions": "2", "changes": "61"}, "updated": [0, 1, 1]}]}
{"author": "dchvn", "sha": "b980e910171a6630cceb420c6de13996492674ed", "commit_date": "2021/10/21 07:03:25", "commit_message": "[SPARK-37083] Inline type hints for python/pyspark/accumulators.py", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/_typing.pyi", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/accumulators.py", "additions": "51", "deletions": "31", "changes": "82"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/accumulators.pyi", "additions": "0", "deletions": "71", "changes": "71"}, "updated": [0, 0, 0]}]}
{"author": "dchvn", "sha": "d596472d95c72e689d6d0cec777c5adc0e46ee4c", "commit_date": "2021/08/25 13:02:40", "commit_message": "[SPARK-36402][PYTHON] Implement series.combine", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/series.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 9]}, {"file": {"name": "python/pyspark/pandas/missing/series.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/series.py", "additions": "168", "deletions": "0", "changes": "168"}, "updated": [0, 0, 10]}, {"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames.py", "additions": "38", "deletions": "0", "changes": "38"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_series.py", "additions": "61", "deletions": "0", "changes": "61"}, "updated": [0, 1, 4]}]}
{"author": "dchvn", "sha": "cc65b8c26c9e33ada63ade7218d2ea869010d0a6", "commit_date": "2021/10/22 06:16:45", "commit_message": "[SPARK-37095][PYTHON] Inline type hints for files in python/pyspark/broadcast.py", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/broadcast.py", "additions": "74", "deletions": "26", "changes": "100"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/broadcast.pyi", "additions": "0", "deletions": "48", "changes": "48"}, "updated": [0, 0, 0]}]}
{"author": "dchvn", "sha": "0df579e7df596f8b960b5716a2959f8b481763b7", "commit_date": "2021/11/08 05:30:17", "commit_message": "[SPARK-37234][PYTHON] Inline type hints for python/pyspark/mllib/stat/_statistics.py\n\nfix\n\nfix", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/mllib/_typing.pyi", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/mllib/stat/_statistics.py", "additions": "72", "deletions": "26", "changes": "98"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/mllib/stat/_statistics.pyi", "additions": "0", "deletions": "63", "changes": "63"}, "updated": [0, 0, 1]}]}
{"author": "LuciferYang", "sha": "95d76ecafd404056e8038c524a80321af6ebfaff", "commit_date": "2021/08/04 03:27:21", "commit_message": "add a new method to avoid file truncate", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java", "additions": "1", "deletions": "4", "changes": "5"}, "updated": [1, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/storage/DiskBlockObjectWriter.scala", "additions": "21", "deletions": "0", "changes": "21"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala", "additions": "1", "deletions": "6", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala", "additions": "1", "deletions": "6", "changes": "7"}, "updated": [1, 2, 3]}]}
{"author": "LuciferYang", "sha": "68cf0a8586ed817bbc318e5be7858da52244c91c", "commit_date": "2021/11/09 10:25:09", "commit_message": "fix ScalaObjectMapper", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/RebaseDateTime.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/RebaseDateTimeSuite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/RocksDBFileManager.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}
{"author": "yaooqinn", "sha": "00cd5057a2846bfa9e61ddaa8ecf6b40dc977251", "commit_date": "2021/11/29 12:02:08", "commit_message": "new fix", "title": "[SPARK-37481][Core][WebUI] Fix disappearance of skipped stages after they retry", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\nWhen skipped stages retry, their skipped info will be lost on the UI, and then we may see a stage with 200 tasks indeed, shows that it only has 3 tasks but its `retry 1` has 15 tasks and completely different inputs/outputs.\r\n\r\nA simple way to reproduce,\r\n\r\n```\r\nbin/spark-sql --packages com.github.yaooqinn:itachi_2.12:0.3.0\r\n```\r\n\r\nand run\r\n\r\n```\r\nselect * from (select v from (values (1), (2), (3) t(v))) t1 join (select stage_id_with_retry(3) from (select v from values (1), (2), (3) t(v) group by v)) t2;\r\n```\r\n\r\nAlso, Detailed in the Gist here - https://gist.github.com/yaooqinn/6acb7b74b343a6a6dffe8401f6b7b45c\r\n\r\n\r\nIn this PR, we increase the nextAttempIds of these skipped stages once they get visited.\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\nfix problems when we have skipped stage retries.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\nYes, the UI will keep the skipped stages info\r\n\r\n![image](https://user-images.githubusercontent.com/8326978/144010378-02a688ce-0ead-4c41-ab9b-bc5fce4f8b90.png)\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->\r\n\r\nmanually as recorded in https://gist.github.com/yaooqinn/6acb7b74b343a6a6dffe8401f6b7b45c\r\n\r\nexisting tests\r\n", "failed_tests": ["org.apache.spark.scheduler.DAGSchedulerSuite", "org.apache.spark.ui.UISeleniumSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/Stage.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/DAGSchedulerSuite.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 1]}]}
{"author": "yaooqinn", "sha": "91889904555522c819992ea3d947e71f0b9eb29a", "commit_date": "2021/12/03 09:07:42", "commit_message": "[SPARK-37532][Core] Limit the length of RDD name", "title": "[SPARK-37532][CORE] Limit the length of RDD name", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\nAPIs like sc.newHadoopFile accepts a string representation of comma separate paths, which could be very very long, and we set it directly as the RDD name. This could be an unfriendly name on the UI and cost tons of driver memory for the whole RDD scope.\r\n\r\n\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\nmake the RDD name on UI more friendly\r\nmake it cost less driver memory\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\nyes, for a single RDD like this, user will see a much short one\r\n\r\n![image](https://user-images.githubusercontent.com/8326978/144568404-bbfa9074-52b7-4a2b-a71a-a73719233f4c.png)\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->\r\n\r\npassing GA\r\n", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/rdd/RDD.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}]}
{"author": "xinrong-databricks", "sha": "8d7caf4b3a5c492aea632751ccb56faf8d143fce", "commit_date": "2021/12/07 05:53:43", "commit_message": "doc", "title": "[SPARK-37563][PYTHON] Implement days, seconds, microseconds properties of TimedeltaIndex", "body": "### What changes were proposed in this pull request?\r\nImplement days, seconds, microseconds properties of TimedeltaIndex\r\n\r\n### Why are the changes needed?\r\nTo be consistent with pandas.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes.\r\n```py\r\n>>> psidx = ps.TimedeltaIndex(\r\n...             [\r\n...                 timedelta(days=10),\r\n...                 timedelta(seconds=20),\r\n...                 timedelta(microseconds=30),\r\n...                 timedelta(milliseconds=40),\r\n...                 timedelta(minutes=50),\r\n...                 timedelta(hours=60),\r\n...                 timedelta(weeks=70),\r\n...             ],\r\n...             name=\"x\",\r\n...         )\r\n\r\n>>> psidx.days\r\nInt64Index([10, 0, 0, 0, 0, 2, 490], dtype='int64', name='x')\r\n>>> psidx.seconds\r\nInt64Index([0, 0, 0, 0, 0, 43200, 0], dtype='int64', name='x')\r\n>>> psidx.microseconds\r\nInt64Index([0, 20000000, 30, 40000, 0, 0, 0], dtype='int64', name='x')\r\n```\r\n### How was this patch tested?\r\nUnit tests.\r\n", "failed_tests": [], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/indexing.rst", "additions": "16", "deletions": "7", "changes": "23"}, "updated": [0, 2, 2]}, {"file": {"name": "python/pyspark/pandas/indexes/timedelta.py", "additions": "68", "deletions": "1", "changes": "69"}, "updated": [0, 2, 2]}, {"file": {"name": "python/pyspark/pandas/missing/indexes.py", "additions": "0", "deletions": "3", "changes": "3"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/pandas/tests/indexes/test_timedelta.py", "additions": "62", "deletions": "0", "changes": "62"}, "updated": [0, 0, 0]}]}
{"author": "wangyum", "sha": "83ea6532e3ff11696be25eaadf7cd5f6a9cc1823", "commit_date": "2021/12/07 05:48:27", "commit_message": "Upgrade mysql-connector-java to 8.0.27", "title": "[SPARK-37565][TESTS] Upgrade mysql-connector-java to 8.0.27", "body": "### What changes were proposed in this pull request?\r\n\r\nThis pr upgrade mysql-connector-java to 8.0.27.\r\n\r\n### Why are the changes needed?\r\n\r\nIt will throw `SSLHandshakeException` if running on Java 11 +:\r\n```\r\nMon Dec 06 22:31:06 GMT-07:00 2021 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.\r\ncom.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure\r\n\r\nThe last packet successfully received from the server was 27 milliseconds ago.  The last packet sent successfully to the server was 20 milliseconds ago.\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\r\n\tat com.mysql.jdbc.Util.handleNewInstance(Util.java:404)\r\n\tat com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:981)\r\n\tat com.mysql.jdbc.ExportControlled.transformSocketToSSLSocket(ExportControlled.java:164)\r\n\tat com.mysql.jdbc.MysqlIO.negotiateSSLConnection(MysqlIO.java:4801)\r\n\tat com.mysql.jdbc.MysqlIO.proceedHandshakeWithPluggableAuthentication(MysqlIO.java:1643)\r\n\tat com.mysql.jdbc.MysqlIO.doHandshake(MysqlIO.java:1215)\r\n\tat com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2255)\r\n\tat com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2286)\r\n\tat com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2085)\r\n\tat com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:795)\r\n\tat com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:44)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\r\n\tat com.mysql.jdbc.Util.handleNewInstance(Util.java:404)\r\n\tat com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:400)\r\n\tat com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:327)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$createConnectionFactory$1(JdbcUtils.scala:64)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:56)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:226)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:35)\r\n\tat org.apache.spark.sql.sources.RelationProvider.createRelation(interfaces.scala:87)\r\n\tat org.apache.spark.sql.sources.RelationProvider.createRelation$(interfaces.scala:83)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:24)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:341)\r\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableCommand.run(createDataSourceTables.scala:86)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:71)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:69)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:80)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:231)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3641)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:105)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:165)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:776)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:67)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3639)\r\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:231)\r\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:102)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:776)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:99)\r\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:619)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:776)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:614)\r\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:661)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:63)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:377)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.$anonfun$processLine$1(SparkSQLCLIDriver.scala:496)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\r\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\r\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processLine(SparkSQLCLIDriver.scala:490)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:282)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:926)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1005)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1014)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: javax.net.ssl.SSLHandshakeException: No appropriate protocol (protocol is disabled or cipher suites are inappropriate)\r\n\tat java.base/sun.security.ssl.HandshakeContext.<init>(HandshakeContext.java:170)\r\n\tat java.base/sun.security.ssl.ClientHandshakeContext.<init>(ClientHandshakeContext.java:98)\r\n\tat java.base/sun.security.ssl.TransportContext.kickstart(TransportContext.java:238)\r\n\tat java.base/sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:394)\r\n\tat java.base/sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:373)\r\n\tat com.mysql.jdbc.ExportControlled.transformSocketToSSLSocket(ExportControlled.java:149)\r\n\t... 67 more\r\n```\r\n\r\nPlease see https://dev.mysql.com/doc/connector-j/8.0/en/connector-j-reference-using-ssl.html for more details.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nNo.\r\n\r\n### How was this patch tested?\r\n\r\nManual test.", "failed_tests": [], "files": [{"file": {"name": "pom.xml", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 5, 24]}]}
{"author": "wangyum", "sha": "02eb80513577b2e40c0f2a3808cfd7551c900878", "commit_date": "2021/12/06 08:04:45", "commit_message": "12000 to 100", "title": "[SPARK-37549][SQL] Support set parallel through data source properties", "body": "### What changes were proposed in this pull request?\r\n\r\nThis pr add support set parallel through data source properties when reading data. For example:\r\n```scala\r\nspark.read.option(\"Parallel\", 1000).parquet(\"path/to/parquet\")\r\n```\r\n```sql\r\nCREATE TABLE very_large_partitioned_bucketed_table (\r\n  id STRING,\r\n  foo STRING,\r\n  bar STRING,\r\n  other STRING,\r\n  dt STRING,\r\n  type STRING)\r\nUSING parquet\r\nOPTIONS (\r\n  compression 'gzip',\r\n  PARALLEL '12000'\r\n)\r\nPARTITIONED BY (dt, type)\r\nCLUSTERED BY (id)\r\nINTO 6000 BUCKETS\r\n```\r\n\r\nOracle has similar feature:\r\nhttps://docs.oracle.com/cd/B19306_01/server.102/b14200/clauses006.htm\r\nhttps://docs.oracle.com/cd/E11882_01/server.112/e25523/parallel002.htm#BEIDFDEH\r\n\r\n### Why are the changes needed?\r\n\r\n1. To decrease the degree of parallelism if it is very large partitioned and bucketed table as it is not always use bucket scan since [SPARK-32859](https://issues.apache.org/jira/browse/SPARK-32859).\r\n2. To increase the degree of parallelism on the stream side if it is `BroadcastNestedLoopJoinExec`.\r\n3. To support setting parallel through hint in the future(Oracle has similar feature: https://docs.oracle.com/cd/E11882_01/server.112/e41573/hintsref.htm#CHDJIGDG).\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nNo.\r\n\r\n\r\n### How was this patch tested?\r\n\r\nUnit test.\r\n", "failed_tests": [], "files": [{"file": {"name": "docs/sql-data-sources-generic-options.md", "additions": "40", "deletions": "0", "changes": "40"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/python/sql/datasource.py", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/r/RSparkSQLExample.R", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FilePartition.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SourceOptions.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/FileSourceParallelSuite.scala", "additions": "61", "deletions": "0", "changes": "61"}, "updated": [0, 0, 0]}]}
{"author": "ulysses-you", "sha": "64890e2eb2799fc783de7138054fcf08eff85603", "commit_date": "2021/12/06 11:58:30", "commit_message": "ShuffledRowRDD get preferred locations order by reduce size", "title": "[SPARK-37559][SQL] ShuffledRowRDD get preferred locations order by reduce size", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nPass `MapOutputStatistics` to `ShuffledRowRDD` in AQE code path so we can do sort according to the origin reduce partition size.\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nThe coalesced partition can contain several reduce partitions. The preferred locations of the RDD partition should be the biggest reduce partition before coalesced. \r\n\r\nSo it can get a better data locality and reduce the network traffic.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nno\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->\r\nAdded test", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/ShuffledRowRDD.scala", "additions": "16", "deletions": "4", "changes": "20"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "30", "deletions": "0", "changes": "30"}, "updated": [0, 2, 3]}]}
{"author": "ulysses-you", "sha": "2ccebe0bb45e58133fcf147f6d125aefaa71dbef", "commit_date": "2021/12/03 06:06:02", "commit_message": "Support reorder tasks during scheduling by shuffle partition size in AQE", "title": "[SPARK-37528][SQL][CORE] Support reorder tasks during scheduling by shuffle partition size in AQE", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nIn order to let task know its input size, we need to add a new method in `org.apache.spark.Partition`. Then at SQL side, we can pass the data size into partition before executing the shuffle read stage (thanks to the stage level scheduler in AQE). So, overall the changes include:\r\n\r\n- Add a new method `predictedInputBytes` in `org.apache.spark.Partition`\r\n- Pass the data size to `ShuffledRowRDD.getPartitions`\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\nThis PR tries to reorder tasks by `predictedInputBytes` with big first. Assume the larger amount of input data takes longer to execute. It can save the whole stage execution time. Let's say we have one stage with 4 tasks and the `defaultParallelism` is 2 and the 4 tasks have different execution time with [1s, 3s, 2s, 4s].\r\n\r\n- in normal, the execution time of the stage is: 7s\r\n- after reorder the tasks, the execution time of the stage is: 5s\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nyes, a new config `spark.scheduler.reorderTasks.enabled` to decide if we allow to reorder tasks.\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->\r\n\r\nAdd test in:\r\n\r\n- org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite\r\n- org.apache.spark.scheduler.DAGSchedulerSuite\r\n", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/Partition.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala", "additions": "14", "deletions": "3", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/ResultTask.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/Task.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/DAGSchedulerSuite.scala", "additions": "45", "deletions": "4", "changes": "49"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/ShuffledRowRDD.scala", "additions": "22", "deletions": "2", "changes": "24"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "55", "deletions": "1", "changes": "56"}, "updated": [1, 2, 3]}]}
{"author": "ulysses-you", "sha": "ed68a25aeadfbcbc3419e446e8e6d3f462b370f4", "commit_date": "2021/11/30 08:45:11", "commit_message": "Support cast aware output partitioning and required if it can up cast", "title": "[SPARK-37502][SQL] Support cast aware output partitioning and required if it can up cast", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nEnhance semantic equals at `Partitioning` to support cast aware output partitioning and required if it can up cast.\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nIf a `Cast` is up cast then it should be without any truncating or precision lose or possible runtime failures. So the output partitioning should be same with/without `Cast` if the `Cast` is up cast.\r\n\r\nLet's say we have a query:\r\n```sql\r\n-- v1: c1 int\r\n-- v2: c2 long\r\n\r\nSELECT * FROM v2 JOIN (SELECT c1, count(*) FROM v1 GROUP BY c1) v1 ON v1.c1 = v2.c2\r\n```\r\n\r\nThe executed plan contains three shuffle nodes which looks like:\r\n```sql\r\nSortMergeJoin\r\n  Exchange(cast(c1 as bigint))\r\n    HashAggregate\r\n      Exchange(c1)\r\n        Scan v1\r\n  Exchange(c2)\r\n    Scan v2\r\n```\r\n\r\nWe can simplify the plan using two shuffle nodes:\r\n```sql\r\nSortMergeJoin\r\n  HashAggregate\r\n    Exchange(c1)\r\n      Scan v1\r\n  Exchange(c2)\r\n    Scan v2\r\n```\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nyes, the plan may be changed\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->\r\nAdd test in:\r\n- org.apache.spark.sql.catalyst.DistributionSuite\r\n- org.apache.spark.sql.SQLQuerySuite", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/physical/PartitioningSemanticEquals.scala", "additions": "39", "deletions": "0", "changes": "39"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/physical/partitioning.scala", "additions": "6", "deletions": "5", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/DistributionSuite.scala", "additions": "39", "deletions": "0", "changes": "39"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala", "additions": "31", "deletions": "1", "changes": "32"}, "updated": [0, 0, 4]}]}
{"author": "ulysses-you", "sha": "7fddb62cc5d6f93b9525162fdf4bd4602903e248", "commit_date": "2021/11/12 13:51:05", "commit_message": "Pull out dynamic partition and bucket sort", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SortExec.scala", "additions": "43", "deletions": "30", "changes": "73"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala", "additions": "13", "deletions": "1", "changes": "14"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormatWriter.scala", "additions": "22", "deletions": "70", "changes": "92"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala", "additions": "7", "deletions": "4", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/V1Writes.scala", "additions": "148", "deletions": "0", "changes": "148"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSink.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/CreateHiveTableAsSelectCommand.scala", "additions": "19", "deletions": "3", "changes": "22"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/InsertIntoHiveTable.scala", "additions": "14", "deletions": "57", "changes": "71"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/SaveAsHiveFile.scala", "additions": "4", "deletions": "7", "changes": "11"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/V1HiveWritesHelper.scala", "additions": "106", "deletions": "0", "changes": "106"}, "updated": [0, 0, 0]}]}
{"author": "cxzl25", "sha": "e1d6a6ce9508b6ec5675fa099043b2ee4387ce70", "commit_date": "2021/12/07 08:00:25", "commit_message": "compatible with different hive versions", "title": "[SPARK-37561][SQL] Avoid loading all functions when obtaining hive's DelegationToken", "body": "### What changes were proposed in this pull request?\r\nUse `Hive.getWithoutRegisterFns` instead of `Hive.get` to avoid loading all permanent functions from the hive meta store.\r\n\r\n### Why are the changes needed?\r\nAt present, when obtaining the delegationToken of hive, all functions will be loaded.\r\nThis is unnecessary, it takes time to load the function, and it also increases the burden on the hive meta store.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo.\r\n\r\n\r\n### How was this patch tested?\r\nmanual test\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/security/HiveDelegationTokenProvider.scala", "additions": "12", "deletions": "1", "changes": "13"}, "updated": [0, 0, 0]}]}
{"author": "summaryzb", "sha": "521d0c5157a92e7a4bf927f8e3266fa49957fd10", "commit_date": "2021/12/07 05:53:03", "commit_message": "[SPARK-37493][CORE] show gc time and duration time of driver in executors page", "title": "[SPARK-37493][CORE] show gc time and duration time of driver in executors page", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\nshow driver's gc time & duration time(equivalent to application time) of driver in both driver side and history side UI\r\n\r\n\r\n### Why are the changes needed?\r\nhelp user to config driver's resource more appropriately\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nyes,user will see driver's gc time & duration time in executors page .\r\nwhen `spark.eventLog.logStageExecutorMetrics` is enabled driver's gc time can be logged.\r\nbefore this change,user always get zero\r\n\r\n![image](https://user-images.githubusercontent.com/37905939/144010082-5ebc1f80-b9f9-4286-ba6a-109700168124.png)\r\n![image](https://user-images.githubusercontent.com/37905939/144010127-389c7b74-f5df-49c2-b600-626825af194e.png)\r\n![image](https://user-images.githubusercontent.com/37905939/144012944-527852b9-681b-4a97-8dad-e4b029408c21.png)\r\n\r\n\r\n### How was this patch tested?\r\nunit tests\r\n", "failed_tests": ["org.apache.spark.deploy.history.HistoryServerSuite", "org.apache.spark.scheduler.EventLoggingListenerSuite", "org.apache.spark.internal.plugin.PluginContainerSuite", "org.apache.spark.ExternalShuffleServiceSuite", "org.apache.spark.scheduler.TaskSetManagerSuite", "org.apache.spark.BarrierStageOnSubmittedSuite", "org.apache.spark.scheduler.SparkListenerWithClusterSuite", "org.apache.spark.deploy.DecommissionWorkerSuite", "org.apache.spark.storage.BlockManagerDecommissionIntegrationSuite", "org.apache.spark.shuffle.HostLocalShuffleReadingSuite", "org.apache.spark.SparkContextSuite", "org.apache.spark.DistributedSuite", "org.apache.spark.resource.ResourceDiscoveryPluginSuite", "org.apache.spark.scheduler.BarrierTaskContextSuite", "org.apache.spark.storage.FallbackStorageSuite", "org.apache.spark.scheduler.WorkerDecommissionSuite", "org.apache.spark.util.JsonProtocolSuite", "org.apache.spark.broadcast.BroadcastSuite", "org.apache.spark.scheduler.WorkerDecommissionExtendedSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/metrics/ExecutorMetricType.scala", "additions": "5", "deletions": "2", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusStore.scala", "additions": "52", "deletions": "2", "changes": "54"}, "updated": [1, 1, 2]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/complete_stage_list_json_expectation.json", "additions": "6", "deletions": "3", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/excludeOnFailure_for_stage_expectation.json", "additions": "6", "deletions": "3", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/excludeOnFailure_node_for_stage_expectation.json", "additions": "12", "deletions": "6", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/executor_list_json_expectation.json", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/executor_list_with_executor_metrics_json_expectation.json", "additions": "9", "deletions": "5", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/executor_memory_usage_expectation.json", "additions": "18", "deletions": "14", "changes": "32"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/executor_node_excludeOnFailure_expectation.json", "additions": "22", "deletions": "18", "changes": "40"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/executor_node_excludeOnFailure_unexcluding_expectation.json", "additions": "9", "deletions": "5", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/executor_resource_information_expectation.json", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/failed_stage_list_json_expectation.json", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/one_stage_attempt_json_details_with_failed_task_expectation.json", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/one_stage_attempt_json_expectation.json", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/one_stage_json_expectation.json", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/one_stage_json_with_details_expectation.json", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/stage_list_json_expectation.json", "additions": "8", "deletions": "4", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/stage_list_with_accumulable_json_expectation.json", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/stage_list_with_peak_metrics_expectation.json", "additions": "6", "deletions": "3", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/stage_with_accumulable_json_expectation.json", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/stage_with_peak_metrics_expectation.json", "additions": "6", "deletions": "3", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/stage_with_summaries_expectation.json", "additions": "8", "deletions": "4", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/EventLoggingListenerSuite.scala", "additions": "29", "deletions": "29", "changes": "58"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/util/JsonProtocolSuite.scala", "additions": "23", "deletions": "13", "changes": "36"}, "updated": [0, 0, 0]}]}
{"author": "c21", "sha": "6d59a4061e7b4a3a317c5c19e5a86c193921c518", "commit_date": "2021/11/24 01:08:09", "commit_message": "Sort aggregate codegen", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.execution.HashAggregationQueryWithControlledFallbackSuite", "org.apache.spark.sql.hive.execution.ObjectHashAggregateSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.execution.HashAggregationQuerySuite", "org.apache.spark.sql.execution.datasources.OrcV2AggregatePushDownSuite", "org.apache.spark.sql.execution.datasources.ParquetV2AggregatePushDownSuite", "org.apache.spark.sql.execution.datasources.OrcV1AggregatePushDownSuite", "org.apache.spark.sql.execution.datasources.ParquetV1AggregatePushDownSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 1, 16]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala", "additions": "15", "deletions": "2", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggregateCodegenSupport.scala", "additions": "336", "deletions": "0", "changes": "336"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala", "additions": "3", "deletions": "276", "changes": "279"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/SortAggregateExec.scala", "additions": "27", "deletions": "3", "changes": "30"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/WholeStageCodegenSuite.scala", "additions": "14", "deletions": "3", "changes": "17"}, "updated": [0, 2, 3]}]}
{"author": "sunchao", "sha": "6fee3e43f22c598182d4ee41daae247e42e2e9a3", "commit_date": "2021/06/09 19:26:29", "commit_message": "wip", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite", "org.apache.spark.sql.sources.DisableUnnecessaryBucketedScanWithHiveSupportSuite", "org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite", "org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite", "org.apache.spark.sql.execution.joins.BroadcastJoinSuite", "org.apache.spark.sql.CachedTableSuite", "org.apache.spark.sql.execution.RemoveRedundantSortsSuite", "org.apache.spark.sql.sources.DisableUnnecessaryBucketedScanWithoutHiveSupportSuite", "org.apache.spark.sql.execution.CoalesceShufflePartitionsSuite", "org.apache.spark.sql.DataFrameSuite", "org.apache.spark.sql.execution.RemoveRedundantSortsSuite", "org.apache.spark.sql.DatasetSuite", "org.apache.spark.sql.execution.exchange.EnsureRequirementsSuite", "org.apache.spark.sql.sources.DisableUnnecessaryBucketedScanWithoutHiveSupportSuite", "org.apache.spark.sql.execution.joins.BroadcastJoinSuite", "org.apache.spark.sql.ExplainSuite", "org.apache.spark.sql.ExplainSuite", "org.apache.spark.sql.execution.PlannerSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/physical/partitioning.scala", "additions": "166", "deletions": "29", "changes": "195"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/DistributionSuite.scala", "additions": "0", "deletions": "38", "changes": "38"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/ShuffleSpecSuite.scala", "additions": "415", "deletions": "0", "changes": "415"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AQEUtils.scala", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/bucketing/DisableUnnecessaryBucketedScan.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "75", "deletions": "43", "changes": "118"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ValidateRequirements.scala", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/joins/ShuffledJoin.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/objects.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinExec.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q17.sf100/explain.txt", "additions": "155", "deletions": "170", "changes": "325"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q17.sf100/simplified.txt", "additions": "74", "deletions": "81", "changes": "155"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q25.sf100/explain.txt", "additions": "155", "deletions": "170", "changes": "325"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q25.sf100/simplified.txt", "additions": "74", "deletions": "81", "changes": "155"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q29.sf100/explain.txt", "additions": "173", "deletions": "188", "changes": "361"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q29.sf100/simplified.txt", "additions": "75", "deletions": "82", "changes": "157"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q47.sf100/explain.txt", "additions": "110", "deletions": "125", "changes": "235"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q47.sf100/simplified.txt", "additions": "80", "deletions": "89", "changes": "169"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q57.sf100/explain.txt", "additions": "110", "deletions": "125", "changes": "235"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q57.sf100/simplified.txt", "additions": "80", "deletions": "89", "changes": "169"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q72.sf100/explain.txt", "additions": "197", "deletions": "207", "changes": "404"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q72.sf100/simplified.txt", "additions": "105", "deletions": "111", "changes": "216"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q24.sf100/explain.txt", "additions": "61", "deletions": "43", "changes": "104"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q24.sf100/simplified.txt", "additions": "36", "deletions": "33", "changes": "69"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q47.sf100/explain.txt", "additions": "110", "deletions": "125", "changes": "235"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q47.sf100/simplified.txt", "additions": "80", "deletions": "89", "changes": "169"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q51a.sf100/explain.txt", "additions": "249", "deletions": "259", "changes": "508"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q51a.sf100/simplified.txt", "additions": "100", "deletions": "106", "changes": "206"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q57.sf100/explain.txt", "additions": "110", "deletions": "125", "changes": "235"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q57.sf100/simplified.txt", "additions": "80", "deletions": "89", "changes": "169"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q64/explain.txt", "additions": "414", "deletions": "424", "changes": "838"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q64/simplified.txt", "additions": "259", "deletions": "265", "changes": "524"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q72.sf100/explain.txt", "additions": "197", "deletions": "207", "changes": "404"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q72.sf100/simplified.txt", "additions": "105", "deletions": "111", "changes": "216"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/PlannerSuite.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/exchange/EnsureRequirementsSuite.scala", "additions": "487", "deletions": "4", "changes": "491"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/sources/BucketedReadSuite.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingJoinSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}]}
{"author": "sunchao", "sha": "7b6e083833c807e44b76bc321d32e9c5857e648f", "commit_date": "2021/11/09 00:02:29", "commit_message": "wip", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/vectorized/ColumnVector.java", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [1, 6, 17]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnarBatchReader.java", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetColumnVector.java", "additions": "359", "deletions": "0", "changes": "359"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetReadState.java", "additions": "42", "deletions": "18", "changes": "60"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase.java", "additions": "13", "deletions": "3", "changes": "16"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader.java", "additions": "56", "deletions": "26", "changes": "82"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader.java", "additions": "100", "deletions": "57", "changes": "157"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedRleValuesReader.java", "additions": "349", "deletions": "29", "changes": "378"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OffHeapColumnVector.java", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OnHeapColumnVector.java", "additions": "27", "deletions": "1", "changes": "28"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/WritableColumnVector.java", "additions": "75", "deletions": "7", "changes": "82"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala", "additions": "21", "deletions": "3", "changes": "24"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/FileBasedDataSourceTest.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcTest.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcV1SchemaPruningSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcV2SchemaPruningSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetColumnIndexSuite.scala", "additions": "13", "deletions": "0", "changes": "13"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormatSuite.scala", "additions": "22", "deletions": "0", "changes": "22"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "326", "deletions": "0", "changes": "326"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaPruningSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetTest.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetVectorizedSuite.scala", "additions": "330", "deletions": "0", "changes": "330"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/vectorized/ColumnVectorSuite.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/vectorized/ColumnarBatchSuite.scala", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 2, 3]}]}
{"author": "sunchao", "sha": "c69cb6e9720e8e1f443aa2c52492c5639928435a", "commit_date": "2021/11/03 23:49:30", "commit_message": "wip", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala", "additions": "47", "deletions": "2", "changes": "49"}, "updated": [0, 1, 2]}, {"file": {"name": "resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/config.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 2]}]}
{"author": "weixiuli", "sha": "16cfd5fae4fa476cfa67cdf0f215c07b691fd1ae", "commit_date": "2021/11/25 11:40:45", "commit_message": "[SPARK-37462][CORE] Avoid unnecessary calculating the number of outstanding fetch requests and RPCS", "title": "[SPARK-37462][CORE] Avoid unnecessary calculating the number of outstanding fetch requests and RPCS", "body": "\r\n### What changes were proposed in this pull request?\r\nAvoid unnecessary calculating the number of outstanding fetch requests and RPCS\r\n\r\n### Why are the changes needed?\r\nIt is unnecessary to calculate the number of outstanding fetch requests and RPCS when the IdleStateEvent is not IDLE or the last request is not timeout.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nNo.\r\n### How was this patch tested?\r\nExist unittests.", "failed_tests": [], "files": [{"file": {"name": "common/network-common/src/main/java/org/apache/spark/network/server/TransportChannelHandler.java", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "oeuf", "sha": "3634265abd18059992bd79c576ff4b959d899063", "commit_date": "2021/12/06 17:30:49", "commit_message": "black format code", "title": "[WIP][SPARK-37553][PYTHON] Fix underscore (`_`) bug in pyspark.pandas.frames.DataFrame.pivot_table", "body": "### What changes were proposed in this pull request?\r\n- Adds example code changes to allow for underscores in (1) the elements for the `columns` arg and (2) for the column names used for the `values` arg when `len(values) > 1`. \r\n\r\n\r\n### Why are the changes needed?\r\nFixes a bug with the method `pyspark.pandas.frames.DataFrame.pivot_table` that causes a `KeyError` when an underscore is present (more details in [SPARK-37553](https://issues.apache.org/jira/browse/SPARK-37553)).\r\n```python\r\n>>> import numpy as np\r\n>>> import pandas as pd\r\n>>> from pyspark import pandas as ps\r\n>>> pdf = pd.DataFrame(\r\n        {\r\n            \"a\": [4, 2, 3, 4, 8, 6],\r\n            \"b_b\": [1, 2, 2, 4, 2, 4],\r\n            \"e\": [10, 20, 20, 40, 20, 40],\r\n            \"c\": [1, 2, 9, 4, 7, 4],\r\n            \"d\": [-1, -2, -3, -4, -5, -6],\r\n        },\r\n        index=np.random.rand(6),\r\n    )\r\n>>> psdf = ps.from_pandas(pdf)\r\n>>> psdf.pivot_table(index=[\"c\"], columns=\"a\", values=[\"b_b\", \"e\"])\r\n\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-8-32d5bb0e1166> in <module>\r\n----> 1 psdf.pivot_table(index=[\"c\"], columns=\"a\", values=[\"b_b\", \"e\"])\r\n\r\n~/.pyenv/versions/3.7.9/envs/venv37/lib/python3.7/site-packages/pyspark/pandas/frame.py in pivot_table(self, values, index, columns, aggfunc, fill_value)\r\n   6053                     column_labels = [\r\n   6054                         tuple(list(column_name_to_index[name.split(\"_\")[1]]) + [name.split(\"_\")[0]])\r\n-> 6055                         for name in data_columns\r\n   6056                     ]\r\n   6057                     column_label_names = (\r\n\r\n~/.pyenv/versions/3.7.9/envs/venv37/lib/python3.7/site-packages/pyspark/pandas/frame.py in <listcomp>(.0)\r\n   6053                     column_labels = [\r\n   6054                         tuple(list(column_name_to_index[name.split(\"_\")[1]]) + [name.split(\"_\")[0]])\r\n-> 6055                         for name in data_columns\r\n   6056                     ]\r\n   6057                     column_label_names = (\r\n\r\nKeyError: 'b'\r\n```\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\n- [x] Add unit tests for code changes\r\n- [] Build package via Github Actions \r\n", "failed_tests": ["pyspark.pandas.tests.test_dataframe"], "files": [{"file": {"name": "python/pyspark/pandas/frame.py", "additions": "28", "deletions": "10", "changes": "38"}, "updated": [0, 2, 10]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "74", "deletions": "0", "changes": "74"}, "updated": [0, 3, 3]}]}
{"author": "ByronHsu", "sha": "108ae961915ba64160e93dfb33bf884fb7f4328a", "commit_date": "2021/12/06 23:20:52", "commit_message": "add type to profiler and add generic", "title": "[SPARK-37152][PYTHON] Inline type hints for python/pyspark/context.py", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nAdd inline type hints for python/pyspark/context.py\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nCurrently, Inline type hints for python/pyspark/context doesn't support type checking within function bodies. So we inline type hints to support that.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nNo\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->\r\nExisting", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/context.py", "additions": "221", "deletions": "154", "changes": "375"}, "updated": [0, 2, 4]}, {"file": {"name": "python/pyspark/context.pyi", "additions": "0", "deletions": "195", "changes": "195"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/util.py", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 5]}]}
{"author": "peter-toth", "sha": "963c423628263a236e513eb3a2e7f3a5483a00b3", "commit_date": "2021/04/21 17:40:19", "commit_message": "[SPARK-34079][SQL] Merging non-correlated scalar subqueries to multi-column scalar subqueries for better reuse", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueries.scala", "additions": "422", "deletions": "0", "changes": "422"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/QueryPlan.scala", "additions": "20", "deletions": "13", "changes": "33"}, "updated": [1, 3, 7]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala", "additions": "32", "deletions": "1", "changes": "33"}, "updated": [2, 8, 16]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreePatterns.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [1, 3, 3]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueriesSuite.scala", "additions": "338", "deletions": "0", "changes": "338"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap.java", "additions": "2", "deletions": "7", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "8", "deletions": "3", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/PlanAdaptiveSubqueries.scala", "additions": "17", "deletions": "6", "changes": "23"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/ReuseAdaptiveSubquery.scala", "additions": "5", "deletions": "3", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala", "additions": "2", "deletions": "8", "changes": "10"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "0", "deletions": "9", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "21", "deletions": "0", "changes": "21"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/reuse/ReuseExchangeAndSubquery.scala", "additions": "10", "deletions": "3", "changes": "13"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala", "additions": "18", "deletions": "2", "changes": "20"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9.sf100/explain.txt", "additions": "183", "deletions": "598", "changes": "781"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9.sf100/simplified.txt", "additions": "65", "deletions": "170", "changes": "235"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9/explain.txt", "additions": "183", "deletions": "598", "changes": "781"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9/simplified.txt", "additions": "65", "deletions": "170", "changes": "235"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala", "additions": "38", "deletions": "0", "changes": "38"}, "updated": [1, 2, 4]}]}
{"author": "sarutak", "sha": "167b681496eaea4a98c4c973bf479c1639863179", "commit_date": "2021/12/06 18:51:15", "commit_message": "Use own realpath script function.", "title": "[SPARK-37529][K8S][TESTS][FOLLOWUP] Allow dev-run-integration-tests.sh to take a custom Dockerfile", "body": "### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nThis PR changes `dev-run-integration-tests.sh` to allow it to take a custom Dockerfile like #34790 did.\r\nWith this change, this script accepts `--docker-file` option, which takes a path to a custom Dockerfile.\r\n```\r\n$ ./dev/run-integration-tests.sh --docker-file /path/to/dockerfile\r\n```\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nAs of #34790, we can specify a custom Dockerfile by `spark.kubernetes.test.dockerFile` property  when we run the K8s integration tests using Maven.\r\nWe can run the integration test via `dev-run-integration-tests.sh` but there is no way to specify a custom Dockerfile.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nNo.\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->\r\nConfirmed that the K8s integration tests run with the following command using `Dockerfile.java17`.\r\n```\r\ncd resource-managers/kubernetes/integration-tests\r\n./dev/dev-run-integration-tests.sh --docker-file ../docker/src/main/dockerfiles/spark/Dockerfile.java17\r\n```", "failed_tests": [], "files": [{"file": {"name": "resource-managers/kubernetes/integration-tests/README.md", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/dev/dev-run-integration-tests.sh", "additions": "23", "deletions": "1", "changes": "24"}, "updated": [0, 2, 2]}]}
{"author": "sarutak", "sha": "891fa1960fbb922353c7cd865a27cbcec315aef9", "commit_date": "2021/11/30 22:07:21", "commit_message": "Enable to mark a job as sampling job.", "title": "[WIP][SPARK-37487][SQL][CORE] Avoid performing CollectMetrics twice if the operation is followed by global sort.", "body": "### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nThis PR fixes an issue that `CollectMetrics` performs twice if it's followed by global sort like as follows.\r\n```\r\nval df = spark.range(100)\r\n  .observe(\r\n    name = \"my_event\",\r\n    min($\"id\").as(\"min_val\"),\r\n    max($\"id\").as(\"max_val\"),\r\n    sum($\"id\"),\r\n    count(when($\"id\" % 2 === 0, 1)).as(\"num_even\"))\r\n  .sort($\"id\".desc)\r\n```\r\n\r\nThe expected statistics calculated by `CollectMetrics` is `[0,99,4950,50]` but the actual result is `[0,99,9900,100]`.\r\nThe reason is that jobs for sampling can run before the global sort, which performs extra `CollectMetrics`.\r\nhttps://github.com/apache/spark/blob/e7fa28930dce468df02b5915e1792ada758a96e3/core/src/main/scala/org/apache/spark/Partitioner.scala#L171\r\nhttps://github.com/apache/spark/blob/e7fa28930dce468df02b5915e1792ada758a96e3/core/src/main/scala/org/apache/spark/Partitioner.scala#L195\r\n\r\nThe solution this PR proposes to introduce a property `spark.job.isSamplingJob` which is intended to be get/set internally.\r\nBefore the sampling jobs run, Spark sets the property, and reset it after the jobs finish.\r\nThen, `CollectMetrics` can judge a task is whether of a sampling job or not.\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nBug fix.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nNo.\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->\r\nNew test.", "failed_tests": ["org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.streaming.StreamingQueryListenerSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.util.DataFrameCallbackSuite", "org.apache.spark.sql.DatasetSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/Partitioner.scala", "additions": "36", "deletions": "31", "changes": "67"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/SparkContext.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/CollectMetricsExec.scala", "additions": "21", "deletions": "17", "changes": "38"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/util/DataFrameCallbackSuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}]}
{"author": "huaxingao", "sha": "decee8df03c5015771ff5f39eb8e0fa759b42ee7", "commit_date": "2021/12/03 07:33:27", "commit_message": "address comments", "title": "[SPARK-37523][SQL] Support optimize skewed partitions in Distribution and Ordering if numPartitions is not specified", "body": "\r\n\r\n### What changes were proposed in this pull request?\r\nSupport optimize skewed partitions in Distribution and Ordering if numPartitions is not specified\r\n\r\n### Why are the changes needed?\r\nWhen doing repartition in distribution and sort, if data source requests for a specific number of partitions, we should not optimize repartition. However, if data source does not request for a specific number of partitions, Spark should optimize repartition and split the skewed partitions if necessary.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n\r\n### How was this patch tested?\r\nExisting and new tests\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 6]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 1, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala", "additions": "10", "deletions": "1", "changes": "11"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DistributionAndOrderingUtils.scala", "additions": "8", "deletions": "7", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/WriteDistributionAndOrderingSuite.scala", "additions": "248", "deletions": "111", "changes": "359"}, "updated": [0, 0, 0]}]}
{"author": "tanelk", "sha": "556776622f04e9e5ea60185d2788d7b65d1781a3", "commit_date": "2021/12/03 14:56:55", "commit_message": "Add UT", "title": "[SPARK-37538][SQL] Replace single projection expand", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nIn the `Optimizer` replace all instances of `Expand` with only 1 projection with a `Project`.\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nBoth grouping sets and distinct aggregations can create `Expand` with only 1 projection. Removing those can improve the performance in two ways:\r\n* Enable optimization rules, that can not work with `Expand`\r\n* Avoid unnecessary copying - `ExpandExec` has `needCopyResult: Boolean = true`\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nNo\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->\r\nNew UT", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "19", "deletions": "1", "changes": "20"}, "updated": [1, 1, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/rules/RuleIdCollection.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [2, 3, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreePatterns.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ReplaceSingleProjectionExpandSuite.scala", "additions": "62", "deletions": "0", "changes": "62"}, "updated": [0, 0, 0]}]}
{"author": "thejdeep", "sha": "e9164fef0bb4aea5d1e990d4508560007544d5e2", "commit_date": "2021/11/16 12:37:18", "commit_message": " ### What changes were proposed in this pull request?\nCurrently there are no speculation metrics available for Spark either at application/job/stage level. This PR is to add some basic speculation metrics for a stage when speculation execution is enabled.\n\nThis is similar to the existing stage level metrics tracking numTotal (total number of speculated tasks), numCompleted (total number of successful speculated tasks), numFailed (total number of failed speculated tasks), numKilled (total number of killed speculated tasks) etc.\n\nWith this new set of metrics, it helps further understanding speculative execution feature in the context of the application and also helps in further tuning the speculative execution config knobs.\n\n ### Why are the changes needed?\nAdditional metrics for speculative execution.\n\n ### Does this PR introduce _any_ user-facing change?\nYes, Stages Page in SHS UI will have an additional table for speculation metrics, if present.\n\n ### How was this patch tested?\nUnit tests added and also tested on our internal platform.", "title": "", "body": "", "failed_tests": ["org.apache.spark.deploy.history.HistoryServerSuite", "org.apache.spark.status.AppStatusListenerWithInMemoryStoreSuite", "org.apache.spark.status.AppStatusListenerSuite"], "files": [{"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/stagepage.js", "additions": "32", "deletions": "0", "changes": "32"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/stagespage-template.html", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/webui.css", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusListener.scala", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusStore.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/LiveEntity.scala", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/api/v1/api.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/storeTypes.scala", "additions": "12", "deletions": "0", "changes": "12"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/application_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/completed_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/limit_app_list_json_expectation.json", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/minDate_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/minEndDate_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/stage_with_speculation_summary_expectation.json", "additions": "507", "deletions": "0", "changes": "507"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/test/resources/spark-events/application_1628109047826_1317105", "additions": "52", "deletions": "0", "changes": "52"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/test/scala/org/apache/spark/deploy/history/HistoryServerSuite.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 2, 4]}, {"file": {"name": "core/src/test/scala/org/apache/spark/status/AppStatusListenerSuite.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 2, 4]}, {"file": {"name": "core/src/test/scala/org/apache/spark/status/AppStatusStoreSuite.scala", "additions": "48", "deletions": "1", "changes": "49"}, "updated": [0, 2, 3]}, {"file": {"name": "core/src/test/scala/org/apache/spark/ui/StagePageSuite.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 2]}, {"file": {"name": "dev/.rat-excludes", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 2, 3]}]}
{"author": "guiyanakuang", "sha": "158fbd655fbdd627d677bdcad007ee66315d5dab", "commit_date": "2021/11/30 07:59:03", "commit_message": "Modify existing test to adapt to current modifications", "title": "[SPARK-37488][CORE] When `TaskLocation` is `HDFSCacheTaskLocation` or `HostTaskLocation`, check if executor is alive on the host", "body": "```java\r\n// The online environment is actually hive partition data imported to tidb, the code logic can be simplified as follows\r\nSparkSession testApp = SparkSession.builder()\r\n    .master(\"local[*]\")\r\n    .appName(\"test app\")\r\n    .enableHiveSupport()\r\n    .getOrCreate();\r\nDataset<Row> dataset = testApp.sql(\"select * from default.test where dt = '20211129'\");\r\ndataset.persist(StorageLevel.MEMORY_AND_DISK());\r\ndataset.count();\r\n```\r\nI have observed that tasks are permanently pending and reruns can always be reproduced.\r\nSince it is only reproducible online, I use the arthas runtime to see the status of the function entries and returns within the `TaskSetManager`.\r\nI replaced the real host to avoid revealing company information\r\nhttps://gist.github.com/guiyanakuang/431584f191645513552a937d16ae8fbd\r\n\r\n`NODE_LOCAL` level, because the persist function is called, the pendingTasks.forHost has a collection of pending tasks, but it points to the machine where the block of partitioned data is located, and since the only resource spark gets is the driver. \r\n\r\nHere is the forHost information I got through the arthas hook\r\n```\r\n    forHost=@HashMap[\r\n        serialVersionUID=@Long[1],\r\n        _loadFactor=@Integer[750],\r\n        table=@HashEntry[][\r\n            @DefaultEntry[(kv: hdfs-loc-1, ArrayBuffer(2, 1))],\r\n            null,\r\n            null,\r\n            null,\r\n            null,\r\n            null,\r\n            null,\r\n            null,\r\n            null,\r\n            @DefaultEntry[(kv: driver-host, ArrayBuffer())],\r\n            null,\r\n            null,\r\n            null,\r\n            null,\r\n            @DefaultEntry[(kv: hdfs-loc-2, ArrayBuffer(2, 1))],\r\n            @DefaultEntry[(kv: hdfs-loc-3, ArrayBuffer(2, 1))],\r\n        ],\r\n```\r\nWhen we can only provide driver resources, getAllowedLocalityLevel limits the attempt level to NODE_LOCAL. But in reality, the task cannot run at this level, and the attempt level cannot reach ANY\r\n\r\n### What changes were proposed in this pull request?\r\nTo solve the above mentioned problem, this pr modifies the initialization logic of `forHost`. \r\nWhen `TaskLocation` is `HDFSCacheTaskLocation` or `HostTaskLocation`, check if executor is alive on the host.\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nIn my online environment it may cause the task to pending permanently, this pr aims to fix this issue.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nNo\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->\r\nAdd unit test", "failed_tests": ["org.apache.spark.scheduler.TaskSchedulerImplSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala", "additions": "10", "deletions": "2", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/TaskSchedulerImplSuite.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/TaskSetManagerSuite.scala", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [0, 0, 0]}]}
{"author": "venkata91", "sha": "ceb61360b16d10165e6844194c9821ee465a20ab", "commit_date": "2021/09/02 04:53:49", "commit_message": "SPARK-33701: Adaptive shuffle merge finalization", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/Dependency.scala", "additions": "31", "deletions": "4", "changes": "35"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/MapOutputTracker.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/SparkEnv.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "27", "deletions": "0", "changes": "27"}, "updated": [0, 1, 6]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala", "additions": "223", "deletions": "59", "changes": "282"}, "updated": [0, 0, 4]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerEvent.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessage.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [1, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/shuffle/ShuffleBlockPusher.scala", "additions": "38", "deletions": "1", "changes": "39"}, "updated": [0, 0, 7]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/DAGSchedulerSuite.scala", "additions": "113", "deletions": "3", "changes": "116"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/test/scala/org/apache/spark/shuffle/ShuffleBlockPusherSuite.scala", "additions": "33", "deletions": "0", "changes": "33"}, "updated": [0, 0, 5]}, {"file": {"name": "docs/configuration.md", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [0, 1, 4]}]}
{"author": "Kimahriman", "sha": "217960e726a5e168414d847a9cf82c9460c690d4", "commit_date": "2021/10/15 11:34:18", "commit_message": "Add codegen support to array transform", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala", "additions": "5", "deletions": "3", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "34", "deletions": "0", "changes": "34"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/higherOrderFunctions.scala", "additions": "405", "deletions": "8", "changes": "413"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [2, 2, 6]}]}
{"author": "Kimahriman", "sha": "a97d081c5287ecf5e60803c5d019000dc1c3a9e0", "commit_date": "2021/11/21 23:08:15", "commit_message": "Consolidate whole stage and non whole stage codegen code", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "43", "deletions": "114", "changes": "157"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/SubexpressionEliminationSuite.scala", "additions": "7", "deletions": "13", "changes": "20"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala", "additions": "23", "deletions": "17", "changes": "40"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "5", "deletions": "9", "changes": "14"}, "updated": [1, 1, 1]}]}
{"author": "zhengruifeng", "sha": "877558e439663d1028028e9a332a5e4e6a18ad6c", "commit_date": "2021/10/22 10:36:04", "commit_message": "init\n\nnit", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "64", "deletions": "1", "changes": "65"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [1, 1, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/rules/RuleIdCollection.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreePatterns.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [1, 5, 9]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/InsertRankLimitSuite.scala", "additions": "165", "deletions": "0", "changes": "165"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "13", "deletions": "0", "changes": "13"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/window/RankLimitExec.scala", "additions": "280", "deletions": "0", "changes": "280"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameWindowFunctionsSuite.scala", "additions": "121", "deletions": "0", "changes": "121"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/benchmark/TopKBenchmark.scala", "additions": "146", "deletions": "0", "changes": "146"}, "updated": [0, 0, 0]}]}
{"author": "Peng-Lei", "sha": "be72b42ebad77a16095af1f47fc0e82bb4afe472", "commit_date": "2021/12/02 09:50:41", "commit_message": "add draft", "title": "[SPARK-37517][SQL] Keep consistent order of columns with user specify for v1 table", "body": "### What changes were proposed in this pull request?\r\n1. keep columns order with user specified instead of put partition columns at last.\r\n2. Modify the `partitionSchema` and `dataSchema` implementation.\r\n\r\n### Why are the changes needed?\r\ndiscuss at [#34719](https://github.com/apache/spark/pull/34719#discussion_r758157813).\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\nAdd test case.\r\n", "failed_tests": ["org.apache.spark.ml.source.image.ImageFileFormatSuite", "org.apache.spark.sql.hive.execution.HiveDDLSuite", "org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/interface.scala", "additions": "6", "deletions": "2", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/rules.scala", "additions": "1", "deletions": "9", "changes": "10"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/charvarchar.sql.out", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/show-create-table.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/ShowCreateTableSuite.scala", "additions": "21", "deletions": "0", "changes": "21"}, "updated": [1, 1, 1]}]}
{"author": "Peng-Lei", "sha": "aa3bc5b799f62ccec5f85f2ee6a2515fad196b7c", "commit_date": "2021/12/01 08:55:41", "commit_message": "add draft", "title": "[SPARK-33898][SQL][FOLLOWUP] Unify the v2 behavior with v1 for `SHOW CREATE TABLE` command", "body": "### What changes were proposed in this pull request?\r\n1. Move the `SHOW CREATE TABLE w/ char/varchar` to `CharVarcharDDLTestBase`\r\n2. Fix the behavior different with v1 command that about the `TBLPROPERTIES`\r\n\r\n### Why are the changes needed?\r\nBefore the [#PR](https://github.com/apache/spark/pull/34719) merge. We should handle some different behavior or bugs between v1 and v2 command.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\nexisted test case.\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ShowCreateTableExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/CharVarcharTestSuite.scala", "additions": "0", "deletions": "10", "changes": "10"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [2, 8, 19]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/CharVarcharDDLTestBase.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [1, 1, 2]}]}
{"author": "Peng-Lei", "sha": "22356f775deacd193203bd6f50e72de2751f2797", "commit_date": "2021/11/25 03:14:50", "commit_message": "add draft", "title": "[SPARK-37381][SQL] Unify v1 and v2 SHOW CREATE TABLE tests", "body": "### What changes were proposed in this pull request?\r\n\r\n1. Move the `SHOW CREATE TABLE` testcase from `DDLParserSuite.scala` to `ShowCreateTableParserSuite.scala`.\r\n2. Move the `SHOW CREATE TABLE` testcase from `DataSourceV2SQLSuite.scala` to`v2.ShowCreateTableSuite` and extract some testcases to `ShowCreateTableBaseSuite`\r\n3. Merge some testcases into one testcase in `sql/ShowCreateTableSuite.scala` and move it to `v1.ShowCreateTableSuite`\r\n4. Move the testcase from `HiveShowCreateTableSuite` to `hive.ShowCreateTableSuite`\r\n5. Extract some testcase from `v1.ShowCreateTableSuite` and `hive.ShowCreateTableSuite` to `ShowCreateTableBaseSuite`\r\n6. Move the `SHOW CREATE TABLE` testcase from `CharVarcharTestSuite.scala` and `SQLViewSuite.scala` to `ShowCreateTableBaseSuite`\r\nThe changes follow the approach of [#30287](https://github.com/apache/spark/pull/30287) [#34305](https://github.com/apache/spark/pull/34305)\r\n\r\n### Why are the changes needed?\r\n\r\n1. The unification will allow to run common `SHOW CREATE TABLE` tests for both DSv1/Hive DSv1 and DSv2\r\n2. We can detect missing features and differences between DSv1 and DSv2 implementations.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\n$  build/sbt -Phive-2.3 -Phive-thriftserver \"test:testOnly *ShowCreateTableSuite\"\r\n\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala", "additions": "0", "deletions": "13", "changes": "13"}, "updated": [0, 3, 10]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/CharVarcharTestSuite.scala", "additions": "0", "deletions": "10", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/ShowCreateTableSuite.scala", "additions": "0", "deletions": "241", "changes": "241"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala", "additions": "0", "deletions": "107", "changes": "107"}, "updated": [1, 5, 15]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/SQLViewSuite.scala", "additions": "3", "deletions": "8", "changes": "11"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/ShowCreateTableParserSuite.scala", "additions": "37", "deletions": "0", "changes": "37"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/ShowCreateTableSuiteBase.scala", "additions": "145", "deletions": "0", "changes": "145"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/v1/ShowCreateTableSuite.scala", "additions": "140", "deletions": "0", "changes": "140"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/v2/ShowCreateTableSuite.scala", "additions": "141", "deletions": "0", "changes": "141"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/command/ShowCreateTableSuite.scala", "additions": "76", "deletions": "142", "changes": "218"}, "updated": [0, 0, 0]}]}
{"author": "yangwwei", "sha": "0f08a5b9ba9e4a7f78179d23cfbf82894d285617", "commit_date": "2021/11/19 23:04:10", "commit_message": "[SPARK-37394] Skip registering to ESS if a customized shuffle manager is configured.", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/shuffle/ShuffleManager.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/storage/BlockManager.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 1]}]}
{"author": "bozhang2820", "sha": "c99ef50fa9cb3b67c08b1180eae9d08ae021b808", "commit_date": "2021/12/02 15:16:34", "commit_message": "Fix test failure and add assersions on test class", "title": "[WIP] Use error-classes for spark-core errors - 1st batch", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nThis change is to refactor the 1st batch of errors in spark-core to use error-classes. \r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nThis is to follow the error class framework in spark-core.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nNo.\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->\r\nExisting unit tests. ", "failed_tests": ["org.apache.spark.SparkThrowableSuite"], "files": [{"file": {"name": "core/src/main/resources/error/error-classes.json", "additions": "39", "deletions": "0", "changes": "39"}, "updated": [0, 1, 6]}, {"file": {"name": "core/src/main/scala/org/apache/spark/executor/ProcfsMetricsGetter.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/serializer/GenericAvroSerializer.scala", "additions": "2", "deletions": "4", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/shuffle/IndexShuffleBlockResolver.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/DependencyUtils.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/ThreadUtils.scala", "additions": "6", "deletions": "3", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/Utils.scala", "additions": "20", "deletions": "20", "changes": "40"}, "updated": [0, 1, 1]}, {"file": {"name": "core/src/test/scala/org/apache/spark/rpc/RpcAddressSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/serializer/KryoSerializerSuite.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 1]}]}
{"author": "Yikun", "sha": "9b14667a48f7d22ac0a17d96de82591dc94f2f2f", "commit_date": "2021/11/15 08:11:56", "commit_message": "Add the ability to creating resources before driver pod", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesDriverSpec.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/KubernetesFeatureConfigStep.scala", "additions": "9", "deletions": "1", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/submit/KubernetesClientApplication.scala", "additions": "16", "deletions": "1", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/submit/KubernetesDriverBuilder.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/DriverCommandFeatureStepSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/submit/ClientSuite.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}]}
{"author": "sathiyapk", "sha": "1604b99ab61fee5eaeb6a05a92a520f500fc8ecb", "commit_date": "2021/11/28 14:22:15", "commit_message": "[SPARK-37475][SQL] Adds scale parameeter to floor and ceil functions", "title": "[SPARK-37475][SQL] Add scale parameter to floor and ceil functions", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nAdds `scale` parameter to `floor`/`ceil` functions in order to allow users to control the rounding position. This feature is proposed in the PR: https://github.com/apache/spark/pull/34593\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nCurrently we support Decimal RoundingModes : HALF_UP (round) and HALF_EVEN (bround). But we have use cases that needs RoundingMode.UP and RoundingMode.DOWN.\r\n\r\nFloor and Ceil functions helps to do this but it doesn't support the position of the rounding. Adding scale parameter to the functions would help us control the rounding positions. \r\n\r\nSnowflake supports `scale` parameter to `floor`/`ceil` :\r\n` FLOOR( <input_expr> [, <scale_expr> ] )`\r\n\r\nREF:\r\nhttps://docs.snowflake.com/en/sql-reference/functions/floor.html\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nNow users can pass `scale` parameter to the `floor` and `ceil` functions.\r\n ```\r\n     > SELECT floor(-0.1);\r\n       -1.0\r\n      > SELECT floor(5);\r\n       5\r\n      > SELECT floor(3.1411, 3);\r\n       3.141\r\n      > SELECT floor(3.1411, -3);\r\n       1000.0\r\n\r\n      > SELECT ceil(-0.1);\r\n       0.0\r\n      > SELECT ceil(5);\r\n       5\r\n      > SELECT ceil(3.1411, 3);\r\n       3.142\r\n      > SELECT ceil(3.1411, -3);\r\n       1000.0\r\n\r\n```\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\nIf benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.\r\n-->\r\nThis patch was tested locally using unit test and git workflow.", "failed_tests": ["pyspark.sql.tests.test_pandas_grouped_map", "org.apache.spark.sql.catalyst.analysis.ResolveAliasesSuite", "org.apache.spark.sql.kafka010.KafkaMicroBatchV2SourceSuite", "org.apache.spark.sql.kafka010.KafkaMicroBatchV1SourceWithAdminSuite", "org.apache.spark.sql.kafka010.KafkaMicroBatchV1SourceSuite", "org.apache.spark.sql.kafka010.KafkaMicroBatchV2SourceWithAdminSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.ExpressionsSchemaSuite", "org.apache.spark.sql.streaming.StreamingAggregationSuite", "org.apache.spark.sql.StatisticsCollectionSuite", "org.apache.spark.sql.streaming.FileStreamSinkV1Suite", "org.apache.spark.sql.streaming.StreamingOuterJoinSuite", "org.apache.spark.sql.streaming.StreamingDeduplicationSuite", "org.apache.spark.sql.streaming.ui.UISeleniumSuite", "org.apache.spark.sql.streaming.StreamingInnerJoinSuite", "org.apache.spark.sql.execution.streaming.MicroBatchExecutionSuite", "org.apache.spark.sql.expressions.ExpressionInfoSuite", "org.apache.spark.sql.streaming.StreamingLeftSemiJoinSuite", "org.apache.spark.sql.streaming.EventTimeWatermarkSuite", "org.apache.spark.sql.streaming.StreamingFullOuterJoinSuite", "org.apache.spark.sql.streaming.FileStreamSinkV2Suite", "org.apache.spark.sql.execution.streaming.sources.ForeachWriterSuite", "org.apache.spark.sql.DataFrameTimeWindowingSuite", "org.apache.spark.sql.execution.PlannerSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/mathExpressions.scala", "additions": "29", "deletions": "57", "changes": "86"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/MathExpressionsSuite.scala", "additions": "122", "deletions": "44", "changes": "166"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/functions.scala", "additions": "22", "deletions": "6", "changes": "28"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/core/src/test/resources/sql-functions/sql-expression-schema.md", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/operators.sql.out", "additions": "11", "deletions": "11", "changes": "22"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/pivot.sql.out", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/postgreSQL/float8.sql.out", "additions": "18", "deletions": "18", "changes": "36"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/postgreSQL/numeric.sql.out", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/udf/udf-pivot.sql.out", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}]}
{"author": "sathiyapk", "sha": "e7f62614f316756f9989099ec73fc54f5d73a05d", "commit_date": "2021/11/14 22:35:58", "commit_message": "SPARK-37324 Adds support for decimal rounding mode up, down, half_down", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/mathExpressions.scala", "additions": "57", "deletions": "19", "changes": "76"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/expressions.scala", "additions": "16", "deletions": "1", "changes": "17"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/types/Decimal.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/PhysicalAggregationSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/functions.scala", "additions": "14", "deletions": "4", "changes": "18"}, "updated": [0, 2, 5]}, {"file": {"name": "sql/core/src/test/resources/sql-functions/sql-expression-schema.md", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/postgreSQL/numeric.sql.out", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q2.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q2/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q78.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q78/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-query-results/v1_4/q2.sql.out", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/MathFunctionsSuite.scala", "additions": "75", "deletions": "0", "changes": "75"}, "updated": [0, 0, 2]}]}
{"author": "sleep1661", "sha": "c30d443d5e113bba870f4c15fbf9afcf95acfbb3", "commit_date": "2021/11/11 12:31:43", "commit_message": "[SPARK-37300][CORE] TaskSchedulerImpl should ignore task finished event if its task was finished state", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala", "additions": "8", "deletions": "1", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/TaskSchedulerImplSuite.scala", "additions": "94", "deletions": "1", "changes": "95"}, "updated": [2, 2, 2]}]}
{"author": "kevincmchen", "sha": "7bec4799fc45d750100768c10be190c0f9b18486", "commit_date": "2021/11/29 13:21:34", "commit_message": " use the same jar class loader in HiveSessionResourceLoader and HiveClient, to avoid the bug like 'loader constraint violation'", "title": "[SPARK-37486][SQL][HIVE]  set the ContextClassLoader before using the `addJars` in `HiveClient` ", "body": "### What changes were proposed in this pull request?\r\nIn `HiveSessionResourceLoader`,  call the  function `addJars` of its supper class firstly and set the contextClassLoader in current thread, and then  use the `addJars` in `HiveClient`. to avoid that different class loaders load the same class. \r\n\r\n### Why are the changes needed?\r\n\r\nwhen using livy to execute sql statements that will use the udf jars located in lakefs, a inner filesystem in Tencent Cloud DLC. it will threw the following exceptions:\r\n\r\n`java.lang.LinkageError: loader constraint violation: loader (instance of sun/misc/Launcher$AppClassLoader) previously initiated loading for a different type with name`     \r\n\r\nMaybe it happens  in some other filesystem.\r\n\r\nThis PR fix the bug  by  setting the setContextClassLoader before using the addJars in HiveClient\r\n\r\n### Why are the changes needed?\r\n\r\nfix  the bug that different class loaders  load the same class. \r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nNO\r\n\r\n### How was this patch tested?\r\n\r\nexisting testsuites", "failed_tests": [], "files": [{"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveSessionStateBuilder.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 2]}]}
{"author": "seayoun", "sha": "ea5c574c8736049efe36bf1e0fb5751590a7236f", "commit_date": "2021/11/27 06:51:20", "commit_message": "mark shuffle partition if it is not empty to cover segment file not exists when disk is missing.", "title": "[SPARK-37473][CORE] BypassMergeSortShuffleWriter may loss data when disk is missing however catagory is present", "body": "### What changes were proposed in this pull request?\r\nWe think it has no data when the segment file not exists when all segment files produced by `BypassMergeSortShuffleWriter` is merging;\r\n\r\nHowever, `file.exists()` may rerurn `false` when then the disk which segment file in on is missing and the root catagory exists; the missing disk only lead `file.exists()` return `false` but no exception. The task will run in peace without current segment file written.\r\n\r\nThe segment data will be ignored  and leading shuffle data loss.\r\n\r\n\r\n### Why are the changes needed?\r\nShuffle may loss partition data leading corretness problems in final data.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n\r\n### How was this patch tested?\r\n the task finished as normal without exception when I delete  a partition file before segment files merge\uff0c and the final data is incorrect;\r\nThe task will retry when apply this patch\r\n", "failed_tests": [], "files": [{"file": {"name": "core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java", "additions": "28", "deletions": "16", "changes": "44"}, "updated": [0, 0, 0]}]}
{"author": "Yaohua628", "sha": "8b8b9fa5da89dd60c7f9dd88de3c117bf32b6cdb", "commit_date": "2021/11/12 00:41:46", "commit_message": "first draft", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/namedExpressions.scala", "additions": "20", "deletions": "1", "changes": "21"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/LogicalPlan.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Relation.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala", "additions": "28", "deletions": "5", "changes": "33"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/PartitionedFileUtil.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormat.scala", "additions": "24", "deletions": "1", "changes": "25"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileScanRDD.scala", "additions": "128", "deletions": "4", "changes": "132"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/LogicalRelation.scala", "additions": "24", "deletions": "2", "changes": "26"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SchemaPruning.scala", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/FileMetadataStructSuite.scala", "additions": "385", "deletions": "0", "changes": "385"}, "updated": [0, 0, 0]}]}
