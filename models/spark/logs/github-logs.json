{"author": "ChenMichael", "sha": "", "commit_date": "2021/09/17 16:32:01", "commit_message": "TBD: trigger???", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/ExplainUtils.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/ExplainSuite.scala", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [0, 0, 5]}]}
{"author": "f-thiele", "sha": "", "commit_date": "2021/09/16 15:24:44", "commit_message": "[SPARK-36782] Avoid blocking dispatcher-BlockManagerMaster during UpdateBlockInfo\n\nDelegate task to threadpool and register callback for succesful\ncompletion. Reply to caller once future finished succesfully.", "title": "", "body": "", "failed_tests": ["pyspark.mllib.tests.test_algorithms", "pyspark.ml.tests.test_algorithms", "org.apache.spark.graphx.lib.PageRankSuite", "org.apache.spark.ml.tuning.JavaCrossValidatorSuite", "org.apache.spark.mllib.clustering.KMeansSuite", "org.apache.spark.ml.classification.DecisionTreeClassifierSuite", "org.apache.spark.ml.tree.impl.GradientBoostedTreesSuite", "org.apache.spark.ml.classification.LogisticRegressionSuite", "org.apache.spark.ml.tuning.CrossValidatorSuite", "org.apache.spark.mllib.classification.LogisticRegressionSuite", "org.apache.spark.ml.classification.GBTClassifierSuite", "org.apache.spark.ml.regression.LinearRegressionSuite", "org.apache.spark.ml.regression.DecisionTreeRegressorSuite", "org.apache.spark.ml.regression.RandomForestRegressorSuite", "org.apache.spark.ml.regression.GBTRegressorSuite", "org.apache.spark.ml.tuning.JavaCrossValidatorSuite", "org.apache.spark.ml.clustering.PowerIterationClusteringSuite", "org.apache.spark.ml.classification.RandomForestClassifierSuite", "org.apache.spark.ml.classification.FMClassifierSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala", "additions": "11", "deletions": "6", "changes": "17"}, "updated": [0, 1, 5]}, {"file": {"name": "core/src/test/scala/org/apache/spark/storage/BlockManagerDecommissionIntegrationSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "viirya", "sha": "8db8b50e0621b46e6572ec72c1cc9aeca2a66807", "commit_date": "2021/09/22 18:49:34", "commit_message": "For review comment.", "title": "[SPARK-36797][SQL] Union should resolve nested columns as top-level columns", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\nThis patch proposes to generalize the resolving-by-position behavior to nested columns for Union.\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\nUnion, by the API definition, resolves columns by position. Currently we only follow this behavior at top-level columns, but not nested columns.\r\n\r\nAs we are making nested columns as first-class citizen, the nested-column-only limitation and the difference between top-level column and nested column do not make sense. We should also resolve nested columns like top-level columns for Union.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\nYes. After this change, Union also resolves nested columns by position.\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\n\r\nAdded tests.\r\n", "failed_tests": ["org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SQLQuerySuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala", "additions": "19", "deletions": "4", "changes": "23"}, "updated": [0, 0, 7]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/types/DataType.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSetOperationsSuite.scala", "additions": "32", "deletions": "9", "changes": "41"}, "updated": [0, 2, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala", "additions": "3", "deletions": "4", "changes": "7"}, "updated": [0, 0, 3]}]}
{"author": "viirya", "sha": "82ccaf18d64f46ffe7b5452f001a8ea68931c439", "commit_date": "2021/09/20 21:46:52", "commit_message": "Update for non-DPP case.", "title": "[SPARK-36809][SQL] Remove broadcast for InSubqueryExec used in DPP", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\nThis patch proposes to remove broadcast variable in `InSubqueryExec` which is used in DPP.\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\nCurrently we include a broadcast variable in `InSubqueryExec`. We use it to hold filtering side query result of DPP. It looks weird because we don't use the result in executors but only need the result in the driver during query planning. We already hold the original result, so basically we hold two copied of query result at this moment.\r\n\r\nAnother thing related is, in `pruningHasBenefit` we estimate if DPP pruning has benefit when the join type does not support broadcast. Due to the broadcast variable above, we also check the filtering side against the config `autoBroadcastJoinThreshold`. The config is not for the purpose and it is not a broadcast join. As the broadcast variable is unnecessary, we can remove this check and leave benefit estimation to overhead and pruning size.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\nYes. We don't rely on `autoBroadcastJoinThreshold` for non-broadcast join DPP.\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\n\r\nUpdated existing test.\r\n", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/PlanAdaptiveDynamicPruningFilters.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/PlanAdaptiveSubqueries.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/dynamicpruning/PartitionPruning.scala", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 2, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala", "additions": "14", "deletions": "9", "changes": "23"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DynamicPartitionPruningSuite.scala", "additions": "8", "deletions": "7", "changes": "15"}, "updated": [0, 2, 3]}]}
{"author": "viirya", "sha": "ba4172076f3f8030510632978a0e47d5b720617a", "commit_date": "2021/07/04 06:37:05", "commit_message": "Use config.", "title": "[WIP][SPARK-33625][SQL] Subexpression elimination for whole-stage codegen in Filter", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\nThis patch proposes to enable whole-stage subexpression elimination for Filter.\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\nWe made subexpression elimination available for whole-stage codegen in ProjectExec. Another one operator that frequently runs into subexpressions, is Filter. We should also make whole-stage codegen subexpression elimination in FilterExec too.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\nNo\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\n\r\nUnit test", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [1, 2, 5]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [2, 3, 10]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 3]}]}
{"author": "viirya", "sha": "", "commit_date": "2021/04/15 01:29:22", "commit_message": "Fix custom metric initialization.", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/metric/CustomMetrics.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLAppStatusListener.scala", "additions": "28", "deletions": "12", "changes": "40"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SparkPlanGraph.scala", "additions": "2", "deletions": "4", "changes": "6"}, "updated": [0, 0, 0]}]}
{"author": "karenfeng", "sha": "", "commit_date": "2021/06/09 21:22:58", "commit_message": "Fix docs issue\n\nSigned-off-by: Karen Feng <karen.feng@databricks.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/SparkError.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "ulysses-you", "sha": "2aa2ee06a8110a9d2d7efe36aadc83afd75b71c8", "commit_date": "2021/09/23 01:33:46", "commit_message": "broadcast", "title": "[SPARK-36823][SQL] Support broadcast nested loop join hint for equi-join", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nAdd a new hint `BROADCAST_NL`\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nFor the join if one side is small and other side is large, the shuffle overhead is also very big. Due to the\r\nbhj limitation, we can only broadcast right side for left join and left side for right join. So for the other case, we can try to use `BroadcastNestedLoopJoin` as the join strategy.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nyes, new hint\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nAdd new test in `JoinHintSuite`", "failed_tests": [], "files": [{"file": {"name": "docs/sql-ref-syntax-qry-select-hints.md", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/hints.scala", "additions": "10", "deletions": "1", "changes": "11"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [1, 2, 6]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/LogicalQueryStageStrategy.scala", "additions": "12", "deletions": "6", "changes": "18"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/JoinHintSuite.scala", "additions": "38", "deletions": "0", "changes": "38"}, "updated": [0, 0, 2]}]}
{"author": "ulysses-you", "sha": "a846ecd5221bc4b21416c9c52552cdaa0e683d0d", "commit_date": "2021/09/17 02:02:01", "commit_message": "cleanup", "title": "[SPARK-34980][SQL] Support coalesce partition through union in AQE", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n- Split plan into several groups, and every child of union is a new group\r\n- Coalesce paritition for every group\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\n#### First Issue\r\nThe rule `CoalesceShufflePartitions` can only coalesce paritition if\r\n* leaf node is ShuffleQueryStage\r\n* all shuffle have same partition number\r\n\r\nWith `Union`, it might break the assumption. Let's say we have such plan\r\n```\r\nUnion\r\n   HashAggregate\r\n      ShuffleQueryStage\r\n   FileScan\r\n```\r\n`CoalesceShufflePartitions` can not optimize it and the result partition would be `shuffle partition + FileScan partition` which can be quite lagre.\r\n\r\nIt's better to support partial optimize with `Union`.\r\n\r\n#### Second Issue\r\nthe coalesce partition formule used the **sum value** as the total input size and it's not friendly for union, see\r\n```\r\n// ShufflePartitionsUtil.coalescePartitions\r\nval totalPostShuffleInputSize = mapOutputStatistics.flatMap(_.map(_.bytesByPartitionId.sum)).sum\r\n```\r\n\r\nSo for such case:\r\n```\r\nUnion\r\n   HashAggregate\r\n      ShuffleQueryStage\r\n   HashAggregate\r\n      ShuffleQueryStage\r\n```\r\nThe `CoalesceShufflePartitions` rule will return an unexpected partition number.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nProbably yes, the result partition might changed.\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nAdd test.", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "78", "deletions": "52", "changes": "130"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/CoalesceShufflePartitionsSuite.scala", "additions": "1", "deletions": "3", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "86", "deletions": "1", "changes": "87"}, "updated": [0, 1, 7]}]}
{"author": "ulysses-you", "sha": "5beb51810dfec69964e570d90d0634e5a8e0499d", "commit_date": "2021/08/13 14:26:41", "commit_message": "fix", "title": "[SPARK-36321][K8S] Do not fail application in kubernetes if name is too long", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nUse short string as executor pod name prefix if app name is long.\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nIf we have a long spark app name and start with k8s master, we will get the execption.\r\n```\r\njava.lang.IllegalArgumentException: 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa-89fe2f7ae71c3570' in spark.kubernetes.executor.podNamePrefix is invalid. must conform https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#dns-label-names and the value length <= 47\r\n\tat org.apache.spark.internal.config.TypedConfigBuilder.$anonfun$checkValue$1(ConfigBuilder.scala:108)\r\n\tat org.apache.spark.internal.config.TypedConfigBuilder.$anonfun$transform$1(ConfigBuilder.scala:101)\r\n\tat scala.Option.map(Option.scala:230)\r\n\tat org.apache.spark.internal.config.OptionalConfigEntry.readFrom(ConfigEntry.scala:239)\r\n\tat org.apache.spark.internal.config.OptionalConfigEntry.readFrom(ConfigEntry.scala:214)\r\n\tat org.apache.spark.SparkConf.get(SparkConf.scala:261)\r\n\tat org.apache.spark.deploy.k8s.KubernetesConf.get(KubernetesConf.scala:67)\r\n\tat org.apache.spark.deploy.k8s.KubernetesExecutorConf.<init>(KubernetesConf.scala:147)\r\n\tat org.apache.spark.deploy.k8s.KubernetesConf$.createExecutorConf(KubernetesConf.scala:231)\r\n\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$requestNewExecutors$2(ExecutorPodsAllocator.scala:367)\r\n```\r\nUse app name as the executor pod name is the Spark internal behavior and we should not make application failure.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nyes\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nAdd test\r\n\r\nthe new log:\r\n```\r\n21/07/28 09:35:53 INFO SparkEnv: Registering OutputCommitCoordinator\r\n21/07/28 09:35:54 INFO Utils: Successfully started service 'SparkUI' on port 41926.\r\n21/07/28 09:35:54 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://:41926\r\n21/07/28 09:35:54 WARN KubernetesClusterManager: Use spark-c460617aeac0fda9 as the executor pod's name prefix due to spark.app.name is too long. Please set 'spark.kubernetes.executor.podNamePrefix' if you need a custom executor pod's name prefix.\r\n21/07/28 09:35:54 INFO SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file\r\n21/07/28 09:35:55 WARN Utils: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.\r\n```\r\n\r\nverify the config:\r\n![image](https://user-images.githubusercontent.com/12025282/127258223-fbcaaac8-451d-4c55-8c09-e802511a510d.png)\r\n\r\nverify the executor pod name\r\n![image](https://user-images.githubusercontent.com/12025282/127258284-be15b862-b826-4440-9a11-023d69c61fc4.png)\r\n", "failed_tests": [], "files": [{"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPi.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPiWithoutAppName.scala", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "4", "deletions": "14", "changes": "18"}, "updated": [1, 1, 5]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala", "additions": "19", "deletions": "7", "changes": "26"}, "updated": [0, 0, 3]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesUtils.scala", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala", "additions": "10", "deletions": "6", "changes": "16"}, "updated": [0, 0, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterManager.scala", "additions": "10", "deletions": "2", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/KubernetesConfSuite.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 0, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 3]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/BasicTestsSuite.scala", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 1, 1]}]}
{"author": "ulysses-you", "sha": "", "commit_date": "2020/12/15 01:33:39", "commit_message": "Merge remote-tracking branch 'upstream/master'", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": ".github/workflows/build_and_test.yml", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 6, 14]}, {"file": {"name": "LICENSE-binary", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "core/pom.xml", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [1, 3, 5]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/PythonRunner.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala", "additions": "29", "deletions": "25", "changes": "54"}, "updated": [1, 3, 8]}, {"file": {"name": "core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 1, 3]}, {"file": {"name": "dev/deps/spark-deps-hadoop-2.7-hive-2.3", "additions": "14", "deletions": "13", "changes": "27"}, "updated": [3, 8, 17]}, {"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "14", "deletions": "12", "changes": "26"}, "updated": [3, 8, 17]}, {"file": {"name": "dev/tox.ini", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 2, 2]}, {"file": {"name": "docs/running-on-kubernetes.md", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [1, 2, 5]}, {"file": {"name": "docs/sql-migration-guide.md", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [2, 4, 14]}, {"file": {"name": "docs/sql-ref-ansi-compliance.md", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [1, 4, 9]}, {"file": {"name": "external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaOffsetReaderAdmin.scala", "additions": "25", "deletions": "56", "changes": "81"}, "updated": [0, 2, 2]}, {"file": {"name": "external/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaRelationSuite.scala", "additions": "5", "deletions": "18", "changes": "23"}, "updated": [0, 2, 3]}, {"file": {"name": "launcher/src/main/java/org/apache/spark/launcher/SparkSubmitCommandBuilder.java", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "pom.xml", "additions": "5", "deletions": "6", "changes": "11"}, "updated": [3, 10, 24]}, {"file": {"name": "project/SparkBuild.scala", "additions": "26", "deletions": "1", "changes": "27"}, "updated": [1, 2, 5]}, {"file": {"name": "python/pyspark/context.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 2, 6]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "32", "deletions": "4", "changes": "36"}, "updated": [2, 3, 6]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Constants.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 1, 3]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/SparkPod.scala", "additions": "10", "deletions": "1", "changes": "11"}, "updated": [1, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/DriverCommandFeatureStep.scala", "additions": "32", "deletions": "5", "changes": "37"}, "updated": [1, 1, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/KubernetesFeatureConfigStep.scala", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [1, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/submit/KubernetesDriverBuilder.scala", "additions": "7", "deletions": "1", "changes": "8"}, "updated": [1, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/ExecutorPodsSnapshot.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 1, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesExecutorBuilder.scala", "additions": "7", "deletions": "1", "changes": "8"}, "updated": [1, 1, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/PodBuilderSuite.scala", "additions": "76", "deletions": "0", "changes": "76"}, "updated": [1, 1, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/DriverCommandFeatureStepSuite.scala", "additions": "50", "deletions": "7", "changes": "57"}, "updated": [1, 1, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/submit/KubernetesDriverBuilderSuite.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [1, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesExecutorBuilderSuite.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [1, 1, 2]}, {"file": {"name": "resource-managers/kubernetes/docker/src/main/dockerfiles/spark/entrypoint.sh", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [1, 1, 3]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/README.md", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/DepsTestsSuite.scala", "additions": "63", "deletions": "22", "changes": "85"}, "updated": [1, 3, 4]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [1, 3, 5]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesTestComponents.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [1, 1, 2]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/ProcessUtils.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [1, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/Utils.scala", "additions": "7", "deletions": "2", "changes": "9"}, "updated": [1, 2, 5]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/tests/py_container_checks.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/tests/python_executable_check.py", "additions": "40", "deletions": "0", "changes": "40"}, "updated": [1, 1, 1]}, {"file": {"name": "resource-managers/yarn/pom.xml", "additions": "0", "deletions": "7", "changes": "7"}, "updated": [1, 3, 5]}, {"file": {"name": "sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 2, 5]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/SupportsPartitionManagement.java", "additions": "9", "deletions": "5", "changes": "14"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/distributions/ClusteredDistribution.java", "additions": "35", "deletions": "0", "changes": "35"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/distributions/Distribution.java", "additions": "28", "deletions": "0", "changes": "28"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/distributions/Distributions.java", "additions": "56", "deletions": "0", "changes": "56"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/distributions/OrderedDistribution.java", "additions": "35", "deletions": "0", "changes": "35"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/distributions/UnspecifiedDistribution.java", "additions": "28", "deletions": "0", "changes": "28"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/expressions/Expressions.java", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/expressions/NullOrdering.java", "additions": "42", "deletions": "0", "changes": "42"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/expressions/SortDirection.java", "additions": "42", "deletions": "0", "changes": "42"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/expressions/SortOrder.java", "additions": "43", "deletions": "0", "changes": "43"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/RequiresDistributionAndOrdering.java", "additions": "57", "deletions": "0", "changes": "57"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/Write.java", "additions": "65", "deletions": "0", "changes": "65"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/WriteBuilder.java", "additions": "29", "deletions": "12", "changes": "41"}, "updated": [2, 2, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/QueryCompilationErrors.scala", "additions": "167", "deletions": "2", "changes": "169"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala", "additions": "72", "deletions": "75", "changes": "147"}, "updated": [1, 11, 33]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [1, 7, 16]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolvePartitionSpec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 3, 8]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/SessionCatalog.scala", "additions": "59", "deletions": "11", "changes": "70"}, "updated": [1, 6, 11]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/CallMethodViaReflection.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala", "additions": "19", "deletions": "8", "changes": "27"}, "updated": [1, 3, 11]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala", "additions": "61", "deletions": "1", "changes": "62"}, "updated": [0, 2, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala", "additions": "37", "deletions": "8", "changes": "45"}, "updated": [3, 12, 33]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/ParseDriver.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala", "additions": "0", "deletions": "16", "changes": "16"}, "updated": [1, 4, 9]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/statements.scala", "additions": "0", "deletions": "8", "changes": "8"}, "updated": [1, 6, 16]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2Commands.scala", "additions": "29", "deletions": "3", "changes": "32"}, "updated": [2, 8, 21]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala", "additions": "7", "deletions": "4", "changes": "11"}, "updated": [1, 2, 5]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/NumberConverter.scala", "additions": "13", "deletions": "51", "changes": "64"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/distributions/distributions.scala", "additions": "59", "deletions": "0", "changes": "59"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/expressions/expressions.scala", "additions": "96", "deletions": "0", "changes": "96"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "24", "deletions": "1", "changes": "25"}, "updated": [1, 14, 37]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisSuite.scala", "additions": "22", "deletions": "0", "changes": "22"}, "updated": [1, 3, 7]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/catalog/ExternalCatalogSuite.scala", "additions": "7", "deletions": "2", "changes": "9"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CastSuite.scala", "additions": "34", "deletions": "7", "changes": "41"}, "updated": [1, 3, 9]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CodeGenerationSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 2]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/MathExpressionsSuite.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [1, 2, 2]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [0, 2, 4]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala", "additions": "37", "deletions": "6", "changes": "43"}, "updated": [2, 9, 24]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/NumberConverterSuite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/TimestampFormatterSuite.scala", "additions": "8", "deletions": "2", "changes": "10"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/InMemoryPartitionTable.scala", "additions": "0", "deletions": "3", "changes": "3"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/SupportsPartitionManagementSuite.scala", "additions": "21", "deletions": "2", "changes": "23"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [1, 4, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala", "additions": "4", "deletions": "19", "changes": "23"}, "updated": [2, 9, 22]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala", "additions": "5", "deletions": "29", "changes": "34"}, "updated": [1, 3, 13]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/command/cache.scala", "additions": "2", "deletions": "50", "changes": "52"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/command/views.scala", "additions": "4", "deletions": "15", "changes": "19"}, "updated": [2, 4, 7]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/CacheTableExec.scala", "additions": "89", "deletions": "0", "changes": "89"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala", "additions": "20", "deletions": "4", "changes": "24"}, "updated": [2, 9, 22]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/RefreshTableExec.scala", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [1, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ShowPartitionsExec.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [1, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetScanBuilder.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStore.scala", "additions": "11", "deletions": "15", "changes": "26"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreConf.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/functions.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 7]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/internal/CatalogImpl.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/internal/SharedState.scala", "additions": "10", "deletions": "3", "changes": "13"}, "updated": [0, 2, 4]}, {"file": {"name": "sql/core/src/test/resources/hive-site.xml", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/datetime.sql", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [1, 3, 5]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/string-functions.sql", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/datetime.sql.out", "additions": "10", "deletions": "1", "changes": "11"}, "updated": [1, 3, 5]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/string-functions.sql.out", "additions": "67", "deletions": "1", "changes": "68"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/datetime-legacy.sql.out", "additions": "9", "deletions": "1", "changes": "10"}, "updated": [1, 3, 5]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/datetime.sql.out", "additions": "9", "deletions": "1", "changes": "10"}, "updated": [1, 3, 5]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/string-functions.sql.out", "additions": "67", "deletions": "1", "changes": "68"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/CachedTableSuite.scala", "additions": "46", "deletions": "0", "changes": "46"}, "updated": [1, 4, 7]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "38", "deletions": "0", "changes": "38"}, "updated": [1, 3, 4]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/ExplainSuite.scala", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [2, 2, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/MathFunctionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala", "additions": "27", "deletions": "14", "changes": "41"}, "updated": [2, 11, 34]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/SQLViewSuite.scala", "additions": "8", "deletions": "17", "changes": "25"}, "updated": [2, 5, 15]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/SQLViewTestSuite.scala", "additions": "50", "deletions": "1", "changes": "51"}, "updated": [1, 5, 5]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/SparkSqlParserSuite.scala", "additions": "0", "deletions": "29", "changes": "29"}, "updated": [0, 2, 6]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/AlterTableAddPartitionSuiteBase.scala", "additions": "19", "deletions": "1", "changes": "20"}, "updated": [1, 3, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/ShowPartitionsSuiteBase.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/v1/AlterTableAddPartitionSuite.scala", "additions": "1", "deletions": "19", "changes": "20"}, "updated": [0, 2, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/v2/AlterTableAddPartitionSuite.scala", "additions": "1", "deletions": "18", "changes": "19"}, "updated": [0, 2, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/StateStoreSuite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 1, 4]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/internal/SharedStateSuite.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkExecuteStatementOperation.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 2, 5]}, {"file": {"name": "sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkSQLSessionManager.scala", "additions": "5", "deletions": "3", "changes": "8"}, "updated": [1, 1, 3]}, {"file": {"name": "sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/HiveThriftServer2Suites.scala", "additions": "118", "deletions": "81", "changes": "199"}, "updated": [1, 1, 3]}, {"file": {"name": "sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/JdbcConnectionUriSuite.scala", "additions": "0", "deletions": "70", "changes": "70"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/SparkMetadataOperationSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/SparkThriftServerProtocolVersionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/ThriftServerWithSparkContextSuite.scala", "additions": "37", "deletions": "0", "changes": "37"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/UISeleniumSuite.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/hive/compatibility/src/test/scala/org/apache/spark/sql/hive/execution/HiveCompatibilitySuite.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [1, 3, 5]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala", "additions": "13", "deletions": "2", "changes": "15"}, "updated": [0, 3, 6]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveExternalCatalogVersionsSuite.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/StatisticsSuite.scala", "additions": "8", "deletions": "4", "changes": "12"}, "updated": [1, 3, 6]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/client/VersionsSuite.scala", "additions": "22", "deletions": "1", "changes": "23"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/HiveDDLSuite.scala", "additions": "103", "deletions": "31", "changes": "134"}, "updated": [2, 6, 10]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/command/AlterTableAddPartitionSuite.scala", "additions": "0", "deletions": "18", "changes": "18"}, "updated": [0, 2, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/test/TestHive.scala", "additions": "2", "deletions": "3", "changes": "5"}, "updated": [0, 2, 7]}]}
{"author": "HyukjinKwon", "sha": "", "commit_date": "2020/08/12 10:07:02", "commit_message": "Test uploading Junit test report artifact", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": ".github/workflows/master.yml", "additions": "12", "deletions": "139", "changes": "151"}, "updated": [0, 3, 9]}, {"file": {"name": ".github/workflows/test_report.yml", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [0, 0, 0]}, {"file": {"name": "common/unsafe/src/test/scala/org/apache/spark/unsafe/types/UTF8StringPropertyCheckSuite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "dev/run-tests.py", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [0, 4, 11]}, {"file": {"name": "python/pyspark/sql/tests/test_arrow.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 2, 6]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 1, 3]}]}
{"author": "pralabhkumar", "sha": "a659e4cbdbef6f2fc7fd8c1d7749a545ad275f32", "commit_date": "2021/09/17 19:54:17", "commit_message": "Commented test cases which suppose to fail for ArrayType(TimeStamp)", "title": "[WIP][SPARK-32285][PYTHON] Add PySpark support for nested timestamps with arrow", "body": "### What changes were proposed in this pull request?\r\nAdded nested timestamp support for Pyspark with Arrow\r\n\r\n\r\n\r\n### Why are the changes needed?\r\nThis change is required to convert ArrayType(TimeStamp) to pandas via arrow. \r\n\r\n\r\n### Does this PR introduce any user-facing change?\r\nYes user will be able to convert DF which contain Arraytype(Timestamp) to pandas\r\n\r\n### How was this patch tested?\r\nunit tests", "failed_tests": ["pyspark.pandas.tests.data_type_ops.test_complex_ops", "pyspark.pandas.tests.test_dataframe"], "files": [{"file": {"name": "python/pyspark/sql/pandas/conversion.py", "additions": "23", "deletions": "6", "changes": "29"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/sql/pandas/types.py", "additions": "43", "deletions": "6", "changes": "49"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_arrow.py", "additions": "72", "deletions": "53", "changes": "125"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_pandas_cogrouped_map.py", "additions": "11", "deletions": "11", "changes": "22"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/tests/test_pandas_grouped_map.py", "additions": "11", "deletions": "11", "changes": "22"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/tests/test_pandas_udf_grouped_agg.py", "additions": "13", "deletions": "13", "changes": "26"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/tests/test_pandas_udf_scalar.py", "additions": "11", "deletions": "11", "changes": "22"}, "updated": [0, 0, 0]}]}
{"author": "pralabhkumar", "sha": "73565a28a55ee52266f04416ba7b1bbe1dcb94dc", "commit_date": "2021/09/13 06:44:48", "commit_message": "Removed curly braces for single import", "title": "[SPARK-36622][CORE] Making spark.history.kerberos.principal _HOST compliant", "body": "### What changes were proposed in this pull request?\r\nspark.history.kerberos.principal can have _HOST , which will be replaced by host canonical address\r\n\r\n### Why are the changes needed?\r\nThis change is required for user to provide prinicipal _HOST complaint . User don't need to hardcode the History server URL . This is in line with Hiveserver2, livy server and other hadoop components.  \r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nYes, users can now add _HOST in the spark.history.kerberos.principal\r\n\r\n### How was this patch tested?\r\n\r\nunit tests/local testing\r\n\r\n", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/deploy/SparkHadoopUtilSuite.scala", "additions": "13", "deletions": "0", "changes": "13"}, "updated": [0, 0, 0]}]}
{"author": "pralabhkumar", "sha": "", "commit_date": "2021/09/03 18:18:41", "commit_message": "Dummy commit", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}]}
{"author": "q2w", "sha": "beb0ef17e61581d9b0996860f89af7c6267743f6", "commit_date": "2021/06/07 05:25:55", "commit_message": "protect access to executorsPendingDecommission by CoarseGrainedSchedulerBackend", "title": "[SPARK-35627][CORE] Decommission executors in batches to not overload network bandwidth", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error message, please read the guideline first:\r\n     https://spark.apache.org/error-message-guidelines.html\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nThis PR is adding a thread which will run at scheduled interval to ask a batch of executors to start decommissioning themselves.\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nCurrenlty, each executor is asked to starts offloading rdd and shuffle blocks as soon it is decommissioned. This can overload the network bandwidth of the application.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nNo\r\n\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nUT in progress", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala", "additions": "43", "deletions": "6", "changes": "49"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/WorkerDecommissionSuite.scala", "additions": "27", "deletions": "0", "changes": "27"}, "updated": [0, 0, 0]}]}
{"author": "q2w", "sha": "c462df9035aaec6624fc90219a1ab7946e743ca5", "commit_date": "2021/06/28 03:45:20", "commit_message": "fix UT faling in BlockManagerDecommissionUnitSuite because of change in number of arguments in replicateBlocks method in BlockManager", "title": "[SPARK-35754][CORE] Add config to put migrating blocks on disk only", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error message, please read the guideline first:\r\n     https://spark.apache.org/error-message-guidelines.html\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nThis PR adds a config which makes block manager decommissioner to migrate block data on disk only. \r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nWhile migrating block data, if enough memory is not available on peer block managers existing blocks are dropped. After this PR migrating blocks won't drop any existing blocks. \r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nNo\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nUT in BlockManagerSuite", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 1, 4]}, {"file": {"name": "core/src/main/scala/org/apache/spark/storage/BlockManager.scala", "additions": "6", "deletions": "3", "changes": "9"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/storage/BlockManagerDecommissioner.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 1, 1]}, {"file": {"name": "core/src/test/scala/org/apache/spark/storage/BlockManagerDecommissionUnitSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/storage/BlockManagerSuite.scala", "additions": "20", "deletions": "0", "changes": "20"}, "updated": [0, 1, 5]}]}
{"author": "rmcyang", "sha": "", "commit_date": "2021/09/03 22:57:21", "commit_message": "SPARK-33781 Improve caching of MergeStatus on the executor side", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/MapOutputTracker.scala", "additions": "127", "deletions": "38", "changes": "165"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 1, 6]}, {"file": {"name": "core/src/test/scala/org/apache/spark/MapOutputTrackerSuite.scala", "additions": "57", "deletions": "4", "changes": "61"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/test/scala/org/apache/spark/MapStatusesSerDeserBenchmark.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}]}
{"author": "xinrong-databricks", "sha": "87172874344406fac65abb26f072c06855674b7f", "commit_date": "2021/09/01 18:59:43", "commit_message": "refactor", "title": "[SPARK-36628][PYTHON] Implement `__getitem__`  of label-based MultiIndex select", "body": "### What changes were proposed in this pull request?\r\nImplement `__getitem__`  of label-based MultiIndex select\r\n\r\n\r\n### Why are the changes needed?\r\nIncrease pandas API coverage\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes, reading values by label-based MultiIndex select is supported now.\r\n\r\n```py\r\n>>> psdf = ps.DataFrame(\r\n...             {\"a\": [1, 2, 3, 4, 5, 6, 7, 8, 9], \"b\": [4, 5, 6, 3, 2, 1, 0, 0, 0]},\r\n...             index=[0, 1, 3, 5, 6, 8, 9, 9, 9],\r\n...         )\r\n>>> psdf = psdf.set_index(\"b\", append=True)\r\n>>> psdf\r\n     a\r\n  b   \r\n0 4  1\r\n1 5  2\r\n3 6  3\r\n5 3  4\r\n6 2  5\r\n8 1  6\r\n9 0  7\r\n  0  8\r\n  0  9\r\n>>> psdf.loc[6, 2]\r\n     a\r\n  b   \r\n6 2  5\r\n```\r\n\r\n### How was this patch tested?\r\nUnit tests.\r\n", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "74", "deletions": "18", "changes": "92"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}]}
{"author": "xinrong-databricks", "sha": "8b7b395c0faa78d8809489eaf6f88a013fcfe2a0", "commit_date": "2021/08/10 21:37:08", "commit_message": "empty", "title": "[WIP][SPARK-36397][PYTHON] Implement DataFrame.mode", "body": "### What changes were proposed in this pull request?\r\nImplement DataFrame.mode (along index axis).\r\n\r\n\r\n### Why are the changes needed?\r\nGet the mode(s) of each element along the selected axis is a common functionality, which is supported in pandas. We should support that.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes. `DataFrame.mode` can be used now.\r\n\r\n```py\r\n>>> psdf = ps.DataFrame(\r\n...     [(\"bird\", 2, 2), (\"mammal\", 4, np.nan), (\"arthropod\", 8, 0), (\"bird\", 2, np.nan)],\r\n...     index=(\"falcon\", \"horse\", \"spider\", \"ostrich\"),\r\n...     columns=(\"species\", \"legs\", \"wings\"),\r\n... )\r\n>>> psdf\r\n           species  legs  wings                                                 \r\nfalcon        bird     2    2.0\r\nhorse       mammal     4    NaN\r\nspider   arthropod     8    0.0\r\nostrich       bird     2    NaN\r\n\r\n>>> psdf.mode()\r\n  species  legs  wings\r\n0    bird   2.0    0.0\r\n1    None   NaN    2.0\r\n\r\n>>> psdf.mode(dropna=False)\r\n  species  legs  wings\r\n0    bird     2    NaN\r\n\r\n>>> psdf.mode(numeric_only=True)\r\n   legs  wings\r\n0   2.0    0.0\r\n1   NaN    2.0\r\n```\r\n\r\n### How was this patch tested?\r\nUnit tests.\r\n", "failed_tests": ["pyspark.pandas.tests.test_dataframe"], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/frame.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "103", "deletions": "1", "changes": "104"}, "updated": [0, 5, 20]}, {"file": {"name": "python/pyspark/pandas/missing/frame.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [0, 2, 6]}]}
{"author": "xinrong-databricks", "sha": "", "commit_date": "2021/04/13 17:37:14", "commit_message": "Misc unit tests", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "dev/sparktestsupport/modules.py", "additions": "12", "deletions": "0", "changes": "12"}, "updated": [2, 4, 4]}, {"file": {"name": "python/pyspark/pandas/tests/test_categorical.py", "additions": "475", "deletions": "0", "changes": "475"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_csv.py", "additions": "434", "deletions": "0", "changes": "434"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_expanding.py", "additions": "306", "deletions": "0", "changes": "306"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "2815", "deletions": "0", "changes": "2815"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "1334", "deletions": "0", "changes": "1334"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_namespace.py", "additions": "337", "deletions": "0", "changes": "337"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_repr.py", "additions": "189", "deletions": "0", "changes": "189"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_reshape.py", "additions": "313", "deletions": "0", "changes": "313"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_rolling.py", "additions": "184", "deletions": "0", "changes": "184"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_sql.py", "additions": "50", "deletions": "0", "changes": "50"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_stats.py", "additions": "412", "deletions": "0", "changes": "412"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_window.py", "additions": "316", "deletions": "0", "changes": "316"}, "updated": [0, 0, 0]}]}
{"author": "gaoyajun02", "sha": "", "commit_date": "2021/06/29 08:40:52", "commit_message": "[SPARK-35920][FOLLOWUP][BUILD] Fix Kryo Shaded dependency", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/pom.xml", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "pom.xml", "additions": "7", "deletions": "2", "changes": "9"}, "updated": [2, 9, 33]}]}
{"author": "zhengruifeng", "sha": "6b434b0baad47063ba1eca1f8f788e35e0af39b3", "commit_date": "2021/09/16 02:51:06", "commit_message": "move agg stringArgs to subclasses", "title": "[SPARK-36638][SQL] Generalize OptimizeSkewedJoin", "body": "### What changes were proposed in this pull request?\r\nThis PR aims to generalize `OptimizeSkewedJoin` to support all patterns that can be handled by current _split-duplicate_ strategy:\r\n\r\n1, find the _splittable_ shuffle query stages by the semantics of internal nodes;\r\n\r\n2, for each _splittable_ shuffle query stage, check whether skew partitions exists, if true, split them into specs;\r\n\r\n3, handle _Combinatorial Explosion_: for each skew partition, check whether the combination number is too large, if so, re-split the stages to keep a reasonable number of combinations. For example, for partition 0, stage A/B/C are split into 100/100/100 specs, respectively. Then there are 1M combinations, which is too large, and will cause performance regression.\r\n\r\n4, attach new specs to shuffle query stages;\r\n\r\n\r\n### Why are the changes needed?\r\nto Generalize OptimizeSkewedJoin \r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\ntwo additional configs are added\r\n\r\n\r\n### How was this patch tested?\r\nexisting testsuites, added testsuites, some cases on our productive system\r\n", "failed_tests": ["org.apache.spark.sql.TPCDSModifiedPlanStabilitySuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilityWithStatsSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "19", "deletions": "0", "changes": "19"}, "updated": [1, 2, 8]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "316", "deletions": "147", "changes": "463"}, "updated": [1, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/BaseAggregateExec.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/SortAggregateExec.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowExec.scala", "additions": "14", "deletions": "7", "changes": "21"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/WholeStageCodegenSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "227", "deletions": "0", "changes": "227"}, "updated": [1, 1, 7]}]}
{"author": "zhengruifeng", "sha": "a9888ba9458d012ab71644bc0808e8123955a824", "commit_date": "2021/07/12 06:18:19", "commit_message": "init\n\ninit\n\ninit", "title": "[SPARK-36087][SQL][WIP] An Impl of skew key detection and data inflation optimization", "body": "### What changes were proposed in this pull request?\r\n1, introduce `ShuffleExecAccumulator` in `ShuffleExchangeExec` to support arbitrary statistics;\r\n\r\n2, impl a key sampling `ShuffleExecAccumulator` to detect skew keys and show debug info on SparkUI;\r\n\r\n3, in `OptimizeSkewedJoin`, estimate the joined size of each partition based on the sampled keys, and split a partition if it is not split yet and its estimated joined size is too larger.\r\n\r\n\r\n### Why are the changes needed?\r\n1, make it easy to add a new statistics which can be used in AQE rules;\r\n2, showing skew info on sparkUI is usefully;\r\n3, spliting partitions based on joined size can resolve data inflation;\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes, new features are added\r\n\r\n\r\n### How was this patch tested?\r\nadded testsuites\r\n", "failed_tests": ["org.apache.spark.sql.TPCDSModifiedPlanStabilityWithStatsSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "36", "deletions": "0", "changes": "36"}, "updated": [0, 8, 16]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeDataInflation.scala", "additions": "125", "deletions": "0", "changes": "125"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeLocalShuffleReader.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "58", "deletions": "15", "changes": "73"}, "updated": [0, 2, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "7", "deletions": "2", "changes": "9"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala", "additions": "79", "deletions": "1", "changes": "80"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExecAccumulator.scala", "additions": "552", "deletions": "0", "changes": "552"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/ShuffleExecAccumulatorSuite.scala", "additions": "155", "deletions": "0", "changes": "155"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "137", "deletions": "1", "changes": "138"}, "updated": [0, 8, 16]}]}
{"author": "zhengruifeng", "sha": "", "commit_date": "2021/04/15 02:22:09", "commit_message": "init", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "mllib-local/src/main/scala/org/apache/spark/ml/linalg/BLAS.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "mllib-local/src/main/scala/org/apache/spark/ml/linalg/Matrices.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 1, 1]}, {"file": {"name": "mllib/src/main/scala/org/apache/spark/ml/classification/GBTClassifier.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "mllib/src/main/scala/org/apache/spark/ml/classification/LogisticRegression.scala", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [1, 1, 3]}, {"file": {"name": "mllib/src/main/scala/org/apache/spark/ml/recommendation/ALS.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [1, 1, 1]}, {"file": {"name": "mllib/src/main/scala/org/apache/spark/ml/regression/GBTRegressor.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}]}
{"author": "shahidki31", "sha": "", "commit_date": "2021/05/11 15:08:33", "commit_message": "Don't allow to set spark.driver.cores=0", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/deploy/SparkSubmitArguments.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}]}
{"author": "warrenzhu25", "sha": "f9057749f7d7d25d396c03a8041a0a55e97148ab", "commit_date": "2021/09/20 17:27:07", "commit_message": "update version in doc", "title": "[SPARK-36793][K8S] Support write container stdout/stderr to file", "body": "### What changes were proposed in this pull request?\r\nSupport write container stdout/stderr to file\r\n\r\n### Why are the changes needed?\r\nIf users want to sidecar logging agent to send stdout/stderr to external log storage,  only way is to change entrypoint.sh, which might break compatibility with community version.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes. User can enable this feature by spark config.\r\n\r\n### How was this patch tested?\r\nAdded UT in BasicDriverFeatureStepSuite and BasicExecutorFeatureStepSuite\r\n", "failed_tests": [], "files": [{"file": {"name": "docs/running-on-kubernetes.md", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [0, 0, 4]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Constants.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicDriverFeatureStep.scala", "additions": "8", "deletions": "1", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala", "additions": "8", "deletions": "1", "changes": "9"}, "updated": [0, 0, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicDriverFeatureStepSuite.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala", "additions": "13", "deletions": "0", "changes": "13"}, "updated": [0, 0, 1]}, {"file": {"name": "resource-managers/kubernetes/docker/src/main/dockerfiles/spark/entrypoint.sh", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [0, 0, 1]}]}
{"author": "warrenzhu25", "sha": "", "commit_date": "2021/06/02 21:33:52", "commit_message": "[SPARK-34777][UI] StagePage input/output size records not show when records greater than zero", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/stagepage.js", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala", "additions": "6", "deletions": "2", "changes": "8"}, "updated": [0, 0, 1]}]}
{"author": "fsamuel-bs", "sha": "279c063d88fb12071f6142386cc3ab66bf369a87", "commit_date": "2021/08/31 17:25:55", "commit_message": "docs", "title": "[SPARK-36627][CORE] Fix java deserialization of proxy classes", "body": "## Upstream SPARK-XXXXX ticket and PR link (if not applicable, explain)\r\nhttps://issues.apache.org/jira/browse/SPARK-36627\r\n\r\n## What changes were proposed in this pull request?\r\nSee issue above for issue description.\r\n\r\n### Why are the changes needed?\r\nSpark deserialization fails with no recourse for the user.\r\n\r\n### Does this PR introduce any user-facing change?\r\nNo.\r\n\r\n### How was this patch tested?\r\nUnit tests.", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/serializer/JavaSerializer.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/java/org/apache/spark/serializer/ProxySerializerTest.java", "additions": "33", "deletions": "0", "changes": "33"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/serializer/JavaSerializerSuite.scala", "additions": "24", "deletions": "2", "changes": "26"}, "updated": [0, 0, 0]}]}
{"author": "senthh", "sha": "2db3bf31572ebd15ca7009fa366f56a37af3f74f", "commit_date": "2021/09/19 18:26:59", "commit_message": "[SPARK-36801][DOCUMENTATION] ADD CHANGES IN SPARK SQL JDBC DOCUMENT", "title": "[SPARK-36801][DOCS] ADD \"All columns are automatically converted to be nullable for compatibility reasons.\" IN SPARK SQL JDBC DOCUMENT", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nAdded a line \"All columns are automatically converted to be nullable for compatibility reasons.\" in Documentation[1].\r\n\r\n Ref:\r\n\r\n[1 ]https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#jdbc-to-other-databases\r\n\r\n \r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n<p>Reading using Spark SQL jdbc DataSource\u00a0 does not maintain nullable type and changes \"non nullable\" columns to \"nullable\".</p>\r\n\r\n<p>\u00a0</p>\r\n\r\n<p>For example:</p>\r\n\r\n<p>mysql&gt; CREATE TABLE Persons(Id int NOT NULL, FirstName varchar(255), LastName varchar(255), Age int);<br>\r\nQuery OK, 0 rows affected (0.04 sec)</p>\r\n\r\n\u00a0Spark-shell:\r\n\r\nval df = spark.read.format(\"jdbc\")\u2028.option(\"database\",\"Test_DB\")\u2028.option(\"user\", \"root\")\u2028.option(\"password\", \"\")\u2028.option(\"driver\", \"com.mysql.cj.jdbc.Driver\")\u2028.option(\"url\", \"jdbc:mysql://localhost:3306/Test_DB\")\u2028.option(\"query\", \"(select * from Persons)\")\u2028.load()\u2028df.printSchema()\r\n\u00a0\r\noutput:\r\n\u00a0\r\nroot\r\n\u2013 Id: integer (nullable = true)\r\n\u2013 FirstName: string (nullable = true)\r\n\u2013 LastName: string (nullable = true)\r\n\u2013 Age: integer (nullable = true)\r\n\r\n<p>So we need to add a note, in Documentation<span class=\"error\">[1]</span>, \"All columns are automatically converted to be nullable for compatibility reasons.\"</p>\r\n\r\n<p>\u00a0Ref:</p>\r\n\r\n<p><span class=\"error\">[1 ]</span><a href=\"https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#jdbc-to-other-databases\" class=\"external-link\" rel=\"nofollow\" title=\"Follow link\">https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#jdbc-to-other-databases</a></p>\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nYes, Document changes\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nTested using \"SKIP_API=1 bundle exec jekyll build\"", "failed_tests": [], "files": [{"file": {"name": "docs/sql-data-sources-jdbc.md", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 1]}]}
{"author": "senthh", "sha": "", "commit_date": "2021/07/29 10:46:49", "commit_message": "SPARK-36327 -  Spark sql creates staging dir inside database directory rather than creating inside table directory", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/SaveAsHiveFile.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "cxzl25", "sha": "bec1e6d7ba85b3486ca7b2e16ecd453c1b98dd6e", "commit_date": "2021/09/18 17:24:39", "commit_message": "Pass queryExecution name in CLI when only select query.", "title": "[SPARK-36799][SQL] Pass queryExecution name in CLI when only select query", "body": "### What changes were proposed in this pull request?\r\nWhen sql is only a select query, call `SQLExecution.withNewExecutionId` and specify `collect` as `executionName` so that `QueryExecutionListener` can get the query.\r\n### Why are the changes needed?\r\nNow when in spark-sql CLI, `QueryExecutionListener` can receive command , but not select query.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\nmanual test.", "failed_tests": [], "files": [{"file": {"name": "sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkSQLDriver.scala", "additions": "6", "deletions": "2", "changes": "8"}, "updated": [0, 0, 0]}]}
{"author": "cxzl25", "sha": "7ec3ccce0e3b7ac445b7ce8224c2eac9bab420bd", "commit_date": "2021/09/08 09:26:29", "commit_message": "trigger test", "title": "[SPARK-35437][SQL] Use expressions to filter Hive partitions at client side", "body": "### What changes were proposed in this pull request?\r\nImprove partition filtering speed and reduce metastore pressure.\r\nWe can first pull all the partition names, filter by expressions, and then obtain detailed information about the corresponding partitions from the MetaStore Server.\r\n\r\n### Why are the changes needed?\r\nWhen `convertFilters` cannot take effect, cannot filter the queried partitions in advance on the hive MetaStore Server. At this time, `getAllPartitionsOf` will get all partition details.\r\n\r\nWhen the Hive client cannot use the server filter, it will first obtain the values of all partitions, and then filter.\r\n\r\nWhen we have a table with a lot of partitions and there is no way to filter it on the MetaStore Server, we will get all the partition details and filter it on the client side. This is slow and puts a lot of pressure on the MetaStore Server.\r\n\r\n\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n\r\n### How was this patch tested?\r\nAdd UT\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/ExternalCatalogUtils.scala", "additions": "27", "deletions": "17", "changes": "44"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "13", "deletions": "0", "changes": "13"}, "updated": [0, 1, 12]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveShim.scala", "additions": "68", "deletions": "11", "changes": "79"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/client/HivePartitionFilteringSuite.scala", "additions": "58", "deletions": "1", "changes": "59"}, "updated": [0, 0, 1]}]}
{"author": "cxzl25", "sha": "d3086a535794dcd4984d30ea66063f8eab5ad95a", "commit_date": "2021/08/09 12:31:16", "commit_message": "Merge remote-tracking branch 'origin' into SPARK-36390", "title": "[SPARK-36390][SQL] Replace SessionState.close with SessionState.detachSession", "body": "### What changes were proposed in this pull request?\r\nSPARK-35286 replace `SessionState.start` with `SessionState.setCurrentSessionState`, but `SessionState.close` will create a `HiveMetaStoreClient` , connect to the Hive Meta Store Server, and then load all functions.\r\n\r\nSPARK-35556 (Remove close HiveClient's SessionState) When the Hive version used is greater than or equal to 2.1, `SessionState.close` is not called and the resource dir of HiveClient is not cleaned up.\r\n\r\n### Why are the changes needed?\r\nClean up the hive resources dir temporary directory.\r\nAvoid wasting resources and accelerate the exit speed.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\nadd UT\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/hive-thriftserver/src/main/java/org/apache/hive/service/cli/session/HiveSessionImpl.java", "additions": "3", "deletions": "15", "changes": "18"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkSQLCLIDriver.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/CliSuite.scala", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveUtils.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 1, 2]}]}
{"author": "cxzl25", "sha": "41d4c4f72f65c8bf2c388d2980240b445522313f", "commit_date": "2021/08/20 07:13:45", "commit_message": "Propagation cause when UDF reflection fails", "title": "[SPARK-36550][SQL] Propagation cause when UDF reflection fails", "body": "### What changes were proposed in this pull request?\r\nWhen the exception is InvocationTargetException, get cause and stack trace.\r\n\r\n### Why are the changes needed?\r\nNow when UDF reflection fails, InvocationTargetException is thrown, but it is not a specific exception.\r\n```\r\nError in query: No handler for Hive UDF 'XXX': java.lang.reflect.InvocationTargetException\r\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n```\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo.\r\n\r\n\r\n### How was this patch tested?\r\nmanual test\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveSessionCatalog.scala", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 0, 0]}]}
{"author": "cxzl25", "sha": "c85ab7aa98873176def007b6e2655dfcdafef1a8", "commit_date": "2021/06/30 08:33:25", "commit_message": "renameFunction use the original name", "title": "[SPARK-35913][SQL] Create hive permanent function with owner name", "body": "### What changes were proposed in this pull request?\r\nCreate hive permanent function with owner name\r\n\r\n\r\n### Why are the changes needed?\r\nThe hive permanent function created by spark does not have an owner name, while the permanent function created by hive has an owner name\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n\r\n### How was this patch tested?\r\nmanual test\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 7]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveShim.scala", "additions": "17", "deletions": "11", "changes": "28"}, "updated": [0, 1, 5]}]}
{"author": "cxzl25", "sha": "", "commit_date": "2020/10/14 12:46:06", "commit_message": "use distinct", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/deploy/DependencyUtils.scala", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 0, 0]}]}
{"author": "baibaichen", "sha": "", "commit_date": "2021/04/05 23:52:48", "commit_message": "[SPARK-34935][SQL] CREATE TABLE LIKE should respect the reserved table properties\n\n### What changes were proposed in this pull request?\n\nCREATE TABLE LIKE should respect the reserved properties of tables and fail if specified, using `spark.sql.legacy.notReserveProperties` to restore.\n\n### Why are the changes needed?\n\nMake DDLs consistently treat reserved properties\n\n### Does this PR introduce _any_ user-facing change?\n\nYES, this is a breaking change as using `create table like` w/ reserved properties will fail.\n\n### How was this patch tested?\n\nnew test\n\nCloses #32025 from yaooqinn/SPARK-34935.\n\nAuthored-by: Kent Yao <yao@apache.org>\nSigned-off-by: Takeshi Yamamuro <yamamuro@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "docs/sql-migration-guide.md", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 4, 11]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/SparkSqlParserSuite.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [1, 1, 1]}]}
{"author": "liangz1", "sha": "", "commit_date": "2021/08/26 07:11:28", "commit_message": "add withMetadata and make withColumn public", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala", "additions": "22", "deletions": "1", "changes": "23"}, "updated": [1, 1, 8]}]}
{"author": "kyoty", "sha": "2bcbcf4db872041b7c7370c96b2513aff1e8c21c", "commit_date": "2021/07/13 17:44:07", "commit_message": "Bump addressable from 2.7.0 to 2.8.0 in /docs\n\nBumps [addressable](https://github.com/sporkmonger/addressable) from 2.7.0 to 2.8.0.\n- [Release notes](https://github.com/sporkmonger/addressable/releases)\n- [Changelog](https://github.com/sporkmonger/addressable/blob/main/CHANGELOG.md)\n- [Commits](https://github.com/sporkmonger/addressable/compare/addressable-2.7.0...addressable-2.8.0)\n\n---\nupdated-dependencies:\n- dependency-name: addressable\n  dependency-type: indirect\n...\n\nSigned-off-by: dependabot[bot] <support@github.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "docs/Gemfile.lock", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "kyoty", "sha": "", "commit_date": "2021/04/17 16:40:55", "commit_message": "Update stagepage.js", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/stagepage.js", "additions": "8", "deletions": "12", "changes": "20"}, "updated": [0, 0, 0]}]}
{"author": "cfmcgrady", "sha": "05a11c880da09799bccfbd3f5dd48c3448315c27", "commit_date": "2021/06/21 05:59:07", "commit_message": "add test", "title": "[SPARK-35688][SQL]Subexpressions should be lazy evaluation in GeneratePredicate", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error message, please read the guideline first:\r\n     https://spark.apache.org/error-message-guidelines.html\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nCurrently the subexpression elimination in GeneraterPredicate is eager execution, add lazy evaluation mode for subexpression elimination. Fix error when the subexression elimination is inside a condition branch and the subexression depends on the other conditions.\r\nFor instance:\r\n```sql\r\n-- c1 as type of array\r\n-- +-------------------------------+\r\n-- |c1                             |\r\n-- +-------------------------------+\r\n-- |[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]|\r\n-- |[1, 2, 3, 4, 5]                |\r\n-- +-------------------------------+\r\nset spark.sql.ansi.enabled = true;\r\nset spark.sql.codegen.wholeStage = false;\r\n\r\nselect * from table_name where size(c1) > 5 and (element_at(c1, 7) = 8 or element_at(c1, 7) = 7);\r\n```\r\nIn this case, `element_at` depends on condition `size(c1) > 5`, before this pr, an exception will throw when we disable codegen.\r\n\r\n\r\nAs the lazy evaluation is expensive (we need to extract a variable and check the subexpr is initialized or not before we use the subexpr), we keep subexpression eager execution in `GenerateUnsafeProjection/GenerateMutableProjection`\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\nFix bug when subexpression elimination enabled.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\nNo.\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\n\r\nExsiting test and new test.", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "115", "deletions": "33", "changes": "148"}, "updated": [0, 3, 7]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/GenerateMutableProjection.scala", "additions": "11", "deletions": "6", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/GeneratePredicate.scala", "additions": "6", "deletions": "4", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/GenerateUnsafeProjection.scala", "additions": "10", "deletions": "7", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CodeGenerationSuite.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 3]}]}
{"author": "cfmcgrady", "sha": "", "commit_date": "2021/05/10 03:18:26", "commit_message": "[SPARK-35316][SQL] UnwrapCastInBinaryComparison support In/InSet predicate", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/UnwrapCastInBinaryComparison.scala", "additions": "15", "deletions": "3", "changes": "18"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/UnwrapCastInBinaryComparisonSuite.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 0, 0]}]}
{"author": "HeartSaVioR", "sha": "", "commit_date": "2021/03/31 08:21:44", "commit_message": "Add test suite", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StreamingSessionWindowStateManager.scala", "additions": "12", "deletions": "0", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/StreamingSessionWindowStateManagerSuite.scala", "additions": "201", "deletions": "0", "changes": "201"}, "updated": [0, 0, 0]}]}
{"author": "BelodengKlaus", "sha": "", "commit_date": "2021/09/16 03:29:03", "commit_message": "[SPARK-36773] Fixed uts to check the compression for parquet", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetCompressionCodecPrecedenceSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "JoshRosen", "sha": "", "commit_date": "2021/09/16 05:15:38", "commit_message": "Fix import ordering", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.execution.SQLMetricsSuite"], "files": [{"file": {"name": "sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/CliSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/HiveThriftServer2Suites.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}]}
{"author": "exmy", "sha": "", "commit_date": "2021/06/01 15:27:58", "commit_message": "SPARK-35596: HighlyCompressedMapStatus should record accurately the size of skewed shuffle blocks", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 1, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/MapStatus.scala", "additions": "12", "deletions": "3", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/MapStatusSuite.scala", "additions": "26", "deletions": "0", "changes": "26"}, "updated": [0, 0, 0]}]}
{"author": "yaooqinn", "sha": "a3b7d08983115c84225ce52f8db3dd16efd5471e", "commit_date": "2021/09/06 05:56:10", "commit_message": "add doc", "title": "[SPARK-36662][SQL] Special timestamps support for path filters -  modifiedBefore/modifiedAfter", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\nSpecial timestamps support for path filters -  modifiedBefore/modifiedAfter\r\n\r\n- epoch\r\n- now\r\n- today\r\n- tomorrow\r\n- yesterday\r\n#### examples\r\n\r\n```scala\r\n    val beforeTodayDF = spark.read.format(\"parquet\")\r\n      // Files modified after the midnight of today are allowed\r\n      .option(\"modifiedBefore\", \"today\")\r\n      .load(\"examples/src/main/resources/dir1\");\r\n    beforeTodayDF.show();\r\n    // +-------------+\r\n    // |         file|\r\n    // +-------------+\r\n    // |file1.parquet|\r\n    // +-------------+\r\n    val afterYesterdayDF = spark.read.format(\"parquet\")\r\n      // Files modified after the midnight of yesterday are allowed\r\n      .option(\"modifiedAfter\", \"yesterday\")\r\n      .load(\"examples/src/main/resources/dir1\");\r\n    afterYesterdayDF.show();\r\n    // +-------------+\r\n    // |         file|\r\n    // +-------------+\r\n    // +-------------+\r\n    // $example off:load_with_modified_time_filter$\r\n```\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\ntheses special values can be useful to be supported in path filters -  modifiedBefore/modifiedAfter. e.g. for daily scheduled jobs\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\nyes, special timestamps are supported by modifiedBefore/modifiedAfter\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\n\r\nnewly added tests", "failed_tests": [], "files": [{"file": {"name": "docs/sql-data-sources-generic-options.md", "additions": "11", "deletions": "4", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 3, 13]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/pathFilters.scala", "additions": "6", "deletions": "3", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/PathFilterSuite.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [0, 0, 0]}]}
{"author": "yaooqinn", "sha": "b989aa3821dbaf6cacccde886ed1051068d43cf5", "commit_date": "2021/09/03 17:21:28", "commit_message": "address comments", "title": "[SPARK-36634][SQL] Support access and read parquet file by column ordinal", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\nAdd a config `spark.sql.parquet.accessByColumnOrdinal` \r\n\r\nWhen true, we access the parquet files by column original instead of catalyst schema mapping at the executor side. \r\n\r\nThis is useful when the parquet file meta is inconsistent with those in Metastore, e.g. creating a table with existing parquet files with column names renamed, the column names are changed by external systems.\r\n\r\n- What if the number of columns/inner fields does not match?\r\n  - if the number of requests columns (M)is greater than the one(N) in the parquet file, the [M, N-1] of the outputs will be filled with nulls, which is also the same as the current name-based mapping\r\n  - if the number of requests columns (M)is less than or equal to the one(N) in the parquet file, the [0, M-1] of fields will be token in order.\r\n- What if the types are not compatible?\r\n  - In this case, it throws unsupportedSchemaColumnConvertError \r\n\r\nBy default, it is off\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\n- What's the concrete use case that requires this feature?\r\n\r\nParquet's schema evolution is implementation-dependent and may causes inconsistency from file to file or file to metastore. Sometimes, 1) the table Spark's processing might be produced by other systems, e.g. renamed by hive https://issues.apache.org/jira/browse/HIVE-6938, 2)some operations that do not introduce a force schema checking, e.g. `set location, `add partition`. With this feature, the users as data consumers can still read those data produced by different providers.\r\n\r\nSee also, https://issues.apache.org/jira/browse/IMPALA-2835\r\n\r\n\r\nbetter data accessibility \r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\nNO\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\n\r\nnewly added test", "failed_tests": ["org.apache.spark.sql.execution.datasources.parquet.ParquetV2SchemaPruningSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV1SchemaPruningSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [1, 1, 12]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport.scala", "additions": "54", "deletions": "21", "changes": "75"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRecordMaterializer.scala", "additions": "6", "deletions": "2", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "27", "deletions": "13", "changes": "40"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "85", "deletions": "0", "changes": "85"}, "updated": [0, 0, 0]}]}
{"author": "yaooqinn", "sha": "4078349ac282f44673918d2c2a40e2a8799ce07a", "commit_date": "2021/06/08 16:23:45", "commit_message": "fix test", "title": "[WIP][SPARK-35677][Core][SQL] Support dynamic range of executor numbers for dynamic allocation", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error message, please read the guideline first:\r\n     https://spark.apache.org/error-message-guidelines.html\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n\r\nCurrently, Spark allows users to set scalability within a Spark application using dynamic allocation. `spark.dynamicAllocation.minExecutors` & `spark.dynamicAllocation.maxExecutors` are used for scaling up and down. Within an application\uff0cSpark tactfully use them to request executors from cluster manager according to the real-time workload. Once set, the range is fixed through the whole application lifecycle. This is not very convenient for long-running application when the range should be changeable for some cases, such as:\r\n1. the cluster manager itself or the queue will scale up and down, which looks very likely to happen in modern cloud platforms\r\n2. the application is long-running, but the timeliness, priority, e.t.c are not only determined by the workload of the application, but also by the traffic across the cluster manager or just different moments\r\n3. e.t.c.\r\n\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\nmake the dynamic allocation for long term Spark applications\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\nConfigs below are changeable:\r\n\r\nspark.dynamicAllocation.maxExecutors \r\nspark.dynamicAllocation.minExecutors \r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nnew tests", "failed_tests": [], "files": [{"file": {"name": "core/src/main/java/org/apache/spark/SparkFirehoseListener.java", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala", "additions": "22", "deletions": "21", "changes": "43"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [1, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/SparkListenerBus.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/ExecutorAllocationManagerSuite.scala", "additions": "31", "deletions": "2", "changes": "33"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/command/SetCommand.scala", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 1, 3]}]}
{"author": "yaooqinn", "sha": "9e8b227e91070c35b98247acdbf4caf022cfaf72", "commit_date": "2021/08/20 13:52:21", "commit_message": "address comments", "title": "[SPARK-36477][SQL] Inferring schema from JSON file shall handle CharConversionException/MalformedInputException", "body": "\r\n<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\nWhen set `ignoreCorruptFiles=true`, reading JSON still fails with corrupt files during inferring schema.\r\n\r\n```scala\r\njava.io.CharConversionException: Unsupported UCS-4 endianness (2143) detected\r\n\tat com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.reportWeirdUCS4(ByteSourceJsonBootstrapper.java:504)\r\n\tat com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.checkUTF32(ByteSourceJsonBootstrapper.java:471)\r\n\tat com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.detectEncoding(ByteSourceJsonBootstrapper.java:144)\r\n\tat com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.constructParser(ByteSourceJsonBootstrapper.java:247)\r\n\tat com.fasterxml.jackson.core.JsonFactory._createParser(JsonFactory.java:1528)\r\n\tat com.fasterxml.jackson.core.JsonFactory.createParser(JsonFactory.java:1034)\r\n\tat org.apache.spark.sql.catalyst.json.CreateJacksonParser$.internalRow(CreateJacksonParser.scala:86)\r\n\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.$anonfun$inferFromDataset$4(JsonDataSource.scala:107)\r\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$3(JsonInferSchema.scala:66)\r\n\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2621)\r\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$2(JsonInferSchema.scala:66)\r\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\r\n\tat scala.collection.TraversableOnce.reduceLeft(TraversableOnce.scala:190)\r\n\tat scala.collection.TraversableOnce.reduceLeft$(TraversableOnce.scala:183)\r\n\tat scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1429)\r\n\tat scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:208)\r\n\tat scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:207)\r\n\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1429)\r\n\tat scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:215)\r\n\tat scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:215)\r\n\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1429)\r\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:81)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n```\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\nbugfix\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nyes, ignoreCorruptFiles will ignore JSON files corrupted\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\n\r\nnew test", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JsonInferSchema.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/json/JsonSuite.scala", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [1, 1, 4]}]}
{"author": "yaooqinn", "sha": "623dc4659e016505d1245bf7637c12d499aa947d", "commit_date": "2021/08/11 04:22:48", "commit_message": "Update sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala\n\nCo-authored-by: Hyukjin Kwon <gurwls223@gmail.com>", "title": "[SPARK-36180][SQL] Store TIMESTAMP_NTZ into hive catalog as TIMESTAMP", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\nThis PR fix a issue that HMS can not recognize timestamp_ntz by mapping timestamp_ntz to `timestamp` of hive\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\nThe hive 2.3.9 does not have 2 timestamp or a type named timestamp_ntz.\r\nFYI, In hive 3.0, the will be a timestamp with local timezone added.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\nno, timestamp_ntz is new and not public yet\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\n\r\nnew test", "failed_tests": [], "files": [{"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala", "additions": "15", "deletions": "14", "changes": "29"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/SQLQuerySuite.scala", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [0, 1, 2]}]}
{"author": "yaooqinn", "sha": "d28f8690f9d0d7c5250d0f8cd49d0f3822df37d3", "commit_date": "2021/06/19 18:50:11", "commit_message": "[SPARK-35828][K8S] Skip retrieving the non-exist driver pod for client mode", "title": "[SPARK-35828][K8S] Skip retrieving the non-exist driver pod for client mode", "body": "\r\n<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error message, please read the guideline first:\r\n     https://spark.apache.org/error-message-guidelines.html\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\nFor a case like,\r\n\r\n```scala\r\nbin/spark-submit  \\\r\n--conf spark.kubernetes.file.upload.path=./ \\\r\n--deploy-mode client \\\r\n--master k8s://https://kubernetes.docker.internal:6443 \\\r\n--conf spark.kubernetes.container.image=yaooqinn/spark:v20210619 \\\r\n-c spark.kubernetes.context=docker-for-desktop_1 \\\r\n--conf spark.kubernetes.executor.podNamePrefix=sparksql \\\r\n--conf spark.dynamicAllocation.shuffleTracking.enabled=true \\\r\n--conf spark.dynamicAllocation.enabled=true \\\r\n--conf spark.kubernetes.driver.pod.name=abc \\\r\n--class org.apache.spark.examples.SparkPi examples/jars/spark-examples_2.12-3.2.0-SNAPSHOT.jar\r\n```\r\n\r\nWhen `spark.kubernetes.driver.pod.name` is specific, we now get the driver pod for whatever the deploy mode is, while the driver pod only exists in cluster mode. So we should skip retrieving it instead of getting the following error:\r\n\r\n```logtalk\r\n21/06/19 16:18:49 ERROR SparkContext: Error initializing SparkContext.\r\norg.apache.spark.SparkException: No pod was found named abc in the cluster in the namespace default (this was supposed to be the driver pod.).\r\n\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$driverPod$2(ExecutorPodsAllocator.scala:81)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$driverPod$1(ExecutorPodsAllocator.scala:79)\r\n\tat scala.Option.map(Option.scala:230)\r\n\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.<init>(ExecutorPodsAllocator.scala:76)\r\n\tat org.apache.spark.scheduler.cluster.k8s.KubernetesClusterManager.createSchedulerBackend(KubernetesClusterManager.scala:118)\r\n\tat org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2969)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:559)\r\n\tat org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2686)\r\n\tat org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:948)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:942)\r\n\tat org.apache.spark.examples.SparkPi$.main(SparkPi.scala:30)\r\n\tat org.apache.spark.examples.SparkPi.main(SparkPi.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\n```\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\nan unused config should stay inoperative instead of failing an application at runtime.\r\n\r\nwhen we switch deploy modes, we do need to justify irrelevant configs.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\nyes, spark.kubernetes.driver.pod.name will cause the client mode app to fail\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\n\r\nnew test added\r\n", "failed_tests": [], "files": [{"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/ExecutorPodsAllocator.scala", "additions": "5", "deletions": "2", "changes": "7"}, "updated": [0, 3, 6]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/scheduler/cluster/k8s/ExecutorPodsAllocatorSuite.scala", "additions": "15", "deletions": "1", "changes": "16"}, "updated": [0, 2, 4]}]}
{"author": "yaooqinn", "sha": "", "commit_date": "2021/04/16 03:26:02", "commit_message": "[SPARK-35102][SQL] Make spark.sql.hive.version read-only", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkSQLEnv.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkSQLSessionManager.scala", "additions": "0", "deletions": "2", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/CliSuite.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/HiveThriftServer2Suites.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveUtils.scala", "additions": "21", "deletions": "12", "changes": "33"}, "updated": [0, 0, 1]}]}
{"author": "sigmod", "sha": "", "commit_date": "2021/04/20 06:38:37", "commit_message": "update", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 13, 47]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/predicates.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/subquery.scala", "additions": "6", "deletions": "5", "changes": "11"}, "updated": [1, 3, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/InsertAdaptiveSparkPlan.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/PlanAdaptiveSubqueries.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "lrytz", "sha": "", "commit_date": "2021/09/09 14:37:09", "commit_message": "Use sed instead of profile to enable scala-parallel-collections on 2.13\n\nWorkaround for the POM issue explained in\nhttps://lists.apache.org/thread.html/rc812979bf41bac070d7d0437bb226e7a778fbe46f18727636daaac5e%40%3Cdev.spark.apache.org%3E", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/pom.xml", "additions": "6", "deletions": "9", "changes": "15"}, "updated": [0, 0, 2]}, {"file": {"name": "dev/change-scala-version.sh", "additions": "8", "deletions": "4", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "pom.xml", "additions": "7", "deletions": "9", "changes": "16"}, "updated": [1, 4, 22]}]}
{"author": "pingsutw", "sha": "", "commit_date": "2021/06/01 09:57:46", "commit_message": "Enable disallow_untyped_defs mypy check for pyspark.pandas.indexing.", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/mypy.ini", "additions": "0", "deletions": "3", "changes": "3"}, "updated": [0, 2, 4]}, {"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [0, 2, 9]}]}
{"author": "luxu1-ms", "sha": "ae35e8cdf757f29abad224d38511541783212e65", "commit_date": "2021/06/03 17:13:44", "commit_message": "update test case for datetime2 mapping", "title": "[SPARK-33743]change TimestampType match to datetime2 instead of datetime for MsSQLServerDialect", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error message, please read the guideline first:\r\n     https://spark.apache.org/error-message-guidelines.html\r\n-->\r\n\r\nSPARK-33743 is to change datetime datatype mapping in JDBC mssqldialect.\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\noverride def getJDBCType(dt: DataType): Option[JdbcType] = dt match {\r\ncase TimestampType => Some(JdbcType(\"DATETIME2\", java.sql.Types.TIMESTAMP))\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nSpark datetime type is timestamp type. This supports a microsecond resolution.\r\nSql supports 2 date time types:\r\n\r\ndatetime can support only milli seconds resolution (0 to 999).\r\ndatetime2 is extension of datetime , is compatible with datetime and supports 0 to 9999999 sub second resolution.\r\ndatetime2 (Transact-SQL) - SQL Server | Microsoft Docs\r\ndatetime (Transact-SQL) - SQL Server | Microsoft Docs\r\n\r\nCurrently MsSQLServerDialect maps timestamp type to datetime. Datetime only allows 3 digits of microseconds. This implies results in errors when writing timestamp with more than 3 digits of microseconds to sql server table. We want to map timestamp to datetime2, which is compatible with datetime but allows 7 digits of microseconds.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nNo.\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nUnit tests were updated and passed in JDBCSuit.scala.\r\nE2E test done with SQL Server.", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/jdbc/MsSqlServerDialect.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "planga82", "sha": "02a265d4eb38a1b562db6ce02e56f739a16d9ce7", "commit_date": "2021/09/09 00:57:53", "commit_message": "Apply only to unresolved regex", "title": "[SPARK-36698][SQL] Allow expand 'quotedRegexColumnNames' in all expressions when it\u2019s expanded to one named expression", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nWith `spark.sql.parser.quotedRegexColumnNames=true` regular expressions are not allowed in expressions like  \r\n``` SELECT `col_.*`/exp FROM (SELECT 3 AS col_a, 1 as exp) ``` \r\n\r\nThis PR propose to improve this behavior and allow this regular expression when it expands to only one named expression. It\u2019s the same behavior in Hive.\r\n\r\nExample 1:\r\n```\r\nSELECT `col_.*`/exp FROM (SELECT 3 AS col_a, 1 as exp) \r\n```\r\ncol_.* expands to col_a:  OK\r\n\r\nExample 2:\r\n```\r\nSELECT `col_.*`/col_b FROM (SELECT 3 AS col_a, 1 as col_b) \r\n```\r\ncol_.* expands to (col_a, col_b) : Fail like now\r\n\r\nExample 3:\r\n```\r\nSELECT `col_a`/exp FROM (SELECT 3 AS col_a, 1 as exp) \r\n```\r\ncol_a expands to col_a:  OK\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nImprove this feature and approaching hive behavior\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nNo\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nUnit testing", "failed_tests": ["org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite", "org.apache.spark.sql.DataFrameSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 2, 18]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [1, 1, 8]}]}
{"author": "planga82", "sha": "533cb7ca056e746ce0d4047662e0ae5574d47c36", "commit_date": "2021/08/15 22:47:03", "commit_message": "Fix style", "title": "[SPARK-36453][SQL] Improve consistency processing floating point special literals", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nSpecial literals in floating point are not consistent between cast and json expressions\r\n```\r\nscala> spark.sql(\"SELECT CAST('+Inf' as Double)\").show\r\n+--------------------+                                                        \r\n|CAST(+Inf AS DOUBLE)|\r\n+--------------------+\r\n|            Infinity|\r\n+--------------------+\r\n```\r\n```\r\nscala> val schema =  StructType(StructField(\"a\", DoubleType) :: Nil)\r\n\r\nscala> Seq(\"\"\"{\"a\" : \"+Inf\"}\"\"\").toDF(\"col1\").select(from_json(col(\"col1\"),schema)).show\r\n+---------------+\r\n|from_json(col1)|\r\n+---------------+\r\n|         {null}|\r\n+---------------+\r\n\r\nscala> Seq(\"\"\"{\"a\" : \"+Inf\"}\"\"\").toDF(\"col\").withColumn(\"col\", from_json(col(\"col\"), StructType.fromDDL(\"a DOUBLE\"))).write.json(\"/tmp/jsontests12345\")\r\nscala> spark.read.schema(StructType(Seq(StructField(\"col\",schema)))).json(\"/tmp/jsontests12345\").show\r\n+------+\r\n|   col|\r\n+------+\r\n|{null}|\r\n+------+\r\n```\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nImprove consistency between operations\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nYes, we are going to support the same special literal in Cast and Json expressions\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nUnit testing and manual testing", "failed_tests": ["pyspark.pandas.tests.data_type_ops.test_string_ops", "pyspark.pandas.tests.test_dataframe", "org.apache.spark.sql.catalyst.expressions.AnsiCastSuite", "org.apache.spark.sql.catalyst.expressions.AnsiCastSuite", "org.apache.spark.sql.catalyst.expressions.CastSuite", "org.apache.spark.sql.catalyst.expressions.TryCastSuite", "org.apache.spark.sql.catalyst.expressions.CastSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SQLQuerySuite", "org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite", "org.apache.spark.sql.DynamicPartitionPruningV1Suite", "org.apache.spark.sql.DynamicPartitionPruningV2Suite", "org.apache.spark.sql.DynamicPartitionPruningV1Suite", "org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite", "org.apache.spark.sql.DataFrameWindowFunctionsSuite", "org.apache.spark.sql.SubquerySuite", "org.apache.spark.sql.DynamicPartitionPruningV2Suite", "org.apache.spark.sql.DataFrameSuite", "org.apache.spark.sql.ColumnExpressionSuite", "org.apache.spark.sql.execution.datasources.json.JsonV1Suite", "org.apache.spark.sql.execution.datasources.json.JsonV2Suite", "org.apache.spark.sql.execution.datasources.json.JsonLegacyTimeParserSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala", "additions": "5", "deletions": "20", "changes": "25"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ExprUtils.scala", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JacksonParser.scala", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [0, 4, 7]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/json/JacksonParserSuite.scala", "additions": "27", "deletions": "12", "changes": "39"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "28", "deletions": "0", "changes": "28"}, "updated": [1, 2, 11]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/json/JsonSuite.scala", "additions": "0", "deletions": "17", "changes": "17"}, "updated": [0, 1, 3]}]}
{"author": "planga82", "sha": "", "commit_date": "2020/09/21 09:15:12", "commit_message": "[SPARK-32867][SQL] When explain, HiveTableRelation show limited message\n\n### What changes were proposed in this pull request?\nIn current mode, when explain a SQL plan with HiveTableRelation, it will show so many info about HiveTableRelation's prunedPartition,  this make plan hard to read, this pr make this information simpler.\n\nBefore:\n![image](https://user-images.githubusercontent.com/46485123/93012078-aeeca080-f5cf-11ea-9286-f5c15eadbee3.png)\n\nFor UT\n```\n test(\"Make HiveTableScanExec message simple\") {\n  withSQLConf(\"hive.exec.dynamic.partition.mode\" -> \"nonstrict\") {\n      withTable(\"df\") {\n        spark.range(30)\n          .select(col(\"id\"), col(\"id\").as(\"k\"))\n          .write\n          .partitionBy(\"k\")\n          .format(\"hive\")\n          .mode(\"overwrite\")\n          .saveAsTable(\"df\")\n\n        val df = sql(\"SELECT df.id, df.k FROM df WHERE df.k < 2\")\n        df.explain(true)\n      }\n    }\n  }\n```\n\nAfter this pr will show\n```\n== Parsed Logical Plan ==\n'Project ['df.id, 'df.k]\n+- 'Filter ('df.k < 2)\n   +- 'UnresolvedRelation [df], []\n\n== Analyzed Logical Plan ==\nid: bigint, k: bigint\nProject [id#11L, k#12L]\n+- Filter (k#12L < cast(2 as bigint))\n   +- SubqueryAlias spark_catalog.default.df\n      +- HiveTableRelation [`default`.`df`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#11L], Partition Cols: [k#12L]]\n\n== Optimized Logical Plan ==\nFilter (isnotnull(k#12L) AND (k#12L < 2))\n+- HiveTableRelation [`default`.`df`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#11L], Partition Cols: [k#12L], Pruned Partitions: [(k=0), (k=1)]]\n\n== Physical Plan ==\nScan hive default.df [id#11L, k#12L], HiveTableRelation [`default`.`df`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#11L], Partition Cols: [k#12L], Pruned Partitions: [(k=0), (k=1)]], [isnotnull(k#12L), (k#12L < 2)]\n\n```\n\nIn my pr, I will construct `HiveTableRelation`'s `simpleString` method to avoid show too much unnecessary info in explain plan. compared to what we had before\uff0cI decrease the detail metadata of each partition and only retain the partSpec to show each partition was pruned. Since for detail information, we always don't see this in Plan but to use DESC EXTENDED statement.\n\n### Why are the changes needed?\nMake plan about HiveTableRelation more readable\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nNo\n\nCloses #29739 from AngersZhuuuu/HiveTableScan-meta-location-info.\n\nAuthored-by: angerszhu <angers.zhu@gmail.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/interface.scala", "additions": "39", "deletions": "2", "changes": "41"}, "updated": [1, 2, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/HiveTableScanSuite.scala", "additions": "61", "deletions": "0", "changes": "61"}, "updated": [1, 1, 1]}]}
{"author": "FatalLin", "sha": "", "commit_date": "2021/04/19 15:31:34", "commit_message": "add new config to enable the function", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [1, 3, 14]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMetastoreCatalog.scala", "additions": "1", "deletions": "4", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveMetastoreCatalogSuite.scala", "additions": "15", "deletions": "18", "changes": "33"}, "updated": [0, 0, 0]}]}
{"author": "ekoifman", "sha": "af5a71ab3d61056f0fa309e89868d57d584d0994", "commit_date": "2021/08/04 18:18:29", "commit_message": "[SPARK-36416][SQL] Add SQL metrics to AdaptiveSparkPlanExec for BHJs and Skew joins\n[SPARK-36416][SQL] fix typo", "title": "[SPARK-36416][SQL] Add SQL metrics to AdaptiveSparkPlanExec for BHJs and Skew joins", "body": "### What changes were proposed in this pull request?\r\n\r\nAdd \"num broadcast joins conversions\" and \"num skew join conversions\"\r\nmetrics to AdaptiveSparkPlanExec to report how many joins were changed to BHJ or had skew mitigated due to AQE.\r\n\r\n### Why are the changes needed?\r\n\r\nTo make it easy to get a sense of how much impact AQE had on an a complex query.\r\n\r\nIt's also useful for systems that collect metrics for later analysis of AQE effectiveness in large production deployment.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\n2 new SQL metrics will now be emitted\r\n\r\n### How was this patch tested?\r\n\r\nExisting and new Unit tests.", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "32", "deletions": "0", "changes": "32"}, "updated": [1, 3, 9]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "56", "deletions": "1", "changes": "57"}, "updated": [1, 4, 16]}]}
{"author": "ekoifman", "sha": "", "commit_date": "2021/01/04 16:14:33", "commit_message": "[SPARK-33875][SQL] Implement DESCRIBE COLUMN for v2 tables\n\n### What changes were proposed in this pull request?\n\nThis PR proposes to implement `DESCRIBE COLUMN` for v2 tables.\n\nNote that `isExnteded` option is not implemented in this PR.\n\n### Why are the changes needed?\n\nParity with v1 tables.\n\n### Does this PR introduce _any_ user-facing change?\n\nYes, now, `DESCRIBE COLUMN` works for v2 tables.\n```scala\nsql(\"CREATE TABLE testcat.tbl (id bigint, data string COMMENT 'hello') USING foo\")\nsql(\"DESCRIBE testcat.tbl data\").show\n```\n```\n+---------+----------+\n|info_name|info_value|\n+---------+----------+\n| col_name|      data|\n|data_type|    string|\n|  comment|     hello|\n+---------+----------+\n```\n\nBefore this PR, the command would fail with: `Describing columns is not supported for v2 tables.`\n\n### How was this patch tested?\n\nAdded new test.\n\nCloses #30881 from imback82/describe_col_v2.\n\nAuthored-by: Terry Kim <yuminkim@gmail.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/QueryCompilationErrors.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [2, 3, 7]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [2, 5, 31]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [1, 3, 20]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolvePartitionSpec.scala", "additions": "5", "deletions": "4", "changes": "9"}, "updated": [1, 2, 11]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/v2ResolutionPlans.scala", "additions": "20", "deletions": "2", "changes": "22"}, "updated": [1, 2, 9]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 6, 41]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2Commands.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 3, 24]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala", "additions": "21", "deletions": "7", "changes": "28"}, "updated": [1, 2, 30]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala", "additions": "28", "deletions": "4", "changes": "32"}, "updated": [1, 4, 27]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [1, 3, 11]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala", "additions": "20", "deletions": "9", "changes": "29"}, "updated": [1, 4, 29]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DescribeColumnExec.scala", "additions": "56", "deletions": "0", "changes": "56"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/describe-table-column.sql", "additions": "7", "deletions": "1", "changes": "8"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/describe-table-column.sql.out", "additions": "21", "deletions": "3", "changes": "24"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/describe.sql.out", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 5]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala", "additions": "47", "deletions": "6", "changes": "53"}, "updated": [1, 3, 38]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/PlanResolutionSuite.scala", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [1, 2, 13]}]}
{"author": "ivoson", "sha": "", "commit_date": "2021/05/17 05:53:52", "commit_message": "modify annotation", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreeNode.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 9]}]}
{"author": "sungpeo", "sha": "a35a6ea8a7c346e43a7e065eceed25aabcfd5a24", "commit_date": "2021/08/25 07:22:15", "commit_message": "SPARK-36582 Spark HistoryPage show 'NotFound' in not logged multiple attempts\n\nattemptId should be always, so deleted hasMultipleAttempts", "title": "[SPARK-36582][UI] Spark HistoryPage show 'NotFound' in not logged multiple attempts", "body": "### What changes were proposed in this pull request?\r\n\r\n`attemptId` should be always, so deleted `hasMultipleAttempts`\r\n\r\n### Why are the changes needed?\r\n\r\nDescribed in https://issues.apache.org/jira/browse/SPARK-36582\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\n#### before change\r\n\r\nWhen all applications in spark history had an attempt only, it doesn't show attemptId\r\n\r\n![Screen Shot 2021-08-26 at 10 47 23 AM](https://user-images.githubusercontent.com/13159599/130886943-36666846-4dca-4687-9e3f-9c6d339207bd.png)\r\n\r\nNow it shows a attemptId column regardless of hasMultipleAttempts.\r\n\r\n![Screen Shot 2021-08-26 at 10 04 43 AM](https://user-images.githubusercontent.com/13159599/130883769-b10916ef-ccaa-4811-8e70-fa27e8a8fceb.png)\r\n\r\n\r\n### How was this patch tested?\r\n\r\nI checked chrome developer tool's console in changed web ui. (History Server's home page)\r\n", "failed_tests": ["org.apache.spark.sql.streaming.FileStreamSinkV2Suite"], "files": [{"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/historypage-template.html", "additions": "0", "deletions": "2", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/historypage.js", "additions": "6", "deletions": "17", "changes": "23"}, "updated": [0, 0, 0]}]}
{"author": "hddong", "sha": "", "commit_date": "2021/04/20 06:13:17", "commit_message": "[SPARK-35143][SQL][SHELL]Add default log level config for spark-sql", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "conf/log4j.properties.template", "additions": "5", "deletions": "3", "changes": "8"}, "updated": [0, 0, 1]}]}
{"author": "fhygh", "sha": "e604f1ba111d208526ec3d49799fed063dc5c990", "commit_date": "2021/09/01 07:22:47", "commit_message": "[SPARK-36604][SQL] timestamp type column stats result consistent with\nthe time zone", "title": "[SPARK-36604][SQL] timestamp type column stats result consistent with the time zone", "body": "\r\n### What changes were proposed in this pull request?\r\ntimestamp type column stats result should consistent with time zone\r\n\r\n\r\n### Why are the changes needed?\r\nfor now timestamp type column stats result is based on UTC TimeZone\uff0cindependent from the time zone. Thus the stats result may not correct\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n\r\n### How was this patch tested?\r\nAdd new test\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/interface.scala", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/StatisticsCollectionSuite.scala", "additions": "39", "deletions": "12", "changes": "51"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/StatisticsSuite.scala", "additions": "29", "deletions": "19", "changes": "48"}, "updated": [0, 0, 0]}]}
{"author": "fhygh", "sha": "691bb5c0add0420ca51eacfb41b384eefb9637cc", "commit_date": "2021/08/17 06:39:37", "commit_message": "[SPARK-36518][Deploy] Spark should support distribute directory to\ncluster", "title": "[SPARK-36518][Deploy] Spark should support distribute directory to cluster", "body": "\r\n### What changes were proposed in this pull request?\r\nsupport distribute directory to cluster via --files\r\nbefore:\r\n[root@kwephispra41893 spark]# ll /opt/ygh/testdir/\r\ntotal 8\r\ndrwxr-xr-x 2 root root 4096 Aug 17 16:07 dd1\r\ndrwxr-xr-x 2 root root 4096 Aug 17 16:07 dd2\r\n-rw-r--r-- 1 root root    0 Aug 17 16:07 t1.txt\r\n-rw-r--r-- 1 root root    0 Aug 17 16:07 t2.txt\r\n-rw-r--r-- 1 root root    0 Aug 17 16:07 t3.conf\r\n\r\nspark-shell --master yarn --files file:///opt/ygh/testdir\r\n![image](https://user-images.githubusercontent.com/25889738/129689226-d63cc7f6-c529-4c6f-a94d-d48c062dbf29.png)\r\n\r\nafter:\r\nspark-shell --master yarn --files file:///opt/ygh/testdir\r\n![image](https://user-images.githubusercontent.com/25889738/129689406-432c796c-52e5-49c1-8016-9eec151257fb.png)\r\n\r\n\r\n### Why are the changes needed?\r\nwhen we submit spark application we can't directly distribute a directory to cluster\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo, user do not need change any code\r\n\r\n\r\n### How was this patch tested?\r\ntested by existing UT\r\n", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/util/DependencyUtils.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "fhygh", "sha": "0abed27967ad76b9b30d70d47d907388aa417b98", "commit_date": "2021/06/26 23:47:00", "commit_message": "[SPARK-35902][Core]spark.driver.log.dfsDir with hdfs scheme failed", "title": "[SPARK-35902][Core] spark.driver.log.dfsDir with hdfs scheme failed", "body": "### What changes were proposed in this pull request?\r\nwhen persist driver logs in client mode to dfs, log dir should support scheme path\r\n\r\n\r\n### Why are the changes needed?\r\nwhen spark.driver.log.dfsDir start with scheme like hdfs\uff0cit failed  \r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nno, user do not need change any code\r\n\r\n\r\n### How was this patch tested?\r\ntested by existing UT\r\n", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/util/logging/DriverLogger.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}]}
{"author": "Kimahriman", "sha": "8b53f7b2193836265437201d27409ae7b0619475", "commit_date": "2021/07/22 13:02:02", "commit_message": "Track conditionally evaluated expressions to resolve as subexpressions for cases they are already being evaluated", "title": "[SPARK-35564][SQL] Support subexpression elimination for conditionally evaluated expressions", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error message, please read the guideline first:\r\n     https://spark.apache.org/error-message-guidelines.html\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nI am proposing to add support for conditionally evaluated expressions during subexpression elimination. Currently, only expressions that will definitely be always at least twice are candidates for subexpression elimination. This PR updates that logic so that expressions that are always evaluated at least once and conditionally evaluated at least once are also candidates for subexpression elimination. This helps optimize a common case during data normalization and cleaning and want to null out values that don't match a certain pattern, where you have something like:\r\n\r\n```\r\ntransformed = F.regexp_replace(F.lower(F.trim('my_column')))\r\ndf.withColumn('normalized_value', F.when(F.length(transformed) > 0, transformed))\r\n```\r\nor\r\n```\r\ndf.withColumn('normalized_value', F.when(transformed.rlike(<some regex>), transformed))\r\n```\r\n\r\nIn these cases, `transformed` will always be fully calculated twice, because it might only be needed once. I am proposing creating a subexpression for `transformed` in this case.\r\n\r\nIn practice I've seen a decrease in runtime and codegen size of 10-30% in our production pipelines that heavily make use of this type of logic.\r\n\r\nThe only potential downside is creating extra subexpressions, and therefore function calls, more than necessary. This should only be an issue for certain edge cases where your conditional overwhelming evaluates to false. And then the only overhead is running your conditional logic potentially in a separate function rather than inlined in the codegen. I added a config to control this behavior if that is actually a real concern to anyone, but I'd be happy to just remove the config.\r\n\r\nI also updated some of the existing logic for common expressions in coalesce and when that are actually better handled by the new logic, since you are only guaranteed to have the first value of a Coalesce evaluated, as well as the first conditional of a CaseWhen expression.\r\n\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nTo increase the performance of conditional expressions.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nNo, just performance improvements.\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nNew and updated UT.\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala", "additions": "113", "deletions": "76", "changes": "189"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [2, 6, 18]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/SubexpressionEliminationSuite.scala", "additions": "55", "deletions": "13", "changes": "68"}, "updated": [0, 0, 4]}]}
{"author": "Kimahriman", "sha": "", "commit_date": "2021/05/06 00:41:00", "commit_message": "Use StructType merging for unionByName with null filling", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveUnion.scala", "additions": "34", "deletions": "111", "changes": "145"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/SchemaPruning.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 4, 20]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/types/StructType.scala", "additions": "10", "deletions": "14", "changes": "24"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SchemaMergeUtils.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSetOperationsSuite.scala", "additions": "110", "deletions": "33", "changes": "143"}, "updated": [0, 0, 0]}]}
{"author": "noslowerdna", "sha": "", "commit_date": "2021/08/24 14:29:26", "commit_message": "[SPARK-36419][CORE] Optionally move final aggregation in RDD.treeAggregate to executor\n\n## What changes were proposed in this pull request?\n\nMove final iteration of aggregation of RDD.treeAggregate to an executor with one partition and fetch that result to the driver\n\n## Why are the changes needed?\n1. RDD.fold pulls all shuffle partitions to the driver to merge the result\n        a. Driver becomes a single point of failure in the case that there are a lot of partitions to do the final aggregation on\n2. Shuffle machinery at executors is much more robust/fault tolerant compared to fetching results to driver.\n\n## Does this PR introduce any user-facing change?\nThe previous behavior always did the final aggregation in the driver. The user can now (optionally) provide a boolean config (default = false) ENABLE_EXECUTOR_TREE_AGGREGATE to do that final aggregation in a single partition executor before fetching the results to the driver. The only additional cost is that the user will see an extra stage in their job.\n\n## How was this patch tested?\nThis patch was tested via unit tests, and also tested on a cluster.\nThe screenshots showing the extra stage on a cluster are attached below (before vs after).\n![before](https://user-images.githubusercontent.com/24758726/128249830-eefc4bda-f737-4d68-960e-1d1907762538.png)\n![after](https://user-images.githubusercontent.com/24758726/128249838-be70bc95-9f39-489c-be17-c9c80c4846a4.png)\n\nCloses #33644 from akpatnam25/SPARK-36419.\n\nAuthored-by: Aravind Patnam <apatnam@linkedin.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala", "additions": "13", "deletions": "0", "changes": "13"}, "updated": [1, 1, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/rdd/RDD.scala", "additions": "31", "deletions": "1", "changes": "32"}, "updated": [1, 1, 2]}, {"file": {"name": "core/src/test/java/test/org/apache/spark/JavaAPISuite.java", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [1, 1, 1]}, {"file": {"name": "core/src/test/scala/org/apache/spark/rdd/RDDSuite.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [1, 1, 1]}]}
{"author": "maropu", "sha": "3be38824fec054d1556c7e347def0b54413e563d", "commit_date": "2021/07/16 02:22:14", "commit_message": "Update the golden file", "title": "[SPARK-34819][SQL] MapType supports comparable semantics", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error message, please read the guideline first:\r\n     https://spark.apache.org/error-message-guidelines.html\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nThis PR proposes to support comparable semantics for map types.\r\n\r\nNOTE: This PR is the rework of #31967(@WangGuangxin)/#15970(@hvanhovell).\r\n\r\nThe approach of the PR is similar to `NormalizeFloatingNumbers` and it has the same restriction; in the plan optimizing phase, a new rule named `NormalizeMaps` inserts an expression `SortMapKeys` to make sure two maps having the same key value pairs but with different key ordering are equal (e.g., Map('a' -> 1, 'b' -> 2) should equal to Map('b' -> 2, 'a' -> 1). As for aggregates, this rule is applied in the physical planning phase because all the grouping exprs are not extracted during the logical phase (This is the same restriction with `NormalizeFloatingNumbers`).\r\n\r\nThe major differences from `NormalizeFloatingNumbers` are as follows;\r\n - The rule covers all the binary comparisons (`EqualTo`, `GreaterThan`, ...) and `In`/`InSet` in a plan (`NormalizeFloatingNumbers` is applied only into the `EqualTo` comparison in a join plan, an equi-join).\r\n - This rule does not apply `normalize` recursively and just adds a `SortMapKeys` expr just on each top-level expr (e.g., top-level grouping expr and left/right side expr of binary comparisons).\r\n - This rule additionally handles `SortOrder`s in sort-related plans.\r\n\r\nFor sorting map entries, I reused the array ordering logic (See: `MapType.compare` and `CodegenContext.genComp`) because keys and values in map entries follow the array format; it checks if key arrays in two maps are the same first, an then check if value arrays are the same. \r\n\r\nNOTE: Adding duplicate `SortMapKeys` exprs in a binary comparison tree is a known issue; for example, in a query below, `MapType`'s column, `a`, is sorted twice;\r\n```\r\nscala> Seq((Map(1->1), Map(1->2), Map(1->1))).toDF(\"a\", \"b\", \"c\").write.saveAsTable(\"t\")\r\nscala> sql(\"select * from t where a = b and a = c\").explain()\r\n== Physical Plan ==\r\n*(1) Filter ((sortmapkeys(a#35) = sortmapkeys(b#36)) AND (sortmapkeys(a#35) = sortmapkeys(c#37)))\r\n+- FileScan parquet default.t[a#35,b#36,c#37] Batched: false, DataFilters: [(sortmapkeys(a#35) = sortmapkeys(b#36)), (sortmapkeys(a#35) = sortmapkeys(c#37))], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/maropu/Repositories/spark/spark-master/spark-warehouse/t], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<a:map<int,int>,b:map<int,int>,c:map<int,int>>\r\n```\r\nBut, I don't have a smart idea to avoid it in this PR for now. Probably, I think common subexpression elimination in filter plans can solve it, but Spark does not have the optimization now. (Fro more details, see the previous @viirya PR: https://github.com/apache/spark/pull/30565).\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nTo improve map usability.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nYes, a user can use map-typed data in GROUP BY, ORDER BY, and PARTITION BY in WINDOW clauses.\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nAdd unit tests.", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeMapData.java", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala", "additions": "0", "deletions": "16", "changes": "16"}, "updated": [0, 3, 15]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "27", "deletions": "0", "changes": "27"}, "updated": [1, 3, 10]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ordering.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeFloatingNumbers.scala", "additions": "22", "deletions": "4", "changes": "26"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeMaps.scala", "additions": "192", "deletions": "0", "changes": "192"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "8", "deletions": "2", "changes": "10"}, "updated": [0, 2, 13]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/ArrayBasedMapData.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/MapData.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/TypeUtils.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/types/ArrayType.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/types/MapType.scala", "additions": "28", "deletions": "0", "changes": "28"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala", "additions": "15", "deletions": "25", "changes": "40"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/ExpressionTypeCheckingSuite.scala", "additions": "2", "deletions": "17", "changes": "19"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/PredicateSuite.scala", "additions": "0", "deletions": "8", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeFloatingPointNumbersSuite.scala", "additions": "63", "deletions": "1", "changes": "64"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeMapsSuite.scala", "additions": "108", "deletions": "0", "changes": "108"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/ComplexDataSuite.scala", "additions": "22", "deletions": "16", "changes": "38"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "7", "deletions": "32", "changes": "39"}, "updated": [0, 1, 6]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala", "additions": "33", "deletions": "0", "changes": "33"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/comparable-map-explain.sql", "additions": "34", "deletions": "0", "changes": "34"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/comparable-map.sql", "additions": "182", "deletions": "0", "changes": "182"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/comparable-map-explain.sql.out", "additions": "130", "deletions": "0", "changes": "130"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/comparable-map.sql.out", "additions": "986", "deletions": "0", "changes": "986"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/pivot.sql.out", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/udf/udf-pivot.sql.out", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameAggregateSuite.scala", "additions": "8", "deletions": "10", "changes": "18"}, "updated": [1, 2, 6]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameFunctionsSuite.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSetOperationsSuite.scala", "additions": "0", "deletions": "19", "changes": "19"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 2, 7]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/sources/BucketedWriteSuite.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 0, 0]}]}
{"author": "maropu", "sha": "", "commit_date": "2020/08/20 13:03:03", "commit_message": "Invokes GitHub Actions", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": ".github/workflows/build_and_test.yml", "additions": "1", "deletions": "4", "changes": "5"}, "updated": [2, 2, 2]}, {"file": {"name": "dev/run-tests.py", "additions": "2", "deletions": "16", "changes": "18"}, "updated": [0, 1, 11]}]}
{"author": "eddyxu", "sha": "", "commit_date": "2021/03/02 15:14:19", "commit_message": "[SPARK-34558][SQL] warehouse path should be qualified ahead of populating and use\n\n### What changes were proposed in this pull request?\n\nCurrently, the warehouse path gets fully qualified in the caller side for creating a database, table, partition, etc. An unqualified path is populated into Spark and Hadoop confs, which leads to inconsistent API behaviors.  We should make it qualified ahead.\n\nWhen the value is a relative path `spark.sql.warehouse.dir=lakehouse`, some behaviors become inconsistent, for example.\n\nIf the default database is absent at runtime, the app fails with\n\n```java\nCaused by: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: file:./lakehouse\n\tat org.apache.hadoop.fs.Path.initialize(Path.java:263)\n\tat org.apache.hadoop.fs.Path.<init>(Path.java:254)\n\tat org.apache.hadoop.hive.metastore.Warehouse.getDnsPath(Warehouse.java:133)\n\tat org.apache.hadoop.hive.metastore.Warehouse.getDnsPath(Warehouse.java:137)\n\tat org.apache.hadoop.hive.metastore.Warehouse.getWhRoot(Warehouse.java:150)\n\tat org.apache.hadoop.hive.metastore.Warehouse.getDefaultDatabasePath(Warehouse.java:163)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB_core(HiveMetaStore.java:636)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:655)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:431)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:79)\n\t... 73 more\n```\n\nIf the default database is present at runtime, the app can work with it, and if we create a database, it gets fully qualified, for example\n\n```sql\nspark-sql> create database test;\nTime taken: 0.052 seconds\nspark-sql> desc database test;\nDatabase Name\ttest\nComment\nLocation\tfile:/Users/kentyao/Downloads/spark/spark-3.2.0-SNAPSHOT-bin-20210226/lakehouse/test.db\nOwner\tkentyao\nTime taken: 0.023 seconds, Fetched 4 row(s)\n```\n\nAnother thing is that the log becomes nubilous, for example.\n\n```logtalk\n21/02/27 13:54:17 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('datalake').\n21/02/27 13:54:17 INFO SharedState: Warehouse path is 'lakehouse'.\n```\n\n### Why are the changes needed?\n\nfix bug and ambiguity\n### Does this PR introduce _any_ user-facing change?\n\nyes, the path now resolved with proper order - `warehouse->database->table->partition`\n\n### How was this patch tested?\n\nw/ ut added\n\nCloses #31671 from yaooqinn/SPARK-34558.\n\nAuthored-by: Kent Yao <yao@apache.org>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/internal/SharedState.scala", "additions": "9", "deletions": "7", "changes": "16"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SparkSessionBuilderSuite.scala", "additions": "34", "deletions": "7", "changes": "41"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveSharedStateSuite.scala", "additions": "12", "deletions": "8", "changes": "20"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveSparkSubmitSuite.scala", "additions": "7", "deletions": "5", "changes": "12"}, "updated": [1, 1, 2]}]}
{"author": "holdenk", "sha": "", "commit_date": "2020/11/18 20:39:00", "commit_message": "[SPARK-32381][CORE][SQL][FOLLOWUP] More cleanup on HadoopFSUtils\n\n### What changes were proposed in this pull request?\n\nThis PR is a follow-up of #29471 and does the following improvements for `HadoopFSUtils`:\n1. Removes the extra `filterFun` from the listing API and combines it with the `filter`.\n2. Removes `SerializableBlockLocation` and `SerializableFileStatus` given that `BlockLocation` and `FileStatus` are already serializable.\n3. Hides the `isRootLevel` flag from the top-level API.\n\n### Why are the changes needed?\n\nMain purpose is to simplify the logic within `HadoopFSUtils` as well as cleanup the API.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting unit tests (e.g., `FileIndexSuite`)\n\nCloses #29959 from sunchao/hadoop-fs-utils-followup.\n\nAuthored-by: Chao Sun <sunchao@apple.com>\nSigned-off-by: Holden Karau <hkarau@apple.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/util/HadoopFSUtils.scala", "additions": "19", "deletions": "85", "changes": "104"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/command/CommandUtils.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InMemoryFileIndex.scala", "additions": "11", "deletions": "8", "changes": "19"}, "updated": [1, 1, 2]}]}
{"author": "aokolnychyi", "sha": "ad57d05af8108f60a676e71b53bb8e4514cdb972", "commit_date": "2021/06/17 20:26:36", "commit_message": "[SPARK-35801][SQL] Support DELETE operations that require rewriting data", "title": "[WIP][SPARK-35801][SQL] Support DELETE operations that require rewriting data", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error message, please read the guideline first:\r\n     https://spark.apache.org/error-message-guidelines.html\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\nThis WIP PR shows how we can use the proposed API in SPARK-35801 (per [design doc](https://docs.google.com/document/d/12Ywmc47j3l2WF4anG5vL4qlrhT2OKigb7_EbIKhxg60)) to support DELETE statements that require rewriting data.\r\n\r\n**Note**: This PR must be split into a number of smaller PRs if we decide to adopt this approach. All changes are grouped here only to simplify the review process and support the design doc.\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\nThese changes are required so that Spark can provide support for DELETE, UPDATE, MERGE statements.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\nYes, this PR introduces a set of new APIs for Data Source V2.\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\n\r\nThis PR comes with a trivial test. More tests to come.\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/SupportsRowLevelOperations.java", "additions": "43", "deletions": "0", "changes": "43"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/DeltaBatchWrite.java", "additions": "30", "deletions": "0", "changes": "30"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/DeltaWrite.java", "additions": "30", "deletions": "0", "changes": "30"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/DeltaWriter.java", "additions": "38", "deletions": "0", "changes": "38"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/DeltaWriterFactory.java", "additions": "33", "deletions": "0", "changes": "33"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/RowLevelOperation.java", "additions": "90", "deletions": "0", "changes": "90"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/RowLevelOperationBuilder.java", "additions": "34", "deletions": "0", "changes": "34"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/SupportsDelta.java", "additions": "35", "deletions": "0", "changes": "35"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/InternalRowProjection.scala", "additions": "91", "deletions": "0", "changes": "91"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [2, 7, 20]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/RewriteDeleteFromTable.scala", "additions": "186", "deletions": "0", "changes": "186"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/ReplaceNullWithFalseInPredicate.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/SimplifyConditionalsInPredicate.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/planning/patterns.scala", "additions": "56", "deletions": "0", "changes": "56"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2Commands.scala", "additions": "72", "deletions": "5", "changes": "77"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/write/RowLevelOperationTable.scala", "additions": "47", "deletions": "0", "changes": "47"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Implicits.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/PullupCorrelatedPredicatesSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryRowLevelOperationTable.scala", "additions": "155", "deletions": "0", "changes": "155"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryRowLevelOperationTableCatalog.scala", "additions": "46", "deletions": "0", "changes": "46"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryTable.scala", "additions": "29", "deletions": "7", "changes": "36"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [2, 3, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala", "additions": "7", "deletions": "2", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala", "additions": "10", "deletions": "1", "changes": "11"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/OptimizeMetadataOnlyDeleteFromTable.scala", "additions": "63", "deletions": "0", "changes": "63"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ReplaceRowLevelOperations.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala", "additions": "30", "deletions": "1", "changes": "31"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2Writes.scala", "additions": "21", "deletions": "2", "changes": "23"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2Exec.scala", "additions": "77", "deletions": "6", "changes": "83"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/DeleteFromTableSuite.scala", "additions": "89", "deletions": "0", "changes": "89"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/PlanResolutionSuite.scala", "additions": "9", "deletions": "6", "changes": "15"}, "updated": [0, 0, 3]}]}
{"author": "aokolnychyi", "sha": "", "commit_date": "2021/06/15 07:15:47", "commit_message": "[SPARK-35779][SQL] Dynamic filtering for Data Source V2", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "external/avro/src/test/scala/org/apache/spark/sql/avro/AvroRowReaderSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "external/avro/src/test/scala/org/apache/spark/sql/avro/AvroSuite.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/read/SupportsRuntimeFiltering.java", "additions": "61", "deletions": "0", "changes": "61"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryTable.scala", "additions": "61", "deletions": "5", "changes": "66"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala", "additions": "14", "deletions": "1", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/BatchScanExec.scala", "additions": "60", "deletions": "7", "changes": "67"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala", "additions": "7", "deletions": "3", "changes": "10"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/dynamicpruning/CleanupDynamicPruningFilters.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/dynamicpruning/PartitionPruning.scala", "additions": "30", "deletions": "2", "changes": "32"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DynamicPartitionPruningSuite.scala", "additions": "120", "deletions": "85", "changes": "205"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/FileBasedDataSourceSuite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcV2SchemaPruningSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/ui/SQLAppStatusListenerSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 2]}]}
{"author": "WangGuangxin", "sha": "", "commit_date": "2020/09/15 05:38:01", "commit_message": "[SPARK-32884][TESTS] Mark TPCDSQuery*Suite as ExtendedSQLTest\n\n### What changes were proposed in this pull request?\n\nThis PR aims to mark the following suite as `ExtendedSQLTest` to reduce GitHub Action test time.\n- TPCDSQuerySuite\n- TPCDSQueryANSISuite\n- TPCDSQueryWithStatsSuite\n\n### Why are the changes needed?\n\nCurrently, the longest GitHub Action task is `Build and test / Build modules: sql - other tests` with `1h 57m 10s` while `Build and test / Build modules: sql - slow tests` takes `42m 20s`. With this PR, we can move the workload from `other tests` to `slow tests` task and reduce the total waiting time about 7 ~ 8 minutes.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo. This is a test-only change.\n\n### How was this patch tested?\n\nPass the GitHub Action with the reduced running time.\n\nCloses #29755 from dongjoon-hyun/SPARK-SLOWTEST.\n\nAuthored-by: Dongjoon Hyun <dongjoon@apache.org>\nSigned-off-by: HyukjinKwon <gurwls223@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/TPCDSQuerySuite.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [1, 1, 5]}]}
{"author": "dnskr", "sha": "7d4d4e39a31a815f97b55228f1123c7f0edb2afa", "commit_date": "2021/08/14 10:33:16", "commit_message": "Merge branch 'apache:master' into docs/SPARK-36510", "title": "[SPARK-36510][DOCS] Add spark.redaction.string.regex property to the docs", "body": "### What changes were proposed in this pull request?\r\nThe PR fixes [SPARK-36510](https://issues.apache.org/jira/browse/SPARK-36510) by adding missing `spark.redaction.string.regex` property to the docs\r\n\r\n### Why are the changes needed?\r\nThe property referred by `spark.sql.redaction.string.regex` description as its default value\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\nNot needed for docs\r\n", "failed_tests": [], "files": [{"file": {"name": "docs/configuration.md", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [1, 2, 3]}]}
{"author": "dnskr", "sha": "", "commit_date": "2021/08/14 05:31:21", "commit_message": "[SPARK-34952][SQL][FOLLOWUP] Normalize pushed down aggregate col name and group by col name\n\n### What changes were proposed in this pull request?\nNormalize pushed down aggregate col names and group by col names ...\n\n### Why are the changes needed?\nto handle case sensitive col names\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nModify existing test\n\nCloses #33739 from huaxingao/normalize.\n\nAuthored-by: Huaxin Gao <huaxin_gao@apple.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala", "additions": "7", "deletions": "3", "changes": "10"}, "updated": [1, 2, 5]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 1, 4]}]}
{"author": "wankunde", "sha": "7f823ae04db1cbc70577e04a827699191c7b3bf9", "commit_date": "2021/06/10 14:07:06", "commit_message": "[SPAKR-35713]Bug fix for thread leak in JobCancellationSuite", "title": "[SPARK-35713]Bug fix for thread leak in JobCancellationSuite", "body": "### What changes were proposed in this pull request?\r\n\r\nBug fix for thread leak in JobCancellationSuite UT\r\n\r\n### Why are the changes needed?\r\n\r\nWhen we call Thread.interrupt() method, that thread's interrupt status will be set but it may not really interrupt.\r\nSo when spark task runs in an infinite loop, spark context may fail to interrupt the task thread, and resulting in thread leak. \r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nNo\r\n\r\n### How was this patch tested?\r\n\r\nTest case \"task reaper kills JVM if killed tasks keep running for too long\" in JobCancellationSuite\r\n", "failed_tests": [], "files": [{"file": {"name": "core/src/test/scala/org/apache/spark/JobCancellationSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "wankunde", "sha": "", "commit_date": "2021/03/10 06:55:27", "commit_message": "[SPARK-34681][SQL] Fix bug for full outer shuffled hash join when building left side with non-equal condition\n\n### What changes were proposed in this pull request?\n\nFor full outer shuffled hash join with building hash map on left side, and having non-equal condition, the join can produce wrong result.\n\nThe root cause is `boundCondition` in `HashJoin.scala` always assumes the left side row is `streamedPlan` and right side row is `buildPlan` ([streamedPlan.output ++ buildPlan.output](https://github.com/apache/spark/blob/branch-3.1/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashJoin.scala#L141)). This is valid assumption, except for full outer + build left case.\n\nThe fix is to correct `boundCondition` in `HashJoin.scala` to handle full outer + build left case properly. See reproduce in https://issues.apache.org/jira/browse/SPARK-32399?focusedCommentId=17298414&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17298414 .\n\n### Why are the changes needed?\n\nFix data correctness bug.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nChanged the test in `OuterJoinSuite.scala` to cover full outer shuffled hash join.\nBefore this change, the unit test `basic full outer join using ShuffledHashJoin` in `OuterJoinSuite.scala` is failed.\n\nCloses #31792 from c21/join-bugfix.\n\nAuthored-by: Cheng Su <chengsu@fb.com>\nSigned-off-by: Dongjoon Hyun <dhyun@apple.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashJoin.scala", "additions": "7", "deletions": "1", "changes": "8"}, "updated": [2, 2, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/joins/OuterJoinSuite.scala", "additions": "10", "deletions": "12", "changes": "22"}, "updated": [1, 1, 1]}]}
{"author": "sumeetgajjar", "sha": "", "commit_date": "2021/05/13 23:30:17", "commit_message": "Intial fix v2", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala", "additions": "1", "deletions": "3", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala", "additions": "64", "deletions": "28", "changes": "92"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/Utils.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/scala/org/apache/spark/storage/BlockManagerSuite.scala", "additions": "16", "deletions": "4", "changes": "20"}, "updated": [0, 0, 1]}]}
{"author": "yeshengm", "sha": "", "commit_date": "2021/08/06 20:47:26", "commit_message": "[SPARK-36448] Exceptions in NoSuchItemException.scala have to be case classes to preserve specific exceptions", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.kafka010.KafkaMicroBatchV2SourceWithAdminSuite", "org.apache.spark.sql.hive.execution.command.ShowTablesSuite", "org.apache.spark.sql.execution.command.v1.ShowTablesSuite", "org.apache.spark.sql.execution.datasources.v2.V2SessionCatalogNamespaceSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/NoSuchItemException.scala", "additions": "23", "deletions": "11", "changes": "34"}, "updated": [0, 0, 0]}]}
{"author": "zsxwing", "sha": "", "commit_date": "2021/04/23 22:33:11", "commit_message": "fix", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/streaming/continuous/ContinuousSuite.scala", "additions": "13", "deletions": "9", "changes": "22"}, "updated": [0, 0, 0]}]}
{"author": "tooptoop4", "sha": "", "commit_date": "2021/07/14 02:36:50", "commit_message": "[SQL] Warn if less files visible after stats write\n\n[SQL] Warn if less files visible after stats write", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/BasicWriteStatsTracker.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "Cedric-Magnan", "sha": "", "commit_date": "2021/08/09 09:18:06", "commit_message": "[SPARK-36271][SQL] Unify V1 insert check field name before prepare writter\n\n### What changes were proposed in this pull request?\nUnify DataSource V1 insert schema check field name before prepare writer.\nAnd in this PR we add check for avro V1 insert too.\n\n### Why are the changes needed?\nUnify code and add check for avro V1 insert too.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nAdded UT\n\nCloses #33566 from AngersZhuuuu/SPARK-36271.\n\nAuthored-by: Angerszhuuuu <angers.zhu@gmail.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "external/avro/src/test/scala/org/apache/spark/sql/avro/AvroSuite.scala", "additions": "22", "deletions": "0", "changes": "22"}, "updated": [1, 3, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceUtils.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [1, 2, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [1, 2, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/FileWrite.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetWrite.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/FileBasedDataSourceSuite.scala", "additions": "22", "deletions": "0", "changes": "22"}, "updated": [1, 2, 4]}]}
{"author": "linar-jether", "sha": "b324a5de0fa4842f6af340967d8397ffe27d2903", "commit_date": "2021/06/07 14:04:46", "commit_message": "Merge branch 'master' into pandas-rdd-to-spark-df-SPARK-3284", "title": "[SPARK-32846][SQL][PYTHON] Support createDataFrame from an RDD of pd.DataFrames", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\nAdded support to `createDataFrame` to receive an RDD of `pd.DataFrame` objects, and convert them using arrow into an RDD of record batches which is then directly converted to a spark DF.\r\n\r\nAdded a `pandasRDD` flag to `createDataFrame` to distinguish between `RDD[pd.DataFrame]` and other RDDs without peeking into their content.\r\n\r\n```python\r\nfrom pyspark.sql import SparkSession\r\nimport pyspark\r\nimport pyarrow as pa\r\nimport numpy as np\r\nimport pandas as pd\r\nimport re\r\n\r\nspark = SparkSession \\\r\n    .builder \\\r\n    .master(\"local\") \\\r\n    .appName(\"Python RDD[pd.DataFrame] to spark DF example\") \\\r\n    .getOrCreate()\r\n\r\n# Enable Arrow-based columnar data transfers\r\nspark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\r\nsc = spark.sparkContext\r\n\r\n# Create a spark DF from an RDD of pandas DFs\r\nprdd = sc.range(0, 4).map(lambda x: pd.DataFrame([[x,]*4], columns=list('ABCD')))\r\n\r\nprdd_large = sc.range(0, 32, numSlices=32). \\\r\n    map(lambda x: pd.DataFrame(np.random.randint(0, 100, size=(40 << 15, 4)), columns=list('ABCD')))\r\n\r\ndf = spark.createDataFrame(prdd, schema=None, pandasRDD=True)\r\ndf.toPandas()\r\n```\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\n\r\n### How was this patch tested?\r\nAdded a new test using for creating a spark DF from an RDD of pandas dataframes. \r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\n", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/sql/pandas/conversion.py", "additions": "108", "deletions": "25", "changes": "133"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/pandas/types.py", "additions": "36", "deletions": "0", "changes": "36"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/session.py", "additions": "20", "deletions": "7", "changes": "27"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_arrow.py", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [0, 1, 1]}]}
{"author": "brandondahler", "sha": "2314d1d41f26973a27f6576aeec2f7c6de6dff4f", "commit_date": "2021/06/14 15:09:40", "commit_message": "[SPARK-35739][SQL] Add Java-compatible Dataset.join overloads", "title": "[SPARK-35739][SQL] Add Java-compatible Dataset.join overloads", "body": "### What changes were proposed in this pull request?\r\nAdds 3 new syntactic sugar overloads to Dataset's join method as proposed in [SPARK-35739](https://issues.apache.org/jira/browse/SPARK-35739).\r\n\r\n### Why are the changes needed?\r\nImproved development experience for developers using Spark SQL, specifically when coding in Java.  \r\n\r\nPrior to changes the Seq overloads required developers to use less-known Java-to-Scala converter methods that made code less readable.  The overloads internalize those converter calls for two of the new methods and the third method adds a single-item overload that is useful for both Java and Scala.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes, the three new overloads technically constitute an API change to the Dataset class.  These overloads are net-new and have been commented appropriately in line with the existing methods.\r\n\r\n### How was this patch tested?\r\nTest cases were not added because it is unclear to me where/how syntactic sugar overloads fit into the testing suites (if at all).  Happy to add them if I can be pointed in the correct direction.\r\n\r\n* Changes were tested in Scala via spark-shell.\r\n* Changes were tested in Java by modifying an example:\r\n  ```\r\n  diff --git a/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java b/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\r\n  index 86a9045d8a..342810c1e6 100644\r\n  --- a/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\r\n  +++ b/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\r\n  @@ -124,6 +124,10 @@ public class JavaSparkSQLExample {\r\n       // |-- age: long (nullable = true)\r\n       // |-- name: string (nullable = true)\r\n\r\n  +    df.join(df, new String[] {\"age\"}).show();\r\n  +    df.join(df, \"age\", \"left\").show();\r\n  +    df.join(df, new String[] {\"age\"}, \"left\").show();\r\n  +\r\n       // Select only the \"name\" column\r\n       df.select(\"name\").show();\r\n       // +-------+\r\n  ```", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala", "additions": "79", "deletions": "2", "changes": "81"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameJoinSuite.scala", "additions": "55", "deletions": "0", "changes": "55"}, "updated": [0, 0, 0]}]}
{"author": "brandondahler", "sha": "", "commit_date": "2021/08/14 05:31:21", "commit_message": "[SPARK-34952][SQL][FOLLOWUP] Normalize pushed down aggregate col name and group by col name\n\n### What changes were proposed in this pull request?\nNormalize pushed down aggregate col names and group by col names ...\n\n### Why are the changes needed?\nto handle case sensitive col names\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nModify existing test\n\nCloses #33739 from huaxingao/normalize.\n\nAuthored-by: Huaxin Gao <huaxin_gao@apple.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala", "additions": "7", "deletions": "3", "changes": "10"}, "updated": [1, 2, 5]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 1, 4]}]}
{"author": "Yikun", "sha": "3f5102d5be8240053b7092b329ba71f67220770c", "commit_date": "2021/05/07 08:33:51", "commit_message": "address nit", "title": "[SPARK-35173][SQL][PYTHON] Add multiple columns adding support", "body": "### What changes were proposed in this pull request?\r\nThis PR added the multiple columns adding support for Spark scala/java/python API.\r\n- Expose `withColumns` with Map input as public API in Scala/Java\r\n- Add `withColumns` in PySpark\r\n\r\nThere was also some discussion about adding multiple columns in past JIRA([SPARK-1225](https://issues.apache.org/jira/browse/SPARK-12225), [SPARK-26224](https://issues.apache.org/jira/browse/SPARK-26224)) and [ML](http://apache-spark-developers-list.1001551.n3.nabble.com/DISCUSS-Multiple-columns-adding-replacing-support-in-PySpark-DataFrame-API-td31164.html).\r\n\r\n### Why are the changes needed?\r\nThere were a private method `withColumns` can add columns at one pass [1]:\r\nhttps://github.com/apache/spark/blob/b5241c97b17a1139a4ff719bfce7f68aef094d95/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L2402\r\n\r\nHowever, it was not exposed as public API in Scala/Java, and also PySpark user can only use `withColumn` to add\u00a0one column or replacing the existing one column\u00a0that\u00a0has the same name. \r\n\r\nFor example, if the PySpark user want to add multiple columns, they should call `withColumn` again and again like:\r\n```Python\r\ndf.withColumn(\"key1\", col(\"key1\")).withColumn(\"key2\", col(\"key2\")).withColumn(\"key3\", col(\"key3\"))\r\n```\r\nAfter this patch, the user can use the `withColumn` with columns list args complete columns adding at one pass:\r\n```Python\r\ndf.withColumn({\"key1\":  col(\"key1\"), \"key2\":col(\"key2\"), \"key3\": col(\"key3\")})\r\n```\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes, this PR exposes `withColumns` as public API, and also adds `withColumns` API in PySpark .\r\n\r\n\r\n### How was this patch tested?\r\n- Add new multiple columns adding test, passed\r\n- Existing test, passed", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/sql/dataframe.py", "additions": "32", "deletions": "0", "changes": "32"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/sql/dataframe.pyi", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_dataframe.py", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/java/test/org/apache/spark/sql/JavaDataFrameSuite.java", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "15", "deletions": "3", "changes": "18"}, "updated": [0, 0, 4]}]}
{"author": "Yikun", "sha": "c6d4f21ca368bbc7ba4236dcd9d09904e7b82e5b", "commit_date": "2021/07/08 01:32:50", "commit_message": "Address doctest and use unittest countTestCases", "title": "[SPARK-35721][PYTHON] Path level discover for python unittests", "body": "### What changes were proposed in this pull request?\r\nAdd path level discover for python unittests.\r\n![image](https://user-images.githubusercontent.com/1736354/124094503-6bdeb980-da8b-11eb-9bbe-b086024f6902.png)\r\n\r\nChange list:\r\n- Introduce a **python_discover_paths** in modules.\r\n- Add **_discover_python_unittests** function: it would be called in pthon/run-tests.py to load test module.\r\n- Add **_append_discovred_goals function**: call _discover_python_unittests to refresh m.python_test_goals\r\n- if modules have python_test_goals or **python_discover_paths** would also be considered as python tests.\r\n- Fix: Move logging.basicConfig to head to make sure logging config before any possible logging print.\r\n- Fix: Change python/pyspark/testing/utils.py SPARK_HOME use _find_spark_home to get value.\r\n- Fix: export py4j PYTHONPATH before run test.\r\n\r\nNote:\r\n- **Why use walk_packages but not unittest.defaultTestLoader.discover?** we use `pkgutil.walk_packages` and `unittest.defaultTestLoader.loadTestsFromModule` to load test modules, consider we will add doctest discover in future, we can add something like blow as the impletations of doctest discover: \r\n```python\r\nimport doctest\r\n\r\ndef _contain_doctests_class(module):\r\n    suite = doctest.DocTestSuite(module)\r\n    if suite.countTestCases():\r\n        return True\r\n    else:\r\n        return False\r\n```\r\n- **Why we doesn't add doctests in here**? Currently, not all modules doctests are added to `python_test_goals`, that means these doctests doesn't be excuted, so better add discover doctests in a separate PR.\r\n\r\n- **What's the deps of discover?** the test discover will do real import for every modules, so we need install **all deps of PySpark test modules** before run-tests otherwise the ImportError would be raised.\r\n\r\n\r\n\r\n### Why are the changes needed?\r\nNow we need to specify the python test cases by manually when we add a new testcase. Sometime, we forgot to add the testcase to module list, the testcase would not be executed.\r\n\r\nSuch as:\r\n\r\npyspark-core pyspark.tests.test_pin_thread\r\n\r\nThus we need some auto-discover way to find all testcase rather than specified every case by manually.\r\n\r\nrelated: https://github.com/apache/spark/pull/32867\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n\r\n### How was this patch tested?\r\n1. Add doc tests for _discover_python_unittests.\r\n2. Compare the CI results (this patch and before), see diff in:\r\nBuild modules: pyspark-sql, pyspark-mllib, pyspark-resource: https://www.diffchecker.com/4RAQydBB\r\nBuild modules: pyspark-core, pyspark-streaming, pyspark-ml: https://www.diffchecker.com/F1ccZDKG\r\nBuild modules: pyspark-pandas\uff1ahttps://www.diffchecker.com/eBDne4uA\r\nBuild modules: pyspark-pandas-slow\uff1ahttps://www.diffchecker.com/lySQGrhA\r\n3. local test for python modules:\r\n./dev/run-tests --parallelism 2 --modules \"pyspark-sql\"", "failed_tests": [], "files": [{"file": {"name": ".github/workflows/build_and_test.yml", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 2, 22]}, {"file": {"name": "dev/run-tests.py", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 1, 6]}, {"file": {"name": "dev/sparktestsupport/modules.py", "additions": "39", "deletions": "129", "changes": "168"}, "updated": [0, 4, 16]}, {"file": {"name": "python/pyspark/pandas/tests/indexes/test_base.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 2, 7]}, {"file": {"name": "python/pyspark/pandas/tests/indexes/test_datetime.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 2, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 2, 10]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 2, 5]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 2, 3]}, {"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 2, 4]}, {"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames_groupby.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 2, 4]}, {"file": {"name": "python/pyspark/pandas/tests/test_series.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 4, 8]}, {"file": {"name": "python/pyspark/pandas/tests/test_stats.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 3, 6]}, {"file": {"name": "python/pyspark/testing/utils.py", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 1]}, {"file": {"name": "python/run-tests.py", "additions": "119", "deletions": "2", "changes": "121"}, "updated": [0, 0, 1]}]}
{"author": "Yikun", "sha": "b2dbe9b6c9225267b7a3160e59be13530c7b7f52", "commit_date": "2021/04/21 09:34:25", "commit_message": "Add columns batch adding support for PySpark.", "title": "[WIP][SPARK-35173][PYTHON][SQL] Add multiple columns adding support for PySpark", "body": "### What changes were proposed in this pull request?\r\nThis PR added the multiple columns adding support for PySpark.dataframe.withColumn.\r\n\r\n### Why are the changes needed?\r\nNow, the spark private method `withColumns` can add columns at one pass [1]:\r\nhttps://github.com/apache/spark/blob/b5241c97b17a1139a4ff719bfce7f68aef094d95/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L2402\r\nbut the PySpark user can only use `withColumn` to add\u00a0one column or replacing the existing one column\u00a0that\u00a0has the same name. \r\n\r\nFor example, if the PySpark user want to add multiple columns, they should call `withColumn` again and again like:\r\n```Python\r\nself.df.withColumn(\"key1\", col(\"key1\")).withColumn(\"key2\", col(\"key2\")).withColumn(\"key3\", col(\"key3\"))\r\n```\r\nAfter this patch, the user can use the `withColumn` with columns list args complete columns adding at one pass:\r\n```Python\r\nself.df.withColumn([\"key1\", \"key2\", \"key3\"], [col(\"key1\"), col(\"key2\"), col(\"key3\")])\r\n```\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes, the input types of withColumn are changed, the PySpark can use withColumn to add multiple columns directly.\r\n\r\n\r\n### How was this patch tested?\r\n- Add new multiple columns adding test, passed\r\n- Existing test, passed", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/sql/dataframe.py", "additions": "26", "deletions": "7", "changes": "33"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/dataframe.pyi", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_dataframe.py", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}]}
{"author": "Yikun", "sha": "63a31fee864b9462ca435b11a449c3205282a3d5", "commit_date": "2021/04/29 05:29:31", "commit_message": "Add pyspark_3.1_to_3.2 into index", "title": "[SPARK-35176][PYTHON] Standardize input validation error type", "body": "### What changes were proposed in this pull request?\r\nThis PR corrects some exception type when the function input params are failed to validate due to TypeError.\r\nIn order to convenient to review, there are 3 commits in this PR:\r\n- Standardize input validation error type on sql\r\n- Standardize input validation error type on ml\r\n- Standardize input validation error type on pandas\r\n\r\n### Why are the changes needed?\r\nAs suggestion from Python exception doc [1]: \"Raised when an operation or function is applied to an object of inappropriate type.\", but there are many Value error are raised in some pyspark code, this patch fix them.\r\n\r\n[1] https://docs.python.org/3/library/exceptions.html#TypeError\r\n\r\nNote that: this patch only addresses the exsiting some wrong raise type for input validation, the input validation decorator/framework which mentioned in [SPARK-35176](https://issues.apache.org/jira/browse/SPARK-35176), would be submited in a speparated patch.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes, code can raise the right TypeError instead of ValueError.\r\n\r\n### How was this patch tested?\r\nExisting test case and UT", "failed_tests": [], "files": [{"file": {"name": "python/docs/source/migration_guide/index.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/docs/source/migration_guide/pyspark_3.1_to_3.2.rst", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/base.py", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/classification.py", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 2, 2]}, {"file": {"name": "python/pyspark/ml/evaluation.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/param/__init__.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/regression.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/ml/tests/test_base.py", "additions": "18", "deletions": "5", "changes": "23"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/tests/test_evaluation.py", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/tests/test_param.py", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/mllib/linalg/distributed.py", "additions": "2", "deletions": "4", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/mllib/tests/test_linalg.py", "additions": "12", "deletions": "1", "changes": "13"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/base.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 4]}, {"file": {"name": "python/pyspark/pandas/config.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 4]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "19", "deletions": "19", "changes": "38"}, "updated": [0, 0, 5]}, {"file": {"name": "python/pyspark/pandas/generic.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 3, 8]}, {"file": {"name": "python/pyspark/pandas/groupby.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 6]}, {"file": {"name": "python/pyspark/pandas/indexes/base.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 5]}, {"file": {"name": "python/pyspark/pandas/indexes/multi.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 5]}, {"file": {"name": "python/pyspark/pandas/namespace.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 5]}, {"file": {"name": "python/pyspark/pandas/plot/core.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 3]}, {"file": {"name": "python/pyspark/pandas/series.py", "additions": "11", "deletions": "11", "changes": "22"}, "updated": [0, 0, 5]}, {"file": {"name": "python/pyspark/pandas/strings.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 4]}, {"file": {"name": "python/pyspark/pandas/tests/indexes/test_base.py", "additions": "11", "deletions": "11", "changes": "22"}, "updated": [0, 3, 3]}, {"file": {"name": "python/pyspark/pandas/tests/test_config.py", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "26", "deletions": "26", "changes": "52"}, "updated": [0, 2, 4]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_namespace.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 5, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_series.py", "additions": "16", "deletions": "16", "changes": "32"}, "updated": [0, 2, 3]}, {"file": {"name": "python/pyspark/pandas/tests/test_series_string.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_utils.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/pandas/utils.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 5]}, {"file": {"name": "python/pyspark/sql/dataframe.py", "additions": "31", "deletions": "28", "changes": "59"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_dataframe.py", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/tests/test_functions.py", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/taskcontext.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "Yikun", "sha": "", "commit_date": "2021/04/14 15:07:05", "commit_message": "[SPARK-35061][BUILD] Upgrade pycodestyle from 2.6.0 to 2.7.0\n\n### What changes were proposed in this pull request?\n\nThis PR bumps up the version of pycodestyle from 2.6.0 to 2.7.0 released a month ago.\n\n### Why are the changes needed?\n\n2.7.0 includes three major fixes below (see https://readthedocs.org/projects/pycodestyle/downloads/pdf/latest/):\n\n- Fix physical checks (such as W191) at end of file. PR #961.\n- Add --indent-size option (defaulting to 4). PR #970.\n- W605: fix escaped crlf false positive on windows. PR #976\n\nThe first and third ones could be useful for dev to detect the styles.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo, dev-only.\n\n### How was this patch tested?\n\nManually tested locally.\n\nCloses #32160 from HyukjinKwon/SPARK-35061.\n\nAuthored-by: HyukjinKwon <gurwls223@apache.org>\nSigned-off-by: Dongjoon Hyun <dhyun@apple.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "dev/lint-python", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 2, 3]}]}
{"author": "Yikun", "sha": "e46233af0bbd8f125a0b31f045628e239d0c8382", "commit_date": "2021/06/30 15:03:00", "commit_message": "Path level discover in python/run-test.py", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": ".github/workflows/build_and_test.yml", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 1, 26]}, {"file": {"name": "dev/run-tests.py", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [1, 1, 6]}, {"file": {"name": "dev/sparktestsupport/modules.py", "additions": "39", "deletions": "129", "changes": "168"}, "updated": [4, 6, 16]}, {"file": {"name": "python/pyspark/pandas/tests/indexes/test_base.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 7]}, {"file": {"name": "python/pyspark/pandas/tests/indexes/test_datetime.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 10]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 5]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 3]}, {"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 4]}, {"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames_groupby.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 4]}, {"file": {"name": "python/pyspark/pandas/tests/test_series.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [3, 3, 7]}, {"file": {"name": "python/pyspark/pandas/tests/test_stats.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 5]}, {"file": {"name": "python/pyspark/testing/utils.py", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 1]}, {"file": {"name": "python/run-tests.py", "additions": "119", "deletions": "2", "changes": "121"}, "updated": [0, 0, 1]}]}
{"author": "Yikun", "sha": "0915bf6bb053a7fe53c5eb7ba52a81fc26957c8b", "commit_date": "2021/06/30 15:03:00", "commit_message": "Path level discover in python/run-test.py", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": ".github/workflows/build_and_test.yml", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 1, 26]}, {"file": {"name": "dev/run-tests.py", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [1, 1, 6]}, {"file": {"name": "dev/sparktestsupport/modules.py", "additions": "40", "deletions": "130", "changes": "170"}, "updated": [4, 6, 16]}, {"file": {"name": "python/pyspark/pandas/tests/indexes/test_base.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 7]}, {"file": {"name": "python/pyspark/pandas/tests/indexes/test_datetime.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 10]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 5]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 3]}, {"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 4]}, {"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames_groupby.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 4]}, {"file": {"name": "python/pyspark/pandas/tests/test_series.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [3, 3, 7]}, {"file": {"name": "python/pyspark/pandas/tests/test_stats.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 5]}, {"file": {"name": "python/pyspark/testing/utils.py", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 1]}, {"file": {"name": "python/run-tests.py", "additions": "118", "deletions": "2", "changes": "120"}, "updated": [0, 0, 1]}]}
{"author": "Yikun", "sha": "05ff042611c27270e9b059e4fd0483c7194672c1", "commit_date": "2021/06/29 08:56:13", "commit_message": "[SPARK-35721][PYTHON] Path level discover for python unittests\n\n### What changes were proposed in this pull request?\nAdd path level discover for python unittests.\n\n### Why are the changes needed?\nNow we need to specify the python test cases by manually when we add a new testcase. Sometime, we forgot to add the testcase to module list, the testcase would not be executed.\n\nSuch as:\n- pyspark-core pyspark.tests.test_pin_thread\n\nThus we need some auto-discover way to find all testcase rather than specified every case by manually.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nAdd below code in end of `dev/sparktestsupport/modules.py`\n```python\nfor m in sorted(all_modules):\n    for g in sorted(m.python_test_goals):\n        print(m.name, g)\n```\nCompare the result before and after:\nhttps://www.diffchecker.com/iO3FvhKL\n\nCloses #32867 from Yikun/SPARK_DISCOVER_TEST.\n\nAuthored-by: Yikun Jiang <yikunkero@gmail.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": ".github/workflows/build_and_test.yml", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 25]}, {"file": {"name": "dev/run-tests.py", "additions": "9", "deletions": "1", "changes": "10"}, "updated": [1, 1, 6]}, {"file": {"name": "dev/sparktestsupport/modules.py", "additions": "86", "deletions": "140", "changes": "226"}, "updated": [3, 5, 15]}, {"file": {"name": "python/pyspark/pandas/tests/indexes/test_base.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 1, 6]}, {"file": {"name": "python/pyspark/pandas/tests/indexes/test_datetime.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 1, 5]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 1, 9]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 1, 4]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 1, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 1, 3]}, {"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames_groupby.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 1, 3]}, {"file": {"name": "python/pyspark/pandas/tests/test_series.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_stats.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 1, 4]}, {"file": {"name": "python/pyspark/testing/utils.py", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 1]}]}
{"author": "Yikun", "sha": "8e0065acce17a938482bb102afb6d99ef8d65a8a", "commit_date": "2021/06/29 11:15:11", "commit_message": "Add missing test modules check", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "dev/sparktestsupport/modules.py", "additions": "17", "deletions": "1", "changes": "18"}, "updated": [3, 5, 15]}]}
{"author": "Yikun", "sha": "ce80943c94b6e220467f153b4ef981b5b0da2e99", "commit_date": "2021/06/29 11:15:11", "commit_message": "Add missing test modules check", "title": "", "body": "", "failed_tests": ["org.apache.spark.scheduler.BasicSchedulerIntegrationSuite"], "files": [{"file": {"name": "dev/sparktestsupport/modules.py", "additions": "13", "deletions": "0", "changes": "13"}, "updated": [3, 5, 15]}]}
{"author": "Yikun", "sha": "33a9eaf0fd7f01f56a1f795a283a782053d4ac63", "commit_date": "2021/06/29 05:27:00", "commit_message": "discover-assert", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "dev/sparktestsupport/modules.py", "additions": "16", "deletions": "1", "changes": "17"}, "updated": [1, 3, 13]}]}
{"author": "Yikun", "sha": "32bfa94dbc780ffc575b3c83408902277c8959f3", "commit_date": "2021/04/15 02:30:07", "commit_message": "remove --ff-only and add --squash", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": ".github/workflows/build_and_test.yml", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [2, 7, 14]}]}
{"author": "zwangsheng", "sha": "4517c2cffc461a8a4196a92ec3fe893c851329c2", "commit_date": "2021/06/02 06:10:03", "commit_message": "do some optimizations", "title": "[WIP] [SPARK-35572] [K8S] add hostNetwork feature to executor", "body": "### What changes were proposed in this pull request?\r\nadd hostNetwork feature to executor\r\nmodify BasicExecutorFeatureStep add hostNetwork in executorpod \r\n\r\n### Why are the changes needed?\r\nIn the process of the company's business promotion, the function of k8s hostNetwork is used. But it is found that spark does not support enable hostNetwork in the configuration.\r\nFound that this function will still be commonly used, so I decided to add a perceptible parameter to enable hostNetwork in spark way.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nyes, add new config spark.kubernetes.executor.hostNetwork.enable\r\n\r\n### How was this patch tested?\r\nadd unit test", "failed_tests": [], "files": [{"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 3]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 1, 1]}]}
{"author": "zwangsheng", "sha": "", "commit_date": "2021/08/12 03:50:27", "commit_message": "modify exit executor log logic", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 3]}]}
{"author": "timarmstrong", "sha": "", "commit_date": "2021/07/02 00:45:57", "commit_message": "remove tab", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/test/scala/org/apache/spark/ThreadAudit.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "IonutBoicuAms", "sha": "", "commit_date": "2021/08/11 16:10:15", "commit_message": "fix bug in in disable unnecessary bucketed scan", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/bucketing/DisableUnnecessaryBucketedScan.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/sources/DisableUnnecessaryBucketedScanSuite.scala", "additions": "26", "deletions": "0", "changes": "26"}, "updated": [0, 0, 0]}]}
{"author": "pan3793", "sha": "", "commit_date": "2021/04/19 15:38:50", "commit_message": "[MINOR][SQL] Remove Antlr4 workaround", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/ParseDriver.scala", "additions": "1", "deletions": "12", "changes": "13"}, "updated": [0, 0, 0]}]}
{"author": "mickjermsurawong-stripe", "sha": "", "commit_date": "2020/01/14 07:08:25", "commit_message": "add filter tests for explicitness in sql schema", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "28", "deletions": "1", "changes": "29"}, "updated": [1, 1, 5]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/test/SQLTestData.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 1]}]}
{"author": "jerqi", "sha": "8e2f4fb0b48cb96cc7b6a774269022f2bd2a3882", "commit_date": "2021/08/09 15:20:22", "commit_message": "modify", "title": "[SPARK-36223][SQL][TEST] Cover 3 kinds of join in the TPCDSQueryTestSuite", "body": "### What changes were proposed in this pull request?\r\nIn current github actions we run TPCDSQueryTestSuite for tpcds benchmark. But it's only tested under default configurations. Since we have added the `spark.sql.join.forceApplyShuffledHashJoin` config. Now we can test all 3 join strategies in TPCDS to improve the coverage.\r\n\r\n### Why are the changes needed?\r\nImprove the coverage of join strategies in the TPCS.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo, only for testing.\r\n\r\n### How was this patch tested?\r\nNo need.", "failed_tests": ["org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/TPCDSQueryTestSuite.scala", "additions": "40", "deletions": "4", "changes": "44"}, "updated": [0, 0, 0]}]}
{"author": "jerqi", "sha": "", "commit_date": "2021/05/03 18:37:07", "commit_message": "[SPARK-35297][CORE][DOC][MINOR] Modify the comment about the executor", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/executor/Executor.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}]}
{"author": "WeichenXu123", "sha": "", "commit_date": "2021/05/11 01:34:15", "commit_message": "update", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "mllib/src/test/scala/org/apache/spark/ml/tuning/CrossValidatorSuite.scala", "additions": "0", "deletions": "2", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "mllib/src/test/scala/org/apache/spark/ml/tuning/TrainValidationSplitSuite.scala", "additions": "0", "deletions": "2", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/ml/tuning.py", "additions": "24", "deletions": "28", "changes": "52"}, "updated": [0, 0, 0]}]}
{"author": "tiehexue", "sha": "", "commit_date": "2021/04/25 12:20:32", "commit_message": "revert import statement to stay with scalastyle.", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "external/kafka-0-10/src/main/scala/org/apache/spark/streaming/kafka010/DirectKafkaInputDStream.scala", "additions": "10", "deletions": "8", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "external/kafka-0-10/src/main/scala/org/apache/spark/streaming/kafka010/KafkaUtils.scala", "additions": "5", "deletions": "4", "changes": "9"}, "updated": [0, 0, 0]}]}
{"author": "robert3005", "sha": "e3754487a8fc49622059a47e403945f2850731f2", "commit_date": "2021/08/06 18:25:53", "commit_message": "boun", "title": "[SPARK-21195][CORE] MetricSystem should pick up dynamically registered metrics in sources", "body": "### What changes were proposed in this pull request?\r\nMetricSystem picks up new metrics from sources that are added throughout execution. If you do measurements via dynamic proxies you might not want to redeclare all metrics that the proxies will create and you'd prefer them to get populated as they're being produced. Right now all sources are processed only onceat startup and metrics are picked up only if they have been registered statically at compile time. Behaviour I am proposing lets you not have to declare metrics in two places.\r\n\r\nThis had been previously suggested in https://github.com/apache/spark/pull/18406 and https://github.com/apache/spark/pull/29980. I have reduced the scope of the change to just dynamic metric registration.\r\n\r\n### Why are the changes needed?\r\nCurrently there's no way to access MetricRegistry that MetricsSystem uses to hold its state and as such it's not possible to reprocess a source. MetricsSystem throws if any metric had already been registered previously.\r\n\r\nn.b. the MetricRegistry is added as a constructor argument to make testing easier but could as well be accessed via reflection as a private variable.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\nAdded tests", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala", "additions": "66", "deletions": "19", "changes": "85"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/metrics/MetricsSystemSuite.scala", "additions": "30", "deletions": "9", "changes": "39"}, "updated": [0, 0, 0]}, {"file": {"name": "streaming/src/test/scala/org/apache/spark/streaming/StreamingContextSuite.scala", "additions": "6", "deletions": "3", "changes": "9"}, "updated": [0, 0, 0]}]}
{"author": "zheniantoushipashi", "sha": "6b75f43fee3b83d1e14e09ed3a5e588336602913", "commit_date": "2021/07/03 07:23:45", "commit_message": "[SPARK-36005][SQL] The canCast method of type of char/varchar is modified to be consistent with StringType", "title": "[SPARK-36005][SQL] The canCast method of type of char/varchar is modified to be consistent with StringType", "body": "\r\n### What changes were proposed in this pull request?\r\n\r\n\r\nThe canCast method of type of char/varchar is modified to be consistent with StringType\r\n\r\n\r\nthe method cast will change the type char/varchar to StringType \r\n\r\n  def cast(to: DataType): Column = withExpr {\r\n    val cast = Cast(expr, CharVarcharUtils.replaceCharVarcharWithStringForCast(to))\r\n    cast.setTagValue(Cast.USER_SPECIFIED_CAST, true)\r\n    cast\r\n  }\r\n\r\n\r\nThe canCast method of type of char/varchar  must   be consistent with StringType\r\n\r\n\r\n\r\n\r\n### Why are the changes needed?\r\n\r\nBefore I used stringType instead of char/varchar, my application code has the logic to judge using canCast. There was no problem before, but now it\u2019s changed to char/varchar, and the judgment of canCast fails. If it doesn\u2019t pass, I Need to change a lot of application code\r\n\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nno\r\n\r\n\r\n### How was this patch tested?\r\n\r\ni add UT\u3002\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [1, 5, 16]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CastSuite.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 5, 16]}]}
{"author": "zheniantoushipashi", "sha": "", "commit_date": "2021/07/03 07:23:45", "commit_message": "[SPARK-36005][SQL] The canCast method of type of char/varchar is modified to be consistent with StringType", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala", "additions": "9", "deletions": "9", "changes": "18"}, "updated": [1, 5, 16]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CastSuite.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 5, 16]}]}
{"author": "Run-Lin", "sha": "", "commit_date": "2021/04/25 10:22:47", "commit_message": "In YARN mode, for better user experience, when Spark is started, not only the AppID is printed, but the Tracking URL is also printed to allow users to better track Spark Job", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala", "additions": "8", "deletions": "1", "changes": "9"}, "updated": [0, 1, 1]}]}
{"author": "ReachInfi", "sha": "05dd8b760a994284f2f83ade49869d43835e3e51", "commit_date": "2021/07/15 20:49:19", "commit_message": "Update build_and_test.yml", "title": "[SPARK-36118][SQL] Add bitmap functions for Spark SQL", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nadd functions of bitmap building and computing cardinality for Spark SQL, If this is ok, I will update function.scala and FunctionRegistry.scala.\r\n\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nBitmaps are used more and more widely, and many frameworks have native support, such as Clickhouse\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nNo\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nCI, it performs well on billions of rows based on our real demand", "failed_tests": ["pyspark.mllib.tests.test_streaming_algorithms"], "files": [{"file": {"name": ".github/workflows/build_and_test.yml", "additions": "4", "deletions": "20", "changes": "24"}, "updated": [3, 4, 21]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/bitmap.scala", "additions": "176", "deletions": "0", "changes": "176"}, "updated": [0, 0, 0]}]}
{"author": "rashtao", "sha": "", "commit_date": "2021/04/23 08:01:55", "commit_message": "reverted default JsonFactory creation", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JacksonGenerator.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}]}
{"author": "turboFei", "sha": "", "commit_date": "2021/04/23 02:37:11", "commit_message": "[SPARK-21499][FOLLOWUP] Update doc for registering Spark UDAFs in Spark SQL queries", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "docs/sql-ref-functions-udf-aggregate.md", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "FridaPolished", "sha": "", "commit_date": "2021/04/09 18:52:55", "commit_message": "[SPARK-34963][SQL] Fix nested column pruning for extracting case-insensitive struct field from array of struct\n\n### What changes were proposed in this pull request?\n\nThis patch proposes a fix of nested column pruning for extracting case-insensitive struct field from array of struct.\n\n### Why are the changes needed?\n\nUnder case-insensitive mode, nested column pruning rule cannot correctly push down extractor of a struct field of an array of struct, e.g.,\n\n```scala\nval query = spark.table(\"contacts\").select(\"friends.First\", \"friends.MiDDle\")\n```\n\nError stack:\n```\n[info]   java.lang.IllegalArgumentException: Field \"First\" does not exist.\n[info] Available fields:\n[info]   at org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)\n[info]   at org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)\n[info]   at scala.collection.MapLike$class.getOrElse(MapLike.scala:128)\n[info]   at scala.collection.AbstractMap.getOrElse(Map.scala:59)\n[info]   at org.apache.spark.sql.types.StructType.apply(StructType.scala:273)\n[info]   at org.apache.spark.sql.execution.ProjectionOverSchema$$anonfun$getProjection$3.apply(ProjectionOverSchema.scala:44)\n[info]   at org.apache.spark.sql.execution.ProjectionOverSchema$$anonfun$getProjection$3.apply(ProjectionOverSchema.scala:41)\n```\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nUnit test\n\nCloses #32059 from viirya/fix-array-nested-pruning.\n\nAuthored-by: Liang-Chi Hsieh <viirya@gmail.com>\nSigned-off-by: Liang-Chi Hsieh <viirya@gmail.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ProjectionOverSchema.scala", "additions": "7", "deletions": "2", "changes": "9"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/SelectedField.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/SchemaPruningSuite.scala", "additions": "42", "deletions": "0", "changes": "42"}, "updated": [1, 1, 1]}]}
{"author": "eejbyfeldt", "sha": "", "commit_date": "2021/07/10 10:04:47", "commit_message": "Add and adopt DataFrameSuite tests cases\n\nThe test cases are from:\nhttps://github.com/apache/spark/pull/33205", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.HiveExternalCatalogVersionsSuite", "org.apache.spark.sql.DataFrameSuite"], "files": [{"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "51", "deletions": "1", "changes": "52"}, "updated": [0, 3, 7]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/test/SQLTestData.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 1]}]}
{"author": "sammyjmoseley", "sha": "", "commit_date": "2021/07/15 14:02:15", "commit_message": "fix diff", "title": "", "body": "", "failed_tests": ["pyspark.pandas.indexes.base", "pyspark.pandas.tests.indexes.test_base"], "files": [{"file": {"name": "build/zinc-0.3.15/bin/nailgun", "additions": "0", "deletions": "50", "changes": "50"}, "updated": [0, 0, 0]}, {"file": {"name": "build/zinc-0.3.15/bin/ng/darwin32/ng", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "build/zinc-0.3.15/bin/ng/darwin64/ng", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "build/zinc-0.3.15/bin/ng/linux32/ng", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "build/zinc-0.3.15/bin/ng/linux64/ng", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "build/zinc-0.3.15/bin/ng/linuxppc64le/ng", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "build/zinc-0.3.15/bin/ng/win32/ng.exe", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "build/zinc-0.3.15/bin/zinc", "additions": "0", "deletions": "257", "changes": "257"}, "updated": [0, 0, 0]}, {"file": {"name": "build/zinc-0.3.15/lib/compiler-interface-sources.jar", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "build/zinc-0.3.15/lib/incremental-compiler.jar", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "build/zinc-0.3.15/lib/nailgun-server.jar", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "build/zinc-0.3.15/lib/sbt-interface.jar", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "build/zinc-0.3.15/lib/scala-compiler.jar", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "build/zinc-0.3.15/lib/scala-library.jar", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "build/zinc-0.3.15/lib/scala-reflect.jar", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}, {"file": {"name": "build/zinc-0.3.15/lib/zinc.jar", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 0, 0]}]}
{"author": "vkorukanti", "sha": "", "commit_date": "2021/06/15 06:26:31", "commit_message": "[SPARK-35763][SS] Add a new copy method to StateStoreCustomMetric", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStore.scala", "additions": "10", "deletions": "3", "changes": "13"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/SymmetricHashJoinStateManager.scala", "additions": "1", "deletions": "9", "changes": "10"}, "updated": [0, 2, 2]}]}
{"author": "shardulm94", "sha": "f39e7c997562c67b4fa9a374e993744512ad3b84", "commit_date": "2021/07/22 21:07:10", "commit_message": "Fix compile error after revert", "title": "[SPARK-36215][SHUFFLE] Add logging for slow fetches to diagnose external shuffle service issues", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\nAdd logging to `ShuffleBlockFetcherIterator` to log \"slow\" fetches, where slow is defined by two confs: `spark.reducer.shuffleFetchSlowLogThreshold.time` and `spark.reducer.shuffleFetchSlowLogThreshold.bytesPerSec`\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\n\r\n### Why are the changes needed?\r\nCurrently we can see from the metrics that a task or stage has slow fetches, and the logs indicate *all* of the shuffle servers those tasks were fetching from, but often this is a big set (dozens or even hundreds) and narrowing down which one caused issues can be very difficult. This change makes it easier to understand which fetch is \"slow\".\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nAdds two configs `spark.reducer.shuffleFetchSlowLogThreshold.time` and `spark.reducer.shuffleFetchSlowLogThreshold.bytesPerSec`\r\n\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nAdded unit test", "failed_tests": ["pyspark.mllib.tests.test_streaming_algorithms"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/TestUtils.scala", "additions": "32", "deletions": "1", "changes": "33"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [1, 2, 5]}, {"file": {"name": "core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala", "additions": "43", "deletions": "5", "changes": "48"}, "updated": [0, 1, 4]}, {"file": {"name": "core/src/test/scala/org/apache/spark/storage/ShuffleBlockFetcherIteratorSuite.scala", "additions": "47", "deletions": "3", "changes": "50"}, "updated": [0, 0, 2]}, {"file": {"name": "docs/configuration.md", "additions": "22", "deletions": "0", "changes": "22"}, "updated": [0, 0, 1]}]}
{"author": "shardulm94", "sha": "", "commit_date": "2021/02/23 14:25:50", "commit_message": "Fix tests [Take 2]", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/HiveQuerySuite.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 2]}]}
{"author": "wForget", "sha": "d37c84358b6fe1574941feb6943f16bd0277dd3d", "commit_date": "2021/04/29 06:48:02", "commit_message": "[SPARK-35270][SQL][CORE] Remove the use of guava in order to upgrade guava version to 27.", "title": "[SPARK-35270][SQL][CORE] Remove the use of guava in order to upgrade guava version to 27", "body": "### What changes were proposed in this pull request?\r\n\r\nRemove the use of guava in order to upgrade guava version to 27.\r\n\r\n\r\n### Why are the changes needed?\r\n\r\nHadoop 3.2.2 uses Guava 27, the change is for the guava version upgrade.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nno\r\n\r\n\r\n### How was this patch tested?\r\n\r\nModify the guava version to 27.0-jre, and then compile.", "failed_tests": [], "files": [{"file": {"name": "common/kvstore/src/main/java/org/apache/spark/util/kvstore/InMemoryStore.java", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "common/network-common/src/main/java/org/apache/spark/network/protocol/AbstractMessage.java", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/RemoteBlockPushResolver.java", "additions": "8", "deletions": "6", "changes": "14"}, "updated": [0, 0, 1]}, {"file": {"name": "common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/protocol/FinalizeShuffleMerge.java", "additions": "9", "deletions": "6", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/protocol/MergeStatuses.java", "additions": "8", "deletions": "6", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/protocol/PushBlockStream.java", "additions": "12", "deletions": "9", "changes": "21"}, "updated": [0, 0, 0]}, {"file": {"name": "common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/protocol/mesos/RegisterDriver.java", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/rdd/RDDOperationScope.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "mllib/src/main/scala/org/apache/spark/mllib/linalg/Matrices.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "mllib/src/main/scala/org/apache/spark/mllib/linalg/distributed/BlockMatrix.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "mllib/src/main/scala/org/apache/spark/mllib/tree/model/InformationGainStats.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "mllib/src/main/scala/org/apache/spark/mllib/tree/model/Predict.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala", "additions": "2", "deletions": "3", "changes": "5"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/java/test/org/apache/spark/sql/JavaDatasetSuite.java", "additions": "12", "deletions": "13", "changes": "25"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveShim.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}]}
{"author": "hanover-fiste", "sha": "", "commit_date": "2021/08/12 13:35:28", "commit_message": "[SPARK-35881][SQL][FOLLOWUP] Remove the AQE post stage creation extension\n\n### What changes were proposed in this pull request?\n\nThis is a followup of #33140\n\nIt turns out that we may be able to complete the AQE and columnar execution integration without the AQE post stage creation extension. The rule `ApplyColumnarRulesAndInsertTransitions` can add to-columnar transition if the shuffle/broadcast supports columnar.\n\n### Why are the changes needed?\n\nremove APIs that are not needed.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo, the APIs are not released yet.\n\n### How was this patch tested?\n\nexisting and manual tests\n\nCloses #33701 from cloud-fan/aqe.\n\nAuthored-by: Wenchen Fan <wenchen@databricks.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/SparkSessionExtensions.scala", "additions": "0", "deletions": "20", "changes": "20"}, "updated": [1, 2, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "10", "deletions": "6", "changes": "16"}, "updated": [1, 4, 11]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala", "additions": "1", "deletions": "6", "changes": "7"}, "updated": [1, 2, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [1, 2, 2]}]}
{"author": "rahulsmahadev", "sha": "", "commit_date": "2021/06/29 22:19:52", "commit_message": "Fix build", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.streaming.FlatMapGroupsWithStateSuite"], "files": [{"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/UnsupportedOperationsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/KeyValueGroupedDataset.scala", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 3, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/FlatMapGroupsWithStateExecHelper.scala", "additions": "0", "deletions": "2", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/streaming/FlatMapGroupsWithStateSuite.scala", "additions": "3", "deletions": "4", "changes": "7"}, "updated": [0, 1, 1]}]}
{"author": "kotlovs", "sha": "814e391a0f7df53c7a1ad96c895c80f5745e591e", "commit_date": "2021/04/15 13:48:35", "commit_message": "Close SparkContext after the Main method has finished, to allow SparkApplication on K8S to complete", "title": "[SPARK-34674][CORE][K8S] Close SparkContext after the Main method has finished", "body": "### What changes were proposed in this pull request?\r\nClose SparkContext after the Main method has finished, to allow SparkApplication on K8S to complete.\r\nThis is fixed version of [merged and reverted PR](https://github.com/apache/spark/pull/32081).\r\n\r\n### Why are the changes needed?\r\nif I don't call the method sparkContext.stop() explicitly, then a Spark driver process doesn't terminate even after its Main method has been completed. This behaviour is different from spark on yarn, where the manual sparkContext stopping is not required. It looks like, the problem is in using non-daemon threads, which prevent the driver jvm process from terminating.\r\nSo I have inserted code that closes sparkContext automatically.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\nManually on the production AWS EKS environment in my company.\r\n", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [1, 3, 7]}]}
{"author": "kotlovs", "sha": "a7efe992484c9012fb8c5d7aae07ddb4de67459b", "commit_date": "2021/04/14 23:36:43", "commit_message": "Add logging", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 2, 6]}]}
{"author": "kotlovs", "sha": "7fbd41b73feceb108522d8771edd04cb2471da89", "commit_date": "2021/04/07 12:57:29", "commit_message": "Close SparkContext after the Main method has finished, to allow SparkApplication on K8S to complete", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 0, 4]}]}
{"author": "JkSelf", "sha": "", "commit_date": "2021/06/04 08:06:21", "commit_message": "fix the failed uts", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "5", "deletions": "2", "changes": "7"}, "updated": [0, 2, 4]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DynamicPartitionPruningSuite.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 1, 2]}]}
{"author": "steven-aerts", "sha": "", "commit_date": "2021/07/02 11:57:52", "commit_message": "[SPARK-35985][SQL] push partitionFilters for empty readDataSchema\n\nthis commit makes sure that for File Source V2 partition filters are\nalso taken into account when the readDataSchema is empty.\nThis is the case for queries like:\n\n  SELECT count(*) FROM tbl WHERE partition=foo\n  SELECT input_file_name() FROM tbl WHERE partition=foo", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/PruneFileSourcePartitionsSuite.scala", "additions": "25", "deletions": "1", "changes": "26"}, "updated": [0, 0, 0]}]}
{"author": "geekyouth", "sha": "", "commit_date": "2021/07/01 03:40:00", "commit_message": "[SPARK-35714][FOLLOW-UP][CORE] Use a shared stopping flag for WorkerWatcher to avoid the duplicate System.exit\n\n### What changes were proposed in this pull request?\n\nThis PR proposes to let `WorkerWatcher` reuse the `stopping` flag in `CoarseGrainedExecutorBackend` to avoid the duplicate call of `System.exit`.\n\n### Why are the changes needed?\n\nAs a followup of https://github.com/apache/spark/pull/32868, this PR tries to give a more robust fix.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nPass existing tests.\n\nCloses #33028 from Ngone51/spark-35714-followup.\n\nLead-authored-by: yi.wu <yi.wu@databricks.com>\nCo-authored-by: wuyi <yi.wu@databricks.com>\nSigned-off-by: yi.wu <yi.wu@databricks.com>\n(cherry picked from commit 868a59470650cc12272de0d0b04c6d98b1fe076d)\nSigned-off-by: yi.wu <yi.wu@databricks.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/deploy/worker/WorkerWatcher.scala", "additions": "8", "deletions": "9", "changes": "17"}, "updated": [1, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala", "additions": "19", "deletions": "14", "changes": "33"}, "updated": [1, 3, 4]}]}
{"author": "leoluan2009", "sha": "", "commit_date": "2020/09/16 06:05:35", "commit_message": "[SPARK-32861][SQL] GenerateExec should require column ordering\n\n### What changes were proposed in this pull request?\nThis PR updates the `RemoveRedundantProjects` rule to make `GenerateExec` require column ordering.\n\n### Why are the changes needed?\n`GenerateExec` was originally considered as a node that does not require column ordering. However, `GenerateExec` binds its input rows directly with its `requiredChildOutput` without using the child's output schema.\nIn `doExecute()`:\n```scala\nval proj = UnsafeProjection.create(output, output)\n```\nIn `doConsume()`:\n```scala\nval values = if (requiredChildOutput.nonEmpty) {\n  input\n} else {\n  Seq.empty\n}\n```\nIn this case, changing input column ordering will result in `GenerateExec` binding the wrong schema to the input columns. For example, if we do not require child columns to be ordered, the `requiredChildOutput` [a, b, c] will directly bind to the schema of the input columns [c, b, a], which is incorrect:\n```\nGenerateExec explode(array(a, b, c)), [a, b, c], false, [d]\n  HashAggregate(keys=[a, b, c], functions=[], output=[c, b, a])\n    ...\n```\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nUnit test\n\nCloses #29734 from allisonwang-db/generator.\n\nAuthored-by: allisonwang-db <66282705+allisonwang-db@users.noreply.github.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/RemoveRedundantProjects.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/RemoveRedundantProjectsSuite.scala", "additions": "48", "deletions": "6", "changes": "54"}, "updated": [1, 1, 2]}]}
{"author": "weixiuli", "sha": "", "commit_date": "2021/09/01 08:23:46", "commit_message": " [SPARK-36635][SQL] Fix a mismatch issue that select name expressions as string type", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.parser.ExpressionParserSuite", "org.apache.spark.sql.catalyst.parser.DDLParserSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.DateFunctionsSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala", "additions": "26", "deletions": "0", "changes": "26"}, "updated": [0, 2, 7]}]}
{"author": "dtarima", "sha": "", "commit_date": "2021/07/08 02:46:55", "commit_message": "[SPARK-36036] [CORE] Add JIRA number to test description.", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/test/scala/org/apache/spark/storage/BlockManagerSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 5]}]}
{"author": "tomvanbussel", "sha": "", "commit_date": "2021/06/27 14:56:29", "commit_message": "Fix", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/RowToColumnConverterSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "matthewrj", "sha": "", "commit_date": "2021/06/19 20:49:49", "commit_message": "Fix compile error", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "graphx/src/main/scala/org/apache/spark/graphx/GraphXUtils.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "shipra-a", "sha": "a7e97ddf3c46641d9c8c298d09c4c4e4ac6024ae", "commit_date": "2021/07/01 00:13:09", "commit_message": "remove unintended changes", "title": "[WIP][SPARK-31973][SQL] Skip partial aggregates in run-time if reduction ratio is low", "body": "### What changes were proposed in this pull request?\r\n\r\nThis PR builds on top of https://github.com/apache/spark/pull/28804. In addition to the other PR, one other change is that the partial aggregation hashmap is freed as soon as partial aggregation is disabled to prevent off-heap OOMs.\r\n\r\n\r\n### Why are the changes needed?\r\n\r\nThis change can help improve query performance.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nNo\r\n\r\n### How was this patch tested?\r\n\r\nExisting and additional unit tests.\r\n\r\n", "failed_tests": ["org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "42", "deletions": "0", "changes": "42"}, "updated": [2, 3, 13]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap.java", "additions": "13", "deletions": "0", "changes": "13"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala", "additions": "206", "deletions": "49", "changes": "255"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashMapGenerator.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/WholeStageCodegenSuite.scala", "additions": "37", "deletions": "0", "changes": "37"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/metric/SQLMetricsSuite.scala", "additions": "45", "deletions": "41", "changes": "86"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/AggregationQuerySuite.scala", "additions": "34", "deletions": "27", "changes": "61"}, "updated": [0, 0, 1]}]}
{"author": "clarkead", "sha": "", "commit_date": "2020/08/26 17:34:49", "commit_message": "[MINOR][PYTHON] Fix typo in a docsting of RDD.toDF\n\n### What changes were proposed in this pull request?\n\nFixes typo in docsting of `toDF`\n\n### Why are the changes needed?\n\nThe third argument of `toDF` is actually `sampleRatio`.\nrelated discussion: https://github.com/apache/spark/pull/12746#discussion-diff-62704834\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nThis patch doesn't affect any logic, so existing tests should cover it.\n\nCloses #29551 from unirt/minor_fix_docs.\n\nAuthored-by: unirt <lunirtc@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/sql/session.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [2, 2, 5]}]}
{"author": "PavithraRamachandran", "sha": "", "commit_date": "2021/06/24 12:44:59", "commit_message": "address cve CVE-2015-523 in protypebuf-java jar", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-2.7-hive-2.3", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 19]}, {"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 21]}, {"file": {"name": "pom.xml", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 7, 32]}]}
{"author": "opensky142857", "sha": "3e27a7e119b135fe0c050a840edd4f57a16c7e02", "commit_date": "2021/05/28 07:04:45", "commit_message": "refine code", "title": "[SPARK-35531][SQL] Can not insert into hive bucket table if create table with upper case schema", "body": "### What changes were proposed in this pull request?\r\n\r\nwhen convert to HiveTable, respect table schema cases.\r\n\r\n### Why are the changes needed?\r\n\r\nWhen user create a hive bucket table with upper case schema, the table schema will be stored as lower cases while bucket column info will stay the same with user input.\r\n\r\nif we try to insert into this table, an HiveException reports bucket column is not in table schema. \r\n\r\nhere is a simple repro\r\n\r\n```scala\r\nspark.sql(\"\"\"\r\n  CREATE TABLE TEST1(\r\n    V1 BIGINT,\r\n    S1 INT)\r\n  PARTITIONED BY (PK BIGINT)\r\n  CLUSTERED BY (V1)\r\n  SORTED BY (S1)\r\n  INTO 200 BUCKETS\r\n  STORED AS PARQUET \"\"\").show\r\n\r\nspark.sql(\"INSERT INTO TEST1 SELECT * FROM VALUES(1,1,1)\").show\r\n```\r\n\r\nError message:\r\n```\r\nscala> spark.sql(\"INSERT INTO TEST1 SELECT * FROM VALUES(1,1,1)\").show\r\norg.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: Bucket columns V1 is not part of the table columns ([FieldSchema(name:v1, type:bigint, comment:null), FieldSchema(name:s1, type:int, comment:null)]\r\n  at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:112)\r\n  at org.apache.spark.sql.hive.HiveExternalCatalog.listPartitions(HiveExternalCatalog.scala:1242)\r\n  at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.listPartitions(ExternalCatalogWithListener.scala:254)\r\n  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.listPartitions(SessionCatalog.scala:1166)\r\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:103)\r\n  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\r\n  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\r\n  at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:120)\r\n  at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:228)\r\n  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r\n  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\r\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r\n  at org.apache.spark.sql.Dataset.<init>(Dataset.scala:228)\r\n  at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\r\n  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\r\n  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\r\n  at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)\r\n  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\r\n  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)\r\n  ... 47 elided\r\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Bucket columns V1 is not part of the table columns ([FieldSchema(name:v1, type:bigint, comment:null), FieldSchema(name:s1, type:int, comment:null)]\r\n  at org.apache.hadoop.hive.ql.metadata.Table.setBucketCols(Table.java:552)\r\n  at org.apache.spark.sql.hive.client.HiveClientImpl$.toHiveTable(HiveClientImpl.scala:1082)\r\n  at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$getPartitions$1(HiveClientImpl.scala:732)\r\n  at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:291)\r\n  at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:224)\r\n  at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:223)\r\n  at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:273)\r\n  at org.apache.spark.sql.hive.client.HiveClientImpl.getPartitions(HiveClientImpl.scala:731)\r\n  at org.apache.spark.sql.hive.client.HiveClient.getPartitions(HiveClient.scala:222)\r\n  at org.apache.spark.sql.hive.client.HiveClient.getPartitions$(HiveClient.scala:218)\r\n  at org.apache.spark.sql.hive.client.HiveClientImpl.getPartitions(HiveClientImpl.scala:91)\r\n  at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$listPartitions$1(HiveExternalCatalog.scala:1245)\r\n  at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)\r\n  ... 69 more\r\n```\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nNo\r\n\r\n### How was this patch tested?\r\n\r\nUT\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala", "additions": "11", "deletions": "2", "changes": "13"}, "updated": [0, 1, 6]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/InsertSuite.scala", "additions": "65", "deletions": "0", "changes": "65"}, "updated": [0, 0, 0]}]}
{"author": "kbendick", "sha": "", "commit_date": "2020/11/03 22:20:58", "commit_message": "Remove outdated comment", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": ".github/labeler.yml", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}]}
{"author": "kbendick", "sha": "9f9ccf7c8fb5e2569f7832b3147c89b404196dcf", "commit_date": "2021/06/23 04:39:14", "commit_message": "Increase stack size in build with maven", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": ".github/workflows/build_and_test.yml", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 3, 26]}, {"file": {"name": "pom.xml", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 8, 32]}]}
{"author": "kbendick", "sha": "f276fcf5d7db71bba424935cf320eac53440d16c", "commit_date": "2020/10/29 21:37:59", "commit_message": "Set up new PR labeler workflow and begin porting the probot labeler config to the new format", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": ".github/labeler.yml", "additions": "153", "deletions": "0", "changes": "153"}, "updated": [0, 0, 0]}, {"file": {"name": ".github/workflows/build_and_test.yml", "additions": "422", "deletions": "414", "changes": "836"}, "updated": [1, 6, 11]}, {"file": {"name": ".github/workflows/labeler.yml", "additions": "30", "deletions": "0", "changes": "30"}, "updated": [0, 0, 0]}, {"file": {"name": ".github/workflows/stale.yml", "additions": "21", "deletions": "21", "changes": "42"}, "updated": [0, 0, 0]}, {"file": {"name": ".github/workflows/test_report.yml", "additions": "23", "deletions": "23", "changes": "46"}, "updated": [0, 1, 2]}, {"file": {"name": "R/README.md", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "yos1p", "sha": "", "commit_date": "2021/06/07 05:03:54", "commit_message": "Remove ps_10mins.ipynb", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/docs/source/getting_started/ps_10mins.ipynb", "additions": "0", "deletions": "14471", "changes": "14471"}, "updated": [0, 1, 1]}, {"file": {"name": "python/docs/source/getting_started/ps_install.rst", "additions": "0", "deletions": "145", "changes": "145"}, "updated": [0, 1, 1]}, {"file": {"name": "python/docs/source/getting_started/ps_videos_blogs.rst", "additions": "0", "deletions": "130", "changes": "130"}, "updated": [0, 1, 1]}]}
{"author": "lizhangdatabricks", "sha": "", "commit_date": "2021/06/22 06:30:05", "commit_message": "Improve ScalaDoc and error msgs; Add Java unit tests", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/GroupStateImpl.scala", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/streaming/TestGroupState.scala", "additions": "27", "deletions": "24", "changes": "51"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/java/test/org/apache/spark/sql/JavaDatasetSuite.java", "additions": "92", "deletions": "0", "changes": "92"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/streaming/FlatMapGroupsWithStateSuite.scala", "additions": "5", "deletions": "2", "changes": "7"}, "updated": [0, 0, 0]}]}
{"author": "VasilyKolpakov", "sha": "", "commit_date": "2021/05/12 20:34:21", "commit_message": "Fix memory leak in ExecutorAllocationListener", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala", "additions": "26", "deletions": "15", "changes": "41"}, "updated": [0, 0, 1]}]}
{"author": "copperybean", "sha": "", "commit_date": "2021/05/20 05:35:18", "commit_message": "Create scala.yml", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": ".github/workflows/scala.yml", "additions": "22", "deletions": "0", "changes": "22"}, "updated": [0, 0, 0]}]}
{"author": "haiyangsun-db", "sha": "", "commit_date": "2021/06/08 15:53:43", "commit_message": "Use copy-on-write semantics for SQLConf registered configurations.", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "65", "deletions": "33", "changes": "98"}, "updated": [0, 3, 14]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/plans/SQLHelper.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/RuntimeConfig.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/api/python/PythonSQLUtils.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/internal/SharedState.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}]}
{"author": "xuechendi", "sha": "", "commit_date": "2021/05/17 09:00:47", "commit_message": "Add AutoCloseable close to MemoryStore\n\nThis commit is aimed to close some object may not be close by JVM GC\nFixes upon @srowen reviews:\n1. Add try catch to skip NonFatal exception during close\n2. Add an async caller to avoid close may hold BlockManager lock for too long\n3. Add UT to clarify this PR\n\nSigned-off-by: Chendi Xue <chendi.xue@intel.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/pom.xml", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala", "additions": "40", "deletions": "1", "changes": "41"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/storage/MemoryStoreSuite.scala", "additions": "105", "deletions": "0", "changes": "105"}, "updated": [0, 0, 0]}]}
{"author": "nolanliou", "sha": "", "commit_date": "2021/05/26 06:49:57", "commit_message": "fix(pyspark): fix OverflowError in partitionBy function", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/rdd.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}]}
{"author": "satishgopalani", "sha": "", "commit_date": "2021/05/04 17:36:54", "commit_message": "Fixing load of maxTriggerDelayMs", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaSource.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}]}
{"author": "satishgopalani", "sha": "090688510e82d925fe2fb2688c40f94de73b2b0c", "commit_date": "2021/05/03 17:20:12", "commit_message": "Adding Smart Trigger which skips batch if the number of available records in Kafka are less than minOffsetsPerTrigger", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "docs/structured-streaming-kafka-integration.md", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaMicroBatchStream.scala", "additions": "56", "deletions": "3", "changes": "59"}, "updated": [0, 0, 0]}, {"file": {"name": "external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaSource.scala", "additions": "65", "deletions": "7", "changes": "72"}, "updated": [0, 0, 0]}, {"file": {"name": "external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaSourceProvider.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 2, 2]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/read/streaming/CompositeReadLimit.java", "additions": "61", "deletions": "0", "changes": "61"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/read/streaming/ReadLimit.java", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/read/streaming/ReadMinRows.java", "additions": "55", "deletions": "0", "changes": "55"}, "updated": [0, 0, 0]}]}
{"author": "alkispoly-db", "sha": "", "commit_date": "2021/05/29 01:44:17", "commit_message": "Fixed tests for Null values", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/stat/StatFunctions.scala", "additions": "46", "deletions": "69", "changes": "115"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameStatSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "mariosmeim-db", "sha": "", "commit_date": "2021/05/20 10:01:00", "commit_message": "revert import changes", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/jdbc/MySQLDialect.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCSuite.scala", "additions": "9", "deletions": "1", "changes": "10"}, "updated": [0, 0, 1]}]}
{"author": "keerthanvasist", "sha": "", "commit_date": "2021/06/03 17:03:56", "commit_message": "[Python][Bug] Fix ambiguous reference in functions.py column()\n\nFixes TypeError: 'str' object is not callable in 3.1.2", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/sql/functions.py", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [2, 3, 5]}]}
{"author": "mggger", "sha": "", "commit_date": "2021/06/04 08:47:05", "commit_message": "Merge branch 'apache:master' into master", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/SessionCatalog.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/catalog/SessionCatalogSuite.scala", "additions": "2", "deletions": "11", "changes": "13"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/DDLSuite.scala", "additions": "1", "deletions": "6", "changes": "7"}, "updated": [1, 1, 1]}]}
{"author": "huskysun", "sha": "", "commit_date": "2021/05/31 03:54:17", "commit_message": "Revert change on k8s example, fix yarn example\n\nThis is because, in bash, '#' can't come after '\\'.", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "docs/submitting-applications.md", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 1]}]}
{"author": "ankurdave", "sha": "", "commit_date": "2021/05/21 19:11:06", "commit_message": "[SPARK-35486][CORE] TaskMemoryManager: retry if other task takes memory freed by partial self-spill", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/java/org/apache/spark/memory/TaskMemoryManager.java", "additions": "10", "deletions": "2", "changes": "12"}, "updated": [0, 0, 0]}]}
{"author": "felixcheung", "sha": "", "commit_date": "2021/05/24 00:19:27", "commit_message": "change SparkR maintainer", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "R/pkg/DESCRIPTION", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}
{"author": "jacobhjkim", "sha": "", "commit_date": "2020/09/03 11:56:03", "commit_message": "[SPARK-32755][SQL] Maintain the order of expressions in AttributeSet and ExpressionSet\n\n### What changes were proposed in this pull request?\nThis PR changes `AttributeSet` and `ExpressionSet` to maintain the insertion order of the elements. More specifically, we:\n- change the underlying data structure of `AttributeSet` from `HashSet` to `LinkedHashSet` to maintain the insertion order.\n- `ExpressionSet` already uses a list to keep track of the expressions, however, since it is extending Scala's immutable.Set class, operations such as map and flatMap are delegated to the immutable.Set itself. This means that the result of these operations is not an instance of ExpressionSet anymore, rather it's a implementation picked up by the parent class. We also remove this inheritance from `immutable.Set `and implement the needed methods directly. ExpressionSet has a very specific semantics and it does not make sense to extend `immutable.Set` anyway.\n- change the `PlanStabilitySuite` to not sort the attributes, to be able to catch changes in the order of expressions in different runs.\n\n### Why are the changes needed?\nExpressions identity is based on the `ExprId` which is an auto-incremented number. This means that the same query can yield a query plan with different expression ids in different runs. `AttributeSet` and `ExpressionSet` internally use a `HashSet` as the underlying data structure, and therefore cannot guarantee the a fixed order of operations in different runs. This can be problematic in cases we like to check for plan changes in different runs.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nPasses `PlanStabilitySuite` after regenerating the golden files.\n\nCloses #29598 from dbaliafroozeh/FixOrderOfExpressions.\n\nAuthored-by: Ali Afroozeh <ali.afroozeh@databricks.com>\nSigned-off-by: herman <herman@databricks.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala-2.12/org/apache/spark/sql/catalyst/expressions/ExpressionSet.scala", "additions": "78", "deletions": "18", "changes": "96"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/AttributeSet.scala", "additions": "7", "deletions": "3", "changes": "10"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/CostBasedJoinReorder.scala", "additions": "11", "deletions": "11", "changes": "22"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [1, 1, 13]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/LogicalPlan.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/QueryPlanConstraints.scala", "additions": "16", "deletions": "18", "changes": "34"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala", "additions": "16", "deletions": "16", "changes": "32"}, "updated": [1, 1, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q10.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q10.sf100/simplified.txt", "additions": "20", "deletions": "20", "changes": "40"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q10/simplified.txt", "additions": "20", "deletions": "20", "changes": "40"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q19.sf100/simplified.txt", "additions": "22", "deletions": "22", "changes": "44"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q19/simplified.txt", "additions": "22", "deletions": "22", "changes": "44"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q27.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q27.sf100/simplified.txt", "additions": "41", "deletions": "41", "changes": "82"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q27/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q27/simplified.txt", "additions": "44", "deletions": "44", "changes": "88"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q3.sf100/simplified.txt", "additions": "13", "deletions": "13", "changes": "26"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q3/simplified.txt", "additions": "11", "deletions": "11", "changes": "22"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q34.sf100/simplified.txt", "additions": "18", "deletions": "18", "changes": "36"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q34/simplified.txt", "additions": "18", "deletions": "18", "changes": "36"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q42.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q42.sf100/simplified.txt", "additions": "14", "deletions": "14", "changes": "28"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q42/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q42/simplified.txt", "additions": "14", "deletions": "14", "changes": "28"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q43.sf100/simplified.txt", "additions": "12", "deletions": "12", "changes": "24"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q43/simplified.txt", "additions": "12", "deletions": "12", "changes": "24"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q46.sf100/simplified.txt", "additions": "23", "deletions": "23", "changes": "46"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q46/simplified.txt", "additions": "24", "deletions": "24", "changes": "48"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q52.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q52.sf100/simplified.txt", "additions": "12", "deletions": "12", "changes": "24"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q52/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q52/simplified.txt", "additions": "12", "deletions": "12", "changes": "24"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q53.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q53.sf100/simplified.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q53/simplified.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q55.sf100/simplified.txt", "additions": "12", "deletions": "12", "changes": "24"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q55/simplified.txt", "additions": "12", "deletions": "12", "changes": "24"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q59.sf100/simplified.txt", "additions": "25", "deletions": "25", "changes": "50"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q59/simplified.txt", "additions": "25", "deletions": "25", "changes": "50"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q63.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q63.sf100/simplified.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q63/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q63/simplified.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q65.sf100/simplified.txt", "additions": "25", "deletions": "25", "changes": "50"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q65/simplified.txt", "additions": "24", "deletions": "24", "changes": "48"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q68.sf100/simplified.txt", "additions": "24", "deletions": "24", "changes": "48"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q68/simplified.txt", "additions": "24", "deletions": "24", "changes": "48"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q7.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q7.sf100/simplified.txt", "additions": "17", "deletions": "17", "changes": "34"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q7/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q7/simplified.txt", "additions": "18", "deletions": "18", "changes": "36"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q73.sf100/simplified.txt", "additions": "16", "deletions": "16", "changes": "32"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q73/simplified.txt", "additions": "16", "deletions": "16", "changes": "32"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q79.sf100/simplified.txt", "additions": "19", "deletions": "19", "changes": "38"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q79/simplified.txt", "additions": "19", "deletions": "19", "changes": "38"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q89.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q89.sf100/simplified.txt", "additions": "20", "deletions": "20", "changes": "40"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q89/simplified.txt", "additions": "19", "deletions": "19", "changes": "38"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q98.sf100/simplified.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q98/simplified.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/ss_max.sf100/simplified.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/ss_max/simplified.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q1.sf100/simplified.txt", "additions": "18", "deletions": "18", "changes": "36"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q1/simplified.txt", "additions": "18", "deletions": "18", "changes": "36"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q10.sf100/simplified.txt", "additions": "16", "deletions": "16", "changes": "32"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q10/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q10/simplified.txt", "additions": "16", "deletions": "16", "changes": "32"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q11.sf100/simplified.txt", "additions": "41", "deletions": "41", "changes": "82"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q11/simplified.txt", "additions": "40", "deletions": "40", "changes": "80"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q12.sf100/simplified.txt", "additions": "12", "deletions": "12", "changes": "24"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q12/simplified.txt", "additions": "12", "deletions": "12", "changes": "24"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q13.sf100/simplified.txt", "additions": "19", "deletions": "19", "changes": "38"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q13/simplified.txt", "additions": "19", "deletions": "19", "changes": "38"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a.sf100/simplified.txt", "additions": "78", "deletions": "78", "changes": "156"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a/simplified.txt", "additions": "75", "deletions": "75", "changes": "150"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b.sf100/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b.sf100/simplified.txt", "additions": "71", "deletions": "71", "changes": "142"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b/explain.txt", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b/simplified.txt", "additions": "68", "deletions": "68", "changes": "136"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q15.sf100/simplified.txt", "additions": "8", "deletions": "8", "changes": "16"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q15/simplified.txt", "additions": "11", "deletions": "11", "changes": "22"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q16.sf100/simplified.txt", "additions": "19", "deletions": "19", "changes": "38"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q16/simplified.txt", "additions": "19", "deletions": "19", "changes": "38"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q17.sf100/explain.txt", "additions": "8", "deletions": "8", "changes": "16"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q17.sf100/simplified.txt", "additions": "24", "deletions": "24", "changes": "48"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q17/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q17/simplified.txt", "additions": "26", "deletions": "26", "changes": "52"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q18.sf100/simplified.txt", "additions": "24", "deletions": "24", "changes": "48"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q18/simplified.txt", "additions": "24", "deletions": "24", "changes": "48"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q19.sf100/simplified.txt", "additions": "23", "deletions": "23", "changes": "46"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q19/simplified.txt", "additions": "22", "deletions": "22", "changes": "44"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q2.sf100/simplified.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q2/simplified.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q20.sf100/simplified.txt", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q20/simplified.txt", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q21.sf100/simplified.txt", "additions": "16", "deletions": "16", "changes": "32"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q21/simplified.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q22.sf100/simplified.txt", "additions": "11", "deletions": "11", "changes": "22"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q22/simplified.txt", "additions": "13", "deletions": "13", "changes": "26"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23a.sf100/simplified.txt", "additions": "44", "deletions": "44", "changes": "88"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23a/simplified.txt", "additions": "44", "deletions": "44", "changes": "88"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23b.sf100/simplified.txt", "additions": "59", "deletions": "59", "changes": "118"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23b/simplified.txt", "additions": "55", "deletions": "55", "changes": "110"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24a.sf100/explain.txt", "additions": "11", "deletions": "11", "changes": "22"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24a.sf100/simplified.txt", "additions": "57", "deletions": "57", "changes": "114"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24a/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24a/simplified.txt", "additions": "50", "deletions": "50", "changes": "100"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24b.sf100/explain.txt", "additions": "11", "deletions": "11", "changes": "22"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24b.sf100/simplified.txt", "additions": "57", "deletions": "57", "changes": "114"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24b/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24b/simplified.txt", "additions": "50", "deletions": "50", "changes": "100"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q25.sf100/explain.txt", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q25.sf100/simplified.txt", "additions": "26", "deletions": "26", "changes": "52"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q25/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q25/simplified.txt", "additions": "28", "deletions": "28", "changes": "56"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q26.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q26.sf100/simplified.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q26/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q26/simplified.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q27.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q27.sf100/simplified.txt", "additions": "19", "deletions": "19", "changes": "38"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q27/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q27/simplified.txt", "additions": "19", "deletions": "19", "changes": "38"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q28.sf100/simplified.txt", "additions": "36", "deletions": "36", "changes": "72"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q28/simplified.txt", "additions": "36", "deletions": "36", "changes": "72"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q29.sf100/explain.txt", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q29.sf100/simplified.txt", "additions": "27", "deletions": "27", "changes": "54"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q29/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q29/simplified.txt", "additions": "29", "deletions": "29", "changes": "58"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q3.sf100/simplified.txt", "additions": "13", "deletions": "13", "changes": "26"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q3/simplified.txt", "additions": "12", "deletions": "12", "changes": "24"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q30.sf100/simplified.txt", "additions": "27", "deletions": "27", "changes": "54"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q30/simplified.txt", "additions": "27", "deletions": "27", "changes": "54"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q31.sf100/simplified.txt", "additions": "57", "deletions": "57", "changes": "114"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q31/simplified.txt", "additions": "55", "deletions": "55", "changes": "110"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q32.sf100/simplified.txt", "additions": "12", "deletions": "12", "changes": "24"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q32/simplified.txt", "additions": "11", "deletions": "11", "changes": "22"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q33.sf100/simplified.txt", "additions": "30", "deletions": "30", "changes": "60"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q33/simplified.txt", "additions": "31", "deletions": "31", "changes": "62"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q34.sf100/simplified.txt", "additions": "18", "deletions": "18", "changes": "36"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q34/simplified.txt", "additions": "18", "deletions": "18", "changes": "36"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q35.sf100/simplified.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q35/simplified.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q36.sf100/simplified.txt", "additions": "16", "deletions": "16", "changes": "32"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q36/simplified.txt", "additions": "16", "deletions": "16", "changes": "32"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q37.sf100/simplified.txt", "additions": "14", "deletions": "14", "changes": "28"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q37/simplified.txt", "additions": "14", "deletions": "14", "changes": "28"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q38.sf100/simplified.txt", "additions": "43", "deletions": "43", "changes": "86"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q38/simplified.txt", "additions": "37", "deletions": "37", "changes": "74"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q39a.sf100/simplified.txt", "additions": "31", "deletions": "31", "changes": "62"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q39a/simplified.txt", "additions": "31", "deletions": "31", "changes": "62"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q39b.sf100/simplified.txt", "additions": "31", "deletions": "31", "changes": "62"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q39b/simplified.txt", "additions": "31", "deletions": "31", "changes": "62"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q4.sf100/simplified.txt", "additions": "60", "deletions": "60", "changes": "120"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q4/simplified.txt", "additions": "59", "deletions": "59", "changes": "118"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q40.sf100/simplified.txt", "additions": "20", "deletions": "20", "changes": "40"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q40/simplified.txt", "additions": "16", "deletions": "16", "changes": "32"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q41.sf100/simplified.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q41/simplified.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q42.sf100/simplified.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q42/simplified.txt", "additions": "14", "deletions": "14", "changes": "28"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q43.sf100/simplified.txt", "additions": "12", "deletions": "12", "changes": "24"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q43/simplified.txt", "additions": "12", "deletions": "12", "changes": "24"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q44.sf100/simplified.txt", "additions": "18", "deletions": "18", "changes": "36"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q44/simplified.txt", "additions": "18", "deletions": "18", "changes": "36"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q45.sf100/simplified.txt", "additions": "21", "deletions": "21", "changes": "42"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q45/simplified.txt", "additions": "20", "deletions": "20", "changes": "40"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q46.sf100/simplified.txt", "additions": "21", "deletions": "21", "changes": "42"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q46/simplified.txt", "additions": "24", "deletions": "24", "changes": "48"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q47.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q47.sf100/simplified.txt", "additions": "45", "deletions": "45", "changes": "90"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q47/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q47/simplified.txt", "additions": "38", "deletions": "38", "changes": "76"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q48.sf100/simplified.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q48/simplified.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q49.sf100/explain.txt", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q49.sf100/simplified.txt", "additions": "56", "deletions": "56", "changes": "112"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q49/explain.txt", "additions": "8", "deletions": "8", "changes": "16"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q49/simplified.txt", "additions": "44", "deletions": "44", "changes": "88"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q5.sf100/simplified.txt", "additions": "41", "deletions": "41", "changes": "82"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q5/simplified.txt", "additions": "41", "deletions": "41", "changes": "82"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q50.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q50.sf100/simplified.txt", "additions": "22", "deletions": "22", "changes": "44"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q50/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q50/simplified.txt", "additions": "19", "deletions": "19", "changes": "38"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q51.sf100/simplified.txt", "additions": "32", "deletions": "32", "changes": "64"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q51/simplified.txt", "additions": "32", "deletions": "32", "changes": "64"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q52.sf100/simplified.txt", "additions": "13", "deletions": "13", "changes": "26"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q52/simplified.txt", "additions": "12", "deletions": "12", "changes": "24"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q53.sf100/simplified.txt", "additions": "14", "deletions": "14", "changes": "28"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q53/simplified.txt", "additions": "14", "deletions": "14", "changes": "28"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q54.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q54.sf100/simplified.txt", "additions": "31", "deletions": "31", "changes": "62"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q54/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q54/simplified.txt", "additions": "32", "deletions": "32", "changes": "64"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q55.sf100/simplified.txt", "additions": "13", "deletions": "13", "changes": "26"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q55/simplified.txt", "additions": "12", "deletions": "12", "changes": "24"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q56.sf100/simplified.txt", "additions": "35", "deletions": "35", "changes": "70"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q56/simplified.txt", "additions": "35", "deletions": "35", "changes": "70"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q57.sf100/simplified.txt", "additions": "42", "deletions": "42", "changes": "84"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q57/simplified.txt", "additions": "37", "deletions": "37", "changes": "74"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q58.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q58.sf100/simplified.txt", "additions": "28", "deletions": "28", "changes": "56"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q58/simplified.txt", "additions": "28", "deletions": "28", "changes": "56"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q59.sf100/simplified.txt", "additions": "20", "deletions": "20", "changes": "40"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q59/simplified.txt", "additions": "20", "deletions": "20", "changes": "40"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q6.sf100/simplified.txt", "additions": "19", "deletions": "19", "changes": "38"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q6/simplified.txt", "additions": "18", "deletions": "18", "changes": "36"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q60.sf100/simplified.txt", "additions": "34", "deletions": "34", "changes": "68"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q60/simplified.txt", "additions": "34", "deletions": "34", "changes": "68"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q61.sf100/simplified.txt", "additions": "31", "deletions": "31", "changes": "62"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q61/simplified.txt", "additions": "34", "deletions": "34", "changes": "68"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q62.sf100/simplified.txt", "additions": "16", "deletions": "16", "changes": "32"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q62/simplified.txt", "additions": "17", "deletions": "17", "changes": "34"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q63.sf100/simplified.txt", "additions": "14", "deletions": "14", "changes": "28"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q63/simplified.txt", "additions": "14", "deletions": "14", "changes": "28"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q64.sf100/simplified.txt", "additions": "87", "deletions": "87", "changes": "174"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q64/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q64/simplified.txt", "additions": "88", "deletions": "88", "changes": "176"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q65.sf100/simplified.txt", "additions": "25", "deletions": "25", "changes": "50"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q65/simplified.txt", "additions": "24", "deletions": "24", "changes": "48"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q66.sf100/simplified.txt", "additions": "33", "deletions": "33", "changes": "66"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q66/simplified.txt", "additions": "33", "deletions": "33", "changes": "66"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q67.sf100/simplified.txt", "additions": "19", "deletions": "19", "changes": "38"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q67/simplified.txt", "additions": "19", "deletions": "19", "changes": "38"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q68.sf100/simplified.txt", "additions": "21", "deletions": "21", "changes": "42"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q68/simplified.txt", "additions": "24", "deletions": "24", "changes": "48"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q69.sf100/simplified.txt", "additions": "16", "deletions": "16", "changes": "32"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q69/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q69/simplified.txt", "additions": "16", "deletions": "16", "changes": "32"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q7.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q7.sf100/simplified.txt", "additions": "18", "deletions": "18", "changes": "36"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q7/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q7/simplified.txt", "additions": "18", "deletions": "18", "changes": "36"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q70.sf100/simplified.txt", "additions": "23", "deletions": "23", "changes": "46"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q70/simplified.txt", "additions": "23", "deletions": "23", "changes": "46"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q71.sf100/simplified.txt", "additions": "22", "deletions": "22", "changes": "44"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q71/simplified.txt", "additions": "22", "deletions": "22", "changes": "44"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q72.sf100/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q72.sf100/simplified.txt", "additions": "35", "deletions": "35", "changes": "70"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q72/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q72/simplified.txt", "additions": "32", "deletions": "32", "changes": "64"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q73.sf100/simplified.txt", "additions": "16", "deletions": "16", "changes": "32"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q73/simplified.txt", "additions": "16", "deletions": "16", "changes": "32"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q74.sf100/simplified.txt", "additions": "34", "deletions": "34", "changes": "68"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q74/simplified.txt", "additions": "33", "deletions": "33", "changes": "66"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q75.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q75.sf100/simplified.txt", "additions": "103", "deletions": "103", "changes": "206"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q75/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q75/simplified.txt", "additions": "78", "deletions": "78", "changes": "156"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q76.sf100/simplified.txt", "additions": "26", "deletions": "26", "changes": "52"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q76/simplified.txt", "additions": "26", "deletions": "26", "changes": "52"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q77.sf100/simplified.txt", "additions": "40", "deletions": "40", "changes": "80"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q77/simplified.txt", "additions": "40", "deletions": "40", "changes": "80"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q78.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q78.sf100/simplified.txt", "additions": "50", "deletions": "50", "changes": "100"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q78/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q78/simplified.txt", "additions": "35", "deletions": "35", "changes": "70"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q79.sf100/simplified.txt", "additions": "19", "deletions": "19", "changes": "38"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q79/simplified.txt", "additions": "19", "deletions": "19", "changes": "38"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q8.sf100/simplified.txt", "additions": "14", "deletions": "14", "changes": "28"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q8/simplified.txt", "additions": "14", "deletions": "14", "changes": "28"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q80.sf100/simplified.txt", "additions": "52", "deletions": "52", "changes": "104"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q80/simplified.txt", "additions": "52", "deletions": "52", "changes": "104"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q81.sf100/simplified.txt", "additions": "26", "deletions": "26", "changes": "52"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q81/simplified.txt", "additions": "26", "deletions": "26", "changes": "52"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q82.sf100/simplified.txt", "additions": "13", "deletions": "13", "changes": "26"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q82/simplified.txt", "additions": "13", "deletions": "13", "changes": "26"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q83.sf100/simplified.txt", "additions": "21", "deletions": "21", "changes": "42"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q83/simplified.txt", "additions": "24", "deletions": "24", "changes": "48"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q84.sf100/simplified.txt", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q84/simplified.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q85.sf100/explain.txt", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q85.sf100/simplified.txt", "additions": "28", "deletions": "28", "changes": "56"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q85/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q85/simplified.txt", "additions": "29", "deletions": "29", "changes": "58"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q86.sf100/simplified.txt", "additions": "12", "deletions": "12", "changes": "24"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q86/simplified.txt", "additions": "12", "deletions": "12", "changes": "24"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q87.sf100/simplified.txt", "additions": "43", "deletions": "43", "changes": "86"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q87/simplified.txt", "additions": "37", "deletions": "37", "changes": "74"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q88.sf100/simplified.txt", "additions": "42", "deletions": "42", "changes": "84"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q88/simplified.txt", "additions": "42", "deletions": "42", "changes": "84"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q89.sf100/simplified.txt", "additions": "18", "deletions": "18", "changes": "36"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q89/simplified.txt", "additions": "18", "deletions": "18", "changes": "36"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9.sf100/simplified.txt", "additions": "35", "deletions": "35", "changes": "70"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9/simplified.txt", "additions": "35", "deletions": "35", "changes": "70"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q90.sf100/simplified.txt", "additions": "16", "deletions": "16", "changes": "32"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q90/simplified.txt", "additions": "14", "deletions": "14", "changes": "28"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q91.sf100/simplified.txt", "additions": "22", "deletions": "22", "changes": "44"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q91/simplified.txt", "additions": "20", "deletions": "20", "changes": "40"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q92.sf100/simplified.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q92/simplified.txt", "additions": "16", "deletions": "16", "changes": "32"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q93.sf100/simplified.txt", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q93/simplified.txt", "additions": "11", "deletions": "11", "changes": "22"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q94.sf100/simplified.txt", "additions": "20", "deletions": "20", "changes": "40"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q94/simplified.txt", "additions": "20", "deletions": "20", "changes": "40"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q95.sf100/simplified.txt", "additions": "22", "deletions": "22", "changes": "44"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q95/simplified.txt", "additions": "21", "deletions": "21", "changes": "42"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q96.sf100/simplified.txt", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q96/simplified.txt", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q97.sf100/simplified.txt", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q97/simplified.txt", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q98.sf100/simplified.txt", "additions": "14", "deletions": "14", "changes": "28"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q98/simplified.txt", "additions": "14", "deletions": "14", "changes": "28"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q99.sf100/simplified.txt", "additions": "14", "deletions": "14", "changes": "28"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q99/simplified.txt", "additions": "13", "deletions": "13", "changes": "26"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q10a.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q10a.sf100/simplified.txt", "additions": "16", "deletions": "16", "changes": "32"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q10a/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q10a/simplified.txt", "additions": "16", "deletions": "16", "changes": "32"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q11.sf100/simplified.txt", "additions": "42", "deletions": "42", "changes": "84"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q11/simplified.txt", "additions": "41", "deletions": "41", "changes": "82"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q12.sf100/simplified.txt", "additions": "12", "deletions": "12", "changes": "24"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q12/simplified.txt", "additions": "12", "deletions": "12", "changes": "24"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14.sf100/explain.txt", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14.sf100/simplified.txt", "additions": "71", "deletions": "71", "changes": "142"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14/explain.txt", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14/simplified.txt", "additions": "68", "deletions": "68", "changes": "136"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a.sf100/simplified.txt", "additions": "146", "deletions": "146", "changes": "292"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a/simplified.txt", "additions": "143", "deletions": "143", "changes": "286"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q18a.sf100/simplified.txt", "additions": "77", "deletions": "77", "changes": "154"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q18a/simplified.txt", "additions": "85", "deletions": "85", "changes": "170"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q20.sf100/simplified.txt", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q20/simplified.txt", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q22.sf100/simplified.txt", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q22/simplified.txt", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q22a.sf100/simplified.txt", "additions": "30", "deletions": "30", "changes": "60"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q22a/simplified.txt", "additions": "32", "deletions": "32", "changes": "64"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q24.sf100/explain.txt", "additions": "16", "deletions": "16", "changes": "32"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q24.sf100/simplified.txt", "additions": "60", "deletions": "60", "changes": "120"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q24/explain.txt", "additions": "8", "deletions": "8", "changes": "16"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q24/simplified.txt", "additions": "52", "deletions": "52", "changes": "104"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q27a.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q27a.sf100/simplified.txt", "additions": "44", "deletions": "44", "changes": "88"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q27a/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q27a/simplified.txt", "additions": "44", "deletions": "44", "changes": "88"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q34.sf100/simplified.txt", "additions": "18", "deletions": "18", "changes": "36"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q34/simplified.txt", "additions": "18", "deletions": "18", "changes": "36"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35.sf100/simplified.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35/simplified.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35a.sf100/simplified.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35a/simplified.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q36a.sf100/simplified.txt", "additions": "29", "deletions": "29", "changes": "58"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q36a/simplified.txt", "additions": "29", "deletions": "29", "changes": "58"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q47.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q47.sf100/simplified.txt", "additions": "45", "deletions": "45", "changes": "90"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q47/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q47/simplified.txt", "additions": "38", "deletions": "38", "changes": "76"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q49.sf100/explain.txt", "additions": "8", "deletions": "8", "changes": "16"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q49.sf100/simplified.txt", "additions": "56", "deletions": "56", "changes": "112"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q49/explain.txt", "additions": "12", "deletions": "12", "changes": "24"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q49/simplified.txt", "additions": "44", "deletions": "44", "changes": "88"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q51a.sf100/simplified.txt", "additions": "53", "deletions": "53", "changes": "106"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q51a/simplified.txt", "additions": "55", "deletions": "55", "changes": "110"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q57.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q57.sf100/simplified.txt", "additions": "42", "deletions": "42", "changes": "84"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q57/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q57/simplified.txt", "additions": "37", "deletions": "37", "changes": "74"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q5a.sf100/simplified.txt", "additions": "54", "deletions": "54", "changes": "108"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q5a/simplified.txt", "additions": "54", "deletions": "54", "changes": "108"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q6.sf100/simplified.txt", "additions": "20", "deletions": "20", "changes": "40"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q6/simplified.txt", "additions": "19", "deletions": "19", "changes": "38"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q64.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q64.sf100/simplified.txt", "additions": "87", "deletions": "87", "changes": "174"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q64/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q64/simplified.txt", "additions": "88", "deletions": "88", "changes": "176"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q67a.sf100/simplified.txt", "additions": "55", "deletions": "55", "changes": "110"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q67a/simplified.txt", "additions": "55", "deletions": "55", "changes": "110"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q70a.sf100/simplified.txt", "additions": "38", "deletions": "38", "changes": "76"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q70a/simplified.txt", "additions": "38", "deletions": "38", "changes": "76"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q72.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q72.sf100/simplified.txt", "additions": "35", "deletions": "35", "changes": "70"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q72/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q72/simplified.txt", "additions": "32", "deletions": "32", "changes": "64"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q74.sf100/simplified.txt", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q74/simplified.txt", "additions": "32", "deletions": "32", "changes": "64"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q75.sf100/explain.txt", "additions": "8", "deletions": "8", "changes": "16"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q75.sf100/simplified.txt", "additions": "103", "deletions": "103", "changes": "206"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q75/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q75/simplified.txt", "additions": "78", "deletions": "78", "changes": "156"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q77a.sf100/simplified.txt", "additions": "53", "deletions": "53", "changes": "106"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q77a/simplified.txt", "additions": "53", "deletions": "53", "changes": "106"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q78.sf100/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q78.sf100/simplified.txt", "additions": "50", "deletions": "50", "changes": "100"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q78/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q78/simplified.txt", "additions": "35", "deletions": "35", "changes": "70"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q80a.sf100/simplified.txt", "additions": "65", "deletions": "65", "changes": "130"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q80a/simplified.txt", "additions": "65", "deletions": "65", "changes": "130"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q86a.sf100/simplified.txt", "additions": "25", "deletions": "25", "changes": "50"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q86a/simplified.txt", "additions": "25", "deletions": "25", "changes": "50"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q98.sf100/simplified.txt", "additions": "13", "deletions": "13", "changes": "26"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q98/simplified.txt", "additions": "13", "deletions": "13", "changes": "26"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/PlanStabilitySuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 2, 3]}]}
{"author": "ashrayjain", "sha": "", "commit_date": "2021/05/19 11:08:45", "commit_message": "Mark K8s Secrets and ConfigMaps created by Spark as immutable\n\nKubernetes supports marking secrets and config maps as immutable to gain performance.\n\nhttps://kubernetes.io/docs/concepts/configuration/configmap/#configmap-immutable\nhttps://kubernetes.io/docs/concepts/configuration/secret/#secret-immutable\n\nFor K8s clusters that run many thousands of Spark applications, this can yield significant reduction in load on the kube-apiserver.\nFrom the K8s docs:\n\nFor clusters that extensively use Secrets (at least tens of thousands of unique Secret to Pod mounts), preventing changes to their data has the following advantages:\n- protects you from accidental (or unwanted) updates that could cause applications outages\n- improves performance of your cluster by significantly reducing load on kube-apiserver, by closing watches for secrets marked as immutable.\n\nFor any secrets and config maps we create in Spark that are immutable, we can mark them as immutable by including the following when building\nthe secret/config map: \".withImmutable(true)\"\n\nThis feature has been supported in K8s as beta since K8s 1.19 and as GA since K8s 1.21", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/DriverKubernetesCredentialsFeatureStep.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/HadoopConfDriverFeatureStep.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/KerberosConfDriverFeatureStep.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/PodTemplateConfigMapStep.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/submit/KubernetesClientUtils.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/PodTemplateConfigMapStepSuite.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/submit/ClientSuite.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 1]}]}
{"author": "YuzhouSun", "sha": "", "commit_date": "2021/05/12 05:31:01", "commit_message": "fix bug", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveExternalCatalog.scala", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}]}
{"author": "lipzhu", "sha": "", "commit_date": "2021/04/28 13:02:39", "commit_message": "Automated formatting for Scala Code for Blank Lines", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "dev/.scalafmt.conf", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "pom.xml", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 4, 15]}]}
{"author": "chrisheaththomas", "sha": "", "commit_date": "2021/05/15 13:42:57", "commit_message": "Update job-scheduling.md\n\nTrigger Github build and test action.", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "docs/job-scheduling.md", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}
{"author": "gerashegalov", "sha": "", "commit_date": "2021/05/15 05:52:59", "commit_message": "[SPARK-35408][PYSPARK] Improve parameter validation in DataFrame.show", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/sql/dataframe.py", "additions": "13", "deletions": "1", "changes": "14"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/sql/tests/test_dataframe.py", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 1, 1]}]}
{"author": "luhenry", "sha": "", "commit_date": "2021/01/04 14:20:18", "commit_message": "Correct typo in command to run benchmark", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "mllib-local/src/test/scala/org/apache/spark/ml/linalg/BLASBenchmark.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "LittleCuteBug", "sha": "", "commit_date": "2021/05/14 01:54:08", "commit_message": "[SPARK-32484][SQL] Fix log info BroadcastExchangeExec.scala", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/BroadcastExchangeExec.scala", "additions": "6", "deletions": "5", "changes": "11"}, "updated": [0, 0, 1]}]}
{"author": "gaborgsomogyi", "sha": "", "commit_date": "2020/09/01 07:23:20", "commit_message": "[SPARK-32579][SQL] Implement JDBCScan/ScanBuilder/WriteBuilder\n\n### What changes were proposed in this pull request?\nAdd JDBCScan, JDBCScanBuilder, JDBCWriteBuilder in Datasource V2 JDBC\n\n### Why are the changes needed?\nComplete Datasource V2 JDBC implementation\n\n### Does this PR introduce _any_ user-facing change?\nYes\n\n### How was this patch tested?\nnew tests\n\nCloses #29396 from huaxingao/v2jdbc.\n\nAuthored-by: Huaxin Gao <huaxing@us.ibm.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCScan.scala", "additions": "50", "deletions": "0", "changes": "50"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCScanBuilder.scala", "additions": "70", "deletions": "0", "changes": "70"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCTable.scala", "additions": "25", "deletions": "10", "changes": "35"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCWriteBuilder.scala", "additions": "46", "deletions": "0", "changes": "46"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala", "additions": "224", "deletions": "0", "changes": "224"}, "updated": [1, 1, 1]}]}
{"author": "bonnal-enzo", "sha": "", "commit_date": "2021/05/09 11:52:42", "commit_message": "overload PageRank.runWithOptions and runWithOptionsWithPreviousPageRank with a 'normalized' parameter to trigger or not the normalization", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "graphx/src/main/scala/org/apache/spark/graphx/lib/PageRank.scala", "additions": "65", "deletions": "6", "changes": "71"}, "updated": [0, 0, 0]}]}
{"author": "seayoun", "sha": "", "commit_date": "2021/01/26 08:13:11", "commit_message": "[SPARK-34235][SS] Make spark.sql.hive as a private package\n\n### What changes were proposed in this pull request?\nFollow the comment https://github.com/apache/spark/pull/31271#discussion_r562598983:\n\n- Remove the API tag `Unstable` for `HiveSessionStateBuilder`\n- Add document for spark.sql.hive package to emphasize it's a private package\n\n### Why are the changes needed?\nFollow the rule for a private package.\n\n### Does this PR introduce _any_ user-facing change?\nNo.\n\n### How was this patch tested?\nDoc change only.\n\nCloses #31321 from xuanyuanking/SPARK-34185-follow.\n\nAuthored-by: Yuanjian Li <yuanjian.li@databricks.com>\nSigned-off-by: HyukjinKwon <gurwls223@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 3, 10]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveSessionStateBuilder.scala", "additions": "0", "deletions": "2", "changes": "2"}, "updated": [2, 2, 4]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/package.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [1, 1, 1]}]}
{"author": "yijiacui-db", "sha": "", "commit_date": "2021/04/23 07:49:35", "commit_message": "update test", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "external/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaMicroBatchSourceSuite.scala", "additions": "0", "deletions": "17", "changes": "17"}, "updated": [0, 0, 0]}]}
{"author": "byungsoo-oh", "sha": "", "commit_date": "2021/05/03 06:33:47", "commit_message": "Add message when creating directory", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/test/scala/org/apache/spark/benchmark/BenchmarkBase.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 2]}]}
{"author": "jose-torres", "sha": "", "commit_date": "2021/04/27 16:55:43", "commit_message": "fix check", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/UnsupportedOperationChecker.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/UnsupportedOperationsSuite.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 3]}]}
{"author": "adrian-wang", "sha": "", "commit_date": "2021/04/27 07:00:36", "commit_message": "[DOC] Add JindoFS SDK in cloud integration documents", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "docs/cloud-integration.md", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}]}
{"author": "harupy", "sha": "", "commit_date": "2021/04/20 02:30:08", "commit_message": "specify return type for rawPredictionUDF", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/ml/classification.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}]}
{"author": "andersonm-ibm", "sha": "", "commit_date": "2021/04/20 06:02:19", "commit_message": "Change test to use TestHiveSingleton like other tests in Hive module instead of SharedSparkSession", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/ParquetEncryptionSuite.scala", "additions": "3", "deletions": "4", "changes": "7"}, "updated": [0, 0, 0]}]}
{"author": "sfcoy", "sha": "", "commit_date": "2021/08/26 04:59:18", "commit_message": "[SPARK-36457][DOCS] Review and fix issues in Scala/Java API docs\n\n### What changes were proposed in this pull request?\n\nCompare the 3.2.0 API doc with the latest release version 3.1.2. Fix the following issues:\n\n- Add missing `Since` annotation for new APIs\n- Remove the leaking class/object in API doc\n\n### Why are the changes needed?\n\nImprove API docs\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting UT\n\nCloses #33824 from gengliangwang/auditDoc.\n\nAuthored-by: Gengliang Wang <gengliang@apache.org>\nSigned-off-by: Gengliang Wang <gengliang@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/SparkException.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [2, 2, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/errors/SparkCoreErrors.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 2, 4]}, {"file": {"name": "core/src/main/scala/org/apache/spark/io/MutableCheckedOutputStream.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/MiscellaneousProcessDetails.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 1, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [1, 1, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/DependencyUtils.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "project/SparkBuild.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [1, 1, 6]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/FunctionCatalog.java", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/TruncatableTable.java", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/functions/AggregateFunction.java", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/functions/BoundFunction.java", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/functions/Function.java", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/functions/ScalarFunction.java", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/functions/UnboundFunction.java", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/read/streaming/ReportsSourceMetrics.java", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [2, 4, 15]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/types/UDTRegistration.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/types/UserDefinedType.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/connector/write/V1Write.java", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 1, 1]}]}
{"author": "ChenDou2021", "sha": "", "commit_date": "2021/04/21 09:59:25", "commit_message": "Add whether the path is dir", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/ml/util.py", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/tests/test_readwrite.py", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 0, 0]}]}
{"author": "sander-goos", "sha": "", "commit_date": "2021/04/16 15:15:35", "commit_message": "Remove initial null value of LiveStage.info", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusListener.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/LiveEntity.scala", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 0, 1]}]}
{"author": "nchammas", "sha": "", "commit_date": "2020/08/21 12:23:41", "commit_message": "[SPARK-32682][INFRA] Use workflow_dispatch to enable manual test triggers\n\n### What changes were proposed in this pull request?\n\nThis PR proposes to add a `workflow_dispatch` entry in the GitHub Action script (`build_and_test.yml`). This update can enable developers to run the Spark tests for a specific branch on their own local repository, so I think it might help to check if al the tests can pass before opening a new PR.\n\n<img width=\"944\" alt=\"Screen Shot 2020-08-21 at 16 28 41\" src=\"https://user-images.githubusercontent.com/692303/90866249-96250c80-e3ce-11ea-8496-3dd6683e92ea.png\">\n\n### Why are the changes needed?\n\nTo reduce the pressure of GitHub Actions on the Spark repository.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nManually checked.\n\nCloses #29504 from maropu/DispatchTest.\n\nAuthored-by: Takeshi Yamamuro <yamamuro@apache.org>\nSigned-off-by: Takeshi Yamamuro <yamamuro@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": ".github/workflows/build_and_test.yml", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [2, 3, 3]}, {"file": {"name": "dev/run-tests.py", "additions": "7", "deletions": "1", "changes": "8"}, "updated": [1, 2, 12]}]}
{"author": "gemelen", "sha": "", "commit_date": "2020/09/12 22:02:50", "commit_message": "Increase stack size for sbt", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": ".github/workflows/build_and_test.yml", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 5]}]}
{"author": "izchen", "sha": "", "commit_date": "2020/10/14 03:13:54", "commit_message": "[SPARK-33134][SQL] Return partial results only for root JSON objects\n\n### What changes were proposed in this pull request?\nIn the PR, I propose to restrict the partial result feature only by root JSON objects. JSON datasource as well as `from_json()` will return `null` for malformed nested JSON objects.\n\n### Why are the changes needed?\n1. To not raise exception to users in the PERMISSIVE mode\n2. To fix a regression and to have the same behavior as Spark 2.4.x has\n3. Current implementation of partial result is supposed to work only for root (top-level) JSON objects, and not tested for bad nested complex JSON fields.\n\n### Does this PR introduce _any_ user-facing change?\nYes. Before the changes, the code below:\n```scala\n    val pokerhand_raw = Seq(\"\"\"[{\"cards\": [19], \"playerId\": 123456}]\"\"\").toDF(\"events\")\n    val event = new StructType().add(\"playerId\", LongType).add(\"cards\", ArrayType(new StructType().add(\"id\", LongType).add(\"rank\", StringType)))\n    val pokerhand_events = pokerhand_raw.select(from_json($\"events\", ArrayType(event)).as(\"event\"))\n    pokerhand_events.show\n```\nthrows the exception even in the default **PERMISSIVE** mode:\n```java\njava.lang.ClassCastException: java.lang.Long cannot be cast to org.apache.spark.sql.catalyst.util.ArrayData\n  at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow.getArray(rows.scala:48)\n  at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow.getArray$(rows.scala:48)\n  at org.apache.spark.sql.catalyst.expressions.GenericInternalRow.getArray(rows.scala:195)\n```\n\nAfter the changes:\n```\n+-----+\n|event|\n+-----+\n| null|\n+-----+\n```\n\n### How was this patch tested?\nAdded a test to `JsonFunctionsSuite`.\n\nCloses #30031 from MaxGekk/json-skip-row-wrong-schema.\n\nAuthored-by: Max Gekk <max.gekk@gmail.com>\nSigned-off-by: HyukjinKwon <gurwls223@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JacksonParser.scala", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/JsonFunctionsSuite.scala", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [1, 1, 1]}]}
{"author": "dbaliafroozeh", "sha": "", "commit_date": "2020/08/31 08:39:12", "commit_message": "[SPARK-32747][R][TESTS] Deduplicate configuration set/unset in test_sparkSQL_arrow.R\n\n### What changes were proposed in this pull request?\n\nThis PR proposes to deduplicate configuration set/unset in `test_sparkSQL_arrow.R`.\nSetting `spark.sql.execution.arrow.sparkr.enabled` can be globally done instead of doing it in each test case.\n\n### Why are the changes needed?\n\nTo duduplicate the codes.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo, dev-only\n\n### How was this patch tested?\n\nManually ran the tests.\n\nCloses #29592 from HyukjinKwon/SPARK-32747.\n\nAuthored-by: HyukjinKwon <gurwls223@apache.org>\nSigned-off-by: HyukjinKwon <gurwls223@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "R/pkg/tests/fulltests/test_sparkSQL_arrow.R", "additions": "60", "deletions": "141", "changes": "201"}, "updated": [1, 1, 2]}]}
{"author": "waitinfuture", "sha": "", "commit_date": "2020/11/26 01:19:38", "commit_message": "[SPARK-33562][UI] Improve the style of the checkbox in executor page\n\n### What changes were proposed in this pull request?\n\n1. Remove the fixed width style of class `container-fluid-div`. So that the UI looks clean when the text is long.\n2. Add one space between a checkbox and the text on the right side, which is consistent with the stage page.\n\n### Why are the changes needed?\n\nThe width of class `container-fluid-div` is set as 200px after https://github.com/apache/spark/pull/21688 . This makes the checkbox in the executor page messy.\n![image](https://user-images.githubusercontent.com/1097932/100242069-3bc5ab80-2ee9-11eb-8c7d-96c221398fee.png)\n\nWe should remove the width limit.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n### How was this patch tested?\n\nManual test.\nAfter the changes:\n![image](https://user-images.githubusercontent.com/1097932/100257802-2f4a4e80-2efb-11eb-9eb0-92d6988ad14b.png)\n\nCloses #30500 from gengliangwang/reviseStyle.\n\nAuthored-by: Gengliang Wang <gengliang.wang@databricks.com>\nSigned-off-by: HyukjinKwon <gurwls223@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/executorspage.js", "additions": "9", "deletions": "9", "changes": "18"}, "updated": [1, 1, 3]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/webui.css", "additions": "0", "deletions": "4", "changes": "4"}, "updated": [1, 1, 1]}]}
{"author": "luluorta", "sha": "", "commit_date": "2020/09/22 04:43:17", "commit_message": "[SPARK-32951][SQL] Foldable propagation from Aggregate\n\n### What changes were proposed in this pull request?\nThis PR adds foldable propagation from `Aggregate` as per: https://github.com/apache/spark/pull/29771#discussion_r490412031\n\n### Why are the changes needed?\nThis is an improvement as `Aggregate`'s `aggregateExpressions` can contain foldables that can be propagated up.\n\n### Does this PR introduce _any_ user-facing change?\nNo.\n\n### How was this patch tested?\nNew UT.\n\nCloses #29816 from peter-toth/SPARK-32951-foldable-propagation-from-aggregate.\n\nAuthored-by: Peter Toth <peter.toth@gmail.com>\nSigned-off-by: Dongjoon Hyun <dhyun@apple.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/expressions.scala", "additions": "15", "deletions": "4", "changes": "19"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/FoldablePropagationSuite.scala", "additions": "12", "deletions": "0", "changes": "12"}, "updated": [1, 2, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a.sf100/explain.txt", "additions": "25", "deletions": "25", "changes": "50"}, "updated": [1, 1, 4]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a.sf100/simplified.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [1, 1, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a/explain.txt", "additions": "25", "deletions": "25", "changes": "50"}, "updated": [1, 1, 4]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a/simplified.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [1, 1, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b.sf100/explain.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 4]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b.sf100/simplified.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [1, 1, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b/explain.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 4]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b/simplified.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [1, 1, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q41.sf100/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [1, 1, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q41.sf100/simplified.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 1, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q41/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [1, 1, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q41/simplified.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 1, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14.sf100/explain.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 4]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14.sf100/simplified.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [1, 1, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14/explain.txt", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [1, 1, 4]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14/simplified.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [1, 1, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a.sf100/explain.txt", "additions": "265", "deletions": "265", "changes": "530"}, "updated": [1, 1, 4]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a.sf100/simplified.txt", "additions": "30", "deletions": "30", "changes": "60"}, "updated": [1, 1, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a/explain.txt", "additions": "265", "deletions": "265", "changes": "530"}, "updated": [1, 1, 4]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a/simplified.txt", "additions": "30", "deletions": "30", "changes": "60"}, "updated": [1, 1, 3]}]}
{"author": "Karl-WangSK", "sha": "", "commit_date": "2020/10/29 03:29:43", "commit_message": "Merge remote-tracking branch 'upstream/master'", "title": "", "body": "", "failed_tests": [], "files": []}
{"author": "davidrabinowitz", "sha": "", "commit_date": "2020/11/13 21:26:07", "commit_message": "Adding support for UserDefinedType for Spark SQL Code generator", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "9", "deletions": "7", "changes": "16"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodegenGetValueFromVectorSuite.scala", "additions": "94", "deletions": "0", "changes": "94"}, "updated": [0, 0, 0]}]}
{"author": "MrPowers", "sha": "", "commit_date": "2021/01/20 02:40:37", "commit_message": "[SPARK-33940][BUILD] Upgrade univocity to 2.9.1\n\n### What changes were proposed in this pull request?\n\nupgrade univocity\n\n### Why are the changes needed?\n\ncsv writer actually has an implicit limit on column name length due to univocity-parser 2.9.0,\n\nwhen we initialize a writer https://github.com/uniVocity/univocity-parsers/blob/e09114c6879fa6c2c15e7365abc02cda3e193ff7/src/main/java/com/univocity/parsers/common/AbstractWriter.java#L211, it calls toIdentifierGroupArray which calls valueOf in NormalizedString.java eventually (https://github.com/uniVocity/univocity-parsers/blob/e09114c6879fa6c2c15e7365abc02cda3e193ff7/src/main/java/com/univocity/parsers/common/NormalizedString.java#L205-L209)\n\nin that stringCache.get, it has a maxStringLength cap https://github.com/uniVocity/univocity-parsers/blob/e09114c6879fa6c2c15e7365abc02cda3e193ff7/src/main/java/com/univocity/parsers/common/StringCache.java#L104 which is 1024 by default\n\nmore details at https://github.com/apache/spark/pull/30972 and https://github.com/uniVocity/univocity-parsers/issues/438\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\nexisting UT\n\nCloses #31246 from CodingCat/upgrade_univocity.\n\nAuthored-by: CodingCat <zhunansjtu@gmail.com>\nSigned-off-by: HyukjinKwon <gurwls223@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-2.7-hive-2.3", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [3, 6, 18]}, {"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [3, 6, 18]}, {"file": {"name": "pom.xml", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [3, 6, 26]}]}
{"author": "kuwii", "sha": "", "commit_date": "2020/10/26 00:06:06", "commit_message": "[SPARK-33234][INFRA] Generates SHA-512 using shasum\n\n### What changes were proposed in this pull request?\n\nI am generating the SHA-512 using the standard shasum which also has a better output compared to GPG.\n\n### Why are the changes needed?\n\nWhich makes the hash much easier to verify for users that don't have GPG.\n\nBecause an user having GPG can check the keys but an user without GPG will have a hard time validating the SHA-512 based on the 'pretty printed' format.\n\nApache Spark is the only project where I've seen this format. Most other Apache projects have a one-line hash file.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nThis patch assumes the build system has shasum (it should, but I can't test this).\n\nCloses #30123 from emilianbold/master.\n\nAuthored-by: Emi <emilian.bold@gmail.com>\nSigned-off-by: Dongjoon Hyun <dhyun@apple.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "dev/create-release/release-build.sh", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [1, 1, 5]}]}
{"author": "CodingCat", "sha": "", "commit_date": "2020/08/31 06:43:13", "commit_message": "[SPARK-32740][SQL] Refactor common partitioning/distribution logic to BaseAggregateExec\n\n### What changes were proposed in this pull request?\n\nFor all three different aggregate physical operator: `HashAggregateExec`, `ObjectHashAggregateExec` and `SortAggregateExec`, they have same `outputPartitioning` and `requiredChildDistribution` logic. Refactor these same logic into their super class `BaseAggregateExec` to avoid code duplication and future bugs (similar to `HashJoin` and `ShuffledJoin`).\n\n### Why are the changes needed?\n\nReduce duplicated code across classes and prevent future bugs if we only update one class but forget another. We already did similar refactoring for join (`HashJoin` and `ShuffledJoin`).\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nExisting unit tests as this is pure refactoring and no new logic added.\n\nCloses #29583 from c21/aggregate-refactor.\n\nAuthored-by: Cheng Su <chengsu@fb.com>\nSigned-off-by: Takeshi Yamamuro <yamamuro@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/BaseAggregateExec.scala", "additions": "17", "deletions": "3", "changes": "20"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala", "additions": "1", "deletions": "15", "changes": "16"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "1", "deletions": "14", "changes": "15"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/SortAggregateExec.scala", "additions": "2", "deletions": "16", "changes": "18"}, "updated": [1, 1, 1]}]}
{"author": "coderbond007", "sha": "", "commit_date": "2021/01/03 09:31:38", "commit_message": "[SPARK-33955][SS] Add latest offsets to source progress\n\n### What changes were proposed in this pull request?\n\nThis patch proposes to add latest offset to source progress for streaming queries.\n\n### Why are the changes needed?\n\nCurrently we record start and end offsets per source in streaming process. Latest offset is an important information for streaming process but the progress lacks of this info. We can use it to track the process lag and adjust streaming queries. We should add latest offset to source progress.\n\n### Does this PR introduce _any_ user-facing change?\n\nYes, for new metric about latest source offset in source progress.\n\n### How was this patch tested?\n\nUnit test. Manually test in Spark cluster:\n\n```\n    \"description\" : \"KafkaV2[Subscribe[page_view_events]]\",\n    \"startOffset\" : {\n      \"page_view_events\" : {\n        \"2\" : 582370921,\n        \"4\" : 391910836,\n        \"1\" : 631009201,\n        \"3\" : 406601346,\n        \"0\" : 195799112\n      }\n    },\n    \"endOffset\" : {\n      \"page_view_events\" : {\n        \"2\" : 583764414,\n        \"4\" : 392338002,\n        \"1\" : 632183480,\n        \"3\" : 407101489,\n        \"0\" : 197304028\n      }\n    },\n    \"latestOffset\" : {\n      \"page_view_events\" : {\n        \"2\" : 589852545,\n        \"4\" : 394204277,\n        \"1\" : 637313869,\n        \"3\" : 409286602,\n        \"0\" : 203878962\n      }\n    },\n    \"numInputRows\" : 4999997,\n    \"inputRowsPerSecond\" : 29287.70501405811,\n```\n\nCloses #30988 from viirya/latest-offset.\n\nAuthored-by: Liang-Chi Hsieh <viirya@gmail.com>\nSigned-off-by: Dongjoon Hyun <dhyun@apple.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaMicroBatchStream.scala", "additions": "7", "deletions": "1", "changes": "8"}, "updated": [1, 2, 3]}, {"file": {"name": "external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaSource.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [1, 2, 3]}, {"file": {"name": "project/MimaExcludes.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [1, 2, 7]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/read/streaming/SupportsAdmissionControl.java", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala", "additions": "18", "deletions": "7", "changes": "25"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ProgressReporter.scala", "additions": "11", "deletions": "2", "changes": "13"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [1, 1, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/streaming/progress.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingQueryStatusAndProgressSuite.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [1, 1, 1]}]}
{"author": "Fokko", "sha": "150b903ecb7008908d1063694fcd8db8fda604ff", "commit_date": "2021/08/03 04:43:54", "commit_message": "Bump checkstyle from 8.29 to 8.45\n\nBumps [checkstyle](https://github.com/checkstyle/checkstyle) from 8.29 to 8.45.\n- [Release notes](https://github.com/checkstyle/checkstyle/releases)\n- [Commits](https://github.com/checkstyle/checkstyle/compare/checkstyle-8.29...checkstyle-8.45)\n\nSigned-off-by: dependabot-preview[bot] <support@dependabot.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "pom.xml", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 7, 30]}]}
{"author": "Fokko", "sha": "a64e740764e7773c3b623eabebd953abfac08200", "commit_date": "2021/07/08 04:48:11", "commit_message": "[Security] Bump jetty.version from 9.4.28.v20200408 to 9.4.43.v20210629\n\nBumps `jetty.version` from 9.4.28.v20200408 to 9.4.43.v20210629.\n\nUpdates `jetty-http` from 9.4.28.v20200408 to 9.4.43.v20210629\n- [Release notes](https://github.com/eclipse/jetty.project/releases)\n- [Commits](https://github.com/eclipse/jetty.project/compare/jetty-9.4.28.v20200408...jetty-9.4.43.v20210629)\n\nUpdates `jetty-continuation` from 9.4.28.v20200408 to 9.4.43.v20210629\n\nUpdates `jetty-servlet` from 9.4.28.v20200408 to 9.4.43.v20210629\n- [Release notes](https://github.com/eclipse/jetty.project/releases)\n- [Commits](https://github.com/eclipse/jetty.project/compare/jetty-9.4.28.v20200408...jetty-9.4.43.v20210629)\n\nUpdates `jetty-servlets` from 9.4.28.v20200408 to 9.4.43.v20210629\n- [Release notes](https://github.com/eclipse/jetty.project/releases)\n- [Commits](https://github.com/eclipse/jetty.project/compare/jetty-9.4.28.v20200408...jetty-9.4.43.v20210629)\n\nUpdates `jetty-proxy` from 9.4.28.v20200408 to 9.4.43.v20210629\n- [Release notes](https://github.com/eclipse/jetty.project/releases)\n- [Commits](https://github.com/eclipse/jetty.project/compare/jetty-9.4.28.v20200408...jetty-9.4.43.v20210629)\n\nUpdates `jetty-client` from 9.4.28.v20200408 to 9.4.43.v20210629\n- [Release notes](https://github.com/eclipse/jetty.project/releases)\n- [Commits](https://github.com/eclipse/jetty.project/compare/jetty-9.4.28.v20200408...jetty-9.4.43.v20210629)\n\nUpdates `jetty-util` from 9.4.28.v20200408 to 9.4.43.v20210629\n- [Release notes](https://github.com/eclipse/jetty.project/releases)\n- [Commits](https://github.com/eclipse/jetty.project/compare/jetty-9.4.28.v20200408...jetty-9.4.43.v20210629)\n\nUpdates `jetty-security` from 9.4.28.v20200408 to 9.4.43.v20210629\n- [Release notes](https://github.com/eclipse/jetty.project/releases)\n- [Commits](https://github.com/eclipse/jetty.project/compare/jetty-9.4.28.v20200408...jetty-9.4.43.v20210629)\n\nUpdates `jetty-plus` from 9.4.28.v20200408 to 9.4.43.v20210629\n- [Release notes](https://github.com/eclipse/jetty.project/releases)\n- [Commits](https://github.com/eclipse/jetty.project/compare/jetty-9.4.28.v20200408...jetty-9.4.43.v20210629)\n\nUpdates `jetty-server` from 9.4.28.v20200408 to 9.4.43.v20210629\n- [Release notes](https://github.com/eclipse/jetty.project/releases)\n- [Commits](https://github.com/eclipse/jetty.project/compare/jetty-9.4.28.v20200408...jetty-9.4.43.v20210629)\n\nUpdates `jetty-webapp` from 9.4.28.v20200408 to 9.4.43.v20210629\n- [Release notes](https://github.com/eclipse/jetty.project/releases)\n- [Commits](https://github.com/eclipse/jetty.project/compare/jetty-9.4.28.v20200408...jetty-9.4.43.v20210629)\n\nUpdates `jetty-util-ajax` from 9.4.28.v20200408 to 9.4.43.v20210629\n- [Release notes](https://github.com/eclipse/jetty.project/releases)\n- [Commits](https://github.com/eclipse/jetty.project/compare/jetty-9.4.28.v20200408...jetty-9.4.43.v20210629)\n\nSigned-off-by: dependabot-preview[bot] <support@dependabot.com>", "title": "", "body": "", "failed_tests": ["org.apache.spark.ui.UISuite"], "files": [{"file": {"name": "pom.xml", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [2, 11, 36]}]}
{"author": "Fokko", "sha": "78b9602332d4ef9401f5f031de0782ea3910b774", "commit_date": "2021/04/27 21:24:13", "commit_message": "[Security] Bump commons-io from 2.5 to 2.7\n\nBumps commons-io from 2.5 to 2.7. **This update includes a security fix.**\n\nSigned-off-by: dependabot-preview[bot] <support@dependabot.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "pom.xml", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [2, 4, 15]}]}
{"author": "Fokko", "sha": "", "commit_date": "2020/11/03 04:29:37", "commit_message": "Bump checkstyle from 8.29 to 8.37\n\nBumps [checkstyle](https://github.com/checkstyle/checkstyle) from 8.29 to 8.37.\n- [Release notes](https://github.com/checkstyle/checkstyle/releases)\n- [Commits](https://github.com/checkstyle/checkstyle/compare/checkstyle-8.29...checkstyle-8.37)\n\nSigned-off-by: dependabot-preview[bot] <support@dependabot.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "pom.xml", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 3, 10]}]}
{"author": "hotienvu", "sha": "", "commit_date": "2020/09/11 19:30:02", "commit_message": "refactored and add config option", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/expressions.scala", "additions": "33", "deletions": "34", "changes": "67"}, "updated": [0, 2, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [1, 2, 22]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/OptimizeInSuite.scala", "additions": "46", "deletions": "17", "changes": "63"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ReplaceOperatorSuite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/columnar/PartitionBatchPruningSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 0, 1]}]}
{"author": "guiyanakuang", "sha": "", "commit_date": "2021/09/01 06:27:30", "commit_message": "[SPARK-36607][SQL] Support BooleanType in UnwrapCastInBinaryComparison\n\n### What changes were proposed in this pull request?\nThis PR proposes to add `BooleanType` support to the `UnwrapCastInBinaryComparison` optimizer that is currently supports `NumericType` only.\n\nThe main idea is to treat `BooleanType` as 1 bit integer so that we can utilize all optimizations already defined in `UnwrapCastInBinaryComparison`.\n\nThis work is an extension of SPARK-24994 and SPARK-32858\n\n### Why are the changes needed?\nCurrent implementation of Spark without this PR cannot properly optimize the filter for the following case\n```\nSELECT * FROM t WHERE boolean_field = 2\n```\nThe above query creates a filter of `cast(boolean_field, int) = 2`. The casting prevents from pushing down the filter. In contrast, this PR creates a `false` filter and returns early as there cannot be such a matching rows anyway (empty results.)\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nPassed existing tests\n```\nbuild/sbt \"catalyst/test\"\nbuild/sbt \"sql/test\"\n```\nAdded unit tests\n```\nbuild/sbt \"catalyst/testOnly *UnwrapCastInBinaryComparisonSuite   -- -z SPARK-36607\"\nbuild/sbt \"sql/testOnly *UnwrapCastInComparisonEndToEndSuite  -- -z SPARK-36607\"\n```\n\nCloses #33865 from kazuyukitanimura/SPARK-36607.\n\nAuthored-by: Kazuyuki Tanimura <ktanimura@apple.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/UnwrapCastInBinaryComparison.scala", "additions": "5", "deletions": "4", "changes": "9"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/UnwrapCastInBinaryComparisonSuite.scala", "additions": "47", "deletions": "10", "changes": "57"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/UnwrapCastInComparisonEndToEndSuite.scala", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [1, 1, 1]}]}
{"author": "mayurdb", "sha": "", "commit_date": "2020/07/09 11:47:19", "commit_message": "Revert \"Additional checks on deciding the pruning side\"\n\nThis reverts commit 89664b4f657811ef3c30eb77450e8d636742faa0.", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/dynamicpruning/PartitionPruning.scala", "additions": "3", "deletions": "33", "changes": "36"}, "updated": [0, 0, 0]}]}
{"author": "pgillet", "sha": "", "commit_date": "2020/11/12 08:50:32", "commit_message": "[SPARK-33386][SQL] Accessing array elements in ElementAt/Elt/GetArrayItem should failed if index is out of bound\n\n### What changes were proposed in this pull request?\n\nInstead of returning NULL, throws runtime ArrayIndexOutOfBoundsException when ansiMode is enable for `element_at`\uff0c`elt`, `GetArrayItem` functions.\n\n### Why are the changes needed?\n\nFor ansiMode.\n\n### Does this PR introduce any user-facing change?\n\nWhen `spark.sql.ansi.enabled` = true, Spark will throw `ArrayIndexOutOfBoundsException` if out-of-range index when accessing array elements\n\n### How was this patch tested?\n\nAdded UT and existing UT.\n\nCloses #30297 from leanken/leanken-SPARK-33386.\n\nAuthored-by: xuewei.linxuewei <xuewei.linxuewei@alibaba-inc.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "docs/sql-ref-ansi-compliance.md", "additions": "8", "deletions": "1", "changes": "9"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/TypeCoercion.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ProjectionOverSchema.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/SelectedField.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala", "additions": "30", "deletions": "23", "changes": "53"}, "updated": [2, 2, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/complexTypeExtractors.scala", "additions": "53", "deletions": "14", "changes": "67"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala", "additions": "30", "deletions": "3", "changes": "33"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/ComplexTypes.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [1, 4, 18]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala", "additions": "84", "deletions": "52", "changes": "136"}, "updated": [2, 2, 2]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ComplexTypeSuite.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [1, 2, 2]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala", "additions": "31", "deletions": "1", "changes": "32"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/ansi/array.sql", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/array.sql", "additions": "12", "deletions": "0", "changes": "12"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/array.sql.out", "additions": "234", "deletions": "0", "changes": "234"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/array.sql.out", "additions": "66", "deletions": "1", "changes": "67"}, "updated": [1, 1, 1]}]}
{"author": "cchighman", "sha": "", "commit_date": "2020/08/18 03:22:59", "commit_message": "Update Files", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/pathFilters.scala", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/PathFilterSuite.scala", "additions": "22", "deletions": "67", "changes": "89"}, "updated": [0, 0, 0]}]}
{"author": "artiship", "sha": "", "commit_date": "2020/11/05 09:21:17", "commit_message": "[SPARK-30294][SS] Explicitly defines read-only StateStore and optimize for HDFSBackedStateStore\n\n### What changes were proposed in this pull request?\n\nThere's a concept of 'read-only' and 'read+write' state store in Spark which is defined \"implicitly\". Spark doesn't prevent write for 'read-only' state store; Spark just assumes read-only stateful operator will not modify the state store. Given it's not defined explicitly, the instance of state store has to be implemented as 'read+write' even it's being used as 'read-only', which sometimes brings confusion.\n\nFor example, abort() in HDFSBackedStateStore - https://github.com/apache/spark/blob/d38f8167483d4d79e8360f24a8c0bffd51460659/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider.scala#L143-L155\n\nThe comment sounds as if statement works differently between 'read-only' and 'read+write', but that's not true as both state store has state initialized as UPDATING (no difference). So 'read-only' state also creates the temporary file, initializes output streams to write to temporary file, closes output streams, and finally deletes the temporary file. This unnecessary operations are being done per batch/partition.\n\nThis patch explicitly defines 'read-only' StateStore, and enables state store provider to create 'read-only' StateStore instance if requested. Relevant code paths are modified, as well as 'read-only' StateStore implementation for HDFSBackedStateStore is introduced. The new implementation gets rid of unnecessary operations explained above.\n\nIn point of backward-compatibility view, the only thing being changed in public API side is `StateStoreProvider`. The trait `StateStoreProvider` has to be changed to allow requesting 'read-only' StateStore; this patch adds default implementation which leverages 'read+write' StateStore but wrapping with 'write-protected' StateStore instance, so that custom providers don't need to change their code to reflect the change. But if the providers can optimize for read-only workload, they'll be happy to make a change.\n\nPlease note that this patch makes ReadOnlyStateStore extend StateStore and being referred as StateStore, as StateStore is being used in so many places and it's not easy to support both traits if we differentiate them. So unfortunately these write methods are still exposed for read-only state; it just throws UnsupportedOperationException.\n\n### Why are the changes needed?\n\nThe new API opens the chance to optimize read-only state store instance compared with read+write state store instance. HDFSBackedStateStoreProvider is modified to provide read-only version of state store which doesn't deal with temporary file as well as state machine.\n\n### Does this PR introduce any user-facing change?\n\nClearly \"no\" for most end users, and also \"no\" for custom state store providers as it doesn't touch trait `StateStore` as well as provides default implementation for added method in trait `StateStoreProvider`.\n\n### How was this patch tested?\n\nModified UT. Existing UTs ensure the change doesn't break anything.\n\nCloses #26935 from HeartSaVioR/SPARK-30294.\n\nAuthored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>\nSigned-off-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider.scala", "additions": "38", "deletions": "8", "changes": "46"}, "updated": [1, 2, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStore.scala", "additions": "93", "deletions": "18", "changes": "111"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreRDD.scala", "additions": "81", "deletions": "23", "changes": "104"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StreamingAggregationStateManager.scala", "additions": "11", "deletions": "11", "changes": "22"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/package.scala", "additions": "35", "deletions": "0", "changes": "35"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/statefulOperators.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/StateStoreSuite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 3]}]}
{"author": "dongkelun", "sha": "", "commit_date": "2021/08/03 00:12:54", "commit_message": "[SPARK-36373][SQL] DecimalPrecision only add necessary cast\n\n### What changes were proposed in this pull request?\n\nThis pr makes `DecimalPrecision` only add necessary cast similar to [`ImplicitTypeCasts`](https://github.com/apache/spark/blob/96c2919988ddf78d104103876d8d8221e8145baa/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/TypeCoercion.scala#L675-L678). For example:\n```\nEqualTo(AttributeReference(\"d1\", DecimalType(5, 2))(), AttributeReference(\"d2\", DecimalType(2, 1))())\n```\nIt will add a useless cast to _d1_:\n```\n(cast(d1#6 as decimal(5,2)) = cast(d2#7 as decimal(5,2)))\n```\n\n### Why are the changes needed?\n\n1. Avoid adding unnecessary cast. Although it will be removed by  `SimplifyCasts` later.\n2. I'm trying to add an extended rule similar to `PullOutGroupingExpressions`. The current behavior will introduce additional alias. For example: `cast(d1 as decimal(5,2)) as cast_d1`.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nUnit test.\n\nCloses #33602 from wangyum/SPARK-36373.\n\nAuthored-by: Yuming Wang <yumwang@ebay.com>\nSigned-off-by: Yuming Wang <yumwang@ebay.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/DecimalPrecision.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/DecimalPrecisionSuite.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [1, 1, 1]}]}
{"author": "rahij", "sha": "", "commit_date": "2020/10/02 13:16:19", "commit_message": "[SPARK-32741][SQL][FOLLOWUP] Run plan integrity check only for effective plan changes\n\n### What changes were proposed in this pull request?\n\n(This is a followup PR of #29585) The PR modified `RuleExecutor#isPlanIntegral` code for checking if a plan has globally-unique attribute IDs, but this check made Jenkins maven test jobs much longer (See [the Dongjoon comment](https://github.com/apache/spark/pull/29585#issuecomment-702461314) and thanks, dongjoon-hyun !). To recover running time for the Jenkins tests, this PR intends to update the code to run plan integrity check only for effective plans.\n\n### Why are the changes needed?\n\nTo recover running time for Jenkins tests.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nExisting tests.\n\nCloses #29928 from maropu/PR29585-FOLLOWUP.\n\nAuthored-by: Takeshi Yamamuro <yamamuro@apache.org>\nSigned-off-by: Takeshi Yamamuro <yamamuro@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/rules/RuleExecutor.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 2]}]}
{"author": "TJX2014", "sha": "", "commit_date": "2020/08/22 12:32:23", "commit_message": "[SPARK-31792][SS][DOC][FOLLOW-UP] Rephrase the description for some operations\n\n### What changes were proposed in this pull request?\nRephrase the description for some operations to make it clearer.\n\n### Why are the changes needed?\nAdd more detail in the document.\n\n### Does this PR introduce _any_ user-facing change?\nNo, document only.\n\n### How was this patch tested?\nDocument only.\n\nCloses #29269 from xuanyuanking/SPARK-31792-follow.\n\nAuthored-by: Yuanjian Li <yuanjian.li@databricks.com>\nSigned-off-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "docs/web-ui.md", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [1, 1, 1]}]}
{"author": "fqaiser94", "sha": "", "commit_date": "2020/08/23 15:26:15", "commit_message": "Merge branch 'master' of https://github.com/apache/spark", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": ".github/workflows/build_and_test.yml", "additions": "32", "deletions": "11", "changes": "43"}, "updated": [1, 3, 3]}, {"file": {"name": ".github/workflows/test_report.yml", "additions": "24", "deletions": "0", "changes": "24"}, "updated": [0, 2, 2]}, {"file": {"name": "R/pkg/R/functions.R", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "R/pkg/tests/run-all.R", "additions": "6", "deletions": "3", "changes": "9"}, "updated": [0, 1, 2]}, {"file": {"name": "appveyor.yml", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 2]}, {"file": {"name": "common/network-common/src/main/java/org/apache/spark/network/util/TransportConf.java", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ExternalBlockStoreClient.java", "additions": "0", "deletions": "6", "changes": "6"}, "updated": [1, 1, 1]}, {"file": {"name": "core/src/main/java/org/apache/spark/api/plugin/DriverPlugin.java", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 1, 1]}, {"file": {"name": "core/src/main/java/org/apache/spark/shuffle/sort/io/LocalDiskShuffleMapOutputWriter.java", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 1, 1]}, {"file": {"name": "core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java", "additions": "70", "deletions": "0", "changes": "70"}, "updated": [0, 1, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [1, 3, 6]}, {"file": {"name": "core/src/main/scala/org/apache/spark/SparkContext.scala", "additions": "21", "deletions": "6", "changes": "27"}, "updated": [0, 1, 6]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala", "additions": "8", "deletions": "2", "changes": "10"}, "updated": [0, 1, 4]}, {"file": {"name": "core/src/main/scala/org/apache/spark/executor/Executor.scala", "additions": "29", "deletions": "16", "changes": "45"}, "updated": [0, 2, 4]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/Worker.scala", "additions": "0", "deletions": "6", "changes": "6"}, "updated": [0, 1, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 2, 7]}, {"file": {"name": "core/src/main/scala/org/apache/spark/resource/ResourceDiscoveryScriptPlugin.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala", "additions": "29", "deletions": "12", "changes": "41"}, "updated": [0, 1, 6]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/ShuffleMapStage.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/TaskDescription.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala", "additions": "42", "deletions": "8", "changes": "50"}, "updated": [0, 2, 9]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala", "additions": "36", "deletions": "30", "changes": "66"}, "updated": [0, 3, 5]}, {"file": {"name": "core/src/main/scala/org/apache/spark/shuffle/IndexShuffleBlockResolver.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/test/java/org/apache/spark/unsafe/map/AbstractBytesToBytesMapSuite.java", "additions": "24", "deletions": "3", "changes": "27"}, "updated": [0, 1, 1]}, {"file": {"name": "core/src/test/java/test/org/apache/spark/JavaSparkContextSuite.java", "additions": "5", "deletions": "2", "changes": "7"}, "updated": [0, 1, 1]}, {"file": {"name": "core/src/test/scala/org/apache/spark/ExecutorAllocationManagerSuite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 2, 4]}, {"file": {"name": "core/src/test/scala/org/apache/spark/deploy/DecommissionWorkerSuite.scala", "additions": "54", "deletions": "12", "changes": "66"}, "updated": [0, 2, 3]}, {"file": {"name": "core/src/test/scala/org/apache/spark/deploy/SparkSubmitSuite.scala", "additions": "82", "deletions": "1", "changes": "83"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/test/scala/org/apache/spark/deploy/client/AppClientSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/TaskSchedulerImplSuite.scala", "additions": "55", "deletions": "8", "changes": "63"}, "updated": [0, 2, 5]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/TaskSetManagerSuite.scala", "additions": "58", "deletions": "6", "changes": "64"}, "updated": [0, 2, 6]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/WorkerDecommissionExtendedSuite.scala", "additions": "2", "deletions": "3", "changes": "5"}, "updated": [0, 2, 4]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/WorkerDecommissionSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 5]}, {"file": {"name": "core/src/test/scala/org/apache/spark/storage/BlockManagerDecommissionIntegrationSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 3, 7]}, {"file": {"name": "dev/deps/spark-deps-hadoop-2.7-hive-1.2", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 1, 6]}, {"file": {"name": "dev/deps/spark-deps-hadoop-2.7-hive-2.3", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 1, 6]}, {"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 1, 6]}, {"file": {"name": "dev/run-tests.py", "additions": "7", "deletions": "1", "changes": "8"}, "updated": [1, 1, 11]}, {"file": {"name": "docs/job-scheduling.md", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 3]}, {"file": {"name": "docs/monitoring.md", "additions": "4", "deletions": "10", "changes": "14"}, "updated": [0, 3, 6]}, {"file": {"name": "docs/sql-migration-guide.md", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 1, 8]}, {"file": {"name": "docs/sql-performance-tuning.md", "additions": "22", "deletions": "0", "changes": "22"}, "updated": [1, 1, 1]}, {"file": {"name": "docs/sql-ref-syntax-qry-select-groupby.md", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 2]}, {"file": {"name": "docs/sql-ref-syntax-qry-select-hints.md", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "docs/sql-ref.md", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "docs/tuning.md", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [1, 1, 1]}, {"file": {"name": "docs/web-ui.md", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [1, 1, 1]}, {"file": {"name": "external/avro/src/main/java/org/apache/spark/sql/avro/SparkAvroKeyOutputFormat.java", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "external/avro/src/main/scala/org/apache/spark/sql/avro/AvroDataToCatalyst.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 2]}, {"file": {"name": "external/avro/src/main/scala/org/apache/spark/sql/avro/AvroDeserializer.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 2]}, {"file": {"name": "external/avro/src/main/scala/org/apache/spark/sql/avro/AvroOptions.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "external/avro/src/main/scala/org/apache/spark/sql/avro/AvroSerializer.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "external/avro/src/main/scala/org/apache/spark/sql/avro/AvroUtils.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 2]}, {"file": {"name": "external/avro/src/main/scala/org/apache/spark/sql/avro/CatalystDataToAvro.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "external/avro/src/main/scala/org/apache/spark/sql/avro/SchemaConverters.scala", "additions": "18", "deletions": "3", "changes": "21"}, "updated": [1, 1, 2]}, {"file": {"name": "external/avro/src/main/scala/org/apache/spark/sql/v2/avro/AvroDataSourceV2.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 1, 2]}, {"file": {"name": "launcher/src/main/java/org/apache/spark/launcher/LauncherServer.java", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "mllib/src/main/scala/org/apache/spark/ml/feature/CountVectorizer.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [1, 1, 1]}, {"file": {"name": "mllib/src/test/scala/org/apache/spark/ml/feature/CountVectorizerSuite.scala", "additions": "59", "deletions": "15", "changes": "74"}, "updated": [1, 1, 1]}, {"file": {"name": "pom.xml", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [1, 2, 12]}, {"file": {"name": "project/SparkBuild.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [1, 2, 10]}, {"file": {"name": "python/docs/source/reference/pyspark.ml.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 3]}, {"file": {"name": "python/docs/source/reference/pyspark.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 2, 3]}, {"file": {"name": "python/pyspark/ml/clustering.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 3]}, {"file": {"name": "python/pyspark/ml/tests/test_tuning.py", "additions": "120", "deletions": "11", "changes": "131"}, "updated": [1, 1, 2]}, {"file": {"name": "python/pyspark/ml/tuning.py", "additions": "52", "deletions": "15", "changes": "67"}, "updated": [1, 1, 3]}, {"file": {"name": "python/pyspark/sql/functions.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 9]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Constants.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/DecommissionSuite.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 1, 2]}, {"file": {"name": "sbin/decommission-worker.sh", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/TableCatalog.java", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/Row.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/CatalystTypeConverters.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/QueryPlanningTracker.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflection.scala", "additions": "3", "deletions": "4", "changes": "7"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala", "additions": "11", "deletions": "3", "changes": "14"}, "updated": [0, 2, 13]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/encoders/RowEncoder.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/CodeGeneratorWithInterpretedFallback.scala", "additions": "2", "deletions": "3", "changes": "5"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/SpecificInternalRow.scala", "additions": "34", "deletions": "15", "changes": "49"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/ApproxCountDistinctForIntervals.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/HyperLogLogPlusPlus.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "6", "deletions": "8", "changes": "14"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/complexTypeCreator.scala", "additions": "35", "deletions": "75", "changes": "110"}, "updated": [0, 2, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/objects/objects.scala", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [1, 1, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/ComplexTypes.scala", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [0, 4, 13]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/WithFields.scala", "additions": "8", "deletions": "8", "changes": "16"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala", "additions": "21", "deletions": "6", "changes": "27"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/objects.scala", "additions": "9", "deletions": "1", "changes": "10"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/ParserUtils.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/ArrayBasedMapData.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "6", "deletions": "3", "changes": "9"}, "updated": [0, 3, 18]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CodeGenerationSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ComplexTypeSuite.scala", "additions": "18", "deletions": "13", "changes": "31"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ExpressionEvalHelper.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/CombineWithFieldsSuite.scala", "additions": "22", "deletions": "19", "changes": "41"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ObjectSerializerPruningSuite.scala", "additions": "32", "deletions": "1", "changes": "33"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/StarJoinCostBasedReorderSuite.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/complexTypesSuite.scala", "additions": "30", "deletions": "51", "changes": "81"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/Column.scala", "additions": "18", "deletions": "68", "changes": "86"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/AlreadyOptimized.scala", "additions": "37", "deletions": "0", "changes": "37"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/BaseScriptTransformationExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala", "additions": "4", "deletions": "6", "changes": "10"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala", "additions": "9", "deletions": "9", "changes": "18"}, "updated": [0, 2, 7]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 3, 10]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/EliminateNullAwareAntiJoin.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [1, 2, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/compressionSchemes.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/command/functions.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala", "additions": "6", "deletions": "2", "changes": "8"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVDataSource.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonDataSource.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala", "additions": "1", "deletions": "3", "changes": "4"}, "updated": [0, 3, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/FileDataSourceV2.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ShowTablePropertiesExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V1FallbackWriters.scala", "additions": "3", "deletions": "5", "changes": "8"}, "updated": [0, 2, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/csv/CSVDataSourceV2.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/json/JsonDataSourceV2.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/orc/OrcDataSourceV2.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetDataSourceV2.scala", "additions": "5", "deletions": "2", "changes": "7"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/text/TextDataSourceV2.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastHashJoinExec.scala", "additions": "3", "deletions": "15", "changes": "18"}, "updated": [1, 2, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashJoin.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 2, 7]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashedRelation.scala", "additions": "173", "deletions": "12", "changes": "185"}, "updated": [1, 3, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/joins/ShuffledHashJoinExec.scala", "additions": "233", "deletions": "6", "changes": "239"}, "updated": [0, 2, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/joins/ShuffledJoin.scala", "additions": "22", "deletions": "1", "changes": "23"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/joins/SortMergeJoinExec.scala", "additions": "0", "deletions": "20", "changes": "20"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog.scala", "additions": "67", "deletions": "30", "changes": "97"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSinkLog.scala", "additions": "1", "deletions": "9", "changes": "10"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceLog.scala", "additions": "1", "deletions": "5", "changes": "6"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/HDFSMetadataLog.scala", "additions": "53", "deletions": "27", "changes": "80"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLAppStatusListener.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/functions.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/core/src/test/resources/structured-streaming/file-sink-log-version-2.1.0/8", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q10.sf100/explain.txt", "additions": "286", "deletions": "0", "changes": "286"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q10.sf100/simplified.txt", "additions": "81", "deletions": "0", "changes": "81"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q10/explain.txt", "additions": "266", "deletions": "0", "changes": "266"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q10/simplified.txt", "additions": "71", "deletions": "0", "changes": "71"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q19.sf100/explain.txt", "additions": "221", "deletions": "0", "changes": "221"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q19.sf100/simplified.txt", "additions": "58", "deletions": "0", "changes": "58"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q19/explain.txt", "additions": "221", "deletions": "0", "changes": "221"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q19/simplified.txt", "additions": "58", "deletions": "0", "changes": "58"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q27.sf100/explain.txt", "additions": "428", "deletions": "0", "changes": "428"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q27.sf100/simplified.txt", "additions": "113", "deletions": "0", "changes": "113"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q27/explain.txt", "additions": "428", "deletions": "0", "changes": "428"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q27/simplified.txt", "additions": "113", "deletions": "0", "changes": "113"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q3.sf100/explain.txt", "additions": "122", "deletions": "0", "changes": "122"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q3.sf100/simplified.txt", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q3/explain.txt", "additions": "122", "deletions": "0", "changes": "122"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q3/simplified.txt", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q34.sf100/explain.txt", "additions": "218", "deletions": "0", "changes": "218"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q34.sf100/simplified.txt", "additions": "63", "deletions": "0", "changes": "63"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q34/explain.txt", "additions": "203", "deletions": "0", "changes": "203"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q34/simplified.txt", "additions": "54", "deletions": "0", "changes": "54"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q42.sf100/explain.txt", "additions": "122", "deletions": "0", "changes": "122"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q42.sf100/simplified.txt", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q42/explain.txt", "additions": "122", "deletions": "0", "changes": "122"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q42/simplified.txt", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q43.sf100/explain.txt", "additions": "122", "deletions": "0", "changes": "122"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q43.sf100/simplified.txt", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q43/explain.txt", "additions": "122", "deletions": "0", "changes": "122"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q43/simplified.txt", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q46.sf100/explain.txt", "additions": "281", "deletions": "0", "changes": "281"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q46.sf100/simplified.txt", "additions": "87", "deletions": "0", "changes": "87"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q46/explain.txt", "additions": "241", "deletions": "0", "changes": "241"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q46/simplified.txt", "additions": "63", "deletions": "0", "changes": "63"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q52.sf100/explain.txt", "additions": "122", "deletions": "0", "changes": "122"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q52.sf100/simplified.txt", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q52/explain.txt", "additions": "122", "deletions": "0", "changes": "122"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q52/simplified.txt", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q53.sf100/explain.txt", "additions": "180", "deletions": "0", "changes": "180"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q53.sf100/simplified.txt", "additions": "49", "deletions": "0", "changes": "49"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q53/explain.txt", "additions": "180", "deletions": "0", "changes": "180"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q53/simplified.txt", "additions": "49", "deletions": "0", "changes": "49"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q55.sf100/explain.txt", "additions": "122", "deletions": "0", "changes": "122"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q55.sf100/simplified.txt", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q55/explain.txt", "additions": "122", "deletions": "0", "changes": "122"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q55/simplified.txt", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q59.sf100/explain.txt", "additions": "290", "deletions": "0", "changes": "290"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q59.sf100/simplified.txt", "additions": "76", "deletions": "0", "changes": "76"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q59/explain.txt", "additions": "290", "deletions": "0", "changes": "290"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q59/simplified.txt", "additions": "76", "deletions": "0", "changes": "76"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q63.sf100/explain.txt", "additions": "180", "deletions": "0", "changes": "180"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q63.sf100/simplified.txt", "additions": "49", "deletions": "0", "changes": "49"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q63/explain.txt", "additions": "180", "deletions": "0", "changes": "180"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q63/simplified.txt", "additions": "49", "deletions": "0", "changes": "49"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q65.sf100/explain.txt", "additions": "245", "deletions": "0", "changes": "245"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q65.sf100/simplified.txt", "additions": "63", "deletions": "0", "changes": "63"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q65/explain.txt", "additions": "245", "deletions": "0", "changes": "245"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q65/simplified.txt", "additions": "63", "deletions": "0", "changes": "63"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q68.sf100/explain.txt", "additions": "289", "deletions": "0", "changes": "289"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q68.sf100/simplified.txt", "additions": "86", "deletions": "0", "changes": "86"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q68/explain.txt", "additions": "241", "deletions": "0", "changes": "241"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q68/simplified.txt", "additions": "63", "deletions": "0", "changes": "63"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q7.sf100/explain.txt", "additions": "193", "deletions": "0", "changes": "193"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q7.sf100/simplified.txt", "additions": "50", "deletions": "0", "changes": "50"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q7/explain.txt", "additions": "193", "deletions": "0", "changes": "193"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q7/simplified.txt", "additions": "50", "deletions": "0", "changes": "50"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q73.sf100/explain.txt", "additions": "203", "deletions": "0", "changes": "203"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q73.sf100/simplified.txt", "additions": "54", "deletions": "0", "changes": "54"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q73/explain.txt", "additions": "203", "deletions": "0", "changes": "203"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q73/simplified.txt", "additions": "54", "deletions": "0", "changes": "54"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q79.sf100/explain.txt", "additions": "208", "deletions": "0", "changes": "208"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q79.sf100/simplified.txt", "additions": "59", "deletions": "0", "changes": "59"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q79/explain.txt", "additions": "193", "deletions": "0", "changes": "193"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q79/simplified.txt", "additions": "50", "deletions": "0", "changes": "50"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q89.sf100/explain.txt", "additions": "175", "deletions": "0", "changes": "175"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q89.sf100/simplified.txt", "additions": "48", "deletions": "0", "changes": "48"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q89/explain.txt", "additions": "175", "deletions": "0", "changes": "175"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q89/simplified.txt", "additions": "48", "deletions": "0", "changes": "48"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q98.sf100/explain.txt", "additions": "162", "deletions": "0", "changes": "162"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q98.sf100/simplified.txt", "additions": "51", "deletions": "0", "changes": "51"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q98/explain.txt", "additions": "147", "deletions": "0", "changes": "147"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q98/simplified.txt", "additions": "42", "deletions": "0", "changes": "42"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/ss_max.sf100/explain.txt", "additions": "56", "deletions": "0", "changes": "56"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/ss_max.sf100/simplified.txt", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/ss_max/explain.txt", "additions": "56", "deletions": "0", "changes": "56"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/ss_max/simplified.txt", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q1.sf100/explain.txt", "additions": "270", "deletions": "0", "changes": "270"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q1.sf100/simplified.txt", "additions": "74", "deletions": "0", "changes": "74"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q1/explain.txt", "additions": "255", "deletions": "0", "changes": "255"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q1/simplified.txt", "additions": "65", "deletions": "0", "changes": "65"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q10.sf100/explain.txt", "additions": "319", "deletions": "0", "changes": "319"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q10.sf100/simplified.txt", "additions": "95", "deletions": "0", "changes": "95"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q10/explain.txt", "additions": "279", "deletions": "0", "changes": "279"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q10/simplified.txt", "additions": "74", "deletions": "0", "changes": "74"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q11.sf100/explain.txt", "additions": "482", "deletions": "0", "changes": "482"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q11.sf100/simplified.txt", "additions": "158", "deletions": "0", "changes": "158"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q11/explain.txt", "additions": "415", "deletions": "0", "changes": "415"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q11/simplified.txt", "additions": "108", "deletions": "0", "changes": "108"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q12.sf100/explain.txt", "additions": "152", "deletions": "0", "changes": "152"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q12.sf100/simplified.txt", "additions": "47", "deletions": "0", "changes": "47"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q12/explain.txt", "additions": "137", "deletions": "0", "changes": "137"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q12/simplified.txt", "additions": "38", "deletions": "0", "changes": "38"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q13.sf100/explain.txt", "additions": "216", "deletions": "0", "changes": "216"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q13.sf100/simplified.txt", "additions": "57", "deletions": "0", "changes": "57"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q13/explain.txt", "additions": "216", "deletions": "0", "changes": "216"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q13/simplified.txt", "additions": "57", "deletions": "0", "changes": "57"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a.sf100/explain.txt", "additions": "878", "deletions": "0", "changes": "878"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a.sf100/simplified.txt", "additions": "254", "deletions": "0", "changes": "254"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a/explain.txt", "additions": "798", "deletions": "0", "changes": "798"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a/simplified.txt", "additions": "214", "deletions": "0", "changes": "214"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b.sf100/explain.txt", "additions": "810", "deletions": "0", "changes": "810"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b.sf100/simplified.txt", "additions": "231", "deletions": "0", "changes": "231"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b/explain.txt", "additions": "763", "deletions": "0", "changes": "763"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b/simplified.txt", "additions": "204", "deletions": "0", "changes": "204"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q15.sf100/explain.txt", "additions": "180", "deletions": "0", "changes": "180"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q15.sf100/simplified.txt", "additions": "57", "deletions": "0", "changes": "57"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q15/explain.txt", "additions": "150", "deletions": "0", "changes": "150"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q15/simplified.txt", "additions": "39", "deletions": "0", "changes": "39"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q16.sf100/explain.txt", "additions": "250", "deletions": "0", "changes": "250"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q16.sf100/simplified.txt", "additions": "71", "deletions": "0", "changes": "71"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q16/explain.txt", "additions": "235", "deletions": "0", "changes": "235"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q16/simplified.txt", "additions": "62", "deletions": "0", "changes": "62"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q17.sf100/explain.txt", "additions": "314", "deletions": "0", "changes": "314"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q17.sf100/simplified.txt", "additions": "98", "deletions": "0", "changes": "98"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q17/explain.txt", "additions": "269", "deletions": "0", "changes": "269"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q17/simplified.txt", "additions": "71", "deletions": "0", "changes": "71"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q18.sf100/explain.txt", "additions": "294", "deletions": "0", "changes": "294"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q18.sf100/simplified.txt", "additions": "87", "deletions": "0", "changes": "87"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q18/explain.txt", "additions": "264", "deletions": "0", "changes": "264"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q18/simplified.txt", "additions": "69", "deletions": "0", "changes": "69"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q19.sf100/explain.txt", "additions": "251", "deletions": "0", "changes": "251"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q19.sf100/simplified.txt", "additions": "76", "deletions": "0", "changes": "76"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q19/explain.txt", "additions": "221", "deletions": "0", "changes": "221"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q19/simplified.txt", "additions": "58", "deletions": "0", "changes": "58"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q2.sf100/explain.txt", "additions": "233", "deletions": "0", "changes": "233"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q2.sf100/simplified.txt", "additions": "70", "deletions": "0", "changes": "70"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q2/explain.txt", "additions": "218", "deletions": "0", "changes": "218"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q2/simplified.txt", "additions": "61", "deletions": "0", "changes": "61"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q20.sf100/explain.txt", "additions": "152", "deletions": "0", "changes": "152"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q20.sf100/simplified.txt", "additions": "47", "deletions": "0", "changes": "47"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q20/explain.txt", "additions": "137", "deletions": "0", "changes": "137"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q20/simplified.txt", "additions": "38", "deletions": "0", "changes": "38"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q21.sf100/explain.txt", "additions": "155", "deletions": "0", "changes": "155"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q21.sf100/simplified.txt", "additions": "40", "deletions": "0", "changes": "40"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q21/explain.txt", "additions": "155", "deletions": "0", "changes": "155"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q21/simplified.txt", "additions": "40", "deletions": "0", "changes": "40"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q22.sf100/explain.txt", "additions": "170", "deletions": "0", "changes": "170"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q22.sf100/simplified.txt", "additions": "49", "deletions": "0", "changes": "49"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q22/explain.txt", "additions": "155", "deletions": "0", "changes": "155"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q22/simplified.txt", "additions": "40", "deletions": "0", "changes": "40"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23a.sf100/explain.txt", "additions": "655", "deletions": "0", "changes": "655"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23a.sf100/simplified.txt", "additions": "198", "deletions": "0", "changes": "198"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23a/explain.txt", "additions": "542", "deletions": "0", "changes": "542"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23a/simplified.txt", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23b.sf100/explain.txt", "additions": "870", "deletions": "0", "changes": "870"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23b.sf100/simplified.txt", "additions": "268", "deletions": "0", "changes": "268"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23b/explain.txt", "additions": "689", "deletions": "0", "changes": "689"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23b/simplified.txt", "additions": "182", "deletions": "0", "changes": "182"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24a.sf100/explain.txt", "additions": "567", "deletions": "0", "changes": "567"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24a.sf100/simplified.txt", "additions": "179", "deletions": "0", "changes": "179"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24a/explain.txt", "additions": "477", "deletions": "0", "changes": "477"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24a/simplified.txt", "additions": "125", "deletions": "0", "changes": "125"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24b.sf100/explain.txt", "additions": "567", "deletions": "0", "changes": "567"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24b.sf100/simplified.txt", "additions": "179", "deletions": "0", "changes": "179"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24b/explain.txt", "additions": "477", "deletions": "0", "changes": "477"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24b/simplified.txt", "additions": "125", "deletions": "0", "changes": "125"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q25.sf100/explain.txt", "additions": "314", "deletions": "0", "changes": "314"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q25.sf100/simplified.txt", "additions": "98", "deletions": "0", "changes": "98"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q25/explain.txt", "additions": "269", "deletions": "0", "changes": "269"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q25/simplified.txt", "additions": "71", "deletions": "0", "changes": "71"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q26.sf100/explain.txt", "additions": "193", "deletions": "0", "changes": "193"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q26.sf100/simplified.txt", "additions": "50", "deletions": "0", "changes": "50"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q26/explain.txt", "additions": "193", "deletions": "0", "changes": "193"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q26/simplified.txt", "additions": "50", "deletions": "0", "changes": "50"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q27.sf100/explain.txt", "additions": "193", "deletions": "0", "changes": "193"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q27.sf100/simplified.txt", "additions": "50", "deletions": "0", "changes": "50"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q27/explain.txt", "additions": "193", "deletions": "0", "changes": "193"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q27/simplified.txt", "additions": "50", "deletions": "0", "changes": "50"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q28.sf100/explain.txt", "additions": "437", "deletions": "0", "changes": "437"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q28.sf100/simplified.txt", "additions": "107", "deletions": "0", "changes": "107"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q28/explain.txt", "additions": "437", "deletions": "0", "changes": "437"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q28/simplified.txt", "additions": "107", "deletions": "0", "changes": "107"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q29.sf100/explain.txt", "additions": "337", "deletions": "0", "changes": "337"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q29.sf100/simplified.txt", "additions": "104", "deletions": "0", "changes": "104"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q29/explain.txt", "additions": "292", "deletions": "0", "changes": "292"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q29/simplified.txt", "additions": "77", "deletions": "0", "changes": "77"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q3.sf100/explain.txt", "additions": "122", "deletions": "0", "changes": "122"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q3.sf100/simplified.txt", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q3/explain.txt", "additions": "122", "deletions": "0", "changes": "122"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q3/simplified.txt", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q30.sf100/explain.txt", "additions": "333", "deletions": "0", "changes": "333"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q30.sf100/simplified.txt", "additions": "96", "deletions": "0", "changes": "96"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q30/explain.txt", "additions": "303", "deletions": "0", "changes": "303"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q30/simplified.txt", "additions": "78", "deletions": "0", "changes": "78"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q31.sf100/explain.txt", "additions": "663", "deletions": "0", "changes": "663"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q31.sf100/simplified.txt", "additions": "206", "deletions": "0", "changes": "206"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q31/explain.txt", "additions": "563", "deletions": "0", "changes": "563"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q31/simplified.txt", "additions": "150", "deletions": "0", "changes": "150"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q32.sf100/explain.txt", "additions": "175", "deletions": "0", "changes": "175"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q32.sf100/simplified.txt", "additions": "45", "deletions": "0", "changes": "45"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q32/explain.txt", "additions": "175", "deletions": "0", "changes": "175"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q32/simplified.txt", "additions": "45", "deletions": "0", "changes": "45"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q33.sf100/explain.txt", "additions": "378", "deletions": "0", "changes": "378"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q33.sf100/simplified.txt", "additions": "101", "deletions": "0", "changes": "101"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q33/explain.txt", "additions": "378", "deletions": "0", "changes": "378"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q33/simplified.txt", "additions": "101", "deletions": "0", "changes": "101"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q34.sf100/explain.txt", "additions": "218", "deletions": "0", "changes": "218"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q34.sf100/simplified.txt", "additions": "63", "deletions": "0", "changes": "63"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q34/explain.txt", "additions": "203", "deletions": "0", "changes": "203"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q34/simplified.txt", "additions": "54", "deletions": "0", "changes": "54"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q35.sf100/explain.txt", "additions": "329", "deletions": "0", "changes": "329"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q35.sf100/simplified.txt", "additions": "103", "deletions": "0", "changes": "103"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q35/explain.txt", "additions": "274", "deletions": "0", "changes": "274"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q35/simplified.txt", "additions": "73", "deletions": "0", "changes": "73"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q36.sf100/explain.txt", "additions": "180", "deletions": "0", "changes": "180"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q36.sf100/simplified.txt", "additions": "49", "deletions": "0", "changes": "49"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q36/explain.txt", "additions": "180", "deletions": "0", "changes": "180"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q36/simplified.txt", "additions": "49", "deletions": "0", "changes": "49"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q37.sf100/explain.txt", "additions": "175", "deletions": "0", "changes": "175"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q37.sf100/simplified.txt", "additions": "50", "deletions": "0", "changes": "50"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q37/explain.txt", "additions": "160", "deletions": "0", "changes": "160"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q37/simplified.txt", "additions": "41", "deletions": "0", "changes": "41"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q38.sf100/explain.txt", "additions": "393", "deletions": "0", "changes": "393"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q38.sf100/simplified.txt", "additions": "118", "deletions": "0", "changes": "118"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q38/explain.txt", "additions": "328", "deletions": "0", "changes": "328"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q38/simplified.txt", "additions": "81", "deletions": "0", "changes": "81"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q39a.sf100/explain.txt", "additions": "307", "deletions": "0", "changes": "307"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q39a.sf100/simplified.txt", "additions": "86", "deletions": "0", "changes": "86"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q39a/explain.txt", "additions": "292", "deletions": "0", "changes": "292"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q39a/simplified.txt", "additions": "77", "deletions": "0", "changes": "77"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q39b.sf100/explain.txt", "additions": "307", "deletions": "0", "changes": "307"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q39b.sf100/simplified.txt", "additions": "86", "deletions": "0", "changes": "86"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q39b/explain.txt", "additions": "292", "deletions": "0", "changes": "292"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q39b/simplified.txt", "additions": "77", "deletions": "0", "changes": "77"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q4.sf100/explain.txt", "additions": "695", "deletions": "0", "changes": "695"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q4.sf100/simplified.txt", "additions": "231", "deletions": "0", "changes": "231"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q4/explain.txt", "additions": "606", "deletions": "0", "changes": "606"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q4/simplified.txt", "additions": "158", "deletions": "0", "changes": "158"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q40.sf100/explain.txt", "additions": "198", "deletions": "0", "changes": "198"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q40.sf100/simplified.txt", "additions": "56", "deletions": "0", "changes": "56"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q40/explain.txt", "additions": "183", "deletions": "0", "changes": "183"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q40/simplified.txt", "additions": "48", "deletions": "0", "changes": "48"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q41.sf100/explain.txt", "additions": "120", "deletions": "0", "changes": "120"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q41.sf100/simplified.txt", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q41/explain.txt", "additions": "120", "deletions": "0", "changes": "120"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q41/simplified.txt", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q42.sf100/explain.txt", "additions": "122", "deletions": "0", "changes": "122"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q42.sf100/simplified.txt", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q42/explain.txt", "additions": "122", "deletions": "0", "changes": "122"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q42/simplified.txt", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q43.sf100/explain.txt", "additions": "122", "deletions": "0", "changes": "122"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q43.sf100/simplified.txt", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q43/explain.txt", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q43/simplified.txt", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q44.sf100/explain.txt", "additions": "248", "deletions": "0", "changes": "248"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q44.sf100/simplified.txt", "additions": "68", "deletions": "0", "changes": "68"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q44/explain.txt", "additions": "248", "deletions": "0", "changes": "248"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q44/simplified.txt", "additions": "68", "deletions": "0", "changes": "68"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q45.sf100/explain.txt", "additions": "256", "deletions": "0", "changes": "256"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q45.sf100/simplified.txt", "additions": "77", "deletions": "0", "changes": "77"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q45/explain.txt", "additions": "226", "deletions": "0", "changes": "226"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q45/simplified.txt", "additions": "59", "deletions": "0", "changes": "59"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q46.sf100/explain.txt", "additions": "281", "deletions": "0", "changes": "281"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q46.sf100/simplified.txt", "additions": "87", "deletions": "0", "changes": "87"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q46/explain.txt", "additions": "241", "deletions": "0", "changes": "241"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q46/simplified.txt", "additions": "63", "deletions": "0", "changes": "63"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q47.sf100/explain.txt", "additions": "313", "deletions": "0", "changes": "313"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q47.sf100/simplified.txt", "additions": "107", "deletions": "0", "changes": "107"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q47/explain.txt", "additions": "278", "deletions": "0", "changes": "278"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q47/simplified.txt", "additions": "84", "deletions": "0", "changes": "84"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q48.sf100/explain.txt", "additions": "183", "deletions": "0", "changes": "183"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q48.sf100/simplified.txt", "additions": "48", "deletions": "0", "changes": "48"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q48/explain.txt", "additions": "183", "deletions": "0", "changes": "183"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q48/simplified.txt", "additions": "48", "deletions": "0", "changes": "48"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q49.sf100/explain.txt", "additions": "478", "deletions": "0", "changes": "478"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q49.sf100/simplified.txt", "additions": "153", "deletions": "0", "changes": "153"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q49/explain.txt", "additions": "433", "deletions": "0", "changes": "433"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q49/simplified.txt", "additions": "126", "deletions": "0", "changes": "126"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q5.sf100/explain.txt", "additions": "450", "deletions": "0", "changes": "450"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q5.sf100/simplified.txt", "additions": "132", "deletions": "0", "changes": "132"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q5/explain.txt", "additions": "435", "deletions": "0", "changes": "435"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q5/simplified.txt", "additions": "123", "deletions": "0", "changes": "123"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q50.sf100/explain.txt", "additions": "198", "deletions": "0", "changes": "198"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q50.sf100/simplified.txt", "additions": "57", "deletions": "0", "changes": "57"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q50/explain.txt", "additions": "183", "deletions": "0", "changes": "183"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q50/simplified.txt", "additions": "48", "deletions": "0", "changes": "48"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q51.sf100/explain.txt", "additions": "228", "deletions": "0", "changes": "228"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q51.sf100/simplified.txt", "additions": "71", "deletions": "0", "changes": "71"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q51/explain.txt", "additions": "228", "deletions": "0", "changes": "228"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q51/simplified.txt", "additions": "71", "deletions": "0", "changes": "71"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q52.sf100/explain.txt", "additions": "122", "deletions": "0", "changes": "122"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q52.sf100/simplified.txt", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q52/explain.txt", "additions": "122", "deletions": "0", "changes": "122"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q52/simplified.txt", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q53.sf100/explain.txt", "additions": "180", "deletions": "0", "changes": "180"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q53.sf100/simplified.txt", "additions": "49", "deletions": "0", "changes": "49"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q53/explain.txt", "additions": "180", "deletions": "0", "changes": "180"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q53/simplified.txt", "additions": "49", "deletions": "0", "changes": "49"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q54.sf100/explain.txt", "additions": "494", "deletions": "0", "changes": "494"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q54.sf100/simplified.txt", "additions": "142", "deletions": "0", "changes": "142"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q54/explain.txt", "additions": "459", "deletions": "0", "changes": "459"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q54/simplified.txt", "additions": "121", "deletions": "0", "changes": "121"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q55.sf100/explain.txt", "additions": "122", "deletions": "0", "changes": "122"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q55.sf100/simplified.txt", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q55/explain.txt", "additions": "122", "deletions": "0", "changes": "122"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q55/simplified.txt", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q56.sf100/explain.txt", "additions": "378", "deletions": "0", "changes": "378"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q56.sf100/simplified.txt", "additions": "101", "deletions": "0", "changes": "101"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q56/explain.txt", "additions": "378", "deletions": "0", "changes": "378"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q56/simplified.txt", "additions": "101", "deletions": "0", "changes": "101"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q57.sf100/explain.txt", "additions": "313", "deletions": "0", "changes": "313"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q57.sf100/simplified.txt", "additions": "107", "deletions": "0", "changes": "107"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q57/explain.txt", "additions": "278", "deletions": "0", "changes": "278"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q57/simplified.txt", "additions": "84", "deletions": "0", "changes": "84"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q58.sf100/explain.txt", "additions": "477", "deletions": "0", "changes": "477"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q58.sf100/simplified.txt", "additions": "125", "deletions": "0", "changes": "125"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q58/explain.txt", "additions": "477", "deletions": "0", "changes": "477"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q58/simplified.txt", "additions": "125", "deletions": "0", "changes": "125"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q59.sf100/explain.txt", "additions": "249", "deletions": "0", "changes": "249"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q59.sf100/simplified.txt", "additions": "66", "deletions": "0", "changes": "66"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q59/explain.txt", "additions": "249", "deletions": "0", "changes": "249"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q59/simplified.txt", "additions": "66", "deletions": "0", "changes": "66"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q6.sf100/explain.txt", "additions": "331", "deletions": "0", "changes": "331"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q6.sf100/simplified.txt", "additions": "95", "deletions": "0", "changes": "95"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q6/explain.txt", "additions": "301", "deletions": "0", "changes": "301"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q6/simplified.txt", "additions": "77", "deletions": "0", "changes": "77"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q60.sf100/explain.txt", "additions": "378", "deletions": "0", "changes": "378"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q60.sf100/simplified.txt", "additions": "101", "deletions": "0", "changes": "101"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q60/explain.txt", "additions": "378", "deletions": "0", "changes": "378"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q60/simplified.txt", "additions": "101", "deletions": "0", "changes": "101"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q61.sf100/explain.txt", "additions": "414", "deletions": "0", "changes": "414"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q61.sf100/simplified.txt", "additions": "110", "deletions": "0", "changes": "110"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q61/explain.txt", "additions": "396", "deletions": "0", "changes": "396"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q61/simplified.txt", "additions": "105", "deletions": "0", "changes": "105"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q62.sf100/explain.txt", "additions": "183", "deletions": "0", "changes": "183"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q62.sf100/simplified.txt", "additions": "48", "deletions": "0", "changes": "48"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q62/explain.txt", "additions": "183", "deletions": "0", "changes": "183"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q62/simplified.txt", "additions": "48", "deletions": "0", "changes": "48"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q63.sf100/explain.txt", "additions": "180", "deletions": "0", "changes": "180"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q63.sf100/simplified.txt", "additions": "49", "deletions": "0", "changes": "49"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q63/explain.txt", "additions": "180", "deletions": "0", "changes": "180"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q63/simplified.txt", "additions": "49", "deletions": "0", "changes": "49"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q64.sf100/explain.txt", "additions": "1110", "deletions": "0", "changes": "1110"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q64.sf100/simplified.txt", "additions": "367", "deletions": "0", "changes": "367"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q64/explain.txt", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q64/simplified.txt", "additions": "246", "deletions": "0", "changes": "246"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q65.sf100/explain.txt", "additions": "260", "deletions": "0", "changes": "260"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q65.sf100/simplified.txt", "additions": "72", "deletions": "0", "changes": "72"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q65/explain.txt", "additions": "245", "deletions": "0", "changes": "245"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q65/simplified.txt", "additions": "63", "deletions": "0", "changes": "63"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q66.sf100/explain.txt", "additions": "310", "deletions": "0", "changes": "310"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q66.sf100/simplified.txt", "additions": "83", "deletions": "0", "changes": "83"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q66/explain.txt", "additions": "310", "deletions": "0", "changes": "310"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q66/simplified.txt", "additions": "83", "deletions": "0", "changes": "83"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q67.sf100/explain.txt", "additions": "190", "deletions": "0", "changes": "190"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q67.sf100/simplified.txt", "additions": "57", "deletions": "0", "changes": "57"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q67/explain.txt", "additions": "175", "deletions": "0", "changes": "175"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q67/simplified.txt", "additions": "48", "deletions": "0", "changes": "48"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q68.sf100/explain.txt", "additions": "281", "deletions": "0", "changes": "281"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q68.sf100/simplified.txt", "additions": "87", "deletions": "0", "changes": "87"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q68/explain.txt", "additions": "241", "deletions": "0", "changes": "241"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q68/simplified.txt", "additions": "63", "deletions": "0", "changes": "63"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q69.sf100/explain.txt", "additions": "299", "deletions": "0", "changes": "299"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q69.sf100/simplified.txt", "additions": "85", "deletions": "0", "changes": "85"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q69/explain.txt", "additions": "274", "deletions": "0", "changes": "274"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q69/simplified.txt", "additions": "73", "deletions": "0", "changes": "73"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q7.sf100/explain.txt", "additions": "193", "deletions": "0", "changes": "193"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q7.sf100/simplified.txt", "additions": "50", "deletions": "0", "changes": "50"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q7/explain.txt", "additions": "193", "deletions": "0", "changes": "193"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q7/simplified.txt", "additions": "50", "deletions": "0", "changes": "50"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q70.sf100/explain.txt", "additions": "264", "deletions": "0", "changes": "264"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q70.sf100/simplified.txt", "additions": "74", "deletions": "0", "changes": "74"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q70/explain.txt", "additions": "264", "deletions": "0", "changes": "264"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q70/simplified.txt", "additions": "74", "deletions": "0", "changes": "74"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q71.sf100/explain.txt", "additions": "232", "deletions": "0", "changes": "232"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q71.sf100/simplified.txt", "additions": "65", "deletions": "0", "changes": "65"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q71/explain.txt", "additions": "232", "deletions": "0", "changes": "232"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q71/simplified.txt", "additions": "65", "deletions": "0", "changes": "65"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q72.sf100/explain.txt", "additions": "436", "deletions": "0", "changes": "436"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q72.sf100/simplified.txt", "additions": "130", "deletions": "0", "changes": "130"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q72/explain.txt", "additions": "391", "deletions": "0", "changes": "391"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q72/simplified.txt", "additions": "104", "deletions": "0", "changes": "104"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q73.sf100/explain.txt", "additions": "218", "deletions": "0", "changes": "218"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q73.sf100/simplified.txt", "additions": "63", "deletions": "0", "changes": "63"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q73/explain.txt", "additions": "203", "deletions": "0", "changes": "203"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q73/simplified.txt", "additions": "54", "deletions": "0", "changes": "54"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q74.sf100/explain.txt", "additions": "477", "deletions": "0", "changes": "477"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q74.sf100/simplified.txt", "additions": "157", "deletions": "0", "changes": "157"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q74/explain.txt", "additions": "410", "deletions": "0", "changes": "410"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q74/simplified.txt", "additions": "107", "deletions": "0", "changes": "107"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q75.sf100/explain.txt", "additions": "752", "deletions": "0", "changes": "752"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q75.sf100/simplified.txt", "additions": "237", "deletions": "0", "changes": "237"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q75/explain.txt", "additions": "647", "deletions": "0", "changes": "647"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q75/simplified.txt", "additions": "180", "deletions": "0", "changes": "180"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q76.sf100/explain.txt", "additions": "245", "deletions": "0", "changes": "245"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q76.sf100/simplified.txt", "additions": "68", "deletions": "0", "changes": "68"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q76/explain.txt", "additions": "209", "deletions": "0", "changes": "209"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q76/simplified.txt", "additions": "58", "deletions": "0", "changes": "58"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q77.sf100/explain.txt", "additions": "520", "deletions": "0", "changes": "520"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q77.sf100/simplified.txt", "additions": "139", "deletions": "0", "changes": "139"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q77/explain.txt", "additions": "520", "deletions": "0", "changes": "520"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q77/simplified.txt", "additions": "139", "deletions": "0", "changes": "139"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q78.sf100/explain.txt", "additions": "391", "deletions": "0", "changes": "391"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q78.sf100/simplified.txt", "additions": "117", "deletions": "0", "changes": "117"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q78/explain.txt", "additions": "341", "deletions": "0", "changes": "341"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q78/simplified.txt", "additions": "88", "deletions": "0", "changes": "88"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q79.sf100/explain.txt", "additions": "208", "deletions": "0", "changes": "208"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q79.sf100/simplified.txt", "additions": "59", "deletions": "0", "changes": "59"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q79/explain.txt", "additions": "193", "deletions": "0", "changes": "193"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q79/simplified.txt", "additions": "50", "deletions": "0", "changes": "50"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q8.sf100/explain.txt", "additions": "302", "deletions": "0", "changes": "302"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q8.sf100/simplified.txt", "additions": "88", "deletions": "0", "changes": "88"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q8/explain.txt", "additions": "272", "deletions": "0", "changes": "272"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q8/simplified.txt", "additions": "70", "deletions": "0", "changes": "70"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q80.sf100/explain.txt", "additions": "598", "deletions": "0", "changes": "598"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q80.sf100/simplified.txt", "additions": "172", "deletions": "0", "changes": "172"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q80/explain.txt", "additions": "553", "deletions": "0", "changes": "553"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q80/simplified.txt", "additions": "148", "deletions": "0", "changes": "148"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q81.sf100/explain.txt", "additions": "343", "deletions": "0", "changes": "343"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q81.sf100/simplified.txt", "additions": "104", "deletions": "0", "changes": "104"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q81/explain.txt", "additions": "298", "deletions": "0", "changes": "298"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q81/simplified.txt", "additions": "77", "deletions": "0", "changes": "77"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q82.sf100/explain.txt", "additions": "175", "deletions": "0", "changes": "175"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q82.sf100/simplified.txt", "additions": "50", "deletions": "0", "changes": "50"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q82/explain.txt", "additions": "160", "deletions": "0", "changes": "160"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q82/simplified.txt", "additions": "41", "deletions": "0", "changes": "41"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q83.sf100/explain.txt", "additions": "344", "deletions": "0", "changes": "344"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q83.sf100/simplified.txt", "additions": "91", "deletions": "0", "changes": "91"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q83/explain.txt", "additions": "344", "deletions": "0", "changes": "344"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q83/simplified.txt", "additions": "91", "deletions": "0", "changes": "91"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q84.sf100/explain.txt", "additions": "200", "deletions": "0", "changes": "200"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q84.sf100/simplified.txt", "additions": "53", "deletions": "0", "changes": "53"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q84/explain.txt", "additions": "200", "deletions": "0", "changes": "200"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q84/simplified.txt", "additions": "53", "deletions": "0", "changes": "53"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q85.sf100/explain.txt", "additions": "317", "deletions": "0", "changes": "317"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q85.sf100/simplified.txt", "additions": "94", "deletions": "0", "changes": "94"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q85/explain.txt", "additions": "287", "deletions": "0", "changes": "287"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q85/simplified.txt", "additions": "76", "deletions": "0", "changes": "76"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q86.sf100/explain.txt", "additions": "142", "deletions": "0", "changes": "142"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q86.sf100/simplified.txt", "additions": "39", "deletions": "0", "changes": "39"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q86/explain.txt", "additions": "142", "deletions": "0", "changes": "142"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q86/simplified.txt", "additions": "39", "deletions": "0", "changes": "39"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q87.sf100/explain.txt", "additions": "388", "deletions": "0", "changes": "388"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q87.sf100/simplified.txt", "additions": "117", "deletions": "0", "changes": "117"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q87/explain.txt", "additions": "323", "deletions": "0", "changes": "323"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q87/simplified.txt", "additions": "80", "deletions": "0", "changes": "80"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q88.sf100/explain.txt", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q88.sf100/simplified.txt", "additions": "250", "deletions": "0", "changes": "250"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q88/explain.txt", "additions": "960", "deletions": "0", "changes": "960"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q88/simplified.txt", "additions": "250", "deletions": "0", "changes": "250"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q89.sf100/explain.txt", "additions": "175", "deletions": "0", "changes": "175"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q89.sf100/simplified.txt", "additions": "48", "deletions": "0", "changes": "48"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q89/explain.txt", "additions": "175", "deletions": "0", "changes": "175"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q89/simplified.txt", "additions": "48", "deletions": "0", "changes": "48"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9.sf100/explain.txt", "additions": "718", "deletions": "0", "changes": "718"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9.sf100/simplified.txt", "additions": "186", "deletions": "0", "changes": "186"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9/explain.txt", "additions": "718", "deletions": "0", "changes": "718"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9/simplified.txt", "additions": "186", "deletions": "0", "changes": "186"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q90.sf100/explain.txt", "additions": "280", "deletions": "0", "changes": "280"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q90.sf100/simplified.txt", "additions": "74", "deletions": "0", "changes": "74"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q90/explain.txt", "additions": "280", "deletions": "0", "changes": "280"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q90/simplified.txt", "additions": "74", "deletions": "0", "changes": "74"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q91.sf100/explain.txt", "additions": "264", "deletions": "0", "changes": "264"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q91.sf100/simplified.txt", "additions": "71", "deletions": "0", "changes": "71"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q91/explain.txt", "additions": "264", "deletions": "0", "changes": "264"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q91/simplified.txt", "additions": "71", "deletions": "0", "changes": "71"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q92.sf100/explain.txt", "additions": "196", "deletions": "0", "changes": "196"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q92.sf100/simplified.txt", "additions": "50", "deletions": "0", "changes": "50"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q92/explain.txt", "additions": "196", "deletions": "0", "changes": "196"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q92/simplified.txt", "additions": "50", "deletions": "0", "changes": "50"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q93.sf100/explain.txt", "additions": "126", "deletions": "0", "changes": "126"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q93.sf100/simplified.txt", "additions": "38", "deletions": "0", "changes": "38"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q93/explain.txt", "additions": "111", "deletions": "0", "changes": "111"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q93/simplified.txt", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q94.sf100/explain.txt", "additions": "265", "deletions": "0", "changes": "265"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q94.sf100/simplified.txt", "additions": "78", "deletions": "0", "changes": "78"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q94/explain.txt", "additions": "235", "deletions": "0", "changes": "235"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q94/simplified.txt", "additions": "62", "deletions": "0", "changes": "62"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q95.sf100/explain.txt", "additions": "347", "deletions": "0", "changes": "347"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q95.sf100/simplified.txt", "additions": "111", "deletions": "0", "changes": "111"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q95/explain.txt", "additions": "318", "deletions": "0", "changes": "318"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q95/simplified.txt", "additions": "84", "deletions": "0", "changes": "84"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q96.sf100/explain.txt", "additions": "160", "deletions": "0", "changes": "160"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q96.sf100/simplified.txt", "additions": "41", "deletions": "0", "changes": "41"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q96/explain.txt", "additions": "160", "deletions": "0", "changes": "160"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q96/simplified.txt", "additions": "41", "deletions": "0", "changes": "41"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q97.sf100/explain.txt", "additions": "179", "deletions": "0", "changes": "179"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q97.sf100/simplified.txt", "additions": "46", "deletions": "0", "changes": "46"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q97/explain.txt", "additions": "179", "deletions": "0", "changes": "179"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q97/simplified.txt", "additions": "46", "deletions": "0", "changes": "46"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q98.sf100/explain.txt", "additions": "162", "deletions": "0", "changes": "162"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q98.sf100/simplified.txt", "additions": "51", "deletions": "0", "changes": "51"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q98/explain.txt", "additions": "147", "deletions": "0", "changes": "147"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q98/simplified.txt", "additions": "42", "deletions": "0", "changes": "42"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q99.sf100/explain.txt", "additions": "183", "deletions": "0", "changes": "183"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q99.sf100/simplified.txt", "additions": "48", "deletions": "0", "changes": "48"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q99/explain.txt", "additions": "183", "deletions": "0", "changes": "183"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q99/simplified.txt", "additions": "48", "deletions": "0", "changes": "48"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q10a.sf100/explain.txt", "additions": "286", "deletions": "0", "changes": "286"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q10a.sf100/simplified.txt", "additions": "81", "deletions": "0", "changes": "81"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q10a/explain.txt", "additions": "266", "deletions": "0", "changes": "266"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q10a/simplified.txt", "additions": "71", "deletions": "0", "changes": "71"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q11.sf100/explain.txt", "additions": "477", "deletions": "0", "changes": "477"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q11.sf100/simplified.txt", "additions": "157", "deletions": "0", "changes": "157"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q11/explain.txt", "additions": "410", "deletions": "0", "changes": "410"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q11/simplified.txt", "additions": "107", "deletions": "0", "changes": "107"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q12.sf100/explain.txt", "additions": "152", "deletions": "0", "changes": "152"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q12.sf100/simplified.txt", "additions": "47", "deletions": "0", "changes": "47"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q12/explain.txt", "additions": "137", "deletions": "0", "changes": "137"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q12/simplified.txt", "additions": "38", "deletions": "0", "changes": "38"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14.sf100/explain.txt", "additions": "810", "deletions": "0", "changes": "810"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14.sf100/simplified.txt", "additions": "231", "deletions": "0", "changes": "231"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14/explain.txt", "additions": "763", "deletions": "0", "changes": "763"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14/simplified.txt", "additions": "204", "deletions": "0", "changes": "204"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a.sf100/explain.txt", "additions": "1460", "deletions": "0", "changes": "1460"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a.sf100/simplified.txt", "additions": "427", "deletions": "0", "changes": "427"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a/explain.txt", "additions": "1380", "deletions": "0", "changes": "1380"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a/simplified.txt", "additions": "387", "deletions": "0", "changes": "387"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q18a.sf100/explain.txt", "additions": "877", "deletions": "0", "changes": "877"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q18a.sf100/simplified.txt", "additions": "262", "deletions": "0", "changes": "262"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q18a/explain.txt", "additions": "856", "deletions": "0", "changes": "856"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q18a/simplified.txt", "additions": "227", "deletions": "0", "changes": "227"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q20.sf100/explain.txt", "additions": "152", "deletions": "0", "changes": "152"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q20.sf100/simplified.txt", "additions": "47", "deletions": "0", "changes": "47"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q20/explain.txt", "additions": "137", "deletions": "0", "changes": "137"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q20/simplified.txt", "additions": "38", "deletions": "0", "changes": "38"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q22.sf100/explain.txt", "additions": "157", "deletions": "0", "changes": "157"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q22.sf100/simplified.txt", "additions": "48", "deletions": "0", "changes": "48"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q22/explain.txt", "additions": "142", "deletions": "0", "changes": "142"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q22/simplified.txt", "additions": "39", "deletions": "0", "changes": "39"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q22a.sf100/explain.txt", "additions": "316", "deletions": "0", "changes": "316"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q22a.sf100/simplified.txt", "additions": "87", "deletions": "0", "changes": "87"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q22a/explain.txt", "additions": "301", "deletions": "0", "changes": "301"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q22a/simplified.txt", "additions": "78", "deletions": "0", "changes": "78"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q24.sf100/explain.txt", "additions": "532", "deletions": "0", "changes": "532"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q24.sf100/simplified.txt", "additions": "156", "deletions": "0", "changes": "156"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q24/explain.txt", "additions": "487", "deletions": "0", "changes": "487"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q24/simplified.txt", "additions": "129", "deletions": "0", "changes": "129"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q27a.sf100/explain.txt", "additions": "428", "deletions": "0", "changes": "428"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q27a.sf100/simplified.txt", "additions": "113", "deletions": "0", "changes": "113"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q27a/explain.txt", "additions": "428", "deletions": "0", "changes": "428"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q27a/simplified.txt", "additions": "113", "deletions": "0", "changes": "113"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q34.sf100/explain.txt", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q34.sf100/simplified.txt", "additions": "63", "deletions": "0", "changes": "63"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q34/explain.txt", "additions": "203", "deletions": "0", "changes": "203"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q34/simplified.txt", "additions": "54", "deletions": "0", "changes": "54"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35.sf100/explain.txt", "additions": "329", "deletions": "0", "changes": "329"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35.sf100/simplified.txt", "additions": "103", "deletions": "0", "changes": "103"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35/explain.txt", "additions": "274", "deletions": "0", "changes": "274"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35/simplified.txt", "additions": "73", "deletions": "0", "changes": "73"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35a.sf100/explain.txt", "additions": "311", "deletions": "0", "changes": "311"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35a.sf100/simplified.txt", "additions": "98", "deletions": "0", "changes": "98"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35a/explain.txt", "additions": "261", "deletions": "0", "changes": "261"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35a/simplified.txt", "additions": "70", "deletions": "0", "changes": "70"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q36a.sf100/explain.txt", "additions": "289", "deletions": "0", "changes": "289"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q36a.sf100/simplified.txt", "additions": "82", "deletions": "0", "changes": "82"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q36a/explain.txt", "additions": "289", "deletions": "0", "changes": "289"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q36a/simplified.txt", "additions": "82", "deletions": "0", "changes": "82"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q47.sf100/explain.txt", "additions": "313", "deletions": "0", "changes": "313"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q47.sf100/simplified.txt", "additions": "107", "deletions": "0", "changes": "107"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q47/explain.txt", "additions": "278", "deletions": "0", "changes": "278"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q47/simplified.txt", "additions": "84", "deletions": "0", "changes": "84"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q49.sf100/explain.txt", "additions": "478", "deletions": "0", "changes": "478"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q49.sf100/simplified.txt", "additions": "153", "deletions": "0", "changes": "153"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q49/explain.txt", "additions": "433", "deletions": "0", "changes": "433"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q49/simplified.txt", "additions": "126", "deletions": "0", "changes": "126"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q51a.sf100/explain.txt", "additions": "441", "deletions": "0", "changes": "441"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q51a.sf100/simplified.txt", "additions": "139", "deletions": "0", "changes": "139"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q51a/explain.txt", "additions": "426", "deletions": "0", "changes": "426"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q51a/simplified.txt", "additions": "126", "deletions": "0", "changes": "126"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q57.sf100/explain.txt", "additions": "313", "deletions": "0", "changes": "313"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q57.sf100/simplified.txt", "additions": "107", "deletions": "0", "changes": "107"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q57/explain.txt", "additions": "278", "deletions": "0", "changes": "278"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q57/simplified.txt", "additions": "84", "deletions": "0", "changes": "84"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q5a.sf100/explain.txt", "additions": "559", "deletions": "0", "changes": "559"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q5a.sf100/simplified.txt", "additions": "165", "deletions": "0", "changes": "165"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q5a/explain.txt", "additions": "544", "deletions": "0", "changes": "544"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q5a/simplified.txt", "additions": "156", "deletions": "0", "changes": "156"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q6.sf100/explain.txt", "additions": "331", "deletions": "0", "changes": "331"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q6.sf100/simplified.txt", "additions": "95", "deletions": "0", "changes": "95"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q6/explain.txt", "additions": "301", "deletions": "0", "changes": "301"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q6/simplified.txt", "additions": "77", "deletions": "0", "changes": "77"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q64.sf100/explain.txt", "additions": "1110", "deletions": "0", "changes": "1110"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q64.sf100/simplified.txt", "additions": "367", "deletions": "0", "changes": "367"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q64/explain.txt", "additions": "918", "deletions": "0", "changes": "918"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q64/simplified.txt", "additions": "246", "deletions": "0", "changes": "246"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q67a.sf100/explain.txt", "additions": "452", "deletions": "0", "changes": "452"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q67a.sf100/simplified.txt", "additions": "129", "deletions": "0", "changes": "129"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q67a/explain.txt", "additions": "437", "deletions": "0", "changes": "437"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q67a/simplified.txt", "additions": "120", "deletions": "0", "changes": "120"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q70a.sf100/explain.txt", "additions": "373", "deletions": "0", "changes": "373"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q70a.sf100/simplified.txt", "additions": "107", "deletions": "0", "changes": "107"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q70a/explain.txt", "additions": "373", "deletions": "0", "changes": "373"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q70a/simplified.txt", "additions": "107", "deletions": "0", "changes": "107"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q72.sf100/explain.txt", "additions": "436", "deletions": "0", "changes": "436"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q72.sf100/simplified.txt", "additions": "130", "deletions": "0", "changes": "130"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q72/explain.txt", "additions": "391", "deletions": "0", "changes": "391"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q72/simplified.txt", "additions": "104", "deletions": "0", "changes": "104"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q74.sf100/explain.txt", "additions": "477", "deletions": "0", "changes": "477"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q74.sf100/simplified.txt", "additions": "157", "deletions": "0", "changes": "157"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q74/explain.txt", "additions": "410", "deletions": "0", "changes": "410"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q74/simplified.txt", "additions": "107", "deletions": "0", "changes": "107"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q75.sf100/explain.txt", "additions": "752", "deletions": "0", "changes": "752"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q75.sf100/simplified.txt", "additions": "237", "deletions": "0", "changes": "237"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q75/explain.txt", "additions": "647", "deletions": "0", "changes": "647"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q75/simplified.txt", "additions": "180", "deletions": "0", "changes": "180"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q77a.sf100/explain.txt", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q77a.sf100/simplified.txt", "additions": "172", "deletions": "0", "changes": "172"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q77a/explain.txt", "additions": "629", "deletions": "0", "changes": "629"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q77a/simplified.txt", "additions": "172", "deletions": "0", "changes": "172"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q78.sf100/explain.txt", "additions": "391", "deletions": "0", "changes": "391"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q78.sf100/simplified.txt", "additions": "117", "deletions": "0", "changes": "117"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q78/explain.txt", "additions": "341", "deletions": "0", "changes": "341"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q78/simplified.txt", "additions": "88", "deletions": "0", "changes": "88"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q80a.sf100/explain.txt", "additions": "707", "deletions": "0", "changes": "707"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q80a.sf100/simplified.txt", "additions": "205", "deletions": "0", "changes": "205"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q80a/explain.txt", "additions": "662", "deletions": "0", "changes": "662"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q80a/simplified.txt", "additions": "181", "deletions": "0", "changes": "181"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q86a.sf100/explain.txt", "additions": "251", "deletions": "0", "changes": "251"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q86a.sf100/simplified.txt", "additions": "72", "deletions": "0", "changes": "72"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q86a/explain.txt", "additions": "251", "deletions": "0", "changes": "251"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q86a/simplified.txt", "additions": "72", "deletions": "0", "changes": "72"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q98.sf100/explain.txt", "additions": "157", "deletions": "0", "changes": "157"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q98.sf100/simplified.txt", "additions": "50", "deletions": "0", "changes": "50"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q98/explain.txt", "additions": "142", "deletions": "0", "changes": "142"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q98/simplified.txt", "additions": "41", "deletions": "0", "changes": "41"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/ColumnExpressionSuite.scala", "additions": "1", "deletions": "350", "changes": "351"}, "updated": [0, 2, 5]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 1, 4]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DatasetOptimizationSuite.scala", "additions": "11", "deletions": "2", "changes": "13"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/JoinSuite.scala", "additions": "66", "deletions": "0", "changes": "66"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/PlanStabilitySuite.scala", "additions": "327", "deletions": "0", "changes": "327"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SQLQueryTestSuite.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/TPCDSBase.scala", "additions": "65", "deletions": "1", "changes": "66"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/TPCDSQuerySuite.scala", "additions": "8", "deletions": "64", "changes": "72"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/V1WriteFallbackSuite.scala", "additions": "42", "deletions": "0", "changes": "42"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/AlreadyOptimizedSuite.scala", "additions": "85", "deletions": "0", "changes": "85"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/BaseScriptTransformationSuite.scala", "additions": "97", "deletions": "7", "changes": "104"}, "updated": [2, 4, 4]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/SQLViewSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/SparkSqlParserSuite.scala", "additions": "81", "deletions": "2", "changes": "83"}, "updated": [0, 2, 4]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 3, 8]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/columnar/compression/BooleanBitSetSuite.scala", "additions": "26", "deletions": "0", "changes": "26"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/DDLSuite.scala", "additions": "30", "deletions": "2", "changes": "32"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/csv/CSVSuite.scala", "additions": "14", "deletions": "2", "changes": "16"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/json/JsonSuite.scala", "additions": "21", "deletions": "3", "changes": "24"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/joins/HashedRelationSuite.scala", "additions": "79", "deletions": "0", "changes": "79"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/CompactibleFileStreamLogSuite.scala", "additions": "0", "deletions": "2", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/FileStreamSinkLogSuite.scala", "additions": "7", "deletions": "13", "changes": "20"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/ui/SQLAppStatusListenerSuite.scala", "additions": "132", "deletions": "0", "changes": "132"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/v1.2/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcFilterSuite.scala", "additions": "95", "deletions": "2", "changes": "97"}, "updated": [2, 3, 4]}, {"file": {"name": "sql/core/v2.3/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcFilterSuite.scala", "additions": "95", "deletions": "2", "changes": "97"}, "updated": [2, 3, 4]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala", "additions": "23", "deletions": "2", "changes": "25"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/HiveSQLViewSuite.scala", "additions": "21", "deletions": "1", "changes": "22"}, "updated": [0, 1, 1]}, {"file": {"name": "streaming/src/main/scala/org/apache/spark/streaming/scheduler/ExecutorAllocationManager.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 2, 2]}, {"file": {"name": "streaming/src/test/scala/org/apache/spark/streaming/scheduler/ExecutorAllocationManagerSuite.scala", "additions": "2", "deletions": "3", "changes": "5"}, "updated": [0, 2, 2]}]}
{"author": "GuoPhilipse", "sha": "", "commit_date": "2020/08/14 14:18:22", "commit_message": "Merge pull request #23 from apache/master\n\nsync", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": ".github/workflows/master.yml", "additions": "17", "deletions": "11", "changes": "28"}, "updated": [1, 1, 10]}, {"file": {"name": ".github/workflows/test_report.yml", "additions": "35", "deletions": "0", "changes": "35"}, "updated": [1, 1, 1]}, {"file": {"name": "R/pkg/R/functions.R", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "R/pkg/R/utils.R", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 1, 2]}, {"file": {"name": "common/unsafe/src/main/java/org/apache/spark/unsafe/types/UTF8String.java", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/main/java/org/apache/spark/api/java/StorageLevels.java", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/utils.js", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/ExecutorAllocationClient.scala", "additions": "40", "deletions": "0", "changes": "40"}, "updated": [1, 1, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala", "additions": "27", "deletions": "7", "changes": "34"}, "updated": [1, 2, 4]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala", "additions": "10", "deletions": "13", "changes": "23"}, "updated": [1, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 2, 4]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala", "additions": "6", "deletions": "4", "changes": "10"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala", "additions": "110", "deletions": "80", "changes": "190"}, "updated": [1, 3, 7]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 1, 4]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/dynalloc/ExecutorMonitor.scala", "additions": "55", "deletions": "6", "changes": "61"}, "updated": [1, 1, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/storage/BlockManager.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 2, 4]}, {"file": {"name": "core/src/main/scala/org/apache/spark/storage/BlockManagerMaster.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [1, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/storage/StorageLevel.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "core/src/test/scala/org/apache/spark/DistributedSuite.scala", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/test/scala/org/apache/spark/ExecutorAllocationManagerSuite.scala", "additions": "70", "deletions": "1", "changes": "71"}, "updated": [1, 1, 3]}, {"file": {"name": "core/src/test/scala/org/apache/spark/SparkFunSuite.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/test/scala/org/apache/spark/deploy/SparkSubmitUtilsSuite.scala", "additions": "8", "deletions": "8", "changes": "16"}, "updated": [1, 1, 1]}, {"file": {"name": "core/src/test/scala/org/apache/spark/deploy/master/MasterSuite.scala", "additions": "11", "deletions": "3", "changes": "14"}, "updated": [1, 1, 4]}, {"file": {"name": "core/src/test/scala/org/apache/spark/rdd/LocalCheckpointSuite.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/TaskSetManagerSuite.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 1, 5]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/WorkerDecommissionExtendedSuite.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 1, 4]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/WorkerDecommissionSuite.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [1, 2, 4]}, {"file": {"name": "core/src/test/scala/org/apache/spark/storage/BlockManagerDecommissionIntegrationSuite.scala", "additions": "12", "deletions": "7", "changes": "19"}, "updated": [1, 3, 6]}, {"file": {"name": "core/src/test/scala/org/apache/spark/ui/ChromeUISeleniumSuite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 1]}, {"file": {"name": "core/src/test/scala/org/apache/spark/ui/RealBrowserUISeleniumSuite.scala", "additions": "54", "deletions": "2", "changes": "56"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/test/scala/org/apache/spark/ui/UISeleniumSuite.scala", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/test/scala/org/apache/spark/util/JsonProtocolSuite.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 4]}, {"file": {"name": "dev/create-release/release-tag.sh", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 1, 1]}, {"file": {"name": "dev/create-release/release-util.sh", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "dev/create-release/releaseutils.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 3]}, {"file": {"name": "dev/create-release/spark-rm/Dockerfile", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 1, 3]}, {"file": {"name": "dev/lint-python", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 1, 4]}, {"file": {"name": "dev/pip-sanity-check.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "dev/run-tests.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 1, 11]}, {"file": {"name": "dev/tox.ini", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 1, 4]}, {"file": {"name": "docs/_config.yml", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "docs/monitoring.md", "additions": "6", "deletions": "4", "changes": "10"}, "updated": [0, 2, 4]}, {"file": {"name": "docs/pyspark-migration-guide.md", "additions": "1", "deletions": "62", "changes": "63"}, "updated": [0, 1, 3]}, {"file": {"name": "docs/rdd-programming-guide.md", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "docs/running-on-kubernetes.md", "additions": "0", "deletions": "4", "changes": "4"}, "updated": [0, 1, 1]}, {"file": {"name": "docs/sql-ref-syntax-aux-cache-cache-table.md", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 3]}, {"file": {"name": "examples/src/main/python/sql/hive.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "examples/src/main/python/status_api_demo.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 1, 2]}, {"file": {"name": "external/docker-integration-tests/src/test/scala/org/apache/spark/sql/jdbc/PostgresIntegrationSuite.scala", "additions": "38", "deletions": "2", "changes": "40"}, "updated": [0, 2, 2]}, {"file": {"name": "external/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaTestUtils.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 2]}, {"file": {"name": "pom.xml", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 13]}, {"file": {"name": "project/SparkBuild.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 1, 9]}, {"file": {"name": "python/docs/source/migration_guide/index.rst", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 1, 2]}, {"file": {"name": "python/docs/source/migration_guide/pyspark_1.0_1.2_to_1.3.rst", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 1, 1]}, {"file": {"name": "python/docs/source/migration_guide/pyspark_1.4_to_1.5.rst", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [0, 1, 1]}, {"file": {"name": "python/docs/source/migration_guide/pyspark_2.2_to_2.3.rst", "additions": "30", "deletions": "0", "changes": "30"}, "updated": [0, 1, 1]}, {"file": {"name": "python/docs/source/migration_guide/pyspark_2.3.0_to_2.3.1_above.rst", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 1, 1]}, {"file": {"name": "python/docs/source/migration_guide/pyspark_2.3_to_2.4.rst", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 1, 1]}, {"file": {"name": "python/docs/source/migration_guide/pyspark_2.4_to_3.0.rst", "additions": "44", "deletions": "0", "changes": "44"}, "updated": [0, 1, 1]}, {"file": {"name": "python/docs/source/reference/pyspark.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/__init__.py", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/ml/__init__.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/ml/param/__init__.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/ml/pipeline.py", "additions": "0", "deletions": "2", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/ml/regression.py", "additions": "3", "deletions": "5", "changes": "8"}, "updated": [0, 2, 4]}, {"file": {"name": "python/pyspark/ml/stat.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 2, 2]}, {"file": {"name": "python/pyspark/ml/tests/test_algorithms.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/ml/tests/test_base.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/ml/tests/test_evaluation.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/ml/tests/test_feature.py", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/ml/tests/test_image.py", "additions": "2", "deletions": "4", "changes": "6"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/ml/tests/test_linalg.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/ml/tests/test_param.py", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 2, 3]}, {"file": {"name": "python/pyspark/ml/tests/test_persistence.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/ml/tests/test_pipeline.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/ml/tests/test_stat.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/ml/tests/test_training_summary.py", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 1, 6]}, {"file": {"name": "python/pyspark/ml/tests/test_tuning.py", "additions": "1", "deletions": "3", "changes": "4"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/ml/tests/test_wrapper.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/ml/util.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/mllib/classification.py", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/mllib/clustering.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/mllib/feature.py", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/mllib/regression.py", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/mllib/tests/test_algorithms.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/mllib/tests/test_feature.py", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/mllib/tests/test_linalg.py", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/mllib/tests/test_stat.py", "additions": "2", "deletions": "3", "changes": "5"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/mllib/tests/test_streaming_algorithms.py", "additions": "3", "deletions": "4", "changes": "7"}, "updated": [0, 2, 2]}, {"file": {"name": "python/pyspark/mllib/tests/test_util.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/rdd.py", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 1, 3]}, {"file": {"name": "python/pyspark/resource/tests/test_resources.py", "additions": "2", "deletions": "3", "changes": "5"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/sql/avro/functions.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/sql/context.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/sql/functions.py", "additions": "5", "deletions": "4", "changes": "9"}, "updated": [1, 3, 10]}, {"file": {"name": "python/pyspark/sql/pandas/functions.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/sql/readwriter.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 5]}, {"file": {"name": "python/pyspark/sql/session.py", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 1, 3]}, {"file": {"name": "python/pyspark/sql/streaming.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 4]}, {"file": {"name": "python/pyspark/sql/tests/test_arrow.py", "additions": "7", "deletions": "1", "changes": "8"}, "updated": [0, 2, 6]}, {"file": {"name": "python/pyspark/sql/tests/test_catalog.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_column.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/sql/tests/test_conf.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_context.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/sql/tests/test_dataframe.py", "additions": "0", "deletions": "2", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_datasources.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_functions.py", "additions": "2", "deletions": "3", "changes": "5"}, "updated": [0, 1, 3]}, {"file": {"name": "python/pyspark/sql/tests/test_group.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_pandas_cogrouped_map.py", "additions": "4", "deletions": "5", "changes": "9"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/sql/tests/test_pandas_grouped_map.py", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 1, 3]}, {"file": {"name": "python/pyspark/sql/tests/test_pandas_map.py", "additions": "1", "deletions": "3", "changes": "4"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/sql/tests/test_pandas_udf.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_pandas_udf_grouped_agg.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_pandas_udf_scalar.py", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 1, 3]}, {"file": {"name": "python/pyspark/sql/tests/test_pandas_udf_typehints.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/sql/tests/test_pandas_udf_window.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_readwriter.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/sql/tests/test_serde.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_session.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_streaming.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_types.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 2, 4]}, {"file": {"name": "python/pyspark/sql/tests/test_udf.py", "additions": "0", "deletions": "2", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_utils.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/types.py", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 1, 4]}, {"file": {"name": "python/pyspark/storagelevel.py", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/streaming/dstream.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/streaming/tests/test_context.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/streaming/tests/test_dstream.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/streaming/tests/test_kinesis.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/streaming/tests/test_listener.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/testing/utils.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/tests/test_appsubmit.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/tests/test_broadcast.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/tests/test_conf.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/tests/test_context.py", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 2, 4]}, {"file": {"name": "python/pyspark/tests/test_daemon.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/tests/test_join.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/tests/test_pin_thread.py", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/tests/test_profiler.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/tests/test_rdd.py", "additions": "2", "deletions": "3", "changes": "5"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/tests/test_rddbarrier.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/tests/test_readwrite.py", "additions": "1", "deletions": "3", "changes": "4"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/tests/test_serializers.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/tests/test_shuffle.py", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/tests/test_taskcontext.py", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/tests/test_util.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/tests/test_worker.py", "additions": "2", "deletions": "3", "changes": "5"}, "updated": [0, 1, 2]}, {"file": {"name": "resource-managers/kubernetes/docker/src/main/dockerfiles/spark/decom.sh", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala", "additions": "11", "deletions": "16", "changes": "27"}, "updated": [1, 1, 3]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/tests/decommissioning.py", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/SupportsAtomicPartitionManagement.java", "additions": "85", "deletions": "0", "changes": "85"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/SupportsPartitionManagement.java", "additions": "115", "deletions": "0", "changes": "115"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala-2.13/org/apache/spark/sql/catalyst/expressions/ExpressionSet.scala", "additions": "7", "deletions": "1", "changes": "8"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/AlreadyExistException.scala", "additions": "24", "deletions": "6", "changes": "30"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala", "additions": "14", "deletions": "6", "changes": "20"}, "updated": [2, 3, 14]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/NoSuchItemException.scala", "additions": "24", "deletions": "9", "changes": "33"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/UnsupportedOperationChecker.scala", "additions": "7", "deletions": "1", "changes": "8"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package.scala", "additions": "32", "deletions": "14", "changes": "46"}, "updated": [1, 2, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [1, 3, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/ApproxCountDistinctForIntervals.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/HyperLogLogPlusPlus.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/interfaces.scala", "additions": "8", "deletions": "2", "changes": "10"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala", "additions": "9", "deletions": "8", "changes": "17"}, "updated": [1, 1, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/complexTypeCreator.scala", "additions": "75", "deletions": "35", "changes": "110"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/higherOrderFunctions.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JacksonParser.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/ComplexTypes.scala", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "27", "deletions": "5", "changes": "32"}, "updated": [3, 3, 12]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/UpdateFields.scala", "additions": "8", "deletions": "8", "changes": "16"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [1, 1, 10]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/planning/patterns.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreeNode.scala", "additions": "14", "deletions": "9", "changes": "23"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "13", "deletions": "5", "changes": "18"}, "updated": [1, 6, 19]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/types/Metadata.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/util/SchemaUtils.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/RandomDataGenerator.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisSuite.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CastSuite.scala", "additions": "9", "deletions": "8", "changes": "17"}, "updated": [1, 3, 4]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/BinaryComparisonSimplificationSuite.scala", "additions": "1", "deletions": "3", "changes": "4"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/CombineUpdateFieldsSuite.scala", "additions": "19", "deletions": "22", "changes": "41"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/EliminateAggregateFilterSuite.scala", "additions": "75", "deletions": "0", "changes": "75"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/FilterPushdownSuite.scala", "additions": "21", "deletions": "36", "changes": "57"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/JoinReorderSuite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/PullupCorrelatedPredicatesSuite.scala", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ReplaceNullWithFalseInPredicateSuite.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/SimplifyCastsSuite.scala", "additions": "5", "deletions": "4", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/SimplifyConditionalSuite.scala", "additions": "11", "deletions": "12", "changes": "23"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/StarJoinReorderSuite.scala", "additions": "11", "deletions": "8", "changes": "19"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/complexTypesSuite.scala", "additions": "51", "deletions": "30", "changes": "81"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/InMemoryAtomicPartitionTable.scala", "additions": "76", "deletions": "0", "changes": "76"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/InMemoryPartitionTable.scala", "additions": "95", "deletions": "0", "changes": "95"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/SupportsAtomicPartitionManagementSuite.scala", "additions": "126", "deletions": "0", "changes": "126"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/SupportsPartitionManagementSuite.scala", "additions": "143", "deletions": "0", "changes": "143"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/util/SchemaUtilsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedPlainValuesReader.java", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedRleValuesReader.java", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OffHeapColumnVector.java", "additions": "26", "deletions": "0", "changes": "26"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OnHeapColumnVector.java", "additions": "24", "deletions": "0", "changes": "24"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/WritableColumnVector.java", "additions": "12", "deletions": "0", "changes": "12"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/Column.scala", "additions": "68", "deletions": "18", "changes": "86"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/BaseScriptTransformationExec.scala", "additions": "201", "deletions": "36", "changes": "237"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/Columnar.scala", "additions": "14", "deletions": "3", "changes": "17"}, "updated": [0, 2, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala", "additions": "4", "deletions": "6", "changes": "10"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 2, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/RemoveRedundantProjects.scala", "additions": "99", "deletions": "0", "changes": "99"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala", "additions": "8", "deletions": "4", "changes": "12"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "54", "deletions": "15", "changes": "69"}, "updated": [0, 3, 6]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/EliminateNullAwareAntiJoin.scala", "additions": "41", "deletions": "0", "changes": "41"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/QueryStageExec.scala", "additions": "5", "deletions": "3", "changes": "8"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala", "additions": "7", "deletions": "5", "changes": "12"}, "updated": [1, 2, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DaysWritable.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala", "additions": "8", "deletions": "3", "changes": "11"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFiltersBase.scala", "additions": "43", "deletions": "7", "changes": "50"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala", "additions": "1", "deletions": "3", "changes": "4"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/orc/OrcScanBuilder.scala", "additions": "3", "deletions": "4", "changes": "7"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/dynamicpruning/PlanDynamicPruningFilters.scala", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastHashJoinExec.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashJoin.scala", "additions": "30", "deletions": "12", "changes": "42"}, "updated": [0, 1, 6]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashedRelation.scala", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/joins/ShuffledHashJoinExec.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala", "additions": "42", "deletions": "6", "changes": "48"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/functions.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 6]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/jdbc/PostgresDialect.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/cast.sql", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/datetime.sql", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/interval.sql", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/datetime.sql.out", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "15", "deletions": "1", "changes": "16"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/cast.sql.out", "additions": "41", "deletions": "1", "changes": "42"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/datetime-legacy.sql.out", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/datetime.sql.out", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "209", "deletions": "172", "changes": "381"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "128", "deletions": "183", "changes": "311"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "15", "deletions": "1", "changes": "16"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/test_script.py", "additions": "0", "deletions": "0", "changes": "0"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/CTEHintSuite.scala", "additions": "168", "deletions": "0", "changes": "168"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/ColumnExpressionSuite.scala", "additions": "350", "deletions": "1", "changes": "351"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala", "additions": "0", "deletions": "12", "changes": "12"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/TPCDSQuerySuite.scala", "additions": "25", "deletions": "2", "changes": "27"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/TPCDSTableStats.scala", "additions": "503", "deletions": "0", "changes": "503"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/BaseScriptTransformationSuite.scala", "additions": "382", "deletions": "0", "changes": "382"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/ColumnarRulesSuite.scala", "additions": "63", "deletions": "0", "changes": "63"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/RemoveRedundantProjectsSuite.scala", "additions": "133", "deletions": "0", "changes": "133"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/SQLViewSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/TestUncaughtExceptionHandler.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "34", "deletions": "1", "changes": "35"}, "updated": [0, 3, 7]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/FileBasedDataSourceTest.scala", "additions": "39", "deletions": "1", "changes": "40"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategySuite.scala", "additions": "30", "deletions": "0", "changes": "30"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcTest.scala", "additions": "22", "deletions": "0", "changes": "22"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala", "additions": "1", "deletions": "28", "changes": "29"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/joins/HashedRelationSuite.scala", "additions": "12", "deletions": "0", "changes": "12"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/vectorized/ColumnarBatchSuite.scala", "additions": "102", "deletions": "14", "changes": "116"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/streaming/FileStreamSourceSuite.scala", "additions": "149", "deletions": "10", "changes": "159"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingQuerySuite.scala", "additions": "50", "deletions": "1", "changes": "51"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/v1.2/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFilters.scala", "additions": "12", "deletions": "16", "changes": "28"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/v1.2/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcFilterSuite.scala", "additions": "160", "deletions": "116", "changes": "276"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/v2.3/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFilters.scala", "additions": "12", "deletions": "16", "changes": "28"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/v2.3/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcFilterSuite.scala", "additions": "163", "deletions": "119", "changes": "282"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveInspectors.scala", "additions": "5", "deletions": "2", "changes": "7"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveStrategies.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala", "additions": "28", "deletions": "5", "changes": "33"}, "updated": [1, 3, 4]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/HiveScriptTransformationExec.scala", "additions": "98", "deletions": "149", "changes": "247"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/InsertIntoHiveTable.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/HiveSQLViewSuite.scala", "additions": "21", "deletions": "1", "changes": "22"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/HiveScriptTransformationSuite.scala", "additions": "183", "deletions": "152", "changes": "335"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/HiveSerDeReadWriteSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [2, 2, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/SQLQuerySuite.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [1, 1, 3]}, {"file": {"name": "streaming/src/main/scala/org/apache/spark/streaming/scheduler/ExecutorAllocationManager.scala", "additions": "9", "deletions": "1", "changes": "10"}, "updated": [1, 1, 1]}, {"file": {"name": "streaming/src/test/scala/org/apache/spark/streaming/scheduler/ExecutorAllocationManagerSuite.scala", "additions": "39", "deletions": "12", "changes": "51"}, "updated": [1, 1, 1]}]}
{"author": "mingjialiu", "sha": "", "commit_date": "2020/09/10 03:39:34", "commit_message": "Revert \"[Spark 32708] Query optimization fails to reuse exchange with DataSourceV2\"\n\nThis reverts commit dd0fb242277184abda5c6a4cb03bdec4e930e736.", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Relation.scala", "additions": "2", "deletions": "3", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2StringFormat.scala", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 0, 0]}]}
{"author": "bowenli86", "sha": "", "commit_date": "2020/09/12 22:46:06", "commit_message": "[SPARK-32865][DOC] python section in quickstart page doesn't display SPARK_VERSION correctly", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "docs/quick-start.md", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "Ted-Jiang", "sha": "", "commit_date": "2020/08/26 01:46:10", "commit_message": "[SPARK-32620][SQL] Reset the numPartitions metric when DPP is enabled\n\n### What changes were proposed in this pull request?\n\nThis pr reset the `numPartitions` metric when DPP is enabled. Otherwise, it is always a [static value](https://github.com/apache/spark/blob/18cac6a9f0bf4a6d449393f1ee84004623b3c893/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala#L215).\n\n### Why are the changes needed?\n\nFix metric issue.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nUnit test and manual test\n\nFor [this test case](https://github.com/apache/spark/blob/18cac6a9f0bf4a6d449393f1ee84004623b3c893/sql/core/src/test/scala/org/apache/spark/sql/DynamicPartitionPruningSuite.scala#L252-L280).\n\nBefore this pr:\n![image](https://user-images.githubusercontent.com/5399861/90301798-9310b480-ded4-11ea-9294-49bcaba46f83.png)\n\nAfter this pr:\n![image](https://user-images.githubusercontent.com/5399861/90301709-0fef5e80-ded4-11ea-942d-4d45d1dd15bc.png)\n\nCloses #29436 from wangyum/SPARK-32620.\n\nAuthored-by: Yuming Wang <yumwang@ebay.com>\nSigned-off-by: Yuming Wang <wgyumg@gmail.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DynamicPartitionPruningSuite.scala", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [1, 1, 3]}]}
{"author": "erenavsarogullari", "sha": "", "commit_date": "2020/09/03 15:49:17", "commit_message": "[SPARK-32788][SQL] non-partitioned table scan should not have partition filter\n\n### What changes were proposed in this pull request?\n\nThis PR fixes a bug `FileSourceStrategy`, which generates partition filters even if the table is not partitioned. This can confuse `FileSourceScanExec`, which mistakenly think the table is partitioned and tries to update the `numPartitions` metrics, and cause a failure. We should not generate partition filters for non-partitioned table.\n\n### Why are the changes needed?\n\nThe bug was exposed by https://github.com/apache/spark/pull/29436.\n\n### Does this PR introduce _any_ user-facing change?\n\nYes, fix a bug.\n\n### How was this patch tested?\n\nnew test\n\nCloses #29637 from cloud-fan/refactor.\n\nAuthored-by: Wenchen Fan <wenchen@databricks.com>\nSigned-off-by: Yuming Wang <yumwang@ebay.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala", "additions": "3", "deletions": "4", "changes": "7"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala", "additions": "7", "deletions": "3", "changes": "10"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [2, 2, 6]}]}
{"author": "javierivanov", "sha": "", "commit_date": "2021/09/05 13:23:05", "commit_message": "[SPARK-36613][SQL][SS] Use EnumSet as the implementation of Table.capabilities method return value\n\n### What changes were proposed in this pull request?\nThe `Table.capabilities` method return a `java.util.Set` of `TableCapability` enumeration type, which is implemented using `java.util.HashSet` now. Such Set can be replaced `with java.util.EnumSet` because `EnumSet` implementations can be much more efficient compared to other sets.\n\n### Why are the changes needed?\nUse more appropriate data structures.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\n\n- Pass GA or Jenkins Tests.\n- Add a new benchmark to compare `create` and `contains` operation between `EnumSet` and `HashSet`\n\nCloses #33867 from LuciferYang/SPARK-36613.\n\nAuthored-by: yangjie01 <yangjie01@baidu.com>\nSigned-off-by: Sean Owen <srowen@gmail.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaSourceProvider.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/catalyst/benchmarks/EnumTypeSetBenchmark-jdk11-results.txt", "additions": "104", "deletions": "0", "changes": "104"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/benchmarks/EnumTypeSetBenchmark-results.txt", "additions": "104", "deletions": "0", "changes": "104"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/V1Table.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/CreateTablePartitioningValidationSuite.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/EnumTypeSetBenchmark.scala", "additions": "176", "deletions": "0", "changes": "176"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryTable.scala", "additions": "2", "deletions": "3", "changes": "5"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/noop/NoopDataSource.scala", "additions": "2", "deletions": "4", "changes": "6"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/FileTable.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCTable.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/console.scala", "additions": "1", "deletions": "3", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/memory.scala", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/ForeachWriterTable.scala", "additions": "1", "deletions": "3", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/RateStreamProvider.scala", "additions": "1", "deletions": "3", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/TextSocketSourceProvider.scala", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/memory.scala", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/java/test/org/apache/spark/sql/connector/JavaSimpleBatchTable.java", "additions": "2", "deletions": "3", "changes": "5"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/java/test/org/apache/spark/sql/connector/JavaSimpleWritableDataSource.java", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2Suite.scala", "additions": "1", "deletions": "3", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/FileDataSourceV2FallBackSuite.scala", "additions": "2", "deletions": "3", "changes": "5"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/LocalScanSuite.scala", "additions": "2", "deletions": "3", "changes": "5"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/SimpleWritableDataSource.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/TableCapabilityCheckSuite.scala", "additions": "5", "deletions": "3", "changes": "8"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/V1ReadFallbackSuite.scala", "additions": "1", "deletions": "3", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/V1WriteFallbackSuite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/streaming/sources/StreamingDataSourceV2Suite.scala", "additions": "8", "deletions": "9", "changes": "17"}, "updated": [2, 2, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/streaming/test/DataStreamTableAPISuite.scala", "additions": "3", "deletions": "4", "changes": "7"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/streaming/util/BlockOnStopSource.scala", "additions": "1", "deletions": "3", "changes": "4"}, "updated": [1, 1, 1]}]}
{"author": "Louiszr", "sha": "", "commit_date": "2020/08/16 13:43:23", "commit_message": "Merge remote-tracking branch 'upstream/master'", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/CodeGeneratorWithInterpretedFallback.scala", "additions": "2", "deletions": "3", "changes": "5"}, "updated": [1, 1, 1]}]}
{"author": "unirt", "sha": "", "commit_date": "2020/08/26 18:24:35", "commit_message": "Revert \"[SPARK-32481][CORE][SQL] Support truncate table to move data to trash\"\n\nThis reverts commit 5c077f05805bda1d0db3476ebe32624034d4066c.", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/util/Utils.scala", "additions": "1", "deletions": "22", "changes": "23"}, "updated": [2, 2, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "0", "deletions": "13", "changes": "13"}, "updated": [3, 5, 21]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala", "additions": "1", "deletions": "3", "changes": "4"}, "updated": [2, 2, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/DDLSuite.scala", "additions": "0", "deletions": "72", "changes": "72"}, "updated": [2, 3, 4]}]}
{"author": "titsuki", "sha": "", "commit_date": "2020/11/11 15:13:17", "commit_message": "[SPARK-33415][PYTHON][SQL] Don't encode JVM response in Column.__repr__\n\n### What changes were proposed in this pull request?\n\nRemoves encoding of the JVM response in `pyspark.sql.column.Column.__repr__`.\n\n### Why are the changes needed?\n\nAPI consistency and improved readability of the expressions.\n\n### Does this PR introduce _any_ user-facing change?\n\nBefore this change\n\n    col(\"abc\")\n    col(\"w\u0105\u017c\")\n\nresult in\n\n    Column<b'abc'>\n    Column<b'w\\xc4\\x85\\xc5\\xbc'>\n\nAfter this change we'll get\n\n    Column<'abc'>\n    Column<'w\u0105\u017c'>\n\n### How was this patch tested?\n\nExisting tests and manual inspection.\n\nCloses #30322 from zero323/SPARK-33415.\n\nAuthored-by: zero323 <mszymkiewicz@gmail.com>\nSigned-off-by: HyukjinKwon <gurwls223@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/sql/column.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 2, 3]}, {"file": {"name": "python/pyspark/sql/tests/test_column.py", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [1, 1, 3]}]}
{"author": "ptkool", "sha": "", "commit_date": "2020/09/18 13:24:33", "commit_message": "[SPARK-32936][SQL] Pass all `external/avro` module UTs in Scala 2.13\n\n### What changes were proposed in this pull request?\nThis pr fix all 14 failed cases in `external/avro` module in Scala 2.13, the main change of this pr as follow:\n\n- Manual call `toSeq` in `AvroDeserializer#newWriter` and `SchemaConverters#toSqlTypeHelper` method because the object  type for case match is `ArrayBuffer` not `Seq` in Scala 2.13\n\n- Specified `Seq` to `s.c.Seq` when we call `Row.get(i).asInstanceOf[Seq]` because the data maybe `mutable.ArraySeq` but `Seq` is `immutable.Seq` in Scala 2.13\n\n### Why are the changes needed?\nWe need to support a Scala 2.13 build.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\n\n- Scala 2.12: Pass the Jenkins or GitHub Action\n\n- Scala 2.13: Pass 2.13 Build GitHub Action and do the following:\n\n```\ndev/change-scala-version.sh 2.13\nmvn clean install -DskipTests  -pl external/avro -Pscala-2.13 -am\nmvn clean test -pl external/avro -Pscala-2.13\n```\n\n**Before**\n```\nTests: succeeded 197, failed 14, canceled 0, ignored 2, pending 0\n*** 14 TESTS FAILED ***\n```\n\n**After**\n\n```\nTests: succeeded 211, failed 0, canceled 0, ignored 2, pending 0\nAll tests passed.\n```\n\nCloses #29801 from LuciferYang/fix-external-avro-213.\n\nAuthored-by: yangjie01 <yangjie01@baidu.com>\nSigned-off-by: HyukjinKwon <gurwls223@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "external/avro/src/main/scala/org/apache/spark/sql/avro/AvroDeserializer.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 3]}, {"file": {"name": "external/avro/src/main/scala/org/apache/spark/sql/avro/SchemaConverters.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 2]}, {"file": {"name": "external/avro/src/test/scala/org/apache/spark/sql/avro/AvroSuite.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 1, 3]}]}
{"author": "fuwhu", "sha": "", "commit_date": "2020/09/06 11:23:12", "commit_message": "[SPARK-32548][SQL] - Add Application attemptId support to SQL Rest API\n\n### What changes were proposed in this pull request?\nCurrently, Spark Public Rest APIs support Application attemptId except SQL API. This causes `no such app: application_X` issue when the application has `attemptId` (e.g: YARN cluster mode).\n\nPlease find existing and supported Rest endpoints with attemptId.\n```\n// Existing Rest Endpoints\napplications/{appId}/sql\napplications/{appId}/sql/{executionId}\n\n// Rest Endpoints required support\napplications/{appId}/{attemptId}/sql\napplications/{appId}/{attemptId}/sql/{executionId}\n```\nAlso fixing following compile warning on `SqlResourceSuite`:\n```\n[WARNING] [Warn] ~/spark/sql/core/src/test/scala/org/apache/spark/status/api/v1/sql/SqlResourceSuite.scala:67: Reference to uninitialized value edges\n```\n### Why are the changes needed?\nThis causes `no such app: application_X` issue when the application has `attemptId`.\n\n### Does this PR introduce _any_ user-facing change?\nNot yet because SQL Rest API is being planned to release with `Spark 3.1`.\n\n### How was this patch tested?\n1. New Unit tests are added for existing Rest endpoints. `attemptId` seems not coming in `local-mode` and coming in `YARN cluster mode` so could not be added for `attemptId` case (Suggestions are welcome).\n2. Also, patch has been tested manually through both Spark Core and History Server Rest APIs.\n\nCloses #29364 from erenavsarogullari/SPARK-32548.\n\nAuthored-by: Eren Avsarogullari <erenavsarogullari@gmail.com>\nSigned-off-by: Gengliang Wang <gengliang.wang@databricks.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/status/api/v1/sql/ApiSqlRootResource.scala", "additions": "7", "deletions": "2", "changes": "9"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/status/api/v1/sql/SqlResourceSuite.scala", "additions": "2", "deletions": "3", "changes": "5"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/status/api/v1/sql/SqlResourceWithActualMetricsSuite.scala", "additions": "127", "deletions": "0", "changes": "127"}, "updated": [1, 1, 1]}]}
{"author": "William1104", "sha": "", "commit_date": "2021/04/20 03:11:40", "commit_message": "[SPARK-35080][SQL] Only allow a subset of correlated equality predicates when a subquery is aggregated\n\nThis PR updated the `foundNonEqualCorrelatedPred` logic for correlated subqueries in `CheckAnalysis` to only allow correlated equality predicates that guarantee one-to-one mapping between inner and outer attributes, instead of all equality predicates.\n\nTo fix correctness bugs. Before this fix Spark can give wrong results for certain correlated subqueries that pass CheckAnalysis:\nExample 1:\n```sql\ncreate or replace view t1(c) as values ('a'), ('b')\ncreate or replace view t2(c) as values ('ab'), ('abc'), ('bc')\n\nselect c, (select count(*) from t2 where t1.c = substring(t2.c, 1, 1)) from t1\n```\nCorrect results: [(a, 2), (b, 1)]\nSpark results:\n```\n+---+-----------------+\n|c  |scalarsubquery(c)|\n+---+-----------------+\n|a  |1                |\n|a  |1                |\n|b  |1                |\n+---+-----------------+\n```\nExample 2:\n```sql\ncreate or replace view t1(a, b) as values (0, 6), (1, 5), (2, 4), (3, 3);\ncreate or replace view t2(c) as values (6);\n\nselect c, (select count(*) from t1 where a + b = c) from t2;\n```\nCorrect results: [(6, 4)]\nSpark results:\n```\n+---+-----------------+\n|c  |scalarsubquery(c)|\n+---+-----------------+\n|6  |1                |\n|6  |1                |\n|6  |1                |\n|6  |1                |\n+---+-----------------+\n```\nYes. Users will not be able to run queries that contain unsupported correlated equality predicates.\n\nAdded unit tests.\n\nCloses #32179 from allisonwang-db/spark-35080-subquery-bug.\n\nLead-authored-by: allisonwang-db <66282705+allisonwang-db@users.noreply.github.com>\nCo-authored-by: Wenchen Fan <cloud0fan@gmail.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit bad4b6f025de4946112a0897892a97d5ae6822cf)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala", "additions": "65", "deletions": "12", "changes": "77"}, "updated": [2, 4, 9]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala", "additions": "24", "deletions": "0", "changes": "24"}, "updated": [1, 3, 4]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/udf/udf-except.sql.out", "additions": "10", "deletions": "2", "changes": "12"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala", "additions": "10", "deletions": "1", "changes": "11"}, "updated": [1, 2, 4]}]}
{"author": "nikunjb", "sha": "", "commit_date": "2020/08/24 05:06:08", "commit_message": "[SPARK-32352][SQL][FOLLOW-UP][TEST-HADOOP2.7][TEST-HIVE1.2] Exclude partition columns from data columns\n\n### What changes were proposed in this pull request?\n\nThis PR fixes a bug of #29406. #29406 partially pushes down data filter even if it mixed in partition filters. But in some cases partition columns might be in data columns too. It will possibly push down a predicate with partition column to datasource.\n\n### Why are the changes needed?\n\nThe test \"org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite.save()/load() - partitioned table - simple queries - partition columns in data\" is currently failed with hive-1.2 profile in master branch.\n\n```\n[info] - save()/load() - partitioned table - simple queries - partition columns in data *** FAILED *** (1 second, 457 milliseconds)\n[info]   java.util.NoSuchElementException: key not found: p1\n[info]   at scala.collection.immutable.Map$Map2.apply(Map.scala:138)\n[info]   at org.apache.spark.sql.hive.orc.OrcFilters$.buildLeafSearchArgument(OrcFilters.scala:250)\n[info]   at org.apache.spark.sql.hive.orc.OrcFilters$.convertibleFiltersHelper$1(OrcFilters.scala:143)\n[info]   at org.apache.spark.sql.hive.orc.OrcFilters$.$anonfun$convertibleFilters$4(OrcFilters.scala:146)\n[info]   at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)\n[info]   at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n[info]   at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n[info]   at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n[info]   at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)\n[info]   at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)\n[info]   at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)\n[info]   at org.apache.spark.sql.hive.orc.OrcFilters$.convertibleFilters(OrcFilters.scala:145)\n[info]   at org.apache.spark.sql.hive.orc.OrcFilters$.createFilter(OrcFilters.scala:83)\n[info]   at org.apache.spark.sql.hive.orc.OrcFileFormat.buildReader(OrcFileFormat.scala:142)\n```\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nUnit test.\n\nCloses #29526 from viirya/SPARK-32352-followup.\n\nAuthored-by: Liang-Chi Hsieh <viirya@gmail.com>\nSigned-off-by: Takeshi Yamamuro <yamamuro@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [1, 2, 2]}]}
{"author": "colinmjj", "sha": "", "commit_date": "2020/09/24 03:10:01", "commit_message": "[SPARK-32971][K8S][FOLLOWUP] Add `.toSeq` for Scala 2.13 compilation\n\n### What changes were proposed in this pull request?\n\nThis is a follow-up to fix Scala 2.13 compilation at Kubernetes module.\n\n### Why are the changes needed?\n\nTo fix Scala 2.13 compilation.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nPass the GitHub Action Scala 2.13 compilation job.\n\nCloses #29859 from dongjoon-hyun/SPARK-32971-2.\n\nAuthored-by: Dongjoon Hyun <dhyun@apple.com>\nSigned-off-by: Dongjoon Hyun <dhyun@apple.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/MountVolumesFeatureStep.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [2, 2, 3]}]}
{"author": "wangmiao1981", "sha": "", "commit_date": "2021/02/13 15:32:55", "commit_message": "[SPARK-33434][PYTHON][DOCS] Added RuntimeConfig to PySpark docs\n\n### What changes were proposed in this pull request?\nDocumentation for `SparkSession.conf.isModifiable` is missing from the Python API site, so we added a Configuration section to the Spark SQL page to expose docs for the `RuntimeConfig` class (the class containing `isModifiable`). Then a `:class:` reference to `RuntimeConfig` was added to the `SparkSession.conf` docstring to create a link there as well.\n\n### Why are the changes needed?\nNo docs were generated for `pyspark.sql.conf.RuntimeConfig`.\n\n### Does this PR introduce _any_ user-facing change?\nYes--a new Configuration section to the Spark SQL page and a `Returns` section of the `SparkSession.conf` docstring, so this will now show a link to the `pyspark.sql.conf.RuntimeConfig` page. This is a change compared to both the released Spark version and the unreleased master branch.\n\n### How was this patch tested?\nFirst built the Python docs:\n```bash\ncd $SPARK_HOME/docs\nSKIP_SCALADOC=1 SKIP_RDOC=1 SKIP_SQLDOC=1 jekyll serve\n```\nThen verified all pages and links:\n1. Configuration link displayed on the API Reference page, and it clicks through to Spark SQL page:\nhttp://localhost:4000/api/python/reference/index.html\n![image](https://user-images.githubusercontent.com/1160861/107601918-a2f02380-6bed-11eb-9b8f-974a0681a2a9.png)\n\n2. Configuration section displayed on the Spark SQL page, and the RuntimeConfig link clicks through to the RuntimeConfig page:\nhttp://localhost:4000/api/python/reference/pyspark.sql.html#configuration\n![image](https://user-images.githubusercontent.com/1160861/107602058-0d08c880-6bee-11eb-8cbb-ad8c47588085.png)**\n\n3. RuntimeConfig page displayed:\nhttp://localhost:4000/api/python/reference/api/pyspark.sql.conf.RuntimeConfig.html\n![image](https://user-images.githubusercontent.com/1160861/107602278-94eed280-6bee-11eb-95fc-445ea62ac1a4.png)\n\n4. SparkSession.conf page displays the RuntimeConfig link, and it navigates to the RuntimeConfig page:\nhttp://localhost:4000/api/python/reference/api/pyspark.sql.SparkSession.conf.html\n![image](https://user-images.githubusercontent.com/1160861/107602435-1f373680-6bef-11eb-985a-b72432464940.png)\n\nCloses #31483 from Eric-Lemmon/SPARK-33434-document-isModifiable.\n\nAuthored-by: Eric Lemmon <eric@lemmon.cc>\nSigned-off-by: Sean Owen <srowen@gmail.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/docs/source/reference/pyspark.sql.rst", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [1, 2, 2]}, {"file": {"name": "python/pyspark/sql/session.py", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [1, 1, 2]}]}
{"author": "sujithjay", "sha": "", "commit_date": "2020/10/14 03:13:54", "commit_message": "[SPARK-33134][SQL] Return partial results only for root JSON objects\n\n### What changes were proposed in this pull request?\nIn the PR, I propose to restrict the partial result feature only by root JSON objects. JSON datasource as well as `from_json()` will return `null` for malformed nested JSON objects.\n\n### Why are the changes needed?\n1. To not raise exception to users in the PERMISSIVE mode\n2. To fix a regression and to have the same behavior as Spark 2.4.x has\n3. Current implementation of partial result is supposed to work only for root (top-level) JSON objects, and not tested for bad nested complex JSON fields.\n\n### Does this PR introduce _any_ user-facing change?\nYes. Before the changes, the code below:\n```scala\n    val pokerhand_raw = Seq(\"\"\"[{\"cards\": [19], \"playerId\": 123456}]\"\"\").toDF(\"events\")\n    val event = new StructType().add(\"playerId\", LongType).add(\"cards\", ArrayType(new StructType().add(\"id\", LongType).add(\"rank\", StringType)))\n    val pokerhand_events = pokerhand_raw.select(from_json($\"events\", ArrayType(event)).as(\"event\"))\n    pokerhand_events.show\n```\nthrows the exception even in the default **PERMISSIVE** mode:\n```java\njava.lang.ClassCastException: java.lang.Long cannot be cast to org.apache.spark.sql.catalyst.util.ArrayData\n  at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow.getArray(rows.scala:48)\n  at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow.getArray$(rows.scala:48)\n  at org.apache.spark.sql.catalyst.expressions.GenericInternalRow.getArray(rows.scala:195)\n```\n\nAfter the changes:\n```\n+-----+\n|event|\n+-----+\n| null|\n+-----+\n```\n\n### How was this patch tested?\nAdded a test to `JsonFunctionsSuite`.\n\nCloses #30031 from MaxGekk/json-skip-row-wrong-schema.\n\nAuthored-by: Max Gekk <max.gekk@gmail.com>\nSigned-off-by: HyukjinKwon <gurwls223@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JacksonParser.scala", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/JsonFunctionsSuite.scala", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [1, 1, 1]}]}
{"author": "heuermh", "sha": "", "commit_date": "2021/01/06 17:28:22", "commit_message": "[SPARK-34022][DOCS][FOLLOW-UP] Fix typo in SQL built-in function docs\n\n### What changes were proposed in this pull request?\n\nThis PR is a follow-up of #31061. It fixes a typo in a document: `Finctions` -> `Functions`\n\n### Why are the changes needed?\n\nMake the change better documented.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nN/A\n\nCloses #31069 from kiszk/SPARK-34022-followup.\n\nAuthored-by: Kazuaki Ishizaki <ishizaki@jp.ibm.com>\nSigned-off-by: Dongjoon Hyun <dhyun@apple.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/gen-sql-api-docs.py", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [2, 2, 2]}]}
{"author": "rongou", "sha": "", "commit_date": "2020/11/19 21:36:45", "commit_message": "[MINOR] Structured Streaming statistics page indent fix\n\n### What changes were proposed in this pull request?\nStructured Streaming statistics page code contains an indentation issue. This PR fixes it.\n\n### Why are the changes needed?\nIndent fix.\n\n### Does this PR introduce _any_ user-facing change?\nNo.\n\n### How was this patch tested?\nExisting unit tests.\n\nCloses #30434 from gaborgsomogyi/STAT-INDENT-FIX.\n\nAuthored-by: Gabor Somogyi <gabor.g.somogyi@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/streaming/ui/StreamingQueryStatisticsPage.scala", "additions": "27", "deletions": "27", "changes": "54"}, "updated": [1, 2, 2]}]}
{"author": "techaddict", "sha": "", "commit_date": "2020/10/05 16:30:27", "commit_message": "[SPARK-33038][SQL] Combine AQE initial and current plan string when two plans are the same\n\n### What changes were proposed in this pull request?\nThis PR combines the current plan and the initial plan in the AQE query plan string when the two plans are the same. It also removes the `== Current Plan ==` and `== Initial Plan ==` headers:\n\nBefore\n```scala\nAdaptiveSparkPlan isFinalPlan=false\n+- == Current Plan ==\n   SortMergeJoin [key#13], [a#23], Inner\n   :- Sort [key#13 ASC NULLS FIRST], false, 0\n   :  +- Exchange hashpartitioning(key#13, 5), true, [id=#94]\n            ...\n+- == Initial Plan ==\n   SortMergeJoin [key#13], [a#23], Inner\n   :- Sort [key#13 ASC NULLS FIRST], false, 0\n   :  +- Exchange hashpartitioning(key#13, 5), true, [id=#94]\n            ...\n```\nAfter\n```scala\nAdaptiveSparkPlan isFinalPlan=false\n+- SortMergeJoin [key#13], [a#23], Inner\n   :- Sort [key#13 ASC NULLS FIRST], false, 0\n   :  +- Exchange hashpartitioning(key#13, 5), true, [id=#94]\n            ...\n```\nFor SQL `EXPLAIN` output:\nBefore\n```scala\nAdaptiveSparkPlan (8)\n+- == Current Plan ==\n   Sort (7)\n   +- Exchange (6)\n      ...\n+- == Initial Plan ==\n   Sort (7)\n   +- Exchange (6)\n      ...\n```\nAfter\n```scala\nAdaptiveSparkPlan (8)\n+- Sort (7)\n   +- Exchange (6)\n      ...\n```\n\n### Why are the changes needed?\nTo simplify the AQE plan string by removing the redundant plan information.\n\n### Does this PR introduce _any_ user-facing change?\nYes.\n\n### How was this patch tested?\nModified the existing unit test.\n\nCloses #29915 from allisonwang-db/aqe-explain.\n\nAuthored-by: allisonwang-db <66282705+allisonwang-db@users.noreply.github.com>\nSigned-off-by: Xiao Li <gatorsmile@gmail.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "32", "deletions": "18", "changes": "50"}, "updated": [1, 1, 6]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "13", "deletions": "110", "changes": "123"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 1, 8]}]}
{"author": "tcondie", "sha": "", "commit_date": "2020/09/01 13:04:24", "commit_message": "[SPARK-32761][SQL] Allow aggregating multiple foldable distinct expressions\n\n### What changes were proposed in this pull request?\nFor queries with multiple foldable distinct columns, since they will be eliminated during\nexecution, it's not mandatory to let `RewriteDistinctAggregates` handle this case. And\nin the current code, `RewriteDistinctAggregates` *dose* miss some \"aggregating with\nmultiple foldable distinct expressions\" cases.\nFor example: `select count(distinct 2), count(distinct 2, 3)` will be missed.\n\nBut in the planner, this will trigger an error that \"multiple distinct expressions\" are not allowed.\nAs the foldable distinct columns can be eliminated finally, we can allow this in the aggregation\nplanner check.\n\n### Why are the changes needed?\nbug fix\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nadded test case\n\nCloses #29607 from linhongliu-db/SPARK-32761.\n\nAuthored-by: Linhong Liu <linhong.liu@databricks.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [1, 3, 9]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [1, 2, 5]}]}
{"author": "frreiss", "sha": "", "commit_date": "2020/11/04 16:35:10", "commit_message": "[SPARK-33338][SQL] GROUP BY using literal map should not fail\n\n### What changes were proposed in this pull request?\n\nThis PR aims to fix `semanticEquals` works correctly on `GetMapValue` expressions having literal maps with `ArrayBasedMapData` and `GenericArrayData`.\n\n### Why are the changes needed?\n\nThis is a regression from Apache Spark 1.6.x.\n```scala\nscala> sc.version\nres1: String = 1.6.3\n\nscala> sqlContext.sql(\"SELECT map('k1', 'v1')[k] FROM t GROUP BY map('k1', 'v1')[k]\").show\n+---+\n|_c0|\n+---+\n| v1|\n+---+\n```\n\nApache Spark 2.x ~ 3.0.1 raise`RuntimeException` for the following queries.\n```sql\nCREATE TABLE t USING ORC AS SELECT map('k1', 'v1') m, 'k1' k\nSELECT map('k1', 'v1')[k] FROM t GROUP BY 1\nSELECT map('k1', 'v1')[k] FROM t GROUP BY map('k1', 'v1')[k]\nSELECT map('k1', 'v1')[k] a FROM t GROUP BY a\n```\n\n**BEFORE**\n```scala\nCaused by: java.lang.RuntimeException: Couldn't find k#3 in [keys: [k1], values: [v1][k#3]#6]\n\tat scala.sys.package$.error(package.scala:27)\n\tat org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1$$anonfun$applyOrElse$1.apply(BoundAttribute.scala:85)\n\tat org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1$$anonfun$applyOrElse$1.apply(BoundAttribute.scala:79)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n```\n\n**AFTER**\n```sql\nspark-sql> SELECT map('k1', 'v1')[k] FROM t GROUP BY 1;\nv1\nTime taken: 1.278 seconds, Fetched 1 row(s)\nspark-sql> SELECT map('k1', 'v1')[k] FROM t GROUP BY map('k1', 'v1')[k];\nv1\nTime taken: 0.313 seconds, Fetched 1 row(s)\nspark-sql> SELECT map('k1', 'v1')[k] a FROM t GROUP BY a;\nv1\nTime taken: 0.265 seconds, Fetched 1 row(s)\n```\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nPass the CIs with the newly added test case.\n\nCloses #30246 from dongjoon-hyun/SPARK-33338.\n\nAuthored-by: Dongjoon Hyun <dhyun@apple.com>\nSigned-off-by: Dongjoon Hyun <dhyun@apple.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/literals.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 2, 2]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ComplexTypeSuite.scala", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala", "additions": "12", "deletions": "0", "changes": "12"}, "updated": [1, 2, 3]}]}
{"author": "wesleydias", "sha": "", "commit_date": "2020/10/26 12:41:56", "commit_message": "[SPARK-33204][UI] The 'Event Timeline' area cannot be opened when a spark application has some failed jobs\n\n### What changes were proposed in this pull request?\nThe page returned by /jobs in Spark UI will  store the detail information of each job in javascript like this:\n```javascript\n{\n  'className': 'executor added',\n  'group': 'executors',\n  'start': new Date(1602834008978),\n  'content': '<div class=\"executor-event-content\"' +\n    'data-toggle=\"tooltip\" data-placement=\"top\"' +\n    'data-title=\"Executor 3<br>' +\n    'Added at 2020/10/16 15:40:08\"' +\n    'data-html=\"true\">Executor 3 added</div>'\n}\n```\nif an application has a failed job, the failure reason corresponding to the job will be stored in the ` content`  field in the javascript . if the failure  reason contains the character: **'**,   the  javascript code will throw an exception to cause the `event timeline url` had no response \uff0c The following is an example of error json:\n```javascript\n{\n  'className': 'executor removed',\n  'group': 'executors',\n  'start': new Date(1602925908654),\n  'content': '<div class=\"executor-event-content\"' +\n    'data-toggle=\"tooltip\" data-placement=\"top\"' +\n    'data-title=\"Executor 2<br>' +\n    'Removed at 2020/10/17 17:11:48' +\n    '<br>Reason: Container from a bad node: ...   20/10/17 16:00:42 WARN ShutdownHookManager: ShutdownHook **'$anon$2'** timeout...\"' +\n    'data-html=\"true\">Executor 2 removed</div>'\n}\n```\n\nSo we need to considier this special case , if the returned job info contains the character:**'**, just remove it\n\n### Why are the changes needed?\n\nEnsure that the UI page can function normally\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nThis pr only  fixes an exception in a special case, manual test result as blows:\n\n![fixed](https://user-images.githubusercontent.com/52202080/96711638-74490580-13d0-11eb-93e0-b44d9ed5da5c.gif)\n\nCloses #30119 from akiyamaneko/timeline_view_cannot_open.\n\nAuthored-by: neko <echohlne@gmail.com>\nSigned-off-by: Gengliang Wang <gengliang.wang@databricks.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 1, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 1, 1]}]}
{"author": "huaxingao", "sha": "c92ea97c02a4f008446cf88481af9359a86f510d", "commit_date": "2021/08/04 15:50:20", "commit_message": "[SPARK-34952][SQL] Aggregate (Min/Max/Count) push down for Parquet", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.HiveParquetSourceSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [1, 3, 17]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetUtils.scala", "additions": "219", "deletions": "0", "changes": "219"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetPartitionReaderFactory.scala", "additions": "101", "deletions": "22", "changes": "123"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetScan.scala", "additions": "31", "deletions": "3", "changes": "34"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetScanBuilder.scala", "additions": "94", "deletions": "7", "changes": "101"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/FileScanSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetAggregatePushDownSuite.scala", "additions": "520", "deletions": "0", "changes": "520"}, "updated": [0, 0, 0]}]}
{"author": "huaxingao", "sha": "1965f773825e4f6cd14b71d7f3d32550daaad4c5", "commit_date": "2021/09/19 22:36:37", "commit_message": "Migrate CreateTableStatement to v2 command framework", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.DSV2SQLInsertTestSuite", "org.apache.spark.sql.connector.DataSourceV2SQLSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 8]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveCatalogs.scala", "additions": "5", "deletions": "10", "changes": "15"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala", "additions": "7", "deletions": "3", "changes": "10"}, "updated": [0, 2, 8]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/statements.scala", "additions": "0", "deletions": "19", "changes": "19"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2Commands.scala", "additions": "23", "deletions": "4", "changes": "27"}, "updated": [0, 0, 5]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/CatalogV2Util.scala", "additions": "2", "deletions": "7", "changes": "9"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala", "additions": "8", "deletions": "12", "changes": "20"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/rules.scala", "additions": "46", "deletions": "0", "changes": "46"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala", "additions": "7", "deletions": "2", "changes": "9"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala", "additions": "9", "deletions": "6", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/PlanResolutionSuite.scala", "additions": "26", "deletions": "19", "changes": "45"}, "updated": [0, 0, 2]}]}
{"author": "huaxingao", "sha": "14a819aedc631746f037f3bf7e07b65215d1cbd8", "commit_date": "2021/08/16 17:45:03", "commit_message": "[SPARK-36526][SQL] DSV2 Index Support: Add supportsIndex interface", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/index/SupportsIndex.java", "additions": "84", "deletions": "0", "changes": "84"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/index/TableIndex.java", "additions": "90", "deletions": "0", "changes": "90"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/AlreadyExistException.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/NoSuchItemException.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}]}
{"author": "AngersZhuuuu", "sha": "71f6b17c91b92a85913f816085290a0212e46939", "commit_date": "2021/08/25 11:27:56", "commit_message": "fix ut", "title": "[SPARK-36579][SQL] Make spark source stagingDir can use user defined", "body": "### What changes were proposed in this pull request?\r\nMake spark source stagingDir can use user defined like hive\r\n\r\n### Why are the changes needed?\r\nMake spark source stagingDir can use user defined then we can do some optimization on this\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nUser can define staging dir by `spark.sql.source.stagingDir`\r\n\r\n### How was this patch tested?\r\nAdded UT\r\n", "failed_tests": ["org.apache.spark.sql.avro.AvroV1Suite", "org.apache.spark.sql.avro.AvroV2Suite", "org.apache.spark.ml.source.libsvm.LibSVMRelationSuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.execution.HiveDDLSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.InsertSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.execution.datasources.csv.CSVLegacyTimeParserSuite", "org.apache.spark.sql.execution.datasources.text.TextV1Suite", "org.apache.spark.sql.execution.datasources.text.TextV2Suite", "org.apache.spark.sql.execution.datasources.csv.CSVv1Suite", "org.apache.spark.sql.execution.datasources.orc.OrcSourceSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormatV1Suite", "org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormatV2Suite", "org.apache.spark.sql.execution.datasources.json.JsonV1Suite", "org.apache.spark.sql.execution.datasources.json.JsonV2Suite", "org.apache.spark.sql.execution.datasources.json.JsonLegacyTimeParserSuite", "org.apache.spark.sql.execution.datasources.csv.CSVv2Suite", "org.apache.spark.sql.sources.InsertSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/internal/io/FileCommitProtocol.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "12", "deletions": "0", "changes": "12"}, "updated": [0, 2, 18]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala", "additions": "2", "deletions": "11", "changes": "13"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SQLHadoopMapReduceCommitProtocol.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/util/SQLFileCommitProtocolUtils.scala", "additions": "115", "deletions": "0", "changes": "115"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/sources/InsertSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/sources/PartitionedWriteSuite.scala", "additions": "27", "deletions": "0", "changes": "27"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/SaveAsHiveFile.scala", "additions": "7", "deletions": "89", "changes": "96"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/InsertSuite.scala", "additions": "13", "deletions": "8", "changes": "21"}, "updated": [0, 0, 0]}]}
{"author": "AngersZhuuuu", "sha": "892b557875158c6e127de081cff98d06b18e14bf", "commit_date": "2021/08/25 02:25:50", "commit_message": "update", "title": " [SPARK-36563][SQL] dynamicPartitionOverwrite can direct rename to targetPath instead of partition path one by one when targetPath is empty", "body": "### What changes were proposed in this pull request?\r\nWhen target path is empty, we can directly rename stagingDir to targetPath avoid to rename path one by one\r\n\r\n\r\n### Why are the changes needed?\r\nOptimize file commit logic\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nUser can set `spark.sql.source.stagingDir` to enable  direct rename to targetPath instead of partition path one by one when targetPath is empty for dynamicPartitionOverWrite\r\n\r\n\r\n### How was this patch tested?\r\nAdded UT\r\n", "failed_tests": ["org.apache.spark.shuffle.KubernetesLocalDiskShuffleDataIOSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala", "additions": "28", "deletions": "18", "changes": "46"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "12", "deletions": "0", "changes": "12"}, "updated": [0, 2, 18]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SQLHadoopMapReduceCommitProtocol.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/sources/PartitionedWriteSuite.scala", "additions": "27", "deletions": "0", "changes": "27"}, "updated": [0, 0, 0]}]}
{"author": "AngersZhuuuu", "sha": "2f7b98bbfb17eb68f66500ee23030684bb7046fe", "commit_date": "2021/08/12 03:09:27", "commit_message": "[SPARK-36485][SQL] Support cast type constructed string as year month interval", "title": "[SPARK-36485][SQL] Support cast type constructed string as year month interval", "body": "### What changes were proposed in this pull request?\r\nSpark support type constructed string as  year month interval such as \r\n```\r\ninterval '1 year 2 month'\r\ninterval '3 year'\r\n```\r\nAnd PGSQL support\r\n```\r\ncast('1 year 2 month' as interval year)\r\ncast('1 year 2 month' as interval year to month)\r\ncast('1 year 2 month' as interval month)\r\n```\r\netc, spark can support this too.\r\n\r\n\r\n### Why are the changes needed?\r\nSupport cast  same type constructed string as year month interval\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nuser can cast such as `1 year 2 month` string as year month interval\r\n\r\n### How was this patch tested?\r\nAdded UT", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/IntervalUtils.scala", "additions": "29", "deletions": "2", "changes": "31"}, "updated": [0, 1, 14]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/cast.sql", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/cast.sql.out", "additions": "57", "deletions": "1", "changes": "58"}, "updated": [0, 0, 1]}]}
{"author": "AngersZhuuuu", "sha": "91cdc0a6bd992f72d1773b37c5f8b811271aad1c", "commit_date": "2021/08/12 03:53:43", "commit_message": "[SPARK-36486][SQL] Support cast type constructed same string to day time interval", "title": "[SPARK-36486][SQL] Support cast type constructed same string to day time interval", "body": "### What changes were proposed in this pull request?\r\nSpark support type constructed string as  day time interval such as \r\n```\r\ninterval '1 day 2 hour'\r\n```\r\nAnd PGSQL support\r\n```\r\ncast('1 day 2 hour' as interval day)\r\ncast('1 day 2 hour' as interval day to hour)\r\n```\r\netc, spark can support this too.\r\n\r\n\r\n### Why are the changes needed?\r\nSupport cast  same type constructed string as day time interval\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nuser can cast such as `1 day 2 hour` string as day time interval\r\n\r\n### How was this patch tested?\r\nAdded UT", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/IntervalUtils.scala", "additions": "51", "deletions": "2", "changes": "53"}, "updated": [0, 1, 14]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/cast.sql", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/cast.sql.out", "additions": "113", "deletions": "1", "changes": "114"}, "updated": [0, 0, 1]}]}
{"author": "AngersZhuuuu", "sha": "34f9d1ad28c7e9f3cfbae47be72349e45c855fe6", "commit_date": "2021/08/31 09:39:53", "commit_message": "[SPARK-36624][YARN] In yarn client mode, when ApplicationMaster failed with KILLED/FAILED, driver should exit with code not 0", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "docs/running-on-yarn.md", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/config.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/yarn/src/main/scala/org/apache/spark/scheduler/cluster/YarnClientSchedulerBackend.scala", "additions": "9", "deletions": "1", "changes": "10"}, "updated": [0, 0, 0]}]}
{"author": "AngersZhuuuu", "sha": "8058f28c46b122d023b09857ffaec55f02ba09b6", "commit_date": "2021/09/08 06:17:18", "commit_message": "[SPARK-36691][PYSPARK] PythonRunner failed should pass error message to ApplicationMaster too", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/SparkException.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 2, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/PythonRunner.scala", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/Utils.scala", "additions": "75", "deletions": "0", "changes": "75"}, "updated": [1, 1, 5]}, {"file": {"name": "resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala", "additions": "3", "deletions": "4", "changes": "7"}, "updated": [0, 1, 1]}]}
{"author": "AngersZhuuuu", "sha": "d90816e4798ad0ada5af2a7f5e60a405e6b8e98d", "commit_date": "2021/08/18 08:31:54", "commit_message": "[SPARK-36540][YARN]YARN-CLIENT mode should check Shutdown message when AMEndpoint disconencted", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "docs/running-on-yarn.md", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala", "additions": "13", "deletions": "2", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/config.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/yarn/src/main/scala/org/apache/spark/scheduler/cluster/YarnClientSchedulerBackend.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/yarn/src/main/scala/org/apache/spark/scheduler/cluster/YarnSchedulerBackend.scala", "additions": "11", "deletions": "2", "changes": "13"}, "updated": [0, 0, 0]}]}
{"author": "AngersZhuuuu", "sha": "b33ad90675910357f976266099883f8c7067441e", "commit_date": "2021/08/11 02:50:19", "commit_message": "[SPARK-36437][CORE] Add non-unified memory peak metrics", "title": "", "body": "", "failed_tests": ["org.apache.spark.deploy.history.HistoryServerSuite", "org.apache.spark.util.JsonProtocolSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/metrics/ExecutorMetricType.scala", "additions": "12", "deletions": "0", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/api/v1/PrometheusResource.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/complete_stage_list_json_expectation.json", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/excludeOnFailure_for_stage_expectation.json", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/excludeOnFailure_node_for_stage_expectation.json", "additions": "12", "deletions": "0", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/executor_list_json_expectation.json", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/executor_list_with_executor_metrics_json_expectation.json", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/executor_memory_usage_expectation.json", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/executor_node_excludeOnFailure_expectation.json", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/executor_node_excludeOnFailure_unexcluding_expectation.json", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/failed_stage_list_json_expectation.json", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/one_stage_attempt_json_details_with_failed_task_expectation.json", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/one_stage_attempt_json_expectation.json", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/one_stage_json_expectation.json", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/one_stage_json_with_details_expectation.json", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/stage_list_json_expectation.json", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/stage_list_with_accumulable_json_expectation.json", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/stage_list_with_peak_metrics_expectation.json", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/stage_with_accumulable_json_expectation.json", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/stage_with_peak_metrics_expectation.json", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/stage_with_summaries_expectation.json", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/util/JsonProtocolSuite.scala", "additions": "19", "deletions": "8", "changes": "27"}, "updated": [0, 0, 0]}, {"file": {"name": "docs/monitoring.md", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 1]}]}
{"author": "Peng-Lei", "sha": "414aabe1938954d3bf56a7f93d23b6ef469a1622", "commit_date": "2021/09/23 07:12:40", "commit_message": "add draft", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.avro.AvroV1Suite", "org.apache.spark.sql.avro.AvroV2Suite", "org.apache.spark.sql.HiveCharVarcharTestSuite", "org.apache.spark.sql.execution.datasources.FileIndexSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetRebaseDatetimeV2Suite", "org.apache.spark.sql.execution.datasources.parquet.ParquetRebaseDatetimeV1Suite", "org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite", "org.apache.spark.sql.FileSourceCharVarcharTestSuite", "org.apache.spark.sql.sources.InsertSuite"], "files": [{"file": {"name": "R/pkg/tests/fulltests/test_sparkSQL.R", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [1, 2, 3]}, {"file": {"name": "core/src/main/resources/error/error-classes.json", "additions": "70", "deletions": "0", "changes": "70"}, "updated": [0, 2, 4]}, {"file": {"name": "core/src/main/scala/org/apache/spark/SparkException.scala", "additions": "5", "deletions": "3", "changes": "8"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala", "additions": "69", "deletions": "78", "changes": "147"}, "updated": [0, 2, 4]}]}
{"author": "Peng-Lei", "sha": "21a94544c539ee2896be81f3988104820efc0068", "commit_date": "2021/09/24 07:02:49", "commit_message": "add draft", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "docs/sql-ref-ansi-compliance.md", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveCatalogs.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 1, 7]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/statements.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2Commands.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/CatalogManager.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [0, 0, 0]}]}
{"author": "Peng-Lei", "sha": "7912e186729193d315ca01641147e386224bb970", "commit_date": "2021/09/16 08:29:05", "commit_message": "add draft", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.DynamicPartitionPruningV2Suite", "org.apache.spark.sql.DSV2SQLInsertTestSuite", "org.apache.spark.sql.connector.DataSourceV2SQLSuite", "org.apache.spark.sql.DynamicPartitionPruningV2Suite"], "files": [{"file": {"name": "R/pkg/tests/fulltests/test_sparkSQL.R", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [1, 2, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 1, 9]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/internal/CatalogImpl.scala", "additions": "58", "deletions": "23", "changes": "81"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala", "additions": "44", "deletions": "0", "changes": "44"}, "updated": [0, 0, 0]}]}
{"author": "Peng-Lei", "sha": "67d3622b9f836b39da812259ad5d284d5e62c233", "commit_date": "2021/07/14 05:59:17", "commit_message": "Keep consistent with the namespace naming rule", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/CatalogManager.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 7, 16]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 1, 2]}]}
{"author": "Peng-Lei", "sha": "759dd0a179e3a38e6bc8c3b692ac82df0c7ef8bb", "commit_date": "2021/07/01 13:42:53", "commit_message": "Add the show catalogs feature", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "docs/sql-ref-ansi-compliance.md", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 5]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveCatalogs.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [1, 3, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [1, 5, 15]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/statements.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 3, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2Commands.scala", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [1, 3, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/CatalogManager.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [1, 3, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ShowCatalogsExec.scala", "additions": "47", "deletions": "0", "changes": "47"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala", "additions": "21", "deletions": "0", "changes": "21"}, "updated": [1, 1, 1]}]}
{"author": "itholic", "sha": "d930f8925720672cb6120ff73826b662f88bc2a0", "commit_date": "2021/09/27 10:35:34", "commit_message": "[SPARK-36438] Support list-like Python objects for Series comparison", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.data_type_ops.test_categorical_ops"], "files": [{"file": {"name": "python/pyspark/pandas/base.py", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [1, 3, 3]}, {"file": {"name": "python/pyspark/pandas/series.py", "additions": "17", "deletions": "1", "changes": "18"}, "updated": [1, 5, 7]}, {"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames.py", "additions": "69", "deletions": "0", "changes": "69"}, "updated": [0, 3, 5]}, {"file": {"name": "python/pyspark/pandas/tests/test_series.py", "additions": "36", "deletions": "0", "changes": "36"}, "updated": [0, 4, 9]}]}
{"author": "itholic", "sha": "0b79e4aef9b8832accebd18d4d4a8135a6721fec", "commit_date": "2021/09/27 07:38:27", "commit_message": "Implement MultiIndex.equal_levels", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.indexes.test_base"], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/indexing.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 3]}, {"file": {"name": "python/pyspark/pandas/indexes/multi.py", "additions": "37", "deletions": "0", "changes": "37"}, "updated": [1, 1, 2]}, {"file": {"name": "python/pyspark/pandas/tests/indexes/test_base.py", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 5]}, {"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames.py", "additions": "37", "deletions": "0", "changes": "37"}, "updated": [0, 3, 5]}]}
{"author": "ueshin", "sha": "0a43396ce3da47024db39f27ffcc9f28911cf1ab", "commit_date": "2021/09/24 20:45:41", "commit_message": "Inline most of type hint files under pyspark/sql/pandas folder", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/sql/pandas/_typing/protocols/frame.pyi", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/pandas/conversion.py", "additions": "88", "deletions": "32", "changes": "120"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/sql/pandas/conversion.pyi", "additions": "0", "deletions": "59", "changes": "59"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/sql/pandas/group_ops.py", "additions": "32", "deletions": "14", "changes": "46"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/pandas/group_ops.pyi", "additions": "0", "deletions": "49", "changes": "49"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/pandas/map_ops.py", "additions": "11", "deletions": "3", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/pandas/map_ops.pyi", "additions": "0", "deletions": "30", "changes": "30"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/pandas/typehints.py", "additions": "20", "deletions": "4", "changes": "24"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/pandas/types.py", "additions": "28", "deletions": "14", "changes": "42"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/sql/pandas/utils.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}]}
{"author": "ueshin", "sha": "cd0f7070b4a504d2aba57d7e4b71fcc225731603", "commit_date": "2021/09/21 00:08:22", "commit_message": "Implement ps.merge_asof.", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/namespace.py", "additions": "497", "deletions": "0", "changes": "497"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_reshape.py", "additions": "138", "deletions": "0", "changes": "138"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/dataframe.py", "additions": "117", "deletions": "0", "changes": "117"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 0, 8]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/DeduplicateRelations.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 2, 9]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteAsOfJoin.scala", "additions": "89", "deletions": "0", "changes": "89"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/joinTypes.scala", "additions": "21", "deletions": "0", "changes": "21"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala", "additions": "116", "deletions": "0", "changes": "116"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/rules/RuleIdCollection.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreePatterns.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/RewriteAsOfJoinSuite.scala", "additions": "289", "deletions": "0", "changes": "289"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala", "additions": "90", "deletions": "27", "changes": "117"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameAsOfJoinSuite.scala", "additions": "169", "deletions": "0", "changes": "169"}, "updated": [0, 0, 0]}]}
{"author": "dongjoon-hyun", "sha": "7a581fc89fa38f921def1de4924bdae9df9d647e", "commit_date": "2021/09/24 02:33:48", "commit_message": "[SPARK-36837][BUILD] Upgrade Kafka to 3.0.0", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.kafka010.KafkaRelationSuite", "org.apache.spark.sql.kafka010.KafkaRelationSuite", "org.apache.spark.sql.kafka010.KafkaRelationSuite", "org.apache.spark.sql.kafka010.KafkaRelationSuite"], "files": [{"file": {"name": "external/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaTestUtils.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "external/kafka-0-10/src/test/scala/org/apache/spark/streaming/kafka010/KafkaRDDSuite.scala", "additions": "11", "deletions": "4", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "external/kafka-0-10/src/test/scala/org/apache/spark/streaming/kafka010/KafkaTestUtils.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "pom.xml", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [2, 12, 26]}]}
{"author": "c21", "sha": "b22479a85a4d7992b81305df27af50c9915dabf0", "commit_date": "2021/09/17 19:00:08", "commit_message": "Ignore duplicated join keys when building relation for LEFT/ANTI hash join", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.TPCHPlanStabilitySuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilitySuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilityWithStatsSuite", "org.apache.spark.sql.execution.debug.DebuggingSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/BroadcastExchangeExec.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastHashJoinExec.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashJoin.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashedRelation.scala", "additions": "24", "deletions": "12", "changes": "36"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/joins/ShuffledHashJoinExec.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q10.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q10/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q19.sf100/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q19/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q27.sf100/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q27/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q3.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q3/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q34.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q34/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q42.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q42/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q43.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q43/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q46.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q46/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q52.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q52/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q53.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q53/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q55.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q55/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q59.sf100/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q59/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q63.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q63/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q65.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q65/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q68.sf100/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q68/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q7.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q7/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q73.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q73/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q79.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q79/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q89.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q89/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q98.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q98/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q1.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q1/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q10.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q10/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q11.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q11/explain.txt", "additions": "9", "deletions": "9", "changes": "18"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q12.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q12/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q13.sf100/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q13/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a.sf100/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a/explain.txt", "additions": "9", "deletions": "9", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b.sf100/explain.txt", "additions": "8", "deletions": "8", "changes": "16"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b/explain.txt", "additions": "11", "deletions": "11", "changes": "22"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q15.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q15/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q16.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q16/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q17.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q17/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q18.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q18/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q19.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q19/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q2.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q2/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q20.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q20/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q21.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q21/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q22.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q22/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23a.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23a/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23b.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23b/explain.txt", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24a.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24a/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24b.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24b/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q25.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q25/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q26.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q26/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q27.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q27/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q29.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q29/explain.txt", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q3.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q3/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q30.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q30/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q31.sf100/explain.txt", "additions": "8", "deletions": "8", "changes": "16"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q31/explain.txt", "additions": "9", "deletions": "9", "changes": "18"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q32.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q32/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q33.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q33/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q35.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q35/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q36.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q36/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q37.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q37/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q38.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q38/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q39a.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q39a/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q39b.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q39b/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q4.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q4/explain.txt", "additions": "13", "deletions": "13", "changes": "26"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q40.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q40/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q41.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q41/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q42.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q42/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q43.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q43/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q44.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q44/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q45.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q45/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q46.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q46/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q47.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q47/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q48.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q48/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q49.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q49/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q5.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q5/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q50.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q50/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q51.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q51/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q52.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q52/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q53.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q53/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q54.sf100/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q54/explain.txt", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q55.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q55/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q56.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q56/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q57.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q57/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q58.sf100/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q58/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q59.sf100/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q59/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q60.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q60/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q61.sf100/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q61/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q62.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q62/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q63.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q63/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q65.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q65/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q66.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q66/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q67.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q67/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q68.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q68/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q69.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q69/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q7.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q7/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q70.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q70/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q71.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q71/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q72.sf100/explain.txt", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q72/explain.txt", "additions": "9", "deletions": "9", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q73.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q73/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q76.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q76/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q77.sf100/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q77/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q79.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q79/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q8.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q8/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q80.sf100/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q80/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q81.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q81/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q82.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q82/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q83.sf100/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q83/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q84.sf100/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q84/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q85.sf100/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q85/explain.txt", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q86.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q86/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q87.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q87/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q88.sf100/explain.txt", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q88/explain.txt", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q89.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q89/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q90.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q90/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q91.sf100/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q91/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q92.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q92/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q93.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q93/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q94.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q94/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q95.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q95/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q96.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q96/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q97.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q97/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q98.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q98/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q99.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q99/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q10a.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q10a/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q11.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q11/explain.txt", "additions": "9", "deletions": "9", "changes": "18"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q12.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q12/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14.sf100/explain.txt", "additions": "8", "deletions": "8", "changes": "16"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14/explain.txt", "additions": "11", "deletions": "11", "changes": "22"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a.sf100/explain.txt", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a/explain.txt", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q18a.sf100/explain.txt", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q18a/explain.txt", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q20.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q20/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q22.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q22/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q22a.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q22a/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q24.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q24/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q27a.sf100/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q27a/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q34.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q34/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35a.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35a/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q36a.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q36a/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q47.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q47/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q49.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q49/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q51a.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q51a/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q57.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q57/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q5a.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q5a/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q6.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q6/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q64.sf100/explain.txt", "additions": "9", "deletions": "9", "changes": "18"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q64/explain.txt", "additions": "13", "deletions": "13", "changes": "26"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q67a.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q67a/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q70a.sf100/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q70a/explain.txt", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q72.sf100/explain.txt", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q72/explain.txt", "additions": "9", "deletions": "9", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q74.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q74/explain.txt", "additions": "9", "deletions": "9", "changes": "18"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q75.sf100/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q75/explain.txt", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q77a.sf100/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q77a/explain.txt", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q78.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q78/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q80a.sf100/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q80a/explain.txt", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q86a.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q86a/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q98.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "LucaCanali", "sha": "903feb290e31e898705e177c7b3fb8c477567eee", "commit_date": "2021/07/28 09:28:16", "commit_message": "Implement SQL metrics for Python UDF.", "title": "[SPARK-34265][WIP][PYTHON] Instrument Python UDFs using SQL metrics ", "body": "### What changes are proposed in this pull request?\r\n\r\nThis proposes to add SQLMetrics instrumentation for Python UDF execution.\r\nThe proposed metrics are:\r\n\r\n- data sent to Python workers\r\n- data returned from Python workers\r\n- number of rows processed\r\n\r\n\r\n### Why are the changes needed?\r\nThis aims at improving monitoring and performance troubleshooting of Python UDFs.\r\nIn particular as an aid to answer performance-related questions such as:\r\nwhy is the UDF slow?, how much work it has done so far?, etc.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nSQL metrics are made available in the WEB UI.  \r\nSee the following examples:  \r\n\r\n![image1](https://user-images.githubusercontent.com/5243162/127323340-f0132da1-e19c-4d81-b5dc-a534ea9346ee.png)\r\n  \r\n![image2](https://issues.apache.org/jira/secure/attachment/13031153/Python_UDF_instrumentation_lite_BatchEvalPython.png)\r\n\r\n### How was this patch tested?\r\n\r\nManually tested + a Python unit test has been added.\r\n\r\nExample code used for testing:\r\n\r\n```\r\nfrom pyspark.sql.functions import col, pandas_udf\r\nimport time\r\n\r\n@pandas_udf(\"long\")\r\ndef test_pandas(col1):\r\n  time.sleep(0.02)\r\n  return col1 * col1\r\n\r\nspark.udf.register(\"test_pandas\", test_pandas)\r\nspark.sql(\"select rand(42)*rand(51)*rand(12) col1 from range(10000000)\").createOrReplaceTempView(\"t1\")\r\nspark.sql(\"select max(test_pandas(col1)) from t1\").collect()\r\n```\r\n\r\nThis is used to test with more data pushed to the Python workers\r\n\r\n```\r\nfrom pyspark.sql.functions import col, pandas_udf\r\nimport time\r\n\r\n@pandas_udf(\"long\")\r\ndef test_pandas(col1,col2,col3,col4,col5,col6,col7,col8,col9,col10,col11,col12,col13,col14,col15,col16,col17):\r\n  time.sleep(0.02)\r\n  return col1\r\n\r\nspark.udf.register(\"test_pandas\", test_pandas)\r\nspark.sql(\"select rand(42)*rand(51)*rand(12) col1 from range(10000000)\").createOrReplaceTempView(\"t1\")\r\nspark.sql(\"select max(test_pandas(col1,col1+1,col1+2,col1+3,col1+4,col1+5,col1+6,col1+7,col1+8,col1+9,col1+10,col1+11,col1+12,col1+13,col1+14,col1+15,col1+16)) from t1\").collect()\r\n```\r\n\r\nThis is for testing Python UDF (non pandas)\r\n\r\n`from pyspark.sql.functions import udf; spark.range(100).select(udf(lambda x: x/1)(\"id\")).collect()`\r\n  ", "failed_tests": ["org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "dev/sparktestsupport/modules.py", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [1, 1, 10]}, {"file": {"name": "docs/web-ui.md", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [1, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_pandas_sqlmetrics.py", "additions": "58", "deletions": "0", "changes": "58"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/python/AggregateInPandasExec.scala", "additions": "5", "deletions": "2", "changes": "7"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowEvalPythonExec.scala", "additions": "5", "deletions": "2", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowPythonRunner.scala", "additions": "8", "deletions": "1", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/python/BatchEvalPythonExec.scala", "additions": "7", "deletions": "2", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/python/CoGroupedArrowPythonRunner.scala", "additions": "8", "deletions": "1", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/python/FlatMapCoGroupsInPandasExec.scala", "additions": "5", "deletions": "2", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/python/FlatMapGroupsInPandasExec.scala", "additions": "5", "deletions": "2", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/python/MapInPandasExec.scala", "additions": "5", "deletions": "2", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/python/PythonArrowOutput.scala", "additions": "10", "deletions": "1", "changes": "11"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/python/PythonSQLMetrics.scala", "additions": "37", "deletions": "0", "changes": "37"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/python/PythonUDFRunner.scala", "additions": "8", "deletions": "1", "changes": "9"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/python/WindowInPandasExec.scala", "additions": "5", "deletions": "2", "changes": "7"}, "updated": [0, 0, 0]}]}
{"author": "peter-toth", "sha": "d86d2c48a3e6244fc091ae09cb9377ade98f66b0", "commit_date": "2021/04/21 17:40:19", "commit_message": "[SPARK-34079][SQL] Merging non-correlated scalar subqueries to multi-column scalar subqueries for better reuse", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueries.scala", "additions": "416", "deletions": "0", "changes": "416"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala", "additions": "32", "deletions": "1", "changes": "33"}, "updated": [2, 8, 16]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreePatterns.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [1, 3, 3]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueriesSuite.scala", "additions": "282", "deletions": "0", "changes": "282"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap.java", "additions": "2", "deletions": "7", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "8", "deletions": "3", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/PlanAdaptiveSubqueries.scala", "additions": "18", "deletions": "6", "changes": "24"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/ReuseAdaptiveSubquery.scala", "additions": "5", "deletions": "3", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala", "additions": "2", "deletions": "8", "changes": "10"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "0", "deletions": "9", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "21", "deletions": "0", "changes": "21"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/reuse/ReuseExchangeAndSubquery.scala", "additions": "10", "deletions": "3", "changes": "13"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala", "additions": "18", "deletions": "2", "changes": "20"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9.sf100/explain.txt", "additions": "183", "deletions": "598", "changes": "781"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9.sf100/simplified.txt", "additions": "65", "deletions": "170", "changes": "235"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9/explain.txt", "additions": "183", "deletions": "598", "changes": "781"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9/simplified.txt", "additions": "65", "deletions": "170", "changes": "235"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala", "additions": "72", "deletions": "0", "changes": "72"}, "updated": [1, 3, 10]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala", "additions": "37", "deletions": "0", "changes": "37"}, "updated": [1, 2, 4]}]}
{"author": "planga82", "sha": "533cb7ca056e746ce0d4047662e0ae5574d47c36", "commit_date": "2021/08/15 22:47:03", "commit_message": "Fix style", "title": "[SPARK-36453][SQL] Improve consistency processing floating point special literals", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nSpecial literals in floating point are not consistent between cast and json expressions\r\n```\r\nscala> spark.sql(\"SELECT CAST('+Inf' as Double)\").show\r\n+--------------------+                                                        \r\n|CAST(+Inf AS DOUBLE)|\r\n+--------------------+\r\n|            Infinity|\r\n+--------------------+\r\n```\r\n```\r\nscala> val schema =  StructType(StructField(\"a\", DoubleType) :: Nil)\r\n\r\nscala> Seq(\"\"\"{\"a\" : \"+Inf\"}\"\"\").toDF(\"col1\").select(from_json(col(\"col1\"),schema)).show\r\n+---------------+\r\n|from_json(col1)|\r\n+---------------+\r\n|         {null}|\r\n+---------------+\r\n\r\nscala> Seq(\"\"\"{\"a\" : \"+Inf\"}\"\"\").toDF(\"col\").withColumn(\"col\", from_json(col(\"col\"), StructType.fromDDL(\"a DOUBLE\"))).write.json(\"/tmp/jsontests12345\")\r\nscala> spark.read.schema(StructType(Seq(StructField(\"col\",schema)))).json(\"/tmp/jsontests12345\").show\r\n+------+\r\n|   col|\r\n+------+\r\n|{null}|\r\n+------+\r\n```\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nImprove consistency between operations\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nYes, we are going to support the same special literal in Cast and Json expressions\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nUnit testing and manual testing", "failed_tests": ["pyspark.pandas.tests.data_type_ops.test_string_ops", "pyspark.pandas.tests.test_dataframe", "org.apache.spark.sql.catalyst.expressions.AnsiCastSuite", "org.apache.spark.sql.catalyst.expressions.AnsiCastSuite", "org.apache.spark.sql.catalyst.expressions.CastSuite", "org.apache.spark.sql.catalyst.expressions.TryCastSuite", "org.apache.spark.sql.catalyst.expressions.CastSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SQLQuerySuite", "org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite", "org.apache.spark.sql.DynamicPartitionPruningV1Suite", "org.apache.spark.sql.DynamicPartitionPruningV2Suite", "org.apache.spark.sql.DynamicPartitionPruningV1Suite", "org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite", "org.apache.spark.sql.DataFrameWindowFunctionsSuite", "org.apache.spark.sql.SubquerySuite", "org.apache.spark.sql.DynamicPartitionPruningV2Suite", "org.apache.spark.sql.DataFrameSuite", "org.apache.spark.sql.ColumnExpressionSuite", "org.apache.spark.sql.execution.datasources.json.JsonV1Suite", "org.apache.spark.sql.execution.datasources.json.JsonV2Suite", "org.apache.spark.sql.execution.datasources.json.JsonLegacyTimeParserSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala", "additions": "5", "deletions": "20", "changes": "25"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ExprUtils.scala", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JacksonParser.scala", "additions": "10", "deletions": "10", "changes": "20"}, "updated": [0, 4, 7]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/json/JacksonParserSuite.scala", "additions": "27", "deletions": "12", "changes": "39"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "28", "deletions": "0", "changes": "28"}, "updated": [1, 2, 11]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/json/JsonSuite.scala", "additions": "0", "deletions": "17", "changes": "17"}, "updated": [0, 1, 3]}]}
{"author": "planga82", "sha": "02a265d4eb38a1b562db6ce02e56f739a16d9ce7", "commit_date": "2021/09/08 22:53:55", "commit_message": "Implementation & tests", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite", "org.apache.spark.sql.DataFrameSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 2, 18]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [1, 1, 8]}]}
{"author": "dgd-contributor", "sha": "b4bf4d2eb5962a8f5c4baec92927f960c4a2cfb8", "commit_date": "2021/08/19 09:06:26", "commit_message": "resolve suggested change", "title": "[SPARK-36099][CORE] Grouping exception in core/util", "body": "### What changes were proposed in this pull request?\r\nThis PR group exception messages in core/src/main/scala/org/apache/spark/util\r\n\r\n### Why are the changes needed?\r\nIt will largely help with standardization of error messages and its maintenance.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo. Error messages remain unchanged.\r\n\r\n### How was this patch tested?\r\nNo new tests - pass all original tests to make sure it doesn't break any existing behavior.", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/errors/SparkCoreErrors.scala", "additions": "152", "deletions": "5", "changes": "157"}, "updated": [0, 2, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala", "additions": "5", "deletions": "8", "changes": "13"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/DependencyUtils.scala", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/KeyLock.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/ListenerBus.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/NextIterator.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/SerializableBuffer.scala", "additions": "5", "deletions": "3", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/ThreadUtils.scala", "additions": "5", "deletions": "4", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/Utils.scala", "additions": "18", "deletions": "30", "changes": "48"}, "updated": [0, 1, 6]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/collection/AppendOnlyMap.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/collection/CompactBuffer.scala", "additions": "5", "deletions": "4", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala", "additions": "6", "deletions": "5", "changes": "11"}, "updated": [0, 0, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/collection/ImmutableBitSet.scala", "additions": "8", "deletions": "6", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/collection/MedianHeap.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/collection/OpenHashSet.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/collection/PartitionedPairBuffer.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/collection/PrimitiveVector.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/io/ChunkedByteBuffer.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/logging/DriverLogger.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/random/RandomSampler.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}]}
{"author": "dgd-contributor", "sha": "a89af71e932f932e05a22caca449cd9b4872d865", "commit_date": "2021/08/19 05:22:39", "commit_message": "resolve suggested change", "title": "[SPARK-36100][CORE] Grouping exception in core/status", "body": "### What changes were proposed in this pull request?\r\nThis PR group exception messages in core/src/main/scala/org/apache/spark/status\r\n\r\n### Why are the changes needed?\r\nIt will largely help with standardization of error messages and its maintenance.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo. Error messages remain unchanged.\r\n\r\n### How was this patch tested?\r\nNo new tests - pass all original tests to make sure it doesn't break any existing behavior.", "failed_tests": ["org.apache.spark.deploy.history.HistoryServerSuite", "org.apache.spark.status.AppStatusListenerSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/errors/SparkCoreErrors.scala", "additions": "103", "deletions": "0", "changes": "103"}, "updated": [0, 2, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusStore.scala", "additions": "4", "deletions": "5", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/KVUtils.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/LiveEntity.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/api/v1/ApiRootResource.scala", "additions": "9", "deletions": "8", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/api/v1/OneApplicationResource.scala", "additions": "12", "deletions": "12", "changes": "24"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/api/v1/SimpleDateParam.scala", "additions": "3", "deletions": "9", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/api/v1/StagesResource.scala", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [0, 0, 0]}]}
{"author": "dgd-contributor", "sha": "909fe78c89b3e603ea7130b83ce640a9957c8aa5", "commit_date": "2021/08/30 03:51:19", "commit_message": "[SPARK-36303]: Refactor fourteenth set of 20 query execution errors to use error classes", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/resources/error/error-classes.json", "additions": "73", "deletions": "0", "changes": "73"}, "updated": [0, 1, 5]}, {"file": {"name": "core/src/main/scala/org/apache/spark/SparkException.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [1, 3, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/CatalogNotFoundException.scala", "additions": "13", "deletions": "1", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala", "additions": "69", "deletions": "31", "changes": "100"}, "updated": [0, 1, 6]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 2, 14]}, {"file": {"name": "sql/catalyst/src/test/java/org/apache/spark/sql/connector/catalog/CatalogLoadingSuite.java", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/RuntimeConfig.scala", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [0, 0, 0]}]}
{"author": "dgd-contributor", "sha": "6227f49e2f407b9e07c03b3c124eb679248065c1", "commit_date": "2021/08/19 08:26:13", "commit_message": "[SPARK-36296]: Refactor seventh set of 20 query execution errors to use error classes", "title": "", "body": "", "failed_tests": ["org.apache.spark.SparkThrowableSuite"], "files": [{"file": {"name": "core/src/main/resources/error/error-classes.json", "additions": "65", "deletions": "0", "changes": "65"}, "updated": [0, 1, 5]}, {"file": {"name": "core/src/main/scala/org/apache/spark/SparkException.scala", "additions": "21", "deletions": "7", "changes": "28"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala", "additions": "38", "deletions": "31", "changes": "69"}, "updated": [0, 0, 8]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 4]}]}
{"author": "dgd-contributor", "sha": "271ad2d90bdf7a078a3068f796de404397c7634a", "commit_date": "2021/09/16 07:54:52", "commit_message": "[SPARK-36711] Support multi-index in new syntax", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_typedef", "pyspark.pandas.tests.indexes.test_datetime"], "files": [{"file": {"name": "python/pyspark/pandas/accessors.py", "additions": "36", "deletions": "18", "changes": "54"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "16", "deletions": "8", "changes": "24"}, "updated": [1, 3, 11]}, {"file": {"name": "python/pyspark/pandas/groupby.py", "additions": "15", "deletions": "8", "changes": "23"}, "updated": [0, 1, 10]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "26", "deletions": "0", "changes": "26"}, "updated": [0, 1, 6]}, {"file": {"name": "python/pyspark/pandas/typedef/typehints.py", "additions": "159", "deletions": "87", "changes": "246"}, "updated": [1, 2, 2]}]}
{"author": "dgd-contributor", "sha": "65ad6074f481bfc8665684d1f96905c7aca47ba3", "commit_date": "2021/08/27 09:54:40", "commit_message": "[SPARK-36292]: Refactor third set of 20 query execution errors to use error classes", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "core/src/main/resources/error/error-classes.json", "additions": "71", "deletions": "2", "changes": "73"}, "updated": [1, 2, 5]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [2, 4, 14]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala", "additions": "41", "deletions": "25", "changes": "66"}, "updated": [1, 1, 6]}]}
{"author": "dgd-contributor", "sha": "8a3b657745e5dedd40445e965d079233bd4beacc", "commit_date": "2021/09/22 09:45:16", "commit_message": "[SPARK-36742][PYTHON] Fix ps.to_datetime with plurals of keys like years, months, days", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_namespace", "pyspark.pandas.tests.test_series"], "files": [{"file": {"name": "python/pyspark/pandas/namespace.py", "additions": "23", "deletions": "2", "changes": "25"}, "updated": [0, 1, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_namespace.py", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [0, 0, 1]}]}
{"author": "dgd-contributor", "sha": "0a4b38e04b79991e327c9b38f3111b8e6636cab5", "commit_date": "2021/09/24 02:10:42", "commit_message": "[SPARK-36302][SQL] Refactor thirteenth set of 20 query execution errors to use error classes", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/resources/error/error-classes.json", "additions": "68", "deletions": "0", "changes": "68"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala", "additions": "73", "deletions": "57", "changes": "130"}, "updated": [0, 1, 4]}]}
{"author": "dgd-contributor", "sha": "37a2a0de0b71c10e117987d778d80e1d2ea1c822", "commit_date": "2021/08/25 08:02:14", "commit_message": "Add needed classes from pr 36107", "title": "", "body": "", "failed_tests": ["org.apache.spark.SparkThrowableSuite", "org.apache.spark.sql.catalyst.expressions.StringExpressionsSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "core/src/main/resources/error/error-classes.json", "additions": "64", "deletions": "0", "changes": "64"}, "updated": [1, 2, 6]}, {"file": {"name": "core/src/main/scala/org/apache/spark/SparkException.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [1, 1, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala", "additions": "25", "deletions": "25", "changes": "50"}, "updated": [1, 1, 8]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 5, 11]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 5, 9]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/regexp-functions.sql.out", "additions": "12", "deletions": "12", "changes": "24"}, "updated": [0, 0, 0]}]}
{"author": "dgd-contributor", "sha": "b658e3535c7c8e4b6b78163e5fd2069820f6256a", "commit_date": "2021/09/21 01:51:17", "commit_message": "[SPARK-36293][SQL] Refactor fourth set of 20 query execution errors to use error classes", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/resources/error/error-classes.json", "additions": "51", "deletions": "0", "changes": "51"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/AnalysisException.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala", "additions": "28", "deletions": "27", "changes": "55"}, "updated": [1, 2, 4]}]}
{"author": "dgd-contributor", "sha": "838f41303b58701ced288152bb95f8938ebacc66", "commit_date": "2021/07/28 15:38:09", "commit_message": "[SPARK-36096][CORE] Grouping exception in core/resource", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/errors/SparkCoreErrors.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [1, 1, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/resource/ResourceProfile.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/resource/ResourceUtils.scala", "additions": "5", "deletions": "4", "changes": "9"}, "updated": [0, 0, 0]}]}
{"author": "dgd-contributor", "sha": "d5f894eed43d9b0b4f41c846c7c2aca25a74c2dd", "commit_date": "2021/08/25 13:02:40", "commit_message": "[SPARK-36402][PYTHON] Implement series.combine", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_ops_on_diff_frames"], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/series.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 9]}, {"file": {"name": "python/pyspark/pandas/missing/series.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/series.py", "additions": "168", "deletions": "0", "changes": "168"}, "updated": [0, 0, 10]}, {"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames.py", "additions": "38", "deletions": "0", "changes": "38"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_series.py", "additions": "61", "deletions": "0", "changes": "61"}, "updated": [0, 1, 4]}]}
{"author": "dgd-contributor", "sha": "909e6a50094fa288da5de891abd9258d8ae05751", "commit_date": "2021/08/23 15:12:18", "commit_message": "[SPARK-36396] Implement_DataFrame.cov\n\nupdate", "title": "", "body": "", "failed_tests": ["pyspark.pandas.frame"], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/frame.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "180", "deletions": "0", "changes": "180"}, "updated": [0, 0, 13]}, {"file": {"name": "python/pyspark/pandas/missing/frame.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "57", "deletions": "1", "changes": "58"}, "updated": [0, 0, 6]}]}
{"author": "dgd-contributor", "sha": "6f699396ad96604ef69b7cc8aa2745ee470580fe", "commit_date": "2021/09/09 01:23:16", "commit_message": "[SPARK-36671][PYTHON] Support Series.__and__ and Series.__or__ for integral", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.data_type_ops.test_num_ops", "pyspark.pandas.tests.test_ops_on_diff_frames"], "files": [{"file": {"name": "python/pyspark/pandas/data_type_ops/base.py", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [1, 1, 6]}, {"file": {"name": "python/pyspark/pandas/data_type_ops/boolean_ops.py", "additions": "13", "deletions": "4", "changes": "17"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/data_type_ops/num_ops.py", "additions": "50", "deletions": "0", "changes": "50"}, "updated": [0, 0, 5]}, {"file": {"name": "python/pyspark/pandas/tests/data_type_ops/test_num_ops.py", "additions": "68", "deletions": "22", "changes": "90"}, "updated": [1, 1, 6]}, {"file": {"name": "python/pyspark/pandas/tests/data_type_ops/testing_utils.py", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames.py", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 2, 2]}]}
{"author": "dgd-contributor", "sha": "d096bd616a63d5b887a4bc05fa7967c628ab7027", "commit_date": "2021/08/17 09:04:19", "commit_message": "[SPARK-36304]: Refactor fifteenth set of 20 query execution errors to use error classes", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.util.ArrowUtilsSuite", "org.apache.spark.sql.catalyst.encoders.EncoderErrorMessageSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.execution.streaming.sources.TextSocketStreamSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.execution.streaming.sources.RateStreamProviderSuite"], "files": [{"file": {"name": "core/src/main/resources/error/error-classes.json", "additions": "76", "deletions": "2", "changes": "78"}, "updated": [1, 1, 5]}, {"file": {"name": "core/src/main/scala/org/apache/spark/SparkException.scala", "additions": "16", "deletions": "1", "changes": "17"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala", "additions": "69", "deletions": "39", "changes": "108"}, "updated": [0, 0, 9]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/RowTest.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/encoders/EncoderErrorMessageSuite.scala", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/util/ArrowUtilsSuite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/sources/RateStreamProviderSuite.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/sources/TextSocketStreamSuite.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}]}
{"author": "dgd-contributor", "sha": "711ff22baafa74f254b7feb5db3bc461fbd910a3", "commit_date": "2021/07/27 13:56:28", "commit_message": "[SPARK-36102][CORE] Grouping exception in core/deploy", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/FaultToleranceTest.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/RRunner.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/SparkSubmitArguments.scala", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/StandaloneResourceUtils.scala", "additions": "2", "deletions": "5", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/history/ApplicationCache.scala", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/history/EventLogFileWriters.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala", "additions": "7", "deletions": "11", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/master/Master.scala", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionClient.scala", "additions": "13", "deletions": "14", "changes": "27"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolException.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolMessage.scala", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolRequest.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/security/HadoopFSDelegationTokenProvider.scala", "additions": "3", "deletions": "4", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/worker/DriverRunner.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala", "additions": "5", "deletions": "4", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/deploy/worker/ui/LogPage.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/errors/SparkCoreErrors.scala", "additions": "202", "deletions": "2", "changes": "204"}, "updated": [0, 0, 0]}]}
{"author": "venkata91", "sha": "ac1659e156eca5899e1eff765698c9986eec5d4c", "commit_date": "2020/06/10 21:02:55", "commit_message": "Basic speculation metrics summary for a stage", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/stagepage.js", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/stagespage-template.html", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/webui.css", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusListener.scala", "additions": "19", "deletions": "0", "changes": "19"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusStore.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [1, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/LiveEntity.scala", "additions": "26", "deletions": "0", "changes": "26"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/api/v1/api.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/storeTypes.scala", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/application_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/completed_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/limit_app_list_json_expectation.json", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/minDate_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/minEndDate_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/stage_with_speculation_summary_expectation.json", "additions": "507", "deletions": "0", "changes": "507"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/spark-events/application_1628109047826_1317105", "additions": "52", "deletions": "0", "changes": "52"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/deploy/history/HistoryServerSuite.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 1, 4]}, {"file": {"name": "core/src/test/scala/org/apache/spark/status/AppStatusListenerSuite.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/status/AppStatusStoreSuite.scala", "additions": "56", "deletions": "1", "changes": "57"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/ui/StagePageSuite.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "dev/.rat-excludes", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 2]}]}
{"author": "venkata91", "sha": "5b611bbb0857c43747088721c48590d151361e45", "commit_date": "2021/09/02 04:53:49", "commit_message": "SPARK-33701: Adaptive shuffle merge finalization", "title": "", "body": "", "failed_tests": ["org.apache.spark.shuffle.ShuffleBlockPusherSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/Dependency.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/SparkEnv.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [0, 1, 6]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala", "additions": "190", "deletions": "58", "changes": "248"}, "updated": [0, 0, 4]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerEvent.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessage.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [1, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/shuffle/ShuffleBlockPusher.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 7]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/ThreadUtils.scala", "additions": "9", "deletions": "1", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/DAGSchedulerSuite.scala", "additions": "161", "deletions": "3", "changes": "164"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/test/scala/org/apache/spark/shuffle/ShuffleBlockPusherSuite.scala", "additions": "34", "deletions": "0", "changes": "34"}, "updated": [0, 0, 5]}]}
{"author": "ulysses-you", "sha": "5beb51810dfec69964e570d90d0634e5a8e0499d", "commit_date": "2021/08/13 14:26:41", "commit_message": "fix", "title": "[SPARK-36321][K8S] Do not fail application in kubernetes if name is too long", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nUse short string as executor pod name prefix if app name is long.\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nIf we have a long spark app name and start with k8s master, we will get the execption.\r\n```\r\njava.lang.IllegalArgumentException: 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa-89fe2f7ae71c3570' in spark.kubernetes.executor.podNamePrefix is invalid. must conform https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#dns-label-names and the value length <= 47\r\n\tat org.apache.spark.internal.config.TypedConfigBuilder.$anonfun$checkValue$1(ConfigBuilder.scala:108)\r\n\tat org.apache.spark.internal.config.TypedConfigBuilder.$anonfun$transform$1(ConfigBuilder.scala:101)\r\n\tat scala.Option.map(Option.scala:230)\r\n\tat org.apache.spark.internal.config.OptionalConfigEntry.readFrom(ConfigEntry.scala:239)\r\n\tat org.apache.spark.internal.config.OptionalConfigEntry.readFrom(ConfigEntry.scala:214)\r\n\tat org.apache.spark.SparkConf.get(SparkConf.scala:261)\r\n\tat org.apache.spark.deploy.k8s.KubernetesConf.get(KubernetesConf.scala:67)\r\n\tat org.apache.spark.deploy.k8s.KubernetesExecutorConf.<init>(KubernetesConf.scala:147)\r\n\tat org.apache.spark.deploy.k8s.KubernetesConf$.createExecutorConf(KubernetesConf.scala:231)\r\n\tat org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$requestNewExecutors$2(ExecutorPodsAllocator.scala:367)\r\n```\r\nUse app name as the executor pod name is the Spark internal behavior and we should not make application failure.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nyes\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nAdd test\r\n\r\nthe new log:\r\n```\r\n21/07/28 09:35:53 INFO SparkEnv: Registering OutputCommitCoordinator\r\n21/07/28 09:35:54 INFO Utils: Successfully started service 'SparkUI' on port 41926.\r\n21/07/28 09:35:54 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://:41926\r\n21/07/28 09:35:54 WARN KubernetesClusterManager: Use spark-c460617aeac0fda9 as the executor pod's name prefix due to spark.app.name is too long. Please set 'spark.kubernetes.executor.podNamePrefix' if you need a custom executor pod's name prefix.\r\n21/07/28 09:35:54 INFO SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file\r\n21/07/28 09:35:55 WARN Utils: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.\r\n```\r\n\r\nverify the config:\r\n![image](https://user-images.githubusercontent.com/12025282/127258223-fbcaaac8-451d-4c55-8c09-e802511a510d.png)\r\n\r\nverify the executor pod name\r\n![image](https://user-images.githubusercontent.com/12025282/127258284-be15b862-b826-4440-9a11-023d69c61fc4.png)\r\n", "failed_tests": [], "files": [{"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPi.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPiWithoutAppName.scala", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "4", "deletions": "14", "changes": "18"}, "updated": [1, 1, 5]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala", "additions": "19", "deletions": "7", "changes": "26"}, "updated": [0, 0, 3]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesUtils.scala", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala", "additions": "10", "deletions": "6", "changes": "16"}, "updated": [0, 0, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterManager.scala", "additions": "10", "deletions": "2", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/KubernetesConfSuite.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 0, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 3]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/BasicTestsSuite.scala", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 1, 1]}]}
{"author": "ulysses-you", "sha": "4ed226f66643523f3661b53a28517383cf1f0eb5", "commit_date": "2021/09/22 06:39:11", "commit_message": "Support broadcast nested loop join hint for equi-join", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "docs/sql-ref-syntax-qry-select-hints.md", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/hints.scala", "additions": "10", "deletions": "1", "changes": "11"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "14", "deletions": "2", "changes": "16"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/LogicalQueryStageStrategy.scala", "additions": "12", "deletions": "6", "changes": "18"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/JoinHintSuite.scala", "additions": "38", "deletions": "0", "changes": "38"}, "updated": [0, 0, 2]}]}
{"author": "ulysses-you", "sha": "a846ecd5221bc4b21416c9c52552cdaa0e683d0d", "commit_date": "2021/04/07 10:02:05", "commit_message": "Support coalesce partition through union", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.execution.CoalesceShufflePartitionsSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "78", "deletions": "52", "changes": "130"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/CoalesceShufflePartitionsSuite.scala", "additions": "1", "deletions": "3", "changes": "4"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "86", "deletions": "1", "changes": "87"}, "updated": [0, 1, 5]}]}
{"author": "xinrong-databricks", "sha": "8b7b395c0faa78d8809489eaf6f88a013fcfe2a0", "commit_date": "2021/08/10 21:37:08", "commit_message": "empty", "title": "[WIP][SPARK-36397][PYTHON] Implement DataFrame.mode", "body": "### What changes were proposed in this pull request?\r\nImplement DataFrame.mode (along index axis).\r\n\r\n\r\n### Why are the changes needed?\r\nGet the mode(s) of each element along the selected axis is a common functionality, which is supported in pandas. We should support that.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes. `DataFrame.mode` can be used now.\r\n\r\n```py\r\n>>> psdf = ps.DataFrame(\r\n...     [(\"bird\", 2, 2), (\"mammal\", 4, np.nan), (\"arthropod\", 8, 0), (\"bird\", 2, np.nan)],\r\n...     index=(\"falcon\", \"horse\", \"spider\", \"ostrich\"),\r\n...     columns=(\"species\", \"legs\", \"wings\"),\r\n... )\r\n>>> psdf\r\n           species  legs  wings                                                 \r\nfalcon        bird     2    2.0\r\nhorse       mammal     4    NaN\r\nspider   arthropod     8    0.0\r\nostrich       bird     2    NaN\r\n\r\n>>> psdf.mode()\r\n  species  legs  wings\r\n0    bird   2.0    0.0\r\n1    None   NaN    2.0\r\n\r\n>>> psdf.mode(dropna=False)\r\n  species  legs  wings\r\n0    bird     2    NaN\r\n\r\n>>> psdf.mode(numeric_only=True)\r\n   legs  wings\r\n0   2.0    0.0\r\n1   NaN    2.0\r\n```\r\n\r\n### How was this patch tested?\r\nUnit tests.\r\n", "failed_tests": ["pyspark.pandas.tests.test_dataframe"], "files": [{"file": {"name": "python/docs/source/reference/pyspark.pandas/frame.rst", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/frame.py", "additions": "103", "deletions": "1", "changes": "104"}, "updated": [0, 5, 20]}, {"file": {"name": "python/pyspark/pandas/missing/frame.py", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [0, 2, 6]}]}
{"author": "xinrong-databricks", "sha": "87172874344406fac65abb26f072c06855674b7f", "commit_date": "2021/08/31 22:56:40", "commit_message": "test case", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/pandas/indexing.py", "additions": "74", "deletions": "18", "changes": "92"}, "updated": [0, 0, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}]}
{"author": "viirya", "sha": "190fa2b796454125d83a90309b17a1f970e90fe0", "commit_date": "2021/09/20 16:46:14", "commit_message": "Remove unnecessary broadcast check.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "13", "deletions": "0", "changes": "13"}, "updated": [0, 2, 8]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/PlanAdaptiveDynamicPruningFilters.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/PlanAdaptiveSubqueries.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/dynamicpruning/PartitionPruning.scala", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 2, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala", "additions": "14", "deletions": "9", "changes": "23"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DynamicPartitionPruningSuite.scala", "additions": "6", "deletions": "5", "changes": "11"}, "updated": [0, 2, 3]}]}
{"author": "yaooqinn", "sha": "9e8b227e91070c35b98247acdbf4caf022cfaf72", "commit_date": "2021/08/20 13:52:21", "commit_message": "address comments", "title": "[SPARK-36477][SQL] Inferring schema from JSON file shall handle CharConversionException/MalformedInputException", "body": "\r\n<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\nWhen set `ignoreCorruptFiles=true`, reading JSON still fails with corrupt files during inferring schema.\r\n\r\n```scala\r\njava.io.CharConversionException: Unsupported UCS-4 endianness (2143) detected\r\n\tat com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.reportWeirdUCS4(ByteSourceJsonBootstrapper.java:504)\r\n\tat com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.checkUTF32(ByteSourceJsonBootstrapper.java:471)\r\n\tat com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.detectEncoding(ByteSourceJsonBootstrapper.java:144)\r\n\tat com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.constructParser(ByteSourceJsonBootstrapper.java:247)\r\n\tat com.fasterxml.jackson.core.JsonFactory._createParser(JsonFactory.java:1528)\r\n\tat com.fasterxml.jackson.core.JsonFactory.createParser(JsonFactory.java:1034)\r\n\tat org.apache.spark.sql.catalyst.json.CreateJacksonParser$.internalRow(CreateJacksonParser.scala:86)\r\n\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.$anonfun$inferFromDataset$4(JsonDataSource.scala:107)\r\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$3(JsonInferSchema.scala:66)\r\n\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2621)\r\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$2(JsonInferSchema.scala:66)\r\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\r\n\tat scala.collection.TraversableOnce.reduceLeft(TraversableOnce.scala:190)\r\n\tat scala.collection.TraversableOnce.reduceLeft$(TraversableOnce.scala:183)\r\n\tat scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1429)\r\n\tat scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:208)\r\n\tat scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:207)\r\n\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1429)\r\n\tat scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:215)\r\n\tat scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:215)\r\n\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1429)\r\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:81)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n```\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\nbugfix\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nyes, ignoreCorruptFiles will ignore JSON files corrupted\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\n\r\nnew test", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JsonInferSchema.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/json/JsonSuite.scala", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [1, 1, 4]}]}
{"author": "yaooqinn", "sha": "623dc4659e016505d1245bf7637c12d499aa947d", "commit_date": "2021/08/11 04:22:48", "commit_message": "Update sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala\n\nCo-authored-by: Hyukjin Kwon <gurwls223@gmail.com>", "title": "[SPARK-36180][SQL] Store TIMESTAMP_NTZ into hive catalog as TIMESTAMP", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\nThis PR fix a issue that HMS can not recognize timestamp_ntz by mapping timestamp_ntz to `timestamp` of hive\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\nThe hive 2.3.9 does not have 2 timestamp or a type named timestamp_ntz.\r\nFYI, In hive 3.0, the will be a timestamp with local timezone added.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\n\r\nno, timestamp_ntz is new and not public yet\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\n\r\nnew test", "failed_tests": [], "files": [{"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala", "additions": "15", "deletions": "14", "changes": "29"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/SQLQuerySuite.scala", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [0, 1, 2]}]}
{"author": "yaooqinn", "sha": "a3b7d08983115c84225ce52f8db3dd16efd5471e", "commit_date": "2021/09/03 07:38:14", "commit_message": "[SPARK-36662][SQL] special timestamps support for path filters -  modifiedBefore/modifiedAfter", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "docs/sql-data-sources-generic-options.md", "additions": "11", "deletions": "4", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 2, 12]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/pathFilters.scala", "additions": "6", "deletions": "3", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/PathFilterSuite.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [0, 0, 0]}]}
{"author": "yaooqinn", "sha": "b989aa3821dbaf6cacccde886ed1051068d43cf5", "commit_date": "2021/09/01 08:19:44", "commit_message": "[SPARK-36634][SQL] Support access and read parquet file by column index", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.execution.datasources.parquet.ParquetV2SchemaPruningSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV1SchemaPruningSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 1, 11]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport.scala", "additions": "54", "deletions": "21", "changes": "75"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRecordMaterializer.scala", "additions": "6", "deletions": "2", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "27", "deletions": "13", "changes": "40"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "85", "deletions": "0", "changes": "85"}, "updated": [0, 0, 0]}]}
{"author": "LuciferYang", "sha": "a7eff43e2f65385754e2accc016bc51281f4c594", "commit_date": "2021/08/16 06:24:44", "commit_message": "support orc file meta cache", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "36", "deletions": "0", "changes": "36"}, "updated": [0, 3, 17]}, {"file": {"name": "sql/core/benchmarks/FileMetaCacheReadBenchmark-jdk11-results.txt", "additions": "95", "deletions": "0", "changes": "95"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/benchmarks/FileMetaCacheReadBenchmark-results.txt", "additions": "95", "deletions": "0", "changes": "95"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnarBatchReader.java", "additions": "9", "deletions": "1", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileMetaCacheManager.scala", "additions": "94", "deletions": "0", "changes": "94"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFileFormat.scala", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [0, 2, 5]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFileMeta.scala", "additions": "46", "deletions": "0", "changes": "46"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/orc/OrcPartitionReaderFactory.scala", "additions": "18", "deletions": "3", "changes": "21"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/FileMetaCacheSuite.scala", "additions": "80", "deletions": "0", "changes": "80"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/benchmark/FileMetaCacheReadBenchmark.scala", "additions": "128", "deletions": "0", "changes": "128"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/internal/SQLConfSuite.scala", "additions": "24", "deletions": "0", "changes": "24"}, "updated": [0, 1, 1]}]}
{"author": "LuciferYang", "sha": "f200546c1e6df47210d9d68bb55d11a9cc529035", "commit_date": "2021/08/26 07:59:24", "commit_message": "add Guava cache bad case", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ExternalShuffleBlockResolver.java", "additions": "8", "deletions": "1", "changes": "9"}, "updated": [0, 1, 3]}, {"file": {"name": "common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/RemoteBlockPushResolver.java", "additions": "7", "deletions": "1", "changes": "8"}, "updated": [0, 2, 8]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileStatusCache.scala", "additions": "9", "deletions": "1", "changes": "10"}, "updated": [0, 1, 2]}]}
{"author": "LuciferYang", "sha": "cc6f52ec8272d3c10a3641a495ee4be49408d58f", "commit_date": "2021/08/04 03:27:21", "commit_message": "add a new method to avoid file truncate", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java", "additions": "1", "deletions": "4", "changes": "5"}, "updated": [1, 1, 2]}, {"file": {"name": "core/src/main/scala/org/apache/spark/storage/DiskBlockObjectWriter.scala", "additions": "21", "deletions": "0", "changes": "21"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala", "additions": "1", "deletions": "6", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala", "additions": "1", "deletions": "6", "changes": "7"}, "updated": [1, 2, 3]}]}
{"author": "LuciferYang", "sha": "7d0a912471047a58b8c40240e74805b69c78b758", "commit_date": "2021/09/14 06:09:47", "commit_message": "Replace some guava api usage with j.u.Objects api", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "common/kvstore/src/main/java/org/apache/spark/util/kvstore/InMemoryStore.java", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "common/kvstore/src/main/java/org/apache/spark/util/kvstore/KVStoreView.java", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "common/kvstore/src/main/java/org/apache/spark/util/kvstore/LevelDBTypeInfo.java", "additions": "6", "deletions": "4", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "common/network-common/src/main/java/org/apache/spark/network/client/TransportClient.java", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 1]}, {"file": {"name": "common/network-common/src/main/java/org/apache/spark/network/client/TransportClientFactory.java", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "common/network-common/src/main/java/org/apache/spark/network/crypto/AuthEngine.java", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 2]}, {"file": {"name": "common/network-common/src/main/java/org/apache/spark/network/protocol/AbstractMessage.java", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "common/network-common/src/main/java/org/apache/spark/network/protocol/MergedBlockMetaRequest.java", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [0, 0, 1]}, {"file": {"name": "common/network-common/src/main/java/org/apache/spark/network/protocol/MergedBlockMetaSuccess.java", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "common/network-common/src/main/java/org/apache/spark/network/sasl/SparkSaslServer.java", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "common/network-common/src/main/java/org/apache/spark/network/server/BlockPushNonFatalFailure.java", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 2]}, {"file": {"name": "common/network-common/src/main/java/org/apache/spark/network/server/OneForOneStreamManager.java", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 1]}, {"file": {"name": "common/network-common/src/main/java/org/apache/spark/network/server/TransportServer.java", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "common/network-common/src/main/java/org/apache/spark/network/util/JavaUtils.java", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "common/network-common/src/main/java/org/apache/spark/network/util/LimitedInputStream.java", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ExternalBlockHandler.java", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 4]}, {"file": {"name": "common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/MergedBlockMeta.java", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/protocol/AbstractFetchShuffleBlocks.java", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/protocol/BlockPushReturnCode.java", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 0, 1]}, {"file": {"name": "common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/protocol/FinalizeShuffleMerge.java", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/protocol/MergeStatuses.java", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 1]}, {"file": {"name": "common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/protocol/PushBlockStream.java", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/protocol/mesos/RegisterDriver.java", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "common/network-yarn/src/main/java/org/apache/spark/network/yarn/YarnShuffleService.java", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/java/org/apache/spark/api/java/Optional.java", "additions": "2", "deletions": "4", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/rdd/RDDOperationScope.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "mllib/src/main/scala/org/apache/spark/mllib/linalg/Matrices.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 1, 1]}, {"file": {"name": "mllib/src/main/scala/org/apache/spark/mllib/linalg/distributed/BlockMatrix.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "mllib/src/main/scala/org/apache/spark/mllib/tree/model/InformationGainStats.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "mllib/src/main/scala/org/apache/spark/mllib/tree/model/Predict.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala", "additions": "2", "deletions": "3", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/IdentifierImpl.java", "additions": "2", "deletions": "4", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/BatchScanExec.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/java/test/org/apache/spark/sql/JavaDatasetSuite.java", "additions": "12", "deletions": "13", "changes": "25"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveShim.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}]}
{"author": "LuciferYang", "sha": "104b1256a23300b7f7912c7bf37fc7b14ac5099c", "commit_date": "2020/11/24 11:39:06", "commit_message": "add simple support for Parquet", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.execution.datasources.parquet.ParquetV2QuerySuite", "org.apache.spark.sql.execution.datasources.orc.OrcV2QuerySuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [1, 10, 27]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase.java", "additions": "38", "deletions": "8", "changes": "46"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileMetaCacheManager.scala", "additions": "87", "deletions": "0", "changes": "87"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala", "additions": "15", "deletions": "1", "changes": "16"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileMeta.scala", "additions": "45", "deletions": "0", "changes": "45"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetPartitionReaderFactory.scala", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "34", "deletions": "1", "changes": "35"}, "updated": [0, 0, 0]}]}
{"author": "dnskr", "sha": "7d4d4e39a31a815f97b55228f1123c7f0edb2afa", "commit_date": "2021/08/14 10:33:16", "commit_message": "Merge branch 'apache:master' into docs/SPARK-36510", "title": "[SPARK-36510][DOCS] Add spark.redaction.string.regex property to the docs", "body": "### What changes were proposed in this pull request?\r\nThe PR fixes [SPARK-36510](https://issues.apache.org/jira/browse/SPARK-36510) by adding missing `spark.redaction.string.regex` property to the docs\r\n\r\n### Why are the changes needed?\r\nThe property referred by `spark.sql.redaction.string.regex` description as its default value\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\nNot needed for docs\r\n", "failed_tests": [], "files": [{"file": {"name": "docs/configuration.md", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [1, 2, 3]}]}
{"author": "ocworld", "sha": "554127b11daaab6377e69cff7956f5c6c38c2d62", "commit_date": "2021/04/29 12:38:14", "commit_message": "[SPARK-35084] supporting \"--packages\" in k8s cluster mode", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala", "additions": "12", "deletions": "11", "changes": "23"}, "updated": [1, 3, 10]}]}
{"author": "beliefer", "sha": "7df29e5120f2473579f65e6ddab89cd878ca7ca0", "commit_date": "2021/09/01 08:45:24", "commit_message": "DivideYMInterval should consider ansi mode.", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.SQLQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/intervalExpressions.scala", "additions": "45", "deletions": "10", "changes": "55"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/IntervalExpressionsSuite.scala", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 10]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/interval.sql.out", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 9]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/ColumnExpressionSuite.scala", "additions": "26", "deletions": "2", "changes": "28"}, "updated": [0, 0, 0]}]}
{"author": "beliefer", "sha": "fc088707c35365c8f6c9c804f73b4b7374a6415a", "commit_date": "2021/07/30 11:15:07", "commit_message": "Support TimestampNTZ type in Orc file source", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.orc.HiveOrcQuerySuite", "pyspark.pandas.mlflow"], "files": [{"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcAtomicColumnVector.java", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcDeserializer.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFileFormat.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcOutputWriter.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcSerializer.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcUtils.scala", "additions": "47", "deletions": "15", "changes": "62"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/package.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/FileBasedDataSourceSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcQuerySuite.scala", "additions": "42", "deletions": "0", "changes": "42"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/orc/OrcFileFormat.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/orc/HiveOrcSourceSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 0]}]}
{"author": "beliefer", "sha": "86ea00d27e9979d6057a39ac7c0711131a6b6c26", "commit_date": "2021/07/27 07:37:12", "commit_message": "Refactor first set of 20 query parsing errors to use error classes", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.parser.DDLParserSuite"], "files": [{"file": {"name": "core/src/main/resources/error/error-classes.json", "additions": "52", "deletions": "0", "changes": "52"}, "updated": [0, 2, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala", "additions": "0", "deletions": "2", "changes": "2"}, "updated": [0, 2, 22]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala", "additions": "22", "deletions": "29", "changes": "51"}, "updated": [0, 2, 7]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisTest.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala", "additions": "8", "deletions": "3", "changes": "11"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/ErrorParserSuite.scala", "additions": "26", "deletions": "9", "changes": "35"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala", "additions": "15", "deletions": "8", "changes": "23"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/SparkScriptTransformationSuite.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/ShowPartitionsParserSuite.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/TruncateTableParserSuite.scala", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [0, 0, 0]}]}
{"author": "wangyum", "sha": "3e482ffa6a779aeebe67494f9a766d17ac2e2899", "commit_date": "2021/09/24 06:32:59", "commit_message": "Support DPP if there is no selective predicate on the filtering side", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.TPCDSModifiedPlanStabilitySuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilityWithStatsSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/dynamicpruning/PartitionPruning.scala", "additions": "9", "deletions": "14", "changes": "23"}, "updated": [0, 2, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DynamicPartitionPruningSuite.scala", "additions": "27", "deletions": "0", "changes": "27"}, "updated": [0, 2, 3]}]}
{"author": "wangyum", "sha": "83a4dbdd610d0c35e4e460eea58edcf7776daee5", "commit_date": "2021/09/09 13:38:17", "commit_message": "Remove the Sort if it is the child of RepartitionByExpression", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 10]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/CollapseRepartitionSuite.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 0, 0]}]}
{"author": "wangyum", "sha": "1929f30e3335b90b9e78357f76e247167e69b901", "commit_date": "2021/09/05 09:17:19", "commit_message": "Dynamic Bloom Filter pruning", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite"], "files": [{"file": {"name": "common/sketch/src/main/java/org/apache/spark/util/sketch/BloomFilterImpl.java", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 2, 19]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/DynamicPruning.scala", "additions": "58", "deletions": "1", "changes": "59"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/BuildBloomFilter.scala", "additions": "127", "deletions": "0", "changes": "127"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/predicates.scala", "additions": "76", "deletions": "0", "changes": "76"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 2, 10]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "26", "deletions": "0", "changes": "26"}, "updated": [1, 1, 12]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [1, 2, 6]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SubqueryAdaptiveShuffleExec.scala", "additions": "49", "deletions": "0", "changes": "49"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AQEOptimizer.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/InsertAdaptiveSparkPlan.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeBloomFilterJoin.scala", "additions": "48", "deletions": "0", "changes": "48"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/PlanAdaptiveDynamicPruningFilters.scala", "additions": "83", "deletions": "41", "changes": "124"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/PlanAdaptiveSubqueries.scala", "additions": "5", "deletions": "3", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/dynamicpruning/DynamicBloomFilterPruning.scala", "additions": "191", "deletions": "0", "changes": "191"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/dynamicpruning/DynamicPruningHelper.scala", "additions": "107", "deletions": "0", "changes": "107"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/dynamicpruning/PartitionPruning.scala", "additions": "8", "deletions": "70", "changes": "78"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/dynamicpruning/PlanDynamicPruningFilters.scala", "additions": "8", "deletions": "3", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala", "additions": "57", "deletions": "1", "changes": "58"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q12.sf100/explain.txt", "additions": "121", "deletions": "93", "changes": "214"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q12.sf100/simplified.txt", "additions": "11", "deletions": "6", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q20.sf100/explain.txt", "additions": "121", "deletions": "93", "changes": "214"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q20.sf100/simplified.txt", "additions": "11", "deletions": "6", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q37.sf100/explain.txt", "additions": "37", "deletions": "9", "changes": "46"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q37.sf100/simplified.txt", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q50.sf100/explain.txt", "additions": "121", "deletions": "37", "changes": "158"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q50.sf100/simplified.txt", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q81.sf100/explain.txt", "additions": "128", "deletions": "100", "changes": "228"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q81.sf100/simplified.txt", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q82.sf100/explain.txt", "additions": "37", "deletions": "9", "changes": "46"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q82.sf100/simplified.txt", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q93.sf100/explain.txt", "additions": "98", "deletions": "34", "changes": "132"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q93.sf100/simplified.txt", "additions": "14", "deletions": "3", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q98.sf100/explain.txt", "additions": "129", "deletions": "101", "changes": "230"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q98.sf100/simplified.txt", "additions": "11", "deletions": "6", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q12.sf100/explain.txt", "additions": "121", "deletions": "93", "changes": "214"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q12.sf100/simplified.txt", "additions": "11", "deletions": "6", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q20.sf100/explain.txt", "additions": "121", "deletions": "93", "changes": "214"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q20.sf100/simplified.txt", "additions": "11", "deletions": "6", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q98.sf100/explain.txt", "additions": "125", "deletions": "97", "changes": "222"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q98.sf100/simplified.txt", "additions": "11", "deletions": "6", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DynamicBloomFilterJoinPruningSuite.scala", "additions": "225", "deletions": "0", "changes": "225"}, "updated": [0, 0, 0]}]}
{"author": "wangyum", "sha": "3643e3f26ac8755a4da1e83b36e246cb16b48275", "commit_date": "2020/05/26 09:42:15", "commit_message": "Infer IsNotNull for all children of NullIntolerant expression", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "38", "deletions": "22", "changes": "60"}, "updated": [1, 1, 2]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/InferFiltersFromConstraintsSuite.scala", "additions": "32", "deletions": "11", "changes": "43"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q59.sf100/explain.txt", "additions": "98", "deletions": "80", "changes": "178"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q59.sf100/simplified.txt", "additions": "8", "deletions": "3", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q59/explain.txt", "additions": "98", "deletions": "80", "changes": "178"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q59/simplified.txt", "additions": "8", "deletions": "3", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q2.sf100/explain.txt", "additions": "133", "deletions": "54", "changes": "187"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q2.sf100/simplified.txt", "additions": "25", "deletions": "2", "changes": "27"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q2/explain.txt", "additions": "133", "deletions": "54", "changes": "187"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q2/simplified.txt", "additions": "25", "deletions": "2", "changes": "27"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24a.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24a/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24b.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q24b/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q59.sf100/explain.txt", "additions": "133", "deletions": "73", "changes": "206"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q59.sf100/simplified.txt", "additions": "18", "deletions": "3", "changes": "21"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q59/explain.txt", "additions": "133", "deletions": "73", "changes": "206"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q59/simplified.txt", "additions": "18", "deletions": "3", "changes": "21"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q8.sf100/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q8/explain.txt", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q24.sf100/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q24/explain.txt", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/JoinSuite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}]}
{"author": "wangyum", "sha": "824ba80a0a71bb8c5079c45ec5de6bc9ae198699", "commit_date": "2021/07/10 12:43:25", "commit_message": "First commit", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.JoinSuite", "org.apache.spark.sql.streaming.StreamingOuterJoinSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.streaming.StreamingLeftSemiJoinSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/BroadcastJoinOuterJoinStreamSide.scala", "additions": "48", "deletions": "0", "changes": "48"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 7]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 2, 9]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/BroadcastJoinOuterJoinStreamSideSuite.scala", "additions": "108", "deletions": "0", "changes": "108"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 1, 6]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a.sf100/explain.txt", "additions": "634", "deletions": "533", "changes": "1167"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a.sf100/simplified.txt", "additions": "97", "deletions": "70", "changes": "167"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b.sf100/explain.txt", "additions": "569", "deletions": "468", "changes": "1037"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b.sf100/simplified.txt", "additions": "90", "deletions": "63", "changes": "153"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23a/explain.txt", "additions": "275", "deletions": "245", "changes": "520"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23a/simplified.txt", "additions": "75", "deletions": "67", "changes": "142"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23b/explain.txt", "additions": "364", "deletions": "319", "changes": "683"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23b/simplified.txt", "additions": "93", "deletions": "81", "changes": "174"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q40/explain.txt", "additions": "123", "deletions": "108", "changes": "231"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q40/simplified.txt", "additions": "17", "deletions": "13", "changes": "30"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q72/explain.txt", "additions": "65", "deletions": "50", "changes": "115"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q72/simplified.txt", "additions": "13", "deletions": "9", "changes": "22"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q80/explain.txt", "additions": "462", "deletions": "417", "changes": "879"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q80/simplified.txt", "additions": "65", "deletions": "53", "changes": "118"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14.sf100/explain.txt", "additions": "569", "deletions": "468", "changes": "1037"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14.sf100/simplified.txt", "additions": "90", "deletions": "63", "changes": "153"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a.sf100/explain.txt", "additions": "778", "deletions": "677", "changes": "1455"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a.sf100/simplified.txt", "additions": "114", "deletions": "87", "changes": "201"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q72/explain.txt", "additions": "65", "deletions": "50", "changes": "115"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q72/simplified.txt", "additions": "13", "deletions": "9", "changes": "22"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q75/explain.txt", "additions": "614", "deletions": "455", "changes": "1069"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q75/simplified.txt", "additions": "108", "deletions": "66", "changes": "174"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q78/explain.txt", "additions": "320", "deletions": "275", "changes": "595"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q78/simplified.txt", "additions": "51", "deletions": "39", "changes": "90"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q80a/explain.txt", "additions": "532", "deletions": "487", "changes": "1019"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q80a/simplified.txt", "additions": "73", "deletions": "61", "changes": "134"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala", "additions": "21", "deletions": "1", "changes": "22"}, "updated": [0, 3, 8]}]}
{"author": "wangyum", "sha": "65df34649227d1f04065cc76a49778770b433536", "commit_date": "2021/07/21 14:51:57", "commit_message": "Deduplicate the right side of left semi/anti join", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/DeduplicateRightSideOfLeftSemiAntiJoin.scala", "additions": "56", "deletions": "0", "changes": "56"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [2, 2, 13]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/Statistics.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/DeduplicateRightSideOfLeftSemiAntiJoinSuite.scala", "additions": "98", "deletions": "0", "changes": "98"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AQEOptimizer.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/QueryStageExec.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q10.sf100/explain.txt", "additions": "119", "deletions": "103", "changes": "222"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q10.sf100/simplified.txt", "additions": "63", "deletions": "63", "changes": "126"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q10.sf100/explain.txt", "additions": "220", "deletions": "204", "changes": "424"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q10.sf100/simplified.txt", "additions": "48", "deletions": "48", "changes": "96"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a.sf100/explain.txt", "additions": "614", "deletions": "646", "changes": "1260"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a.sf100/simplified.txt", "additions": "143", "deletions": "185", "changes": "328"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b.sf100/explain.txt", "additions": "559", "deletions": "576", "changes": "1135"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b.sf100/simplified.txt", "additions": "151", "deletions": "184", "changes": "335"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q35.sf100/explain.txt", "additions": "230", "deletions": "214", "changes": "444"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q35.sf100/simplified.txt", "additions": "51", "deletions": "51", "changes": "102"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q69.sf100/explain.txt", "additions": "221", "deletions": "183", "changes": "404"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q69.sf100/simplified.txt", "additions": "56", "deletions": "60", "changes": "116"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q95.sf100/explain.txt", "additions": "134", "deletions": "92", "changes": "226"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q95.sf100/simplified.txt", "additions": "42", "deletions": "36", "changes": "78"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q10a.sf100/explain.txt", "additions": "199", "deletions": "183", "changes": "382"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q10a.sf100/simplified.txt", "additions": "42", "deletions": "42", "changes": "84"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14.sf100/explain.txt", "additions": "559", "deletions": "576", "changes": "1135"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14.sf100/simplified.txt", "additions": "151", "deletions": "184", "changes": "335"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a.sf100/explain.txt", "additions": "742", "deletions": "774", "changes": "1516"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a.sf100/simplified.txt", "additions": "158", "deletions": "200", "changes": "358"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35.sf100/explain.txt", "additions": "230", "deletions": "214", "changes": "444"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35.sf100/simplified.txt", "additions": "51", "deletions": "51", "changes": "102"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35a.sf100/explain.txt", "additions": "219", "deletions": "203", "changes": "422"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35a.sf100/simplified.txt", "additions": "49", "deletions": "49", "changes": "98"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [1, 5, 17]}]}
{"author": "wangyum", "sha": "07eda27de9622d97f807a7891af824e203e6b8ff", "commit_date": "2021/07/26 15:19:57", "commit_message": "Push down join condition evaluation", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.optimizer.PushDownJoinConditionEvaluationSuite", "org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite", "org.apache.spark.sql.streaming.StreamingOuterJoinSuite", "org.apache.spark.sql.streaming.StreamingInnerJoinSuite", "org.apache.spark.sql.streaming.StreamingLeftSemiJoinSuite", "org.apache.spark.sql.streaming.StreamingFullOuterJoinSuite", "org.apache.spark.sql.execution.PlannerSuite", "org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 3, 13]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/PullOutJoinCondition.scala", "additions": "75", "deletions": "0", "changes": "75"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/PullOutJoinConditionSuite.scala", "additions": "92", "deletions": "0", "changes": "92"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q58.sf100/explain.txt", "additions": "289", "deletions": "278", "changes": "567"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q58.sf100/simplified.txt", "additions": "24", "deletions": "20", "changes": "44"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q58/explain.txt", "additions": "278", "deletions": "267", "changes": "545"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q58/simplified.txt", "additions": "32", "deletions": "28", "changes": "60"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q83.sf100/explain.txt", "additions": "278", "deletions": "264", "changes": "542"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q83.sf100/simplified.txt", "additions": "27", "deletions": "23", "changes": "50"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q83/explain.txt", "additions": "259", "deletions": "245", "changes": "504"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q83/simplified.txt", "additions": "33", "deletions": "29", "changes": "62"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/JoinSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/PlannerSuite.scala", "additions": "0", "deletions": "8", "changes": "8"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/WholeStageCodegenSuite.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}]}
{"author": "wangyum", "sha": "eb71b8ae8de1fb737eea170e920c24127bcc2b95", "commit_date": "2021/07/17 15:56:31", "commit_message": "Remove the aggregation from left semi/anti join if the same aggregation has already been done on left side", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RemoveRedundantAggregates.scala", "additions": "6", "deletions": "2", "changes": "8"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/DistinctAttributesVisitor.scala", "additions": "100", "deletions": "0", "changes": "100"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/LogicalPlan.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/LogicalPlanDistinctAttributes.scala", "additions": "33", "deletions": "0", "changes": "33"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/RemoveRedundantAggregatesSuite.scala", "additions": "110", "deletions": "11", "changes": "121"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/plans/logical/DistinctAttributesVisitorSuite.scala", "additions": "112", "deletions": "0", "changes": "112"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a.sf100/explain.txt", "additions": "460", "deletions": "481", "changes": "941"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a.sf100/simplified.txt", "additions": "113", "deletions": "118", "changes": "231"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a/explain.txt", "additions": "229", "deletions": "245", "changes": "474"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a/simplified.txt", "additions": "62", "deletions": "64", "changes": "126"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b.sf100/explain.txt", "additions": "393", "deletions": "414", "changes": "807"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b.sf100/simplified.txt", "additions": "106", "deletions": "111", "changes": "217"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b/explain.txt", "additions": "213", "deletions": "229", "changes": "442"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b/simplified.txt", "additions": "62", "deletions": "64", "changes": "126"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q38.sf100/explain.txt", "additions": "143", "deletions": "190", "changes": "333"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q38.sf100/simplified.txt", "additions": "112", "deletions": "125", "changes": "237"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q38/explain.txt", "additions": "79", "deletions": "106", "changes": "185"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q38/simplified.txt", "additions": "60", "deletions": "63", "changes": "123"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q87.sf100/explain.txt", "additions": "143", "deletions": "190", "changes": "333"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q87.sf100/simplified.txt", "additions": "112", "deletions": "125", "changes": "237"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q87/explain.txt", "additions": "79", "deletions": "106", "changes": "185"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q87/simplified.txt", "additions": "60", "deletions": "63", "changes": "123"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14.sf100/explain.txt", "additions": "393", "deletions": "414", "changes": "807"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14.sf100/simplified.txt", "additions": "106", "deletions": "111", "changes": "217"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14/explain.txt", "additions": "213", "deletions": "229", "changes": "442"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14/simplified.txt", "additions": "62", "deletions": "64", "changes": "126"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a.sf100/explain.txt", "additions": "602", "deletions": "623", "changes": "1225"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a.sf100/simplified.txt", "additions": "128", "deletions": "133", "changes": "261"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a/explain.txt", "additions": "279", "deletions": "295", "changes": "574"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a/simplified.txt", "additions": "62", "deletions": "64", "changes": "126"}, "updated": [0, 0, 3]}]}
{"author": "xuzikun2003", "sha": "876c38309d42d3dddb23b5695a363cf517860ebf", "commit_date": "2021/08/19 16:24:47", "commit_message": "Merge branch 'master' of https://github.com/xuzikun2003/spark into Victor/Cp_fix", "title": "[SPARK-36493] Skip Retrieving keytab with SparkFiles.get if keytab found in the CWD of Yarn Container", "body": "### Why are the changes needed?\r\nCurrently we have the following logic to deal with the JDBC keytab provided by the `--files` option\r\n\r\n`if (keytabParam != null && FilenameUtils.getPath(keytabParam).isEmpty)`\r\n`{`     \r\n`    ` `val result = SparkFiles.get(keytabParam)`     \r\n`    ` `logDebug(s\"Keytab path not found, assuming --files, file name used on executor: $result\")`     \r\n`    ` `result` \r\n`}` ` else {`\r\n`    ` `logDebug(\"Keytab path found, assuming manual upload\")`\r\n`    ` `keytabParam`\r\n`}`\r\n\r\n\r\n\r\nSpark has already created the soft link in the current working directory of driver (application master) for the file submitted by the `--files` option in cluster mode. Here is an example.\r\n`testusera1.keytab -> /var/opt/hadoop/temp/nm-local-dir/usercache/testusera1/appcache/application_1628584679772_0003/filecache/12/testusera1.keytab`\r\n\r\n\r\n\r\nMoreover, SparkFiles.get will get a wrong path of keytab for the driver in cluster mode. In cluster mode, the keytab is available at the following location for both the driver and executors\r\n`/var/opt/hadoop/temp/nm-local-dir/usercache/testusera1/appcache/application_1628584679772_0003/container_1628584679772_0030_01_000001/testusera1.keytab`\r\nwhile `SparkFiles.get` brings the following wrong location\r\n`/var/opt/hadoop/temp/nm-local-dir/usercache/testusera1/appcache/application_1628584679772_0003/spark-8fb0f437-c842-4a9f-9612-39de40082e40/userFiles-5075388b-0928-4bc3-a498-7f6c84b27808/testusera1.keytab`\r\n\r\nSo there is no need to call the function `SparkFiles.get` to get the absolute path of the keytab file if it exists at the current working directory of Yarn container. We can directly use the variable `keytabParam` as the keytab file path.\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nWe need to check whether the keytab exists in the current directory. If it is in the current directory, we do not need to call SparkFiles.get to obtain the file. To check whether the keytab exists in the current directory, we use the function call `!Files.exists(Paths.get(keytabParam))`.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nNo\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nExisting unit tests and manual integration tests.", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 0, 2]}]}
{"author": "cxzl25", "sha": "bec1e6d7ba85b3486ca7b2e16ecd453c1b98dd6e", "commit_date": "2021/09/18 17:24:39", "commit_message": "Pass queryExecution name in CLI when only select query.", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkSQLDriver.scala", "additions": "6", "deletions": "2", "changes": "8"}, "updated": [0, 0, 0]}]}
{"author": "cxzl25", "sha": "7ec3ccce0e3b7ac445b7ce8224c2eac9bab420bd", "commit_date": "2021/06/28 11:49:59", "commit_message": "fix conflicts", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/ExternalCatalogUtils.scala", "additions": "27", "deletions": "17", "changes": "44"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "13", "deletions": "0", "changes": "13"}, "updated": [1, 2, 12]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 7]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveShim.scala", "additions": "68", "deletions": "11", "changes": "79"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/client/HivePartitionFilteringSuite.scala", "additions": "58", "deletions": "1", "changes": "59"}, "updated": [0, 0, 1]}]}
{"author": "cxzl25", "sha": "41d4c4f72f65c8bf2c388d2980240b445522313f", "commit_date": "2021/08/20 07:13:45", "commit_message": "Propagation cause when UDF reflection fails", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveSessionCatalog.scala", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 0, 0]}]}
{"author": "cxzl25", "sha": "d3086a535794dcd4984d30ea66063f8eab5ad95a", "commit_date": "2021/08/03 08:53:06", "commit_message": "Replace SessionState.close with SessionState.detachSession", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/hive-thriftserver/src/main/java/org/apache/hive/service/cli/session/HiveSessionImpl.java", "additions": "3", "deletions": "15", "changes": "18"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkSQLCLIDriver.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/CliSuite.scala", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveUtils.scala", "additions": "29", "deletions": "1", "changes": "30"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 1, 4]}]}
{"author": "fhygh", "sha": "691bb5c0add0420ca51eacfb41b384eefb9637cc", "commit_date": "2021/08/17 06:39:37", "commit_message": "[SPARK-36518][Deploy] Spark should support distribute directory to\ncluster", "title": "[SPARK-36518][Deploy] Spark should support distribute directory to cluster", "body": "\r\n### What changes were proposed in this pull request?\r\nsupport distribute directory to cluster via --files\r\nbefore:\r\n[root@kwephispra41893 spark]# ll /opt/ygh/testdir/\r\ntotal 8\r\ndrwxr-xr-x 2 root root 4096 Aug 17 16:07 dd1\r\ndrwxr-xr-x 2 root root 4096 Aug 17 16:07 dd2\r\n-rw-r--r-- 1 root root    0 Aug 17 16:07 t1.txt\r\n-rw-r--r-- 1 root root    0 Aug 17 16:07 t2.txt\r\n-rw-r--r-- 1 root root    0 Aug 17 16:07 t3.conf\r\n\r\nspark-shell --master yarn --files file:///opt/ygh/testdir\r\n![image](https://user-images.githubusercontent.com/25889738/129689226-d63cc7f6-c529-4c6f-a94d-d48c062dbf29.png)\r\n\r\nafter:\r\nspark-shell --master yarn --files file:///opt/ygh/testdir\r\n![image](https://user-images.githubusercontent.com/25889738/129689406-432c796c-52e5-49c1-8016-9eec151257fb.png)\r\n\r\n\r\n### Why are the changes needed?\r\nwhen we submit spark application we can't directly distribute a directory to cluster\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo, user do not need change any code\r\n\r\n\r\n### How was this patch tested?\r\ntested by existing UT\r\n", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/util/DependencyUtils.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "fhygh", "sha": "e604f1ba111d208526ec3d49799fed063dc5c990", "commit_date": "2021/09/01 07:22:47", "commit_message": "[SPARK-36604][SQL] timestamp type column stats result consistent with\nthe time zone", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/interface.scala", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/StatisticsCollectionSuite.scala", "additions": "39", "deletions": "12", "changes": "51"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/StatisticsSuite.scala", "additions": "29", "deletions": "19", "changes": "48"}, "updated": [0, 0, 0]}]}
{"author": "ekoifman", "sha": "af5a71ab3d61056f0fa309e89868d57d584d0994", "commit_date": "2021/08/04 18:18:29", "commit_message": "[SPARK-36416][SQL] Add SQL metrics to AdaptiveSparkPlanExec for BHJs and Skew joins\n[SPARK-36416][SQL] fix typo", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "32", "deletions": "0", "changes": "32"}, "updated": [1, 3, 9]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "56", "deletions": "1", "changes": "57"}, "updated": [1, 4, 16]}]}
{"author": "tanelk", "sha": "38d98ed9e613f181018bb6266d89ab772db84b4c", "commit_date": "2021/08/17 05:51:07", "commit_message": "Merge remote-tracking branch 'origin/master' into SPARK-36496_remove_grouping_literals\n\n# Conflicts:\n#\tsql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "title": "[SPARK-36496][SQL] Remove literals from grouping expressions when using the DataFrame withColumn API", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\nMove the `RemoveLiteralFromGroupExpressions` and `RemoveRepetitionFromGroupExpressions` rules from a separate batch to the `operatorOptimizationBatch`.\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nThe `RemoveLiteralFromGroupExpressions` does not work in some cases if it is in a separate batch.\r\nThe added UT would fail with:\r\n```\r\n[info] - SPARK-36496: Remove literals from grouping expressions *** FAILED *** (2 seconds, 955 milliseconds)\r\n[info]   == FAIL: Plans do not match ===\r\n[info]   !Aggregate [*id#0L, null], [*id#0L, null AS a#0, count(1) AS count#0L]   Aggregate [*id#0L], [*id#0L, null AS a#0, count(1) AS count#0L]\r\n[info]    +- Range (0, 100, step=1, splits=Some(2))                               +- Range (0, 100, step=1, splits=Some(2)) (PlanTest.scala:174)\r\n```\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nNo\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nNew UT", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "4", "deletions": "5", "changes": "9"}, "updated": [0, 5, 14]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [1, 2, 11]}]}
{"author": "brandondahler", "sha": "2314d1d41f26973a27f6576aeec2f7c6de6dff4f", "commit_date": "2021/06/14 15:09:40", "commit_message": "[SPARK-35739][SQL] Add Java-compatible Dataset.join overloads", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala", "additions": "79", "deletions": "2", "changes": "81"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameJoinSuite.scala", "additions": "55", "deletions": "0", "changes": "55"}, "updated": [0, 0, 0]}]}
{"author": "kazuyukitanimura", "sha": "931c9061a99e6f63bd9fc3371a9fe7ac189c423c", "commit_date": "2021/08/31 06:50:05", "commit_message": "[SPARK-36665][SQL] Add more Not operator simplifications\n\n ### What changes were proposed in this pull request?\nThis PR proposes to add more Not operator simplifications in `BooleanSimplification` by applying the following rules\n  - Not(null) == null\n    - e.g. IsNull(Not(...)) can be IsNull(...)\n  - (Not(a) = b) == (a = Not(b))\n    - e.g. Not(...) = true can be (...) = false\n  - (a != b) == (a = Not(b))\n    - e.g. (...) != true can be (...) = false\n\n ### Why are the changes needed?\nThe following query does not push down the filter in the current implementation\n```\nSELECT * FROM t WHERE (not boolean_col) <=> null\n```\nalthough the following equivalent query pushes down the filter as expected.\n```\nSELECT * FROM t WHERE boolean_col <=> null\n```\nThat is because the first query creates `IsNull(Not(boolean_col))` in the current implementation, which should be able to get simplified further to `IsNull(boolean_col)`\nThis PR helps optimizing such cases.\n\n ### Does this PR introduce _any_ user-facing change?\nNo\n\n ### How was this patch tested?\nAdded unit tests\n```\nbuild/sbt \"testOnly *BooleanSimplificationSuite  -- -z SPARK-36665\"\n```", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.optimizer.BooleanSimplificationSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV2FilterSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV1FilterSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 2, 11]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/expressions.scala", "additions": "78", "deletions": "0", "changes": "78"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/rules/RuleIdCollection.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NotPropagationSuite.scala", "additions": "176", "deletions": "0", "changes": "176"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NullDownPropagationSuite.scala", "additions": "59", "deletions": "0", "changes": "59"}, "updated": [0, 0, 0]}]}
{"author": "zengruios", "sha": "e4794d404bb8bb140a9bd5470fee55ab1e1589c9", "commit_date": "2021/08/12 12:14:01", "commit_message": "[SPARK-36494]Add param bucketSpec when create LogicalRelation for the hive table in HiveMetastoreCatalog.covertToLogicalRelation.", "title": "[SPARK-36494]Add param bucketSpec when create LogicalRelation for the hive table in HiveMetastoreCatalog.covertToLogicalRelation", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\nAdd param bucketSpec when create LogicalRelation for the hive table in HiveMetastoreCatalog.covertToLogicalRelation\r\n\r\n\r\n### Why are the changes needed?\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\nIf bucketSpec is not used, SortMergeJoin will do unnecessary shuffle.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nYes. See in SPARK-36494.\r\n\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nTest it in my develop enviroinment.\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMetastoreCatalog.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 1]}]}
{"author": "MyeongKim", "sha": "8389a0f7c7febe1d3c9489ce18b61a1501f6f0d1", "commit_date": "2021/07/25 17:59:42", "commit_message": "Support TPCDSQueryBenchmark in Benchmarks", "title": "", "body": "", "failed_tests": ["org.apache.spark.scheduler.BasicSchedulerIntegrationSuite"], "files": [{"file": {"name": ".github/workflows/benchmark.yml", "additions": "72", "deletions": "4", "changes": "76"}, "updated": [0, 0, 3]}, {"file": {"name": ".github/workflows/build_and_test.yml", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [3, 9, 22]}, {"file": {"name": "core/src/test/scala/org/apache/spark/benchmark/Benchmarks.scala", "additions": "10", "deletions": "2", "changes": "12"}, "updated": [0, 0, 0]}]}
{"author": "sunchao", "sha": "c2ec1f8122ea9b871a1679571f0b20009be39cb7", "commit_date": "2021/09/03 20:48:52", "commit_message": "wip", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "LICENSE-binary", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "assembly/pom.xml", "additions": "19", "deletions": "1", "changes": "20"}, "updated": [0, 0, 0]}, {"file": {"name": "dev/deps/spark-deps-hadoop-2.7-hive-2.3", "additions": "6", "deletions": "26", "changes": "32"}, "updated": [1, 3, 15]}, {"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "7", "deletions": "27", "changes": "34"}, "updated": [1, 3, 15]}, {"file": {"name": "pom.xml", "additions": "152", "deletions": "3", "changes": "155"}, "updated": [1, 4, 23]}, {"file": {"name": "project/SparkBuild.scala", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [0, 2, 6]}, {"file": {"name": "sql/hive-shaded/pom.xml", "additions": "238", "deletions": "0", "changes": "238"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive-thriftserver/pom.xml", "additions": "9", "deletions": "16", "changes": "25"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/pom.xml", "additions": "29", "deletions": "66", "changes": "95"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/client/IsolatedClientLoader.scala", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}]}
{"author": "sunchao", "sha": "622467ba22d2c8e7a97523b9d0985716642603cc", "commit_date": "2021/06/09 19:26:29", "commit_message": "wip", "title": "", "body": "", "failed_tests": ["pyspark.sql.tests.test_pandas_cogrouped_map", "org.apache.spark.sql.JoinSuite", "org.apache.spark.sql.execution.RemoveRedundantSortsSuite", "org.apache.spark.sql.execution.WholeStageCodegenSuite", "org.apache.spark.sql.execution.CoalesceShufflePartitionsSuite", "org.apache.spark.sql.DataFrameSuite", "org.apache.spark.sql.execution.RemoveRedundantSortsSuite", "org.apache.spark.sql.execution.exchange.EnsureRequirementsSuite", "org.apache.spark.sql.execution.PlannerSuite"], "files": [{"file": {"name": "python/pyspark/sql/tests/test_pandas_cogrouped_map.py", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/physical/partitioning.scala", "additions": "188", "deletions": "29", "changes": "217"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/DistributionSuite.scala", "additions": "0", "deletions": "38", "changes": "38"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AQEUtils.scala", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/bucketing/DisableUnnecessaryBucketedScan.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ValidateRequirements.scala", "additions": "0", "deletions": "1", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/joins/ShuffledJoin.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/objects.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinExec.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q17.sf100/explain.txt", "additions": "155", "deletions": "170", "changes": "325"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q17.sf100/simplified.txt", "additions": "74", "deletions": "81", "changes": "155"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q25.sf100/explain.txt", "additions": "155", "deletions": "170", "changes": "325"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q25.sf100/simplified.txt", "additions": "74", "deletions": "81", "changes": "155"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q29.sf100/explain.txt", "additions": "173", "deletions": "188", "changes": "361"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q29.sf100/simplified.txt", "additions": "75", "deletions": "82", "changes": "157"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q47.sf100/explain.txt", "additions": "110", "deletions": "125", "changes": "235"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q47.sf100/simplified.txt", "additions": "80", "deletions": "89", "changes": "169"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q57.sf100/explain.txt", "additions": "110", "deletions": "125", "changes": "235"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q57.sf100/simplified.txt", "additions": "80", "deletions": "89", "changes": "169"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q72.sf100/explain.txt", "additions": "197", "deletions": "207", "changes": "404"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q72.sf100/simplified.txt", "additions": "105", "deletions": "111", "changes": "216"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q24.sf100/explain.txt", "additions": "61", "deletions": "43", "changes": "104"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q24.sf100/simplified.txt", "additions": "36", "deletions": "33", "changes": "69"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q47.sf100/explain.txt", "additions": "110", "deletions": "125", "changes": "235"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q47.sf100/simplified.txt", "additions": "80", "deletions": "89", "changes": "169"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q51a.sf100/explain.txt", "additions": "249", "deletions": "259", "changes": "508"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q51a.sf100/simplified.txt", "additions": "100", "deletions": "106", "changes": "206"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q57.sf100/explain.txt", "additions": "110", "deletions": "125", "changes": "235"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q57.sf100/simplified.txt", "additions": "80", "deletions": "89", "changes": "169"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q64/explain.txt", "additions": "414", "deletions": "424", "changes": "838"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q64/simplified.txt", "additions": "259", "deletions": "265", "changes": "524"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q72.sf100/explain.txt", "additions": "197", "deletions": "207", "changes": "404"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q72.sf100/simplified.txt", "additions": "105", "deletions": "111", "changes": "216"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/PlannerSuite.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/exchange/EnsureRequirementsSuite.scala", "additions": "181", "deletions": "2", "changes": "183"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/sources/BucketedReadSuite.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingJoinSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [1, 1, 1]}]}
{"author": "sunchao", "sha": "5f9f186ab3c4e403dddd0a34d8f8852193af0bc7", "commit_date": "2021/06/15 21:48:11", "commit_message": "initial commit", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.execution.datasources.parquet.ParquetVectorizedSuite", "org.apache.spark.sql.FileBasedDataSourceSuite"], "files": [{"file": {"name": "pom.xml", "additions": "21", "deletions": "0", "changes": "21"}, "updated": [1, 7, 32]}, {"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/vectorized/ColumnVector.java", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [1, 2, 13]}, {"file": {"name": "sql/core/pom.xml", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/parquet/io/ColumnIOUtil.java", "additions": "38", "deletions": "0", "changes": "38"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnarBatchReader.java", "additions": "1", "deletions": "2", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetColumn.java", "additions": "318", "deletions": "0", "changes": "318"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetReadState.java", "additions": "41", "deletions": "18", "changes": "59"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase.java", "additions": "84", "deletions": "22", "changes": "106"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader.java", "additions": "63", "deletions": "32", "changes": "95"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader.java", "additions": "114", "deletions": "57", "changes": "171"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedRleValuesReader.java", "additions": "341", "deletions": "24", "changes": "365"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OffHeapColumnVector.java", "additions": "20", "deletions": "1", "changes": "21"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OnHeapColumnVector.java", "additions": "21", "deletions": "1", "changes": "22"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/WritableColumnVector.java", "additions": "64", "deletions": "6", "changes": "70"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala", "additions": "20", "deletions": "3", "changes": "23"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "5", "deletions": "2", "changes": "7"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "138", "deletions": "38", "changes": "176"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetType.scala", "additions": "68", "deletions": "0", "changes": "68"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/java/org/apache/parquet/column/page/TestDataPage.java", "additions": "44", "deletions": "0", "changes": "44"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/explain.sql.out", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/benchmark/DataSourceReadBenchmark.scala", "additions": "119", "deletions": "3", "changes": "122"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/FileBasedDataSourceTest.scala", "additions": "6", "deletions": "2", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcTest.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcV1SchemaPruningSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcV2SchemaPruningSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetColumnIndexSuite.scala", "additions": "14", "deletions": "1", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "326", "deletions": "0", "changes": "326"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaPruningSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetTest.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetVectorizedSuite.scala", "additions": "751", "deletions": "0", "changes": "751"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/vectorized/ColumnVectorSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/vectorized/ColumnarBatchSuite.scala", "additions": "15", "deletions": "2", "changes": "17"}, "updated": [0, 0, 0]}]}
{"author": "sunchao", "sha": "4bf4533b8c7e9c7b2f264ede6a14cad52c33b194", "commit_date": "2021/06/30 20:22:02", "commit_message": "initial commit", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.data_type_ops.test_binary_ops", "pyspark.pandas.tests.indexes.test_base", "org.apache.spark.sql.execution.datasources.AvroReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.avro.AvroScanSuite", "org.apache.spark.sql.avro.AvroScanSuite", "org.apache.spark.sql.avro.AvroScanSuite", "org.apache.spark.sql.avro.AvroScanSuite", "org.apache.spark.sql.avro.AvroScanSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.avro.AvroCatalystDataConversionSuite", "org.apache.spark.sql.avro.AvroCatalystDataConversionSuite", "org.apache.spark.sql.avro.AvroCatalystDataConversionSuite", "org.apache.spark.sql.avro.AvroCatalystDataConversionSuite", "org.apache.spark.sql.avro.AvroCatalystDataConversionSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.avro.AvroV1Suite", "org.apache.spark.sql.avro.AvroSuite", "org.apache.spark.sql.avro.AvroSuite", "org.apache.spark.sql.avro.AvroSuite", "org.apache.spark.sql.avro.AvroSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.avro.DeprecatedAvroFunctionsSuite", "org.apache.spark.sql.avro.DeprecatedAvroFunctionsSuite", "org.apache.spark.sql.avro.DeprecatedAvroFunctionsSuite", "org.apache.spark.sql.avro.DeprecatedAvroFunctionsSuite", "org.apache.spark.sql.avro.DeprecatedAvroFunctionsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.avro.AvroV1LogicalTypeSuite", "org.apache.spark.sql.avro.AvroLogicalTypeSuite", "org.apache.spark.sql.avro.AvroLogicalTypeSuite", "org.apache.spark.sql.avro.AvroLogicalTypeSuite", "org.apache.spark.sql.avro.AvroLogicalTypeSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.avro.AvroSchemaHelperSuite", "org.apache.spark.sql.avro.AvroSchemaHelperSuite", "org.apache.spark.sql.avro.AvroSchemaHelperSuite", "org.apache.spark.sql.avro.AvroSchemaHelperSuite", "org.apache.spark.sql.avro.AvroSchemaHelperSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.avro.AvroFunctionsSuite", "org.apache.spark.sql.avro.AvroFunctionsSuite", "org.apache.spark.sql.avro.AvroFunctionsSuite", "org.apache.spark.sql.avro.AvroFunctionsSuite", "org.apache.spark.sql.avro.AvroFunctionsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.avro.AvroRowReaderSuite", "org.apache.spark.sql.avro.AvroRowReaderSuite", "org.apache.spark.sql.avro.AvroRowReaderSuite", "org.apache.spark.sql.avro.AvroRowReaderSuite", "org.apache.spark.sql.avro.AvroRowReaderSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.avro.AvroV2Suite", "org.apache.spark.sql.avro.AvroSuite", "org.apache.spark.sql.avro.AvroSuite", "org.apache.spark.sql.avro.AvroSuite", "org.apache.spark.sql.avro.AvroSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.avro.AvroV2LogicalTypeSuite", "org.apache.spark.sql.avro.AvroLogicalTypeSuite", "org.apache.spark.sql.avro.AvroLogicalTypeSuite", "org.apache.spark.sql.avro.AvroLogicalTypeSuite", "org.apache.spark.sql.avro.AvroLogicalTypeSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.avro.JavaAvroFunctionsSuite", "org.apache.spark.sql.avro.JavaAvroFunctionsSuite", "org.apache.spark.sql.avro.JavaAvroFunctionsSuite", "org.apache.spark.sql.execution.datasources.AvroReadSchemaSuite", "org.apache.spark.sql.avro.AvroScanSuite", "org.apache.spark.sql.avro.AvroCatalystDataConversionSuite", "org.apache.spark.sql.avro.AvroV1Suite", "org.apache.spark.sql.avro.DeprecatedAvroFunctionsSuite", "org.apache.spark.sql.avro.AvroV1LogicalTypeSuite", "org.apache.spark.sql.avro.AvroSchemaHelperSuite", "org.apache.spark.sql.avro.AvroFunctionsSuite", "org.apache.spark.sql.avro.AvroRowReaderSuite", "org.apache.spark.sql.avro.AvroV2Suite", "org.apache.spark.sql.avro.AvroV2LogicalTypeSuite", "org.apache.spark.sql.catalyst.analysis.V2OverwriteByExpressionANSIAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2ANSIAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2ANSIAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2ANSIAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.analysis.PullOutNondeterministicSuite", "org.apache.spark.sql.catalyst.analysis.PullOutNondeterministicSuite", "org.apache.spark.sql.catalyst.analysis.PullOutNondeterministicSuite", "org.apache.spark.sql.catalyst.analysis.PullOutNondeterministicSuite", "org.apache.spark.sql.catalyst.analysis.PullOutNondeterministicSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.analysis.V2OverwritePartitionsDynamicANSIAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2ANSIAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2ANSIAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2ANSIAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.analysis.V2AppendDataStrictAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2StrictAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2StrictAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2StrictAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite", "org.apache.spark.sql.catalyst.catalog.ExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.analysis.ResolveHintsSuite", "org.apache.spark.sql.catalyst.analysis.ResolveHintsSuite", "org.apache.spark.sql.catalyst.analysis.ResolveHintsSuite", "org.apache.spark.sql.catalyst.analysis.ResolveHintsSuite", "org.apache.spark.sql.catalyst.analysis.ResolveHintsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.catalog.InMemorySessionCatalogSuite", "org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite", "org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.analysis.ResolveGroupingAnalyticsSuite", "org.apache.spark.sql.catalyst.analysis.ResolveGroupingAnalyticsSuite", "org.apache.spark.sql.catalyst.analysis.ResolveGroupingAnalyticsSuite", "org.apache.spark.sql.catalyst.analysis.ResolveGroupingAnalyticsSuite", "org.apache.spark.sql.catalyst.analysis.ResolveGroupingAnalyticsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisExceptionPositionSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisExceptionPositionSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisExceptionPositionSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisExceptionPositionSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisExceptionPositionSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisExceptionPositionSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisExceptionPositionSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.analysis.ResolveNaturalJoinSuite", "org.apache.spark.sql.catalyst.analysis.ResolveNaturalJoinSuite", "org.apache.spark.sql.catalyst.analysis.ResolveNaturalJoinSuite", "org.apache.spark.sql.catalyst.analysis.ResolveNaturalJoinSuite", "org.apache.spark.sql.catalyst.analysis.ResolveNaturalJoinSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.analysis.V2AppendDataANSIAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2ANSIAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2ANSIAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2ANSIAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.analysis.V2OverwritePartitionsDynamicStrictAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2StrictAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2StrictAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2StrictAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.expressions.SelectedFieldSuite", "org.apache.spark.sql.catalyst.expressions.SelectedFieldSuite", "org.apache.spark.sql.catalyst.expressions.SelectedFieldSuite", "org.apache.spark.sql.catalyst.expressions.SelectedFieldSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.catalog.ExternalCatalogEventSuite", "org.apache.spark.sql.catalyst.catalog.ExternalCatalogEventSuite", "org.apache.spark.sql.catalyst.catalog.ExternalCatalogEventSuite", "org.apache.spark.sql.catalyst.catalog.ExternalCatalogEventSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite", "org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite", "org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite", "org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite", "org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite", "org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite", "org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite", "org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.analysis.SubstituteUnresolvedOrdinalsSuite", "org.apache.spark.sql.catalyst.analysis.SubstituteUnresolvedOrdinalsSuite", "org.apache.spark.sql.catalyst.analysis.SubstituteUnresolvedOrdinalsSuite", "org.apache.spark.sql.catalyst.analysis.SubstituteUnresolvedOrdinalsSuite", "org.apache.spark.sql.catalyst.analysis.SubstituteUnresolvedOrdinalsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.analysis.V2OverwriteByExpressionStrictAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2StrictAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2StrictAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2StrictAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.analysis.ExtractGeneratorSuite", "org.apache.spark.sql.catalyst.analysis.ExtractGeneratorSuite", "org.apache.spark.sql.catalyst.analysis.ExtractGeneratorSuite", "org.apache.spark.sql.catalyst.analysis.ExtractGeneratorSuite", "org.apache.spark.sql.catalyst.analysis.ExtractGeneratorSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.analysis.ResolvedUuidExpressionsSuite", "org.apache.spark.sql.catalyst.analysis.ResolvedUuidExpressionsSuite", "org.apache.spark.sql.catalyst.analysis.ResolvedUuidExpressionsSuite", "org.apache.spark.sql.catalyst.analysis.CreateTablePartitioningValidationSuite", "org.apache.spark.sql.catalyst.analysis.CreateTablePartitioningValidationSuite", "org.apache.spark.sql.catalyst.analysis.CreateTablePartitioningValidationSuite", "org.apache.spark.sql.catalyst.analysis.CreateTablePartitioningValidationSuite", "org.apache.spark.sql.catalyst.analysis.CreateTablePartitioningValidationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.analysis.ResolveAliasesSuite", "org.apache.spark.sql.catalyst.analysis.ResolveAliasesSuite", "org.apache.spark.sql.catalyst.analysis.ResolveAliasesSuite", "org.apache.spark.sql.catalyst.analysis.ResolveAliasesSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.analysis.LookupFunctionsSuite", "org.apache.spark.sql.catalyst.analysis.LookupFunctionsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.analysis.ResolveSubquerySuite", "org.apache.spark.sql.catalyst.analysis.ResolveSubquerySuite", "org.apache.spark.sql.catalyst.analysis.ResolveSubquerySuite", "org.apache.spark.sql.catalyst.analysis.ResolveSubquerySuite", "org.apache.spark.sql.catalyst.analysis.ResolveSubquerySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisExternalCatalogSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisExternalCatalogSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisExternalCatalogSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisExternalCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.optimizer.AggregateOptimizeSuite", "org.apache.spark.sql.catalyst.optimizer.AggregateOptimizeSuite", "org.apache.spark.sql.catalyst.optimizer.AggregateOptimizeSuite", "org.apache.spark.sql.catalyst.optimizer.EliminateSortsSuite", "org.apache.spark.sql.catalyst.optimizer.EliminateSortsSuite", "org.apache.spark.sql.catalyst.optimizer.EliminateSortsSuite", "org.apache.spark.sql.catalyst.analysis.TableLookupCacheSuite", "org.apache.spark.sql.catalyst.analysis.TableLookupCacheSuite", "org.apache.spark.sql.catalyst.analysis.TableLookupCacheSuite", "org.apache.spark.sql.catalyst.analysis.TableLookupCacheSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.analysis.TableLookupCacheSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.connector.catalog.CatalogManagerSuite", "org.apache.spark.sql.connector.catalog.CatalogManagerSuite", "org.apache.spark.sql.connector.catalog.CatalogManagerSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.catalyst.expressions.codegen.BufferHolderSparkSubmitSuite", "org.apache.spark.sql.catalyst.analysis.V2OverwriteByExpressionANSIAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.PullOutNondeterministicSuite", "org.apache.spark.sql.catalyst.analysis.V2OverwritePartitionsDynamicANSIAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisSuite", "org.apache.spark.sql.catalyst.analysis.V2AppendDataStrictAnalysisSuite", "org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite", "org.apache.spark.sql.catalyst.analysis.ResolveHintsSuite", "org.apache.spark.sql.catalyst.catalog.InMemorySessionCatalogSuite", "org.apache.spark.sql.catalyst.analysis.ResolveGroupingAnalyticsSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisExceptionPositionSuite", "org.apache.spark.sql.catalyst.analysis.ResolveNaturalJoinSuite", "org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite", "org.apache.spark.sql.catalyst.analysis.V2AppendDataANSIAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.V2OverwritePartitionsDynamicStrictAnalysisSuite", "org.apache.spark.sql.catalyst.expressions.SelectedFieldSuite", "org.apache.spark.sql.catalyst.catalog.ExternalCatalogEventSuite", "org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite", "org.apache.spark.sql.catalyst.analysis.SubstituteUnresolvedOrdinalsSuite", "org.apache.spark.sql.catalyst.analysis.V2OverwriteByExpressionStrictAnalysisSuite", "org.apache.spark.sql.catalyst.analysis.ExtractGeneratorSuite", "org.apache.spark.sql.catalyst.analysis.ResolvedUuidExpressionsSuite", "org.apache.spark.sql.catalyst.analysis.CreateTablePartitioningValidationSuite", "org.apache.spark.sql.catalyst.analysis.ResolveAliasesSuite", "org.apache.spark.sql.catalyst.analysis.LookupFunctionsSuite", "org.apache.spark.sql.catalyst.analysis.ResolveSubquerySuite", "org.apache.spark.sql.catalyst.analysis.AnalysisExternalCatalogSuite", "org.apache.spark.sql.catalyst.optimizer.AggregateOptimizeSuite", "org.apache.spark.sql.catalyst.optimizer.EliminateSortsSuite", "org.apache.spark.sql.catalyst.analysis.TableLookupCacheSuite", "org.apache.spark.sql.connector.catalog.CatalogManagerSuite", "org.apache.spark.streaming.kafka010.KafkaRDDSuite", "org.apache.spark.streaming.kafka010.KafkaRDDSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.streaming.kafka010.DirectKafkaStreamSuite", "org.apache.spark.streaming.kafka010.DirectKafkaStreamSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.streaming.kafka010.JavaDirectKafkaStreamSuite", "org.apache.spark.streaming.kafka010.JavaKafkaRDDSuite", "org.apache.spark.streaming.kafka010.JavaDirectKafkaStreamSuite", "org.apache.spark.streaming.kafka010.JavaKafkaRDDSuite", "org.apache.spark.streaming.kafka010.KafkaRDDSuite", "org.apache.spark.streaming.kafka010.DirectKafkaStreamSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.HiveParquetSuite", "org.apache.spark.sql.hive.HiveParquetSuite", "org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite", "org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite", "org.apache.spark.sql.hive.orc.HiveOrcQuerySuite", "org.apache.spark.sql.hive.orc.HiveOrcQuerySuite", "org.apache.spark.sql.hive.execution.command.ShowPartitionsSuite", "org.apache.spark.sql.hive.execution.command.ShowPartitionsSuite", "org.apache.spark.sql.hive.execution.command.ShowNamespacesSuite", "org.apache.spark.sql.hive.execution.command.ShowNamespacesSuite", "org.apache.spark.sql.hive.execution.HiveDDLSuite", "org.apache.spark.sql.hive.execution.HiveDDLSuite", "org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite", "org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite", "org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite", "org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite", "org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite", "org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite", "org.apache.spark.sql.hive.execution.command.ShowTablesSuite", "org.apache.spark.sql.hive.execution.command.ShowTablesSuite", "org.apache.spark.sql.hive.execution.HiveQuerySuite", "org.apache.spark.sql.hive.execution.HiveQuerySuite", "org.apache.spark.sql.hive.orc.HiveOrcPartitionDiscoverySuite", "org.apache.spark.sql.hive.orc.HiveOrcPartitionDiscoverySuite", "org.apache.spark.sql.hive.InsertSuite", "org.apache.spark.sql.hive.InsertSuite", "org.apache.spark.sql.hive.CachedTableSuite", "org.apache.spark.sql.hive.CachedTableSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.sources.DisableUnnecessaryBucketedScanWithHiveSupportSuite", "org.apache.spark.sql.sources.DisableUnnecessaryBucketedScanWithHiveSupportSuite", "org.apache.spark.sql.hive.CompressionCodecSuite", "org.apache.spark.sql.hive.CompressionCodecSuite", "org.apache.spark.sql.hive.execution.WindowQuerySuite", "org.apache.spark.sql.hive.execution.WindowQuerySuite", "org.apache.spark.sql.hive.HiveExternalSessionCatalogSuite", "org.apache.spark.sql.hive.HiveExternalSessionCatalogSuite", "org.apache.spark.sql.hive.execution.HiveSQLViewSuite", "org.apache.spark.sql.hive.execution.HiveSQLViewSuite", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.ListTablesSuite", "org.apache.spark.sql.hive.ListTablesSuite", "org.apache.spark.sql.sources.BucketedWriteWithHiveSupportSuite", "org.apache.spark.sql.sources.BucketedWriteWithHiveSupportSuite", "org.apache.spark.sql.hive.execution.HashUDAQuerySuite", "org.apache.spark.sql.hive.execution.UDAQuerySuite", "org.apache.spark.sql.hive.execution.HashUDAQuerySuite", "org.apache.spark.sql.hive.ParquetEncryptionSuite", "org.apache.spark.sql.hive.ParquetEncryptionSuite", "org.apache.spark.sql.hive.execution.PruneHiveTablePartitionsSuite", "org.apache.spark.sql.hive.execution.PruneHiveTablePartitionsSuite", "org.apache.spark.sql.hive.HiveUtilsSuite", "org.apache.spark.sql.hive.HiveUtilsSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveUDAFSuite", "org.apache.spark.sql.hive.execution.HiveUDAFSuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.command.AlterTableRenamePartitionSuite", "org.apache.spark.sql.hive.execution.command.AlterTableRenamePartitionSuite", "org.apache.spark.sql.hive.HiveSessionStateSuite", "org.apache.spark.sql.hive.HiveSessionStateSuite", "org.apache.spark.sql.hive.HiveVariableSubstitutionSuite", "org.apache.spark.sql.hive.HiveVariableSubstitutionSuite", "org.apache.spark.sql.hive.execution.HiveSerDeReadWriteSuite", "org.apache.spark.sql.hive.execution.HiveSerDeReadWriteSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.UDFSuite", "org.apache.spark.sql.hive.UDFSuite", "org.apache.spark.sql.hive.execution.HashAggregationQueryWithControlledFallbackSuite", "org.apache.spark.sql.hive.execution.AggregationQuerySuite", "org.apache.spark.sql.hive.execution.HashAggregationQueryWithControlledFallbackSuite", "org.apache.spark.sql.hive.HiveUserDefinedTypeSuite", "org.apache.spark.sql.hive.HiveUserDefinedTypeSuite", "org.apache.spark.sql.HiveCharVarcharTestSuite", "org.apache.spark.sql.HiveCharVarcharTestSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.BigDataBenchmarkSuite", "org.apache.spark.sql.hive.execution.BigDataBenchmarkSuite", "org.apache.spark.sql.hive.execution.ObjectHashAggregateSuite", "org.apache.spark.sql.hive.execution.ObjectHashAggregateSuite", "org.apache.spark.sql.hive.execution.command.TruncateTableSuite", "org.apache.spark.sql.hive.execution.command.TruncateTableSuite", "org.apache.spark.sql.hive.execution.command.AlterTableRenameSuite", "org.apache.spark.sql.hive.execution.command.AlterTableRenameSuite", "org.apache.spark.sql.hive.execution.SQLMetricsSuite", "org.apache.spark.sql.hive.execution.SQLMetricsSuite", "org.apache.spark.sql.hive.OptimizeHiveMetadataOnlyQuerySuite", "org.apache.spark.sql.hive.OptimizeHiveMetadataOnlyQuerySuite", "org.apache.spark.sql.hive.execution.HiveCommandSuite", "org.apache.spark.sql.hive.execution.HiveCommandSuite", "org.apache.spark.sql.hive.QueryPartitionSuite", "org.apache.spark.sql.hive.QueryPartitionSuite", "org.apache.spark.sql.hive.execution.HashUDAQueryWithControlledFallbackSuite", "org.apache.spark.sql.hive.execution.UDAQuerySuite", "org.apache.spark.sql.hive.execution.HashUDAQueryWithControlledFallbackSuite", "org.apache.spark.sql.hive.ParquetHiveCompatibilitySuite", "org.apache.spark.sql.hive.ParquetHiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.command.MsckRepairTableSuite", "org.apache.spark.sql.hive.execution.command.MsckRepairTableSuite", "org.apache.spark.sql.hive.HiveUDFDynamicLoadSuite", "org.apache.spark.sql.hive.HiveUDFDynamicLoadSuite", "org.apache.spark.sql.hive.TestHiveSuite", "org.apache.spark.sql.hive.TestHiveSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.HiveSQLInsertTestSuite", "org.apache.spark.sql.hive.HiveSQLInsertTestSuite", "org.apache.spark.sql.sources.CommitFailureTestRelationSuite", "org.apache.spark.sql.sources.CommitFailureTestRelationSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.execution.HiveWindowFunctionQueryFileSuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveWindowFunctionQueryFileSuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveWindowFunctionQueryFileSuite", "org.apache.spark.sql.hive.HiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.HiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.ErrorPositionSuite", "org.apache.spark.sql.hive.ErrorPositionSuite", "org.apache.spark.sql.hive.execution.HiveScriptTransformationSuite", "org.apache.spark.sql.hive.execution.HiveScriptTransformationSuite", "org.apache.spark.sql.hive.execution.HashAggregationQuerySuite", "org.apache.spark.sql.hive.execution.AggregationQuerySuite", "org.apache.spark.sql.hive.execution.HashAggregationQuerySuite", "org.apache.spark.sql.hive.execution.command.AlterTableRecoverPartitionsSuite", "org.apache.spark.sql.hive.execution.command.AlterTableRecoverPartitionsSuite", "org.apache.spark.sql.hive.execution.HiveExplainSuite", "org.apache.spark.sql.hive.execution.HiveExplainSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.execution.command.DropTableSuite", "org.apache.spark.sql.hive.execution.command.DropTableSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.execution.command.AlterTableDropPartitionSuite", "org.apache.spark.sql.hive.execution.command.AlterTableDropPartitionSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.HiveCharVarcharDDLTestSuite", "org.apache.spark.sql.HiveCharVarcharDDLTestSuite", "org.apache.spark.sql.hive.execution.command.AlterTableAddPartitionSuite", "org.apache.spark.sql.hive.execution.command.AlterTableAddPartitionSuite", "org.apache.spark.sql.hive.HiveExternalCatalogVersionsSuite", "org.apache.spark.sql.hive.HiveSparkSubmitSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.client.VersionsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.HiveParquetSuite", "org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite", "org.apache.spark.sql.hive.orc.HiveOrcQuerySuite", "org.apache.spark.sql.hive.execution.command.ShowPartitionsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.execution.command.ShowNamespacesSuite", "org.apache.spark.sql.hive.execution.HiveDDLSuite", "org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite", "org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite", "org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite", "org.apache.spark.sql.hive.execution.command.ShowTablesSuite", "org.apache.spark.sql.hive.execution.HiveQuerySuite", "org.apache.spark.sql.hive.orc.HiveOrcPartitionDiscoverySuite", "org.apache.spark.sql.hive.InsertSuite", "org.apache.spark.sql.hive.CachedTableSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.sources.DisableUnnecessaryBucketedScanWithHiveSupportSuite", "org.apache.spark.sql.hive.CompressionCodecSuite", "org.apache.spark.sql.hive.execution.WindowQuerySuite", "org.apache.spark.sql.hive.HiveExternalSessionCatalogSuite", "org.apache.spark.sql.hive.execution.HiveSQLViewSuite", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.ListTablesSuite", "org.apache.spark.sql.sources.BucketedWriteWithHiveSupportSuite", "org.apache.spark.sql.hive.execution.HashUDAQuerySuite", "org.apache.spark.sql.hive.ParquetEncryptionSuite", "org.apache.spark.sql.hive.execution.PruneHiveTablePartitionsSuite", "org.apache.spark.sql.hive.HiveUtilsSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveUDAFSuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.command.AlterTableRenamePartitionSuite", "org.apache.spark.sql.hive.HiveSessionStateSuite", "org.apache.spark.sql.hive.HiveVariableSubstitutionSuite", "org.apache.spark.sql.hive.execution.HiveSerDeReadWriteSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.UDFSuite", "org.apache.spark.sql.hive.execution.HashAggregationQueryWithControlledFallbackSuite", "org.apache.spark.sql.hive.HiveUserDefinedTypeSuite", "org.apache.spark.sql.HiveCharVarcharTestSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.BigDataBenchmarkSuite", "org.apache.spark.sql.hive.execution.ObjectHashAggregateSuite", "org.apache.spark.sql.hive.execution.command.TruncateTableSuite", "org.apache.spark.sql.hive.execution.command.AlterTableRenameSuite", "org.apache.spark.sql.hive.execution.SQLMetricsSuite", "org.apache.spark.sql.hive.OptimizeHiveMetadataOnlyQuerySuite", "org.apache.spark.sql.hive.execution.HiveCommandSuite", "org.apache.spark.sql.hive.QueryPartitionSuite", "org.apache.spark.sql.hive.execution.HashUDAQueryWithControlledFallbackSuite", "org.apache.spark.sql.hive.ParquetHiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.command.MsckRepairTableSuite", "org.apache.spark.sql.hive.HiveUDFDynamicLoadSuite", "org.apache.spark.sql.hive.TestHiveSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.HiveSQLInsertTestSuite", "org.apache.spark.sql.sources.CommitFailureTestRelationSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.execution.HiveWindowFunctionQueryFileSuite", "org.apache.spark.sql.hive.HiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.ErrorPositionSuite", "org.apache.spark.sql.hive.execution.HiveScriptTransformationSuite", "org.apache.spark.sql.hive.execution.HashAggregationQuerySuite", "org.apache.spark.sql.hive.execution.command.AlterTableRecoverPartitionsSuite", "org.apache.spark.sql.hive.execution.HiveExplainSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.execution.command.DropTableSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.execution.command.AlterTableDropPartitionSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.HiveCharVarcharDDLTestSuite", "org.apache.spark.sql.hive.execution.command.AlterTableAddPartitionSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.HiveParquetSuite", "org.apache.spark.sql.hive.HiveParquetSuite", "org.apache.spark.sql.hive.HiveContextCompatibilitySuite", "org.apache.spark.sql.hive.HiveContextCompatibilitySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite", "org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite", "org.apache.spark.sql.hive.orc.HiveOrcQuerySuite", "org.apache.spark.sql.hive.orc.HiveOrcQuerySuite", "org.apache.spark.sql.hive.execution.command.ShowPartitionsSuite", "org.apache.spark.sql.hive.execution.command.ShowPartitionsSuite", "org.apache.spark.sql.hive.execution.command.ShowNamespacesSuite", "org.apache.spark.sql.hive.execution.command.ShowNamespacesSuite", "org.apache.spark.sql.hive.client.HiveClientUserNameSuite", "org.apache.spark.sql.hive.client.HiveClientUserNameSuite", "org.apache.spark.sql.hive.client.HiveClientUserNameSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.client.HiveClientUserNameSuites.runNestedSuite", "org.apache.spark.sql.hive.client.HiveClientUserNameSuite", "org.apache.spark.sql.hive.execution.HiveResolutionSuite", "org.apache.spark.sql.hive.execution.HiveResolutionSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.execution.HiveDDLSuite", "org.apache.spark.sql.hive.execution.HiveDDLSuite", "org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite", "org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite", "org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite", "org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite", "org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite", "org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite", "org.apache.spark.sql.hive.execution.command.ShowTablesSuite", "org.apache.spark.sql.hive.execution.command.ShowTablesSuite", "org.apache.spark.sql.hive.execution.HiveQuerySuite", "org.apache.spark.sql.hive.execution.HiveQuerySuite", "org.apache.spark.sql.hive.orc.HiveOrcPartitionDiscoverySuite", "org.apache.spark.sql.hive.orc.HiveOrcPartitionDiscoverySuite", "org.apache.spark.sql.hive.InsertSuite", "org.apache.spark.sql.hive.InsertSuite", "org.apache.spark.sql.hive.CachedTableSuite", "org.apache.spark.sql.hive.CachedTableSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.sources.DisableUnnecessaryBucketedScanWithHiveSupportSuite", "org.apache.spark.sql.sources.DisableUnnecessaryBucketedScanWithHiveSupportSuite", "org.apache.spark.sql.hive.CompressionCodecSuite", "org.apache.spark.sql.hive.CompressionCodecSuite", "org.apache.spark.sql.hive.execution.WindowQuerySuite", "org.apache.spark.sql.hive.execution.WindowQuerySuite", "org.apache.spark.sql.hive.HiveExternalSessionCatalogSuite", "org.apache.spark.sql.hive.HiveExternalSessionCatalogSuite", "org.apache.spark.sql.hive.execution.HiveSQLViewSuite", "org.apache.spark.sql.hive.execution.HiveSQLViewSuite", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.ListTablesSuite", "org.apache.spark.sql.hive.ListTablesSuite", "org.apache.spark.sql.sources.BucketedWriteWithHiveSupportSuite", "org.apache.spark.sql.sources.BucketedWriteWithHiveSupportSuite", "org.apache.spark.sql.hive.execution.HashUDAQuerySuite", "org.apache.spark.sql.hive.execution.UDAQuerySuite", "org.apache.spark.sql.hive.execution.HashUDAQuerySuite", "org.apache.spark.sql.hive.ParquetEncryptionSuite", "org.apache.spark.sql.hive.ParquetEncryptionSuite", "org.apache.spark.sql.hive.execution.PruneHiveTablePartitionsSuite", "org.apache.spark.sql.hive.execution.PruneHiveTablePartitionsSuite", "org.apache.spark.sql.hive.HiveUtilsSuite", "org.apache.spark.sql.hive.HiveUtilsSuite", "org.apache.spark.sql.hive.HiveShimSuite", "org.apache.spark.sql.hive.HiveShimSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.HiveSharedStateSuite", "org.apache.spark.sql.hive.HiveSharedStateSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.client.HadoopVersionInfoSuite", "org.apache.spark.sql.hive.client.HadoopVersionInfoSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveSerDeSuite", "org.apache.spark.sql.hive.execution.HiveSerDeSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.execution.HiveUDAFSuite", "org.apache.spark.sql.hive.execution.HiveUDAFSuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.command.AlterTableRenamePartitionSuite", "org.apache.spark.sql.hive.execution.command.AlterTableRenamePartitionSuite", "org.apache.spark.sql.hive.HiveSessionStateSuite", "org.apache.spark.sql.hive.HiveSessionStateSuite", "org.apache.spark.sql.hive.HiveVariableSubstitutionSuite", "org.apache.spark.sql.hive.HiveVariableSubstitutionSuite", "org.apache.spark.sql.hive.execution.HiveSerDeReadWriteSuite", "org.apache.spark.sql.hive.execution.HiveSerDeReadWriteSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite", "org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite", "org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite", "org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.UDFSuite", "org.apache.spark.sql.hive.UDFSuite", "org.apache.spark.sql.hive.execution.HashAggregationQueryWithControlledFallbackSuite", "org.apache.spark.sql.hive.execution.AggregationQuerySuite", "org.apache.spark.sql.hive.execution.HashAggregationQueryWithControlledFallbackSuite", "org.apache.spark.sql.hive.HiveUserDefinedTypeSuite", "org.apache.spark.sql.hive.HiveUserDefinedTypeSuite", "org.apache.spark.sql.HiveCharVarcharTestSuite", "org.apache.spark.sql.HiveCharVarcharTestSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.BigDataBenchmarkSuite", "org.apache.spark.sql.hive.execution.BigDataBenchmarkSuite", "org.apache.spark.sql.hive.execution.ObjectHashAggregateSuite", "org.apache.spark.sql.hive.execution.ObjectHashAggregateSuite", "org.apache.spark.sql.hive.execution.command.TruncateTableSuite", "org.apache.spark.sql.hive.execution.command.TruncateTableSuite", "org.apache.spark.sql.hive.execution.command.AlterTableRenameSuite", "org.apache.spark.sql.hive.execution.command.AlterTableRenameSuite", "org.apache.spark.sql.hive.execution.SQLMetricsSuite", "org.apache.spark.sql.hive.execution.SQLMetricsSuite", "org.apache.spark.sql.hive.OptimizeHiveMetadataOnlyQuerySuite", "org.apache.spark.sql.hive.OptimizeHiveMetadataOnlyQuerySuite", "org.apache.spark.sql.hive.execution.HiveCommandSuite", "org.apache.spark.sql.hive.execution.HiveCommandSuite", "org.apache.spark.sql.hive.QueryPartitionSuite", "org.apache.spark.sql.hive.QueryPartitionSuite", "org.apache.spark.sql.hive.execution.HashUDAQueryWithControlledFallbackSuite", "org.apache.spark.sql.hive.execution.UDAQuerySuite", "org.apache.spark.sql.hive.execution.HashUDAQueryWithControlledFallbackSuite", "org.apache.spark.sql.hive.ParquetHiveCompatibilitySuite", "org.apache.spark.sql.hive.ParquetHiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.command.MsckRepairTableSuite", "org.apache.spark.sql.hive.execution.command.MsckRepairTableSuite", "org.apache.spark.sql.hive.HiveUDFDynamicLoadSuite", "org.apache.spark.sql.hive.HiveUDFDynamicLoadSuite", "org.apache.spark.sql.hive.TestHiveSuite", "org.apache.spark.sql.hive.TestHiveSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.HiveSQLInsertTestSuite", "org.apache.spark.sql.hive.HiveSQLInsertTestSuite", "org.apache.spark.sql.sources.CommitFailureTestRelationSuite", "org.apache.spark.sql.sources.CommitFailureTestRelationSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.client.HivePartitionFilteringSuite", "org.apache.spark.sql.hive.client.HivePartitionFilteringSuite", "org.apache.spark.sql.hive.client.HivePartitionFilteringSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.client.HivePartitionFilteringSuites.runNestedSuite", "org.apache.spark.sql.hive.client.HivePartitionFilteringSuite", "org.apache.spark.sql.hive.execution.HiveWindowFunctionQueryFileSuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveWindowFunctionQueryFileSuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.HiveWindowFunctionQueryFileSuite", "org.apache.spark.sql.hive.HiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.HiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.ErrorPositionSuite", "org.apache.spark.sql.hive.ErrorPositionSuite", "org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.hive.execution.HiveScriptTransformationSuite", "org.apache.spark.sql.hive.execution.HiveScriptTransformationSuite", "org.apache.spark.sql.hive.execution.HashAggregationQuerySuite", "org.apache.spark.sql.hive.execution.AggregationQuerySuite", "org.apache.spark.sql.hive.execution.HashAggregationQuerySuite", "org.apache.spark.sql.hive.execution.command.AlterTableRecoverPartitionsSuite", "org.apache.spark.sql.hive.execution.command.AlterTableRecoverPartitionsSuite", "org.apache.spark.sql.hive.execution.HiveExplainSuite", "org.apache.spark.sql.hive.execution.HiveExplainSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.execution.command.DropTableSuite", "org.apache.spark.sql.hive.execution.command.DropTableSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.execution.command.AlterTableDropPartitionSuite", "org.apache.spark.sql.hive.execution.command.AlterTableDropPartitionSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.hive.execution.PruningSuite", "org.apache.spark.sql.hive.execution.PruningSuite", "org.apache.spark.sql.hive.execution.PruningSuite", "org.apache.spark.sql.hive.execution.PruningSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.HiveCharVarcharDDLTestSuite", "org.apache.spark.sql.HiveCharVarcharDDLTestSuite", "org.apache.spark.sql.hive.execution.command.AlterTableAddPartitionSuite", "org.apache.spark.sql.hive.execution.command.AlterTableAddPartitionSuite", "org.apache.spark.sql.hive.JavaDataFrameSuite", "org.apache.spark.sql.hive.JavaDataFrameSuite", "org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite", "org.apache.spark.sql.hive.JavaDataFrameSuite", "org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite", "org.apache.spark.sql.hive.StatisticsSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.HiveParquetSuite", "org.apache.spark.sql.hive.HiveContextCompatibilitySuite", "org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite", "org.apache.spark.sql.hive.orc.HiveOrcQuerySuite", "org.apache.spark.sql.hive.execution.command.ShowPartitionsSuite", "org.apache.spark.sql.hive.HiveExternalCatalogSuite", "org.apache.spark.sql.hive.execution.command.ShowNamespacesSuite", "org.apache.spark.sql.hive.client.HiveClientUserNameSuite", "org.apache.spark.sql.hive.execution.HiveResolutionSuite", "org.apache.spark.sql.hive.execution.HiveDDLSuite", "org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite", "org.apache.spark.sql.sources.BucketedReadWithHiveSupportSuite", "org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite", "org.apache.spark.sql.hive.execution.command.ShowTablesSuite", "org.apache.spark.sql.hive.execution.HiveQuerySuite", "org.apache.spark.sql.hive.orc.HiveOrcPartitionDiscoverySuite", "org.apache.spark.sql.hive.InsertSuite", "org.apache.spark.sql.hive.CachedTableSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.sources.DisableUnnecessaryBucketedScanWithHiveSupportSuite", "org.apache.spark.sql.hive.CompressionCodecSuite", "org.apache.spark.sql.hive.execution.WindowQuerySuite", "org.apache.spark.sql.hive.HiveExternalSessionCatalogSuite", "org.apache.spark.sql.hive.execution.HiveSQLViewSuite", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.ListTablesSuite", "org.apache.spark.sql.sources.BucketedWriteWithHiveSupportSuite", "org.apache.spark.sql.hive.execution.HashUDAQuerySuite", "org.apache.spark.sql.hive.ParquetEncryptionSuite", "org.apache.spark.sql.hive.execution.PruneHiveTablePartitionsSuite", "org.apache.spark.sql.hive.HiveUtilsSuite", "org.apache.spark.sql.hive.HiveShimSuite", "org.apache.spark.sql.hive.HiveSharedStateSuite", "org.apache.spark.sql.hive.client.HadoopVersionInfoSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveSerDeSuite", "org.apache.spark.sql.hive.execution.HiveUDAFSuite", "org.apache.spark.sql.hive.execution.HiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.command.AlterTableRenamePartitionSuite", "org.apache.spark.sql.hive.HiveSessionStateSuite", "org.apache.spark.sql.hive.HiveVariableSubstitutionSuite", "org.apache.spark.sql.hive.execution.HiveSerDeReadWriteSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.UDFSuite", "org.apache.spark.sql.hive.execution.HashAggregationQueryWithControlledFallbackSuite", "org.apache.spark.sql.hive.HiveUserDefinedTypeSuite", "org.apache.spark.sql.HiveCharVarcharTestSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.BigDataBenchmarkSuite", "org.apache.spark.sql.hive.execution.ObjectHashAggregateSuite", "org.apache.spark.sql.hive.execution.command.TruncateTableSuite", "org.apache.spark.sql.hive.execution.command.AlterTableRenameSuite", "org.apache.spark.sql.hive.execution.SQLMetricsSuite", "org.apache.spark.sql.hive.OptimizeHiveMetadataOnlyQuerySuite", "org.apache.spark.sql.hive.execution.HiveCommandSuite", "org.apache.spark.sql.hive.QueryPartitionSuite", "org.apache.spark.sql.hive.execution.HashUDAQueryWithControlledFallbackSuite", "org.apache.spark.sql.hive.ParquetHiveCompatibilitySuite", "org.apache.spark.sql.hive.execution.command.MsckRepairTableSuite", "org.apache.spark.sql.hive.HiveUDFDynamicLoadSuite", "org.apache.spark.sql.hive.TestHiveSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.HiveSQLInsertTestSuite", "org.apache.spark.sql.sources.CommitFailureTestRelationSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.client.HivePartitionFilteringSuite", "org.apache.spark.sql.hive.execution.HiveWindowFunctionQueryFileSuite", "org.apache.spark.sql.hive.HiveMetastoreCatalogSuite", "org.apache.spark.sql.hive.ErrorPositionSuite", "org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite", "org.apache.spark.sql.hive.execution.HiveScriptTransformationSuite", "org.apache.spark.sql.hive.execution.HashAggregationQuerySuite", "org.apache.spark.sql.hive.execution.command.AlterTableRecoverPartitionsSuite", "org.apache.spark.sql.hive.execution.HiveExplainSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.execution.command.DropTableSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.execution.command.AlterTableDropPartitionSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.hive.execution.PruningSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite", "org.apache.spark.sql.HiveCharVarcharDDLTestSuite", "org.apache.spark.sql.hive.execution.command.AlterTableAddPartitionSuite", "org.apache.spark.sql.execution.datasources.BasicWriteTaskStatsTrackerSuite", "org.apache.spark.sql.execution.datasources.BasicWriteTaskStatsTrackerSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV2SchemaPruningSuite", "org.apache.spark.sql.execution.datasources.SchemaPruningSuite", "org.apache.spark.sql.execution.datasources.SchemaPruningSuite", "org.apache.spark.sql.execution.datasources.SchemaPruningSuite", "org.apache.spark.sql.execution.datasources.SchemaPruningSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.TPCDSQuerySuite", "org.apache.spark.sql.TPCDSQuerySuite", "org.apache.spark.sql.TPCDSQuerySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV2FilterSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetFilterSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetFilterSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetFilterSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetFilterSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV1FilterSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetFilterSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetFilterSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetFilterSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetFilterSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.ExpressionsSchemaSuite", "org.apache.spark.sql.ExpressionsSchemaSuite", "org.apache.spark.sql.ExpressionsSchemaSuite", "org.apache.spark.sql.ExpressionsSchemaSuite", "org.apache.spark.sql.ExpressionsSchemaSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.TPCDSQueryWithStatsSuite", "org.apache.spark.sql.TPCDSQuerySuite", "org.apache.spark.sql.TPCDSQuerySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV1SchemaPruningSuite", "org.apache.spark.sql.execution.datasources.SchemaPruningSuite", "org.apache.spark.sql.execution.datasources.SchemaPruningSuite", "org.apache.spark.sql.execution.datasources.SchemaPruningSuite", "org.apache.spark.sql.execution.datasources.SchemaPruningSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.TPCDSQueryANSISuite", "org.apache.spark.sql.TPCDSQuerySuite", "org.apache.spark.sql.TPCDSQuerySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.BasicWriteTaskStatsTrackerSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV2SchemaPruningSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.TPCDSQuerySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV2FilterSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV1FilterSuite", "org.apache.spark.sql.ExpressionsSchemaSuite", "org.apache.spark.sql.TPCDSQueryWithStatsSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV1SchemaPruningSuite", "org.apache.spark.sql.TPCDSQueryANSISuite", "org.apache.spark.sql.SQLQuerySuite", "org.apache.spark.sql.SQLQuerySuite", "org.apache.spark.sql.SQLQuerySuite", "org.apache.spark.sql.SQLQuerySuite", "org.apache.spark.sql.SQLQuerySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.MemorySourceStressSuite", "org.apache.spark.sql.streaming.MemorySourceStressSuite", "org.apache.spark.sql.streaming.MemorySourceStressSuite", "org.apache.spark.sql.streaming.MemorySourceStressSuite", "org.apache.spark.sql.streaming.MemorySourceStressSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.HiveResultSuite", "org.apache.spark.sql.execution.HiveResultSuite", "org.apache.spark.sql.execution.HiveResultSuite", "org.apache.spark.sql.execution.HiveResultSuite", "org.apache.spark.sql.execution.HiveResultSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderSuite", "org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderSuite", "org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderSuite", "org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderSuite", "org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.DSV2CharVarcharDDLTestSuite", "org.apache.spark.sql.execution.command.DSV2CharVarcharDDLTestSuite", "org.apache.spark.sql.execution.command.DSV2CharVarcharDDLTestSuite", "org.apache.spark.sql.execution.command.DSV2CharVarcharDDLTestSuite", "org.apache.spark.sql.execution.command.DSV2CharVarcharDDLTestSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.SparkPlanSuite", "org.apache.spark.sql.execution.SparkPlanSuite", "org.apache.spark.sql.execution.SparkPlanSuite", "org.apache.spark.sql.execution.SparkPlanSuite", "org.apache.spark.sql.execution.SparkPlanSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.StreamingAggregationSuite", "org.apache.spark.sql.streaming.StreamingAggregationSuite", "org.apache.spark.sql.streaming.StreamingAggregationSuite", "org.apache.spark.sql.streaming.StreamingAggregationSuite", "org.apache.spark.sql.streaming.StreamingAggregationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DateFunctionsSuite", "org.apache.spark.sql.DateFunctionsSuite", "org.apache.spark.sql.DateFunctionsSuite", "org.apache.spark.sql.DateFunctionsSuite", "org.apache.spark.sql.DateFunctionsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.NestedDataSourceV1Suite", "org.apache.spark.sql.NestedDataSourceV1Suite", "org.apache.spark.sql.NestedDataSourceV1Suite", "org.apache.spark.sql.NestedDataSourceV1Suite", "org.apache.spark.sql.NestedDataSourceV1Suite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.state.StateStoreSuite", "org.apache.spark.sql.execution.streaming.state.StateStoreSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.state.StateStoreSuite", "org.apache.spark.sql.execution.streaming.state.StateStoreSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.state.StateStoreSuite", "org.apache.spark.sql.execution.streaming.state.StateStoreSuite", "org.apache.spark.sql.execution.streaming.sources.ForeachBatchSinkSuite", "org.apache.spark.sql.execution.streaming.sources.ForeachBatchSinkSuite", "org.apache.spark.sql.execution.streaming.sources.ForeachBatchSinkSuite", "org.apache.spark.sql.execution.streaming.sources.ForeachBatchSinkSuite", "org.apache.spark.sql.execution.streaming.sources.ForeachBatchSinkSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.jdbc.JDBCV2Suite", "org.apache.spark.sql.jdbc.JDBCV2Suite", "org.apache.spark.sql.jdbc.JDBCV2Suite", "org.apache.spark.sql.jdbc.JDBCV2Suite", "org.apache.spark.sql.jdbc.JDBCV2Suite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.continuous.ContinuousQueryStatusAndProgressSuite", "org.apache.spark.sql.streaming.continuous.ContinuousSuite", "org.apache.spark.sql.streaming.continuous.ContinuousSuite", "org.apache.spark.sql.streaming.continuous.ContinuousSuite", "org.apache.spark.sql.streaming.continuous.ContinuousSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite", "org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite", "org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite", "org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite", "org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRenameSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRenameSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRenameSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRenameSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRenameSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.v2.V2SessionCatalogTableSuite", "org.apache.spark.sql.execution.datasources.v2.V2SessionCatalogBaseSuite", "org.apache.spark.sql.execution.datasources.v2.V2SessionCatalogBaseSuite", "org.apache.spark.sql.execution.datasources.v2.V2SessionCatalogBaseSuite", "org.apache.spark.sql.execution.datasources.v2.V2SessionCatalogTableSuite", "org.apache.spark.sql.execution.datasources.v2.V2SessionCatalogBaseSuite", "org.apache.spark.sql.execution.datasources.v2.V2SessionCatalogBaseSuite", "org.apache.spark.sql.execution.datasources.BasicWriteTaskStatsTrackerSuite", "org.apache.spark.sql.execution.datasources.BasicWriteTaskStatsTrackerSuite", "org.apache.spark.sql.execution.arrow.ArrowConvertersSuite", "org.apache.spark.sql.execution.arrow.ArrowConvertersSuite", "org.apache.spark.sql.execution.arrow.ArrowConvertersSuite", "org.apache.spark.sql.execution.arrow.ArrowConvertersSuite", "org.apache.spark.sql.execution.arrow.ArrowConvertersSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.v2.AlterTableDropPartitionSuite", "org.apache.spark.sql.execution.command.v2.AlterTableDropPartitionSuite", "org.apache.spark.sql.execution.command.v2.AlterTableDropPartitionSuite", "org.apache.spark.sql.execution.command.v2.AlterTableDropPartitionSuite", "org.apache.spark.sql.execution.command.v2.AlterTableDropPartitionSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DynamicPartitionPruningV1Suite", "org.apache.spark.sql.DynamicPartitionPruningSuite", "org.apache.spark.sql.DynamicPartitionPruningSuite", "org.apache.spark.sql.DynamicPartitionPruningSuite", "org.apache.spark.sql.DynamicPartitionPruningSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite", "org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite", "org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite", "org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite", "org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.v1.ShowPartitionsSuite", "org.apache.spark.sql.execution.command.v1.ShowPartitionsSuite", "org.apache.spark.sql.execution.command.v1.ShowPartitionsSuite", "org.apache.spark.sql.execution.command.v1.ShowPartitionsSuite", "org.apache.spark.sql.execution.command.v1.ShowPartitionsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.ReplaceNullWithFalseInPredicateEndToEndSuite", "org.apache.spark.sql.ReplaceNullWithFalseInPredicateEndToEndSuite", "org.apache.spark.sql.ReplaceNullWithFalseInPredicateEndToEndSuite", "org.apache.spark.sql.ReplaceNullWithFalseInPredicateEndToEndSuite", "org.apache.spark.sql.ReplaceNullWithFalseInPredicateEndToEndSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.continuous.ContinuousEpochBacklogSuite", "org.apache.spark.sql.streaming.continuous.ContinuousEpochBacklogSuite", "org.apache.spark.sql.streaming.continuous.ContinuousSuite", "org.apache.spark.sql.streaming.continuous.ContinuousSuite", "org.apache.spark.sql.streaming.continuous.ContinuousSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DeprecatedAPISuite", "org.apache.spark.sql.DeprecatedAPISuite", "org.apache.spark.sql.DeprecatedAPISuite", "org.apache.spark.sql.DeprecatedAPISuite", "org.apache.spark.sql.DeprecatedAPISuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.MergedParquetReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.MergedParquetReadSchemaSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.StreamingQueryListenersConfSuite", "org.apache.spark.sql.streaming.StreamingQueryListenersConfSuite", "org.apache.spark.sql.streaming.StreamingQueryListenersConfSuite", "org.apache.spark.sql.streaming.StreamingQueryListenersConfSuite", "org.apache.spark.sql.streaming.StreamingQueryListenersConfSuite", "org.apache.spark.sql.streaming.StreamingQueryListenersConfSuite", "org.apache.spark.sql.streaming.StreamingQueryListenersConfSuite", "org.apache.spark.sql.execution.datasources.OrcReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.OrcReadSchemaSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.jdbc.JDBCSuite", "org.apache.spark.sql.jdbc.JDBCSuite", "org.apache.spark.sql.jdbc.JDBCSuite", "org.apache.spark.sql.jdbc.JDBCSuite", "org.apache.spark.sql.jdbc.JDBCSuite", "org.apache.spark.sql.jdbc.JDBCSuite", "org.apache.spark.sql.jdbc.JDBCSuite", "org.apache.spark.sql.execution.datasources.HeaderCSVReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCTableCatalogSuite", "org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCTableCatalogSuite", "org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCTableCatalogSuite", "org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCTableCatalogSuite", "org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCTableCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DataFrameJoinSuite", "org.apache.spark.sql.DataFrameJoinSuite", "org.apache.spark.sql.DataFrameJoinSuite", "org.apache.spark.sql.DataFrameJoinSuite", "org.apache.spark.sql.DataFrameJoinSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.SQLWindowFunctionSuite", "org.apache.spark.sql.execution.SQLWindowFunctionSuite", "org.apache.spark.sql.execution.SQLWindowFunctionSuite", "org.apache.spark.sql.execution.SQLWindowFunctionSuite", "org.apache.spark.sql.execution.SQLWindowFunctionSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.continuous.EpochCoordinatorSuite", "org.apache.spark.sql.streaming.continuous.EpochCoordinatorSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.sources.StreamingDataSourceV2Suite", "org.apache.spark.sql.streaming.sources.StreamingDataSourceV2Suite", "org.apache.spark.sql.streaming.sources.StreamingDataSourceV2Suite", "org.apache.spark.sql.streaming.sources.StreamingDataSourceV2Suite", "org.apache.spark.sql.streaming.sources.StreamingDataSourceV2Suite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.execution.command.v2.AlterTableRecoverPartitionsSuite", "org.apache.spark.sql.execution.command.v2.AlterTableRecoverPartitionsSuite", "org.apache.spark.sql.execution.command.v2.AlterTableRecoverPartitionsSuite", "org.apache.spark.sql.execution.command.v2.AlterTableRecoverPartitionsSuite", "org.apache.spark.sql.execution.command.v2.AlterTableRecoverPartitionsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.TypedImperativeAggregateSuite", "org.apache.spark.sql.TypedImperativeAggregateSuite", "org.apache.spark.sql.TypedImperativeAggregateSuite", "org.apache.spark.sql.TypedImperativeAggregateSuite", "org.apache.spark.sql.TypedImperativeAggregateSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.binaryfile.BinaryFileFormatSuite", "org.apache.spark.sql.execution.datasources.binaryfile.BinaryFileFormatSuite", "org.apache.spark.sql.execution.datasources.binaryfile.BinaryFileFormatSuite", "org.apache.spark.sql.execution.datasources.binaryfile.BinaryFileFormatSuite", "org.apache.spark.sql.execution.datasources.binaryfile.BinaryFileFormatSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.FileIndexSuite", "org.apache.spark.sql.execution.datasources.FileIndexSuite", "org.apache.spark.sql.execution.datasources.FileIndexSuite", "org.apache.spark.sql.execution.datasources.FileIndexSuite", "org.apache.spark.sql.execution.datasources.FileIndexSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetRebaseDatetimeV2Suite", "org.apache.spark.sql.execution.datasources.parquet.ParquetRebaseDatetimeSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetRebaseDatetimeSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetRebaseDatetimeSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetRebaseDatetimeSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DatasetPrimitiveSuite", "org.apache.spark.sql.DatasetPrimitiveSuite", "org.apache.spark.sql.DatasetPrimitiveSuite", "org.apache.spark.sql.DatasetPrimitiveSuite", "org.apache.spark.sql.DatasetPrimitiveSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.text.WholeTextFileV2Suite", "org.apache.spark.sql.execution.datasources.text.WholeTextFileSuite", "org.apache.spark.sql.execution.datasources.text.WholeTextFileSuite", "org.apache.spark.sql.execution.datasources.text.WholeTextFileSuite", "org.apache.spark.sql.execution.datasources.text.WholeTextFileSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetColumnIndexSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetColumnIndexSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetColumnIndexSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetColumnIndexSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetColumnIndexSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.PersistedViewTestSuite", "org.apache.spark.sql.execution.PersistedViewTestSuite", "org.apache.spark.sql.execution.PersistedViewTestSuite", "org.apache.spark.sql.execution.PersistedViewTestSuite", "org.apache.spark.sql.execution.PersistedViewTestSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.connector.DataSourceV2FunctionSuite", "org.apache.spark.sql.connector.DataSourceV2FunctionSuite", "org.apache.spark.sql.connector.DataSourceV2FunctionSuite", "org.apache.spark.sql.connector.DataSourceV2FunctionSuite", "org.apache.spark.sql.connector.DataSourceV2FunctionSuite", "org.apache.spark.sql.connector.DataSourceV2FunctionSuite", "org.apache.spark.sql.connector.DataSourceV2FunctionSuite", "org.apache.spark.sql.execution.streaming.UpdatingSessionsIteratorSuite", "org.apache.spark.sql.execution.streaming.UpdatingSessionsIteratorSuite", "org.apache.spark.sql.execution.streaming.UpdatingSessionsIteratorSuite", "org.apache.spark.sql.execution.streaming.UpdatingSessionsIteratorSuite", "org.apache.spark.sql.execution.streaming.UpdatingSessionsIteratorSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.connector.V1ReadFallbackWithCatalogSuite", "org.apache.spark.sql.connector.V1ReadFallbackSuite", "org.apache.spark.sql.connector.V1ReadFallbackSuite", "org.apache.spark.sql.connector.V1ReadFallbackSuite", "org.apache.spark.sql.connector.V1ReadFallbackWithCatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.connector.V1WriteFallbackSessionCatalogSuite", "org.apache.spark.sql.execution.streaming.state.StateSchemaCompatibilityCheckerSuite", "org.apache.spark.sql.execution.streaming.state.StateSchemaCompatibilityCheckerSuite", "org.apache.spark.sql.execution.streaming.state.StateSchemaCompatibilityCheckerSuite", "org.apache.spark.sql.execution.streaming.state.StateSchemaCompatibilityCheckerSuite", "org.apache.spark.sql.execution.streaming.state.StateSchemaCompatibilityCheckerSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.internal.CatalogSuite", "org.apache.spark.sql.internal.CatalogSuite", "org.apache.spark.sql.internal.CatalogSuite", "org.apache.spark.sql.internal.CatalogSuite", "org.apache.spark.sql.internal.CatalogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.connector.WriteDistributionAndOrderingSuite", "org.apache.spark.sql.connector.WriteDistributionAndOrderingSuite", "org.apache.spark.sql.connector.WriteDistributionAndOrderingSuite", "org.apache.spark.sql.connector.WriteDistributionAndOrderingSuite", "org.apache.spark.sql.connector.WriteDistributionAndOrderingSuite", "org.apache.spark.sql.connector.WriteDistributionAndOrderingSuite", "org.apache.spark.sql.connector.WriteDistributionAndOrderingSuite", "org.apache.spark.sql.streaming.continuous.ContinuousQueuedDataReaderSuite", "org.apache.spark.sql.streaming.continuous.ContinuousQueuedDataReaderSuite", "org.apache.spark.sql.streaming.continuous.ContinuousQueuedDataReaderSuite", "org.apache.spark.sql.streaming.continuous.ContinuousQueuedDataReaderSuite", "org.apache.spark.sql.streaming.continuous.ContinuousQueuedDataReaderSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.GeneratorFunctionSuite", "org.apache.spark.sql.GeneratorFunctionSuite", "org.apache.spark.sql.GeneratorFunctionSuite", "org.apache.spark.sql.GeneratorFunctionSuite", "org.apache.spark.sql.GeneratorFunctionSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.csv.CSVLegacyTimeParserSuite", "org.apache.spark.sql.execution.datasources.csv.CSVSuite", "org.apache.spark.sql.execution.datasources.csv.CSVSuite", "org.apache.spark.sql.execution.datasources.csv.CSVSuite", "org.apache.spark.sql.execution.datasources.csv.CSVSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorSuite", "org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorSuite", "org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.DataSourceStrategySuite", "org.apache.spark.sql.execution.datasources.DataSourceStrategySuite", "org.apache.spark.sql.execution.datasources.DataSourceStrategySuite", "org.apache.spark.sql.execution.datasources.DataSourceStrategySuite", "org.apache.spark.sql.execution.datasources.DataSourceStrategySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.FileStreamSourceSuite", "org.apache.spark.sql.streaming.FileStreamSourceSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.JoinHintSuite", "org.apache.spark.sql.JoinHintSuite", "org.apache.spark.sql.JoinHintSuite", "org.apache.spark.sql.JoinHintSuite", "org.apache.spark.sql.JoinHintSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DynamicPartitionPruningV2Suite", "org.apache.spark.sql.DynamicPartitionPruningSuite", "org.apache.spark.sql.DynamicPartitionPruningSuite", "org.apache.spark.sql.DynamicPartitionPruningSuite", "org.apache.spark.sql.DynamicPartitionPruningSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.StreamingSessionWindowSuite", "org.apache.spark.sql.streaming.StreamingSessionWindowSuite", "org.apache.spark.sql.streaming.StreamingSessionWindowSuite", "org.apache.spark.sql.streaming.StreamingSessionWindowSuite", "org.apache.spark.sql.streaming.StreamingSessionWindowSuite", "org.apache.spark.sql.streaming.StreamingSessionWindowSuite", "org.apache.spark.sql.streaming.StreamingSessionWindowSuite", "org.apache.spark.sql.DSV2SQLInsertTestSuite", "org.apache.spark.sql.DSV2SQLInsertTestSuite", "org.apache.spark.sql.DSV2SQLInsertTestSuite", "org.apache.spark.sql.DSV2SQLInsertTestSuite", "org.apache.spark.sql.DSV2SQLInsertTestSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.python.ExtractPythonUDFsSuite", "org.apache.spark.sql.execution.python.ExtractPythonUDFsSuite", "org.apache.spark.sql.execution.python.ExtractPythonUDFsSuite", "org.apache.spark.sql.execution.python.ExtractPythonUDFsSuite", "org.apache.spark.sql.execution.python.ExtractPythonUDFsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DynamicPartitionPruningV1Suite", "org.apache.spark.sql.DynamicPartitionPruningSuite", "org.apache.spark.sql.DynamicPartitionPruningSuite", "org.apache.spark.sql.DynamicPartitionPruningSuite", "org.apache.spark.sql.DynamicPartitionPruningSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite", "org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite", "org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite", "org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite", "org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.ui.SQLAppStatusListenerSuite", "org.apache.spark.sql.execution.ui.SQLAppStatusListenerSuite", "org.apache.spark.sql.execution.ui.SQLAppStatusListenerSuite", "org.apache.spark.sql.execution.ui.SQLAppStatusListenerSuite", "org.apache.spark.sql.execution.ui.SQLAppStatusListenerSuite", "org.apache.spark.sql.execution.ui.SQLAppStatusListenerSuite", "org.apache.spark.sql.execution.ui.SQLAppStatusListenerSuite", "org.apache.spark.sql.execution.datasources.orc.OrcV1FilterSuite", "org.apache.spark.sql.execution.datasources.orc.OrcFilterSuite", "org.apache.spark.sql.execution.datasources.orc.OrcFilterSuite", "org.apache.spark.sql.execution.datasources.orc.OrcFilterSuite", "org.apache.spark.sql.execution.datasources.orc.OrcFilterSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.connector.DataSourceV2SQLSuite", "org.apache.spark.sql.DatasetCacheSuite", "org.apache.spark.sql.DatasetCacheSuite", "org.apache.spark.sql.DatasetCacheSuite", "org.apache.spark.sql.DatasetCacheSuite", "org.apache.spark.sql.DatasetCacheSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.JoinSuite", "org.apache.spark.sql.JoinSuite", "org.apache.spark.sql.JoinSuite", "org.apache.spark.sql.JoinSuite", "org.apache.spark.sql.JoinSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.joins.BroadcastJoinSuite", "org.apache.spark.sql.execution.joins.BroadcastJoinSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.UDFSuite", "org.apache.spark.sql.UDFSuite", "org.apache.spark.sql.UDFSuite", "org.apache.spark.sql.UDFSuite", "org.apache.spark.sql.UDFSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRecoverPartitionsSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRecoverPartitionsSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRecoverPartitionsSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRecoverPartitionsSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRecoverPartitionsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.CachedTableSuite", "org.apache.spark.sql.CachedTableSuite", "org.apache.spark.sql.CachedTableSuite", "org.apache.spark.sql.CachedTableSuite", "org.apache.spark.sql.CachedTableSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.ShowTablesParserSuite", "org.apache.spark.sql.execution.command.ShowTablesParserSuite", "org.apache.spark.sql.execution.command.ShowTablesParserSuite", "org.apache.spark.sql.execution.command.ShowTablesParserSuite", "org.apache.spark.sql.execution.command.ShowTablesParserSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.sources.PathOptionSuite", "org.apache.spark.sql.sources.PathOptionSuite", "org.apache.spark.sql.sources.PathOptionSuite", "org.apache.spark.sql.sources.PathOptionSuite", "org.apache.spark.sql.sources.PathOptionSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.AlterTableAddPartitionParserSuite", "org.apache.spark.sql.execution.command.AlterTableAddPartitionParserSuite", "org.apache.spark.sql.execution.command.AlterTableAddPartitionParserSuite", "org.apache.spark.sql.execution.command.AlterTableAddPartitionParserSuite", "org.apache.spark.sql.execution.command.AlterTableAddPartitionParserSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaInferenceSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.GlobalTempViewTestSuite", "org.apache.spark.sql.execution.GlobalTempViewTestSuite", "org.apache.spark.sql.execution.GlobalTempViewTestSuite", "org.apache.spark.sql.execution.GlobalTempViewTestSuite", "org.apache.spark.sql.execution.GlobalTempViewTestSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.SerializationSuite", "org.apache.spark.sql.SerializationSuite", "org.apache.spark.sql.SerializationSuite", "org.apache.spark.sql.SerializationSuite", "org.apache.spark.sql.SerializationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DataFrameWindowFunctionsSuite", "org.apache.spark.sql.DataFrameWindowFunctionsSuite", "org.apache.spark.sql.DataFrameWindowFunctionsSuite", "org.apache.spark.sql.DataFrameWindowFunctionsSuite", "org.apache.spark.sql.DataFrameWindowFunctionsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.text.TextV1Suite", "org.apache.spark.sql.execution.datasources.text.TextSuite", "org.apache.spark.sql.execution.datasources.text.TextSuite", "org.apache.spark.sql.execution.datasources.text.TextSuite", "org.apache.spark.sql.execution.datasources.text.TextSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.sources.BucketedWriteWithoutHiveSupportSuite", "org.apache.spark.sql.sources.BucketedWriteWithoutHiveSupportSuite", "org.apache.spark.sql.sources.BucketedWriteWithoutHiveSupportSuite", "org.apache.spark.sql.sources.BucketedWriteWithoutHiveSupportSuite", "org.apache.spark.sql.sources.BucketedWriteWithoutHiveSupportSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetAvroCompatibilitySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetAvroCompatibilitySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetAvroCompatibilitySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetAvroCompatibilitySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetAvroCompatibilitySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.sources.CreateTableAsSelectSuite", "org.apache.spark.sql.sources.CreateTableAsSelectSuite", "org.apache.spark.sql.sources.CreateTableAsSelectSuite", "org.apache.spark.sql.sources.CreateTableAsSelectSuite", "org.apache.spark.sql.sources.CreateTableAsSelectSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.ColumnarRulesSuite", "org.apache.spark.sql.execution.ColumnarRulesSuite", "org.apache.spark.sql.execution.ColumnarRulesSuite", "org.apache.spark.sql.execution.ColumnarRulesSuite", "org.apache.spark.sql.execution.ColumnarRulesSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.SQLContextSuite", "org.apache.spark.sql.SQLContextSuite", "org.apache.spark.sql.SQLContextSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.UnwrapCastInComparisonEndToEndSuite", "org.apache.spark.sql.UnwrapCastInComparisonEndToEndSuite", "org.apache.spark.sql.UnwrapCastInComparisonEndToEndSuite", "org.apache.spark.sql.UnwrapCastInComparisonEndToEndSuite", "org.apache.spark.sql.UnwrapCastInComparisonEndToEndSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.SetCommandSuite", "org.apache.spark.sql.SetCommandSuite", "org.apache.spark.sql.SetCommandSuite", "org.apache.spark.sql.SetCommandSuite", "org.apache.spark.sql.SetCommandSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.StatisticsCollectionSuite", "org.apache.spark.sql.StatisticsCollectionSuite", "org.apache.spark.sql.StatisticsCollectionSuite", "org.apache.spark.sql.StatisticsCollectionSuite", "org.apache.spark.sql.StatisticsCollectionSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.SSBQuerySuite", "org.apache.spark.sql.SSBQuerySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.StreamingQueryListenerSuite", "org.apache.spark.sql.streaming.StreamingQueryListenerSuite", "org.apache.spark.sql.streaming.StreamingQueryListenerSuite", "org.apache.spark.sql.streaming.StreamingQueryListenerSuite", "org.apache.spark.sql.streaming.StreamingQueryListenerSuite", "org.apache.spark.sql.streaming.StreamingQueryListenerSuite", "org.apache.spark.sql.streaming.StreamingQueryListenerSuite", "org.apache.spark.sql.PlanStabilitySuite", "org.apache.spark.sql.PlanStabilitySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.FileStreamSinkV1Suite", "org.apache.spark.sql.streaming.FileStreamSinkSuite", "org.apache.spark.sql.streaming.FileStreamSinkSuite", "org.apache.spark.sql.streaming.FileStreamSinkSuite", "org.apache.spark.sql.streaming.FileStreamSinkSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.StreamingOuterJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.test.DataFrameReaderWriterSuite", "org.apache.spark.sql.test.DataFrameReaderWriterSuite", "org.apache.spark.sql.test.DataFrameReaderWriterSuite", "org.apache.spark.sql.test.DataFrameReaderWriterSuite", "org.apache.spark.sql.test.DataFrameReaderWriterSuite", "org.apache.spark.sql.test.DataFrameReaderWriterSuite", "org.apache.spark.sql.test.DataFrameReaderWriterSuite", "org.apache.spark.sql.StringFunctionsSuite", "org.apache.spark.sql.StringFunctionsSuite", "org.apache.spark.sql.StringFunctionsSuite", "org.apache.spark.sql.StringFunctionsSuite", "org.apache.spark.sql.StringFunctionsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.NestedDataSourceV2Suite", "org.apache.spark.sql.NestedDataSourceV2Suite", "org.apache.spark.sql.NestedDataSourceV2Suite", "org.apache.spark.sql.NestedDataSourceV2Suite", "org.apache.spark.sql.NestedDataSourceV2Suite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.AlterTableRecoverPartitionsParserSuite", "org.apache.spark.sql.execution.command.AlterTableRecoverPartitionsParserSuite", "org.apache.spark.sql.execution.command.AlterTableRecoverPartitionsParserSuite", "org.apache.spark.sql.execution.command.AlterTableRecoverPartitionsParserSuite", "org.apache.spark.sql.execution.command.AlterTableRecoverPartitionsParserSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetRebaseDatetimeV1Suite", "org.apache.spark.sql.execution.datasources.parquet.ParquetRebaseDatetimeSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetRebaseDatetimeSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetRebaseDatetimeSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetRebaseDatetimeSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.sources.PrunedScanSuite", "org.apache.spark.sql.sources.PrunedScanSuite", "org.apache.spark.sql.sources.PrunedScanSuite", "org.apache.spark.sql.sources.PrunedScanSuite", "org.apache.spark.sql.sources.PrunedScanSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.TruncateTableParserSuite", "org.apache.spark.sql.execution.command.TruncateTableParserSuite", "org.apache.spark.sql.execution.command.TruncateTableParserSuite", "org.apache.spark.sql.execution.command.TruncateTableParserSuite", "org.apache.spark.sql.execution.command.TruncateTableParserSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.CompactibleFileStreamLogSuite", "org.apache.spark.sql.execution.streaming.CompactibleFileStreamLogSuite", "org.apache.spark.sql.execution.streaming.CompactibleFileStreamLogSuite", "org.apache.spark.sql.execution.streaming.CompactibleFileStreamLogSuite", "org.apache.spark.sql.execution.streaming.CompactibleFileStreamLogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.StreamingQueryManagerSuite", "org.apache.spark.sql.streaming.StreamingQueryManagerSuite", "org.apache.spark.sql.streaming.StreamingQueryManagerSuite", "org.apache.spark.sql.streaming.StreamingQueryManagerSuite", "org.apache.spark.sql.streaming.StreamingQueryManagerSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.noop.NoopStreamSuite", "org.apache.spark.sql.execution.datasources.noop.NoopStreamSuite", "org.apache.spark.sql.execution.datasources.noop.NoopStreamSuite", "org.apache.spark.sql.execution.datasources.noop.NoopStreamSuite", "org.apache.spark.sql.execution.datasources.noop.NoopStreamSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.CsvFunctionsSuite", "org.apache.spark.sql.CsvFunctionsSuite", "org.apache.spark.sql.CsvFunctionsSuite", "org.apache.spark.sql.CsvFunctionsSuite", "org.apache.spark.sql.CsvFunctionsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.connector.DataSourceV2DataFrameSessionCatalogSuite", "org.apache.spark.sql.execution.SortSuite", "org.apache.spark.sql.execution.SortSuite", "org.apache.spark.sql.execution.SortSuite", "org.apache.spark.sql.execution.SortSuite", "org.apache.spark.sql.execution.SortSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.SparkSessionBuilderSuite", "org.apache.spark.sql.SparkSessionBuilderSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.continuous.ContinuousStressSuite", "org.apache.spark.sql.streaming.continuous.ContinuousSuite", "org.apache.spark.sql.streaming.continuous.ContinuousSuite", "org.apache.spark.sql.streaming.continuous.ContinuousSuite", "org.apache.spark.sql.streaming.continuous.ContinuousSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.columnar.RefCountedTestCachedBatchSerializerSuite", "org.apache.spark.sql.execution.columnar.RefCountedTestCachedBatchSerializerSuite", "org.apache.spark.sql.execution.columnar.RefCountedTestCachedBatchSerializerSuite", "org.apache.spark.sql.execution.columnar.RefCountedTestCachedBatchSerializerSuite", "org.apache.spark.sql.execution.columnar.RefCountedTestCachedBatchSerializerSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV1QuerySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.sources.FilteredScanSuite", "org.apache.spark.sql.sources.FilteredScanSuite", "org.apache.spark.sql.sources.FilteredScanSuite", "org.apache.spark.sql.sources.FilteredScanSuite", "org.apache.spark.sql.sources.FilteredScanSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.BasicCharVarcharTestSuite", "org.apache.spark.sql.BasicCharVarcharTestSuite", "org.apache.spark.sql.BasicCharVarcharTestSuite", "org.apache.spark.sql.BasicCharVarcharTestSuite", "org.apache.spark.sql.BasicCharVarcharTestSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.SubquerySuite", "org.apache.spark.sql.SubquerySuite", "org.apache.spark.sql.SubquerySuite", "org.apache.spark.sql.SubquerySuite", "org.apache.spark.sql.SubquerySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.ShowPartitionsParserSuite", "org.apache.spark.sql.execution.command.ShowPartitionsParserSuite", "org.apache.spark.sql.execution.command.ShowPartitionsParserSuite", "org.apache.spark.sql.execution.command.ShowPartitionsParserSuite", "org.apache.spark.sql.execution.command.ShowPartitionsParserSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.InMemoryTableMetricSuite", "org.apache.spark.sql.execution.datasources.InMemoryTableMetricSuite", "org.apache.spark.sql.execution.datasources.InMemoryTableMetricSuite", "org.apache.spark.sql.execution.datasources.InMemoryTableMetricSuite", "org.apache.spark.sql.execution.datasources.InMemoryTableMetricSuite", "org.apache.spark.sql.execution.datasources.InMemoryTableMetricSuite", "org.apache.spark.sql.execution.datasources.InMemoryTableMetricSuite", "org.apache.spark.sql.execution.ui.AllExecutionsPageSuite", "org.apache.spark.sql.execution.ui.AllExecutionsPageSuite", "org.apache.spark.sql.execution.ui.AllExecutionsPageSuite", "org.apache.spark.sql.execution.ui.AllExecutionsPageSuite", "org.apache.spark.sql.execution.ui.AllExecutionsPageSuite", "org.apache.spark.sql.execution.ui.AllExecutionsPageSuite", "org.apache.spark.sql.execution.ui.AllExecutionsPageSuite", "org.apache.spark.sql.sources.TableScanSuite", "org.apache.spark.sql.sources.TableScanSuite", "org.apache.spark.sql.sources.TableScanSuite", "org.apache.spark.sql.sources.TableScanSuite", "org.apache.spark.sql.sources.TableScanSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.StreamingDeduplicationSuite", "org.apache.spark.sql.streaming.StreamingDeduplicationSuite", "org.apache.spark.sql.streaming.StreamingDeduplicationSuite", "org.apache.spark.sql.streaming.StreamingDeduplicationSuite", "org.apache.spark.sql.streaming.StreamingDeduplicationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.v1.MsckRepairTableSuite", "org.apache.spark.sql.execution.command.v1.MsckRepairTableSuite", "org.apache.spark.sql.execution.command.v1.MsckRepairTableSuite", "org.apache.spark.sql.execution.command.v1.MsckRepairTableSuite", "org.apache.spark.sql.execution.command.v1.MsckRepairTableSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.MetadataCacheV2Suite", "org.apache.spark.sql.MetadataCacheSuite", "org.apache.spark.sql.MetadataCacheSuite", "org.apache.spark.sql.MetadataCacheSuite", "org.apache.spark.sql.MetadataCacheSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.AggregatingAccumulatorSuite", "org.apache.spark.sql.execution.AggregatingAccumulatorSuite", "org.apache.spark.sql.execution.AggregatingAccumulatorSuite", "org.apache.spark.sql.execution.AggregatingAccumulatorSuite", "org.apache.spark.sql.execution.AggregatingAccumulatorSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.state.StateStoreCompatibilitySuite", "org.apache.spark.sql.execution.streaming.state.StateStoreCompatibilitySuite", "org.apache.spark.sql.execution.streaming.state.StateStoreCompatibilitySuite", "org.apache.spark.sql.execution.streaming.state.StateStoreCompatibilitySuite", "org.apache.spark.sql.execution.streaming.state.StateStoreCompatibilitySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.internal.SQLConfSuite", "org.apache.spark.sql.internal.SQLConfSuite", "org.apache.spark.sql.internal.SQLConfSuite", "org.apache.spark.sql.internal.SQLConfSuite", "org.apache.spark.sql.internal.SQLConfSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.jdbc.JDBCNestedDataSourceSuite", "org.apache.spark.sql.jdbc.JDBCNestedDataSourceSuite", "org.apache.spark.sql.jdbc.JDBCNestedDataSourceSuite", "org.apache.spark.sql.jdbc.JDBCNestedDataSourceSuite", "org.apache.spark.sql.jdbc.JDBCNestedDataSourceSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.DataSourceSuite", "org.apache.spark.sql.execution.datasources.DataSourceSuite", "org.apache.spark.sql.execution.datasources.DataSourceSuite", "org.apache.spark.sql.execution.datasources.DataSourceSuite", "org.apache.spark.sql.execution.datasources.DataSourceSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.text.TextV2Suite", "org.apache.spark.sql.execution.datasources.text.TextSuite", "org.apache.spark.sql.execution.datasources.text.TextSuite", "org.apache.spark.sql.execution.datasources.text.TextSuite", "org.apache.spark.sql.execution.datasources.text.TextSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.MathFunctionsSuite", "org.apache.spark.sql.MathFunctionsSuite", "org.apache.spark.sql.MathFunctionsSuite", "org.apache.spark.sql.MathFunctionsSuite", "org.apache.spark.sql.MathFunctionsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.DeprecatedStreamingAggregationSuite", "org.apache.spark.sql.streaming.DeprecatedStreamingAggregationSuite", "org.apache.spark.sql.streaming.DeprecatedStreamingAggregationSuite", "org.apache.spark.sql.streaming.DeprecatedStreamingAggregationSuite", "org.apache.spark.sql.streaming.DeprecatedStreamingAggregationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.sources.TextSocketStreamSuite", "org.apache.spark.sql.execution.streaming.sources.TextSocketStreamSuite", "org.apache.spark.sql.execution.streaming.sources.TextSocketStreamSuite", "org.apache.spark.sql.execution.streaming.sources.TextSocketStreamSuite", "org.apache.spark.sql.execution.streaming.sources.TextSocketStreamSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.CTEHintSuite", "org.apache.spark.sql.CTEHintSuite", "org.apache.spark.sql.CTEHintSuite", "org.apache.spark.sql.CTEHintSuite", "org.apache.spark.sql.CTEHintSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.ParquetReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ParquetReadSchemaSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.joins.InnerJoinSuite", "org.apache.spark.sql.execution.joins.InnerJoinSuite", "org.apache.spark.sql.execution.joins.InnerJoinSuite", "org.apache.spark.sql.execution.joins.InnerJoinSuite", "org.apache.spark.sql.execution.joins.InnerJoinSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.RowDataSourceStrategySuite", "org.apache.spark.sql.execution.datasources.RowDataSourceStrategySuite", "org.apache.spark.sql.execution.datasources.RowDataSourceStrategySuite", "org.apache.spark.sql.execution.datasources.RowDataSourceStrategySuite", "org.apache.spark.sql.execution.datasources.RowDataSourceStrategySuite", "org.apache.spark.sql.execution.datasources.RowDataSourceStrategySuite", "org.apache.spark.sql.execution.datasources.RowDataSourceStrategySuite", "org.apache.spark.sql.execution.GlobalTempViewSuite", "org.apache.spark.sql.execution.GlobalTempViewSuite", "org.apache.spark.sql.execution.GlobalTempViewSuite", "org.apache.spark.sql.execution.GlobalTempViewSuite", "org.apache.spark.sql.execution.GlobalTempViewSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRecoverPartitionsParallelSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRecoverPartitionsParallelSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRecoverPartitionsParallelSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRecoverPartitionsParallelSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRecoverPartitionsParallelSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.ui.SQLAppStatusListenerMemoryLeakSuite", "org.apache.spark.sql.execution.ui.SQLAppStatusListenerMemoryLeakSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.orc.OrcPartitionDiscoverySuite", "org.apache.spark.sql.execution.datasources.orc.OrcPartitionDiscoverySuite", "org.apache.spark.sql.execution.datasources.orc.OrcPartitionDiscoverySuite", "org.apache.spark.sql.execution.datasources.orc.OrcPartitionDiscoverySuite", "org.apache.spark.sql.execution.datasources.orc.OrcPartitionDiscoverySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.RemoveRedundantSortsSuite", "org.apache.spark.sql.execution.RemoveRedundantSortsSuite", "org.apache.spark.sql.execution.RemoveRedundantSortsSuite", "org.apache.spark.sql.execution.RemoveRedundantSortsSuite", "org.apache.spark.sql.execution.RemoveRedundantSortsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.HadoopFileLinesReaderSuite", "org.apache.spark.sql.execution.datasources.HadoopFileLinesReaderSuite", "org.apache.spark.sql.execution.datasources.HadoopFileLinesReaderSuite", "org.apache.spark.sql.execution.datasources.HadoopFileLinesReaderSuite", "org.apache.spark.sql.execution.datasources.HadoopFileLinesReaderSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DynamicPartitionPruningV2Suite", "org.apache.spark.sql.DynamicPartitionPruningSuite", "org.apache.spark.sql.DynamicPartitionPruningSuite", "org.apache.spark.sql.DynamicPartitionPruningSuite", "org.apache.spark.sql.DynamicPartitionPruningSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.test.GenericFunSpecSuite", "org.apache.spark.sql.test.GenericFunSpecSuite", "org.apache.spark.sql.test.GenericFunSpecSuite", "org.apache.spark.sql.test.GenericFunSpecSuite", "org.apache.spark.sql.test.GenericFunSpecSuite", "org.apache.spark.sql.connector.SupportsCatalogOptionsSuite", "org.apache.spark.sql.connector.SupportsCatalogOptionsSuite", "org.apache.spark.sql.connector.SupportsCatalogOptionsSuite", "org.apache.spark.sql.connector.SupportsCatalogOptionsSuite", "org.apache.spark.sql.connector.SupportsCatalogOptionsSuite", "org.apache.spark.sql.connector.SupportsCatalogOptionsSuite", "org.apache.spark.sql.connector.SupportsCatalogOptionsSuite", "org.apache.spark.sql.DatasetAggregatorSuite", "org.apache.spark.sql.DatasetAggregatorSuite", "org.apache.spark.sql.DatasetAggregatorSuite", "org.apache.spark.sql.DatasetAggregatorSuite", "org.apache.spark.sql.DatasetAggregatorSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.v2.AlterTableAddPartitionSuite", "org.apache.spark.sql.execution.command.v2.AlterTableAddPartitionSuite", "org.apache.spark.sql.execution.command.v2.AlterTableAddPartitionSuite", "org.apache.spark.sql.execution.command.v2.AlterTableAddPartitionSuite", "org.apache.spark.sql.execution.command.v2.AlterTableAddPartitionSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.v2.ShowPartitionsSuite", "org.apache.spark.sql.execution.command.v2.ShowPartitionsSuite", "org.apache.spark.sql.execution.command.v2.ShowPartitionsSuite", "org.apache.spark.sql.execution.command.v2.ShowPartitionsSuite", "org.apache.spark.sql.execution.command.v2.ShowPartitionsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.sources.DisableUnnecessaryBucketedScanWithoutHiveSupportSuite", "org.apache.spark.sql.sources.DisableUnnecessaryBucketedScanWithoutHiveSupportSuite", "org.apache.spark.sql.sources.DisableUnnecessaryBucketedScanWithoutHiveSupportSuite", "org.apache.spark.sql.sources.DisableUnnecessaryBucketedScanWithoutHiveSupportSuite", "org.apache.spark.sql.sources.DisableUnnecessaryBucketedScanWithoutHiveSupportSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.v1.ShowNamespacesSuite", "org.apache.spark.sql.execution.command.v1.ShowNamespacesSuite", "org.apache.spark.sql.execution.command.v1.ShowNamespacesSuite", "org.apache.spark.sql.execution.command.v1.ShowNamespacesSuite", "org.apache.spark.sql.execution.command.v1.ShowNamespacesSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.v1.ShowTablesSuite", "org.apache.spark.sql.execution.command.v1.ShowTablesSuite", "org.apache.spark.sql.execution.command.v1.ShowTablesSuite", "org.apache.spark.sql.execution.command.v1.ShowTablesSuite", "org.apache.spark.sql.execution.command.v1.ShowTablesSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.test.GenericFlatSpecSuite", "org.apache.spark.sql.test.GenericFlatSpecSuite", "org.apache.spark.sql.test.GenericFlatSpecSuite", "org.apache.spark.sql.test.GenericFlatSpecSuite", "org.apache.spark.sql.test.GenericFlatSpecSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.RemoveRedundantProjectsSuite", "org.apache.spark.sql.execution.RemoveRedundantProjectsSuite", "org.apache.spark.sql.execution.RemoveRedundantProjectsSuite", "org.apache.spark.sql.execution.RemoveRedundantProjectsSuite", "org.apache.spark.sql.execution.RemoveRedundantProjectsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManagerSuite", "org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManagerSuite", "org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManagerSuite", "org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManagerSuite", "org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManagerSuite", "org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManagerSuite", "org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManagerSuite", "org.apache.spark.sql.streaming.ui.UISeleniumSuite", "org.apache.spark.sql.streaming.ui.UISeleniumSuite", "org.apache.spark.sql.streaming.ui.UISeleniumSuite", "org.apache.spark.sql.streaming.ui.UISeleniumSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.WholeStageCodegenSuite", "org.apache.spark.sql.execution.WholeStageCodegenSuite", "org.apache.spark.sql.execution.WholeStageCodegenSuite", "org.apache.spark.sql.execution.WholeStageCodegenSuite", "org.apache.spark.sql.execution.WholeStageCodegenSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.CountMinSketchAggQuerySuite", "org.apache.spark.sql.CountMinSketchAggQuerySuite", "org.apache.spark.sql.CountMinSketchAggQuerySuite", "org.apache.spark.sql.CountMinSketchAggQuerySuite", "org.apache.spark.sql.CountMinSketchAggQuerySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.SQLExecutionSuite", "org.apache.spark.sql.execution.SQLExecutionSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.DataSourceScanExecRedactionSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.test.DataStreamTableAPISuite", "org.apache.spark.sql.streaming.test.DataStreamTableAPISuite", "org.apache.spark.sql.streaming.test.DataStreamTableAPISuite", "org.apache.spark.sql.streaming.test.DataStreamTableAPISuite", "org.apache.spark.sql.streaming.test.DataStreamTableAPISuite", "org.apache.spark.sql.streaming.test.DataStreamTableAPISuite", "org.apache.spark.sql.streaming.test.DataStreamTableAPISuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DataFramePivotSuite", "org.apache.spark.sql.DataFramePivotSuite", "org.apache.spark.sql.DataFramePivotSuite", "org.apache.spark.sql.DataFramePivotSuite", "org.apache.spark.sql.DataFramePivotSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.FileStreamStressSuite", "org.apache.spark.sql.streaming.FileStreamStressSuite", "org.apache.spark.sql.streaming.FileStreamStressSuite", "org.apache.spark.sql.streaming.FileStreamStressSuite", "org.apache.spark.sql.streaming.FileStreamStressSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.columnar.CachedBatchSerializerSuite", "org.apache.spark.sql.execution.columnar.CachedBatchSerializerSuite", "org.apache.spark.sql.execution.columnar.CachedBatchSerializerSuite", "org.apache.spark.sql.execution.columnar.CachedBatchSerializerSuite", "org.apache.spark.sql.execution.columnar.CachedBatchSerializerSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.VectorizedParquetReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.VectorizedParquetReadSchemaSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.state.RocksDBSuite", "org.apache.spark.sql.execution.streaming.state.RocksDBSuite", "org.apache.spark.sql.execution.streaming.state.RocksDBSuite", "org.apache.spark.sql.execution.streaming.state.RocksDBSuite", "org.apache.spark.sql.execution.streaming.state.RocksDBSuite", "org.apache.spark.sql.execution.streaming.state.RocksDBSuite", "org.apache.spark.sql.execution.streaming.state.RocksDBSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.text.WholeTextFileV1Suite", "org.apache.spark.sql.execution.datasources.text.WholeTextFileSuite", "org.apache.spark.sql.execution.datasources.text.WholeTextFileSuite", "org.apache.spark.sql.execution.datasources.text.WholeTextFileSuite", "org.apache.spark.sql.execution.datasources.text.WholeTextFileSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.v1.AlterTableAddPartitionSuite", "org.apache.spark.sql.execution.command.v1.AlterTableAddPartitionSuite", "org.apache.spark.sql.execution.command.v1.AlterTableAddPartitionSuite", "org.apache.spark.sql.execution.command.v1.AlterTableAddPartitionSuite", "org.apache.spark.sql.execution.command.v1.AlterTableAddPartitionSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.ui.StreamingQueryStatusListenerSuite", "org.apache.spark.sql.streaming.ui.StreamingQueryStatusListenerSuite", "org.apache.spark.sql.streaming.ui.StreamingQueryStatusListenerSuite", "org.apache.spark.sql.streaming.ui.StreamingQueryStatusListenerSuite", "org.apache.spark.sql.streaming.ui.StreamingQueryStatusListenerSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.UserDefinedTypeSuite", "org.apache.spark.sql.UserDefinedTypeSuite", "org.apache.spark.sql.UserDefinedTypeSuite", "org.apache.spark.sql.UserDefinedTypeSuite", "org.apache.spark.sql.UserDefinedTypeSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.DropTableParserSuite", "org.apache.spark.sql.execution.command.DropTableParserSuite", "org.apache.spark.sql.execution.command.DropTableParserSuite", "org.apache.spark.sql.execution.command.DropTableParserSuite", "org.apache.spark.sql.execution.command.DropTableParserSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DSV2CharVarcharTestSuite", "org.apache.spark.sql.DSV2CharVarcharTestSuite", "org.apache.spark.sql.DSV2CharVarcharTestSuite", "org.apache.spark.sql.DSV2CharVarcharTestSuite", "org.apache.spark.sql.DSV2CharVarcharTestSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.SparkSessionExtensionSuite", "org.apache.spark.sql.SparkSessionExtensionSuite", "org.apache.spark.sql.SparkSessionExtensionSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DataFrameSelfJoinSuite", "org.apache.spark.sql.DataFrameSelfJoinSuite", "org.apache.spark.sql.DataFrameSelfJoinSuite", "org.apache.spark.sql.DataFrameSelfJoinSuite", "org.apache.spark.sql.DataFrameSelfJoinSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.StreamingInnerJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.connector.V1ReadFallbackWithDataFrameReaderSuite", "org.apache.spark.sql.connector.V1ReadFallbackSuite", "org.apache.spark.sql.connector.V1ReadFallbackSuite", "org.apache.spark.sql.connector.V1ReadFallbackSuite", "org.apache.spark.sql.connector.V1ReadFallbackSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.CoalesceShufflePartitionsSuite", "org.apache.spark.sql.execution.CoalesceShufflePartitionsSuite", "org.apache.spark.sql.execution.CoalesceShufflePartitionsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.CSVReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.ApproximatePercentileQuerySuite", "org.apache.spark.sql.ApproximatePercentileQuerySuite", "org.apache.spark.sql.ApproximatePercentileQuerySuite", "org.apache.spark.sql.ApproximatePercentileQuerySuite", "org.apache.spark.sql.ApproximatePercentileQuerySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DataFrameSuite", "org.apache.spark.sql.DataFrameSuite", "org.apache.spark.sql.DataFrameSuite", "org.apache.spark.sql.DataFrameSuite", "org.apache.spark.sql.DataFrameSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DataFrameNaFunctionsSuite", "org.apache.spark.sql.DataFrameNaFunctionsSuite", "org.apache.spark.sql.DataFrameNaFunctionsSuite", "org.apache.spark.sql.DataFrameNaFunctionsSuite", "org.apache.spark.sql.DataFrameNaFunctionsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.orc.OrcFilterSuite", "org.apache.spark.sql.execution.datasources.orc.OrcFilterSuite", "org.apache.spark.sql.execution.datasources.orc.OrcFilterSuite", "org.apache.spark.sql.execution.datasources.orc.OrcFilterSuite", "org.apache.spark.sql.execution.datasources.orc.OrcFilterSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.noop.NoopSuite", "org.apache.spark.sql.execution.datasources.noop.NoopSuite", "org.apache.spark.sql.execution.datasources.noop.NoopSuite", "org.apache.spark.sql.execution.datasources.noop.NoopSuite", "org.apache.spark.sql.execution.datasources.noop.NoopSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.sources.SaveLoadSuite", "org.apache.spark.sql.sources.SaveLoadSuite", "org.apache.spark.sql.sources.SaveLoadSuite", "org.apache.spark.sql.sources.SaveLoadSuite", "org.apache.spark.sql.sources.SaveLoadSuite", "org.apache.spark.sql.sources.SaveLoadSuite", "org.apache.spark.sql.sources.SaveLoadSuite", "org.apache.spark.sql.FileScanSuite", "org.apache.spark.sql.FileScanSuite", "org.apache.spark.sql.FileScanSuite", "org.apache.spark.sql.FileScanSuite", "org.apache.spark.sql.FileScanSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.MicroBatchExecutionSuite", "org.apache.spark.sql.execution.streaming.MicroBatchExecutionSuite", "org.apache.spark.sql.execution.streaming.MicroBatchExecutionSuite", "org.apache.spark.sql.execution.streaming.MicroBatchExecutionSuite", "org.apache.spark.sql.execution.streaming.MicroBatchExecutionSuite", "org.apache.spark.sql.execution.streaming.MicroBatchExecutionSuite", "org.apache.spark.sql.execution.streaming.MicroBatchExecutionSuite", "org.apache.spark.sql.execution.datasources.csv.CSVv1Suite", "org.apache.spark.sql.execution.datasources.csv.CSVSuite", "org.apache.spark.sql.execution.datasources.csv.CSVSuite", "org.apache.spark.sql.execution.datasources.csv.CSVSuite", "org.apache.spark.sql.execution.datasources.csv.CSVSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.FileBasedDataSourceSuite", "org.apache.spark.sql.FileBasedDataSourceSuite", "org.apache.spark.sql.FileBasedDataSourceSuite", "org.apache.spark.sql.FileBasedDataSourceSuite", "org.apache.spark.sql.FileBasedDataSourceSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.ExtraStrategiesSuite", "org.apache.spark.sql.ExtraStrategiesSuite", "org.apache.spark.sql.ExtraStrategiesSuite", "org.apache.spark.sql.ExtraStrategiesSuite", "org.apache.spark.sql.ExtraStrategiesSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.orc.OrcV1SchemaPruningSuite", "org.apache.spark.sql.execution.datasources.SchemaPruningSuite", "org.apache.spark.sql.execution.datasources.SchemaPruningSuite", "org.apache.spark.sql.execution.datasources.SchemaPruningSuite", "org.apache.spark.sql.execution.datasources.SchemaPruningSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.connector.DataSourceV2SQLSessionCatalogSuite", "org.apache.spark.sql.execution.streaming.OffsetSeqLogSuite", "org.apache.spark.sql.execution.streaming.OffsetSeqLogSuite", "org.apache.spark.sql.execution.streaming.OffsetSeqLogSuite", "org.apache.spark.sql.execution.streaming.OffsetSeqLogSuite", "org.apache.spark.sql.execution.streaming.OffsetSeqLogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.internal.DeprecatedCreateExternalTableSuite", "org.apache.spark.sql.internal.DeprecatedCreateExternalTableSuite", "org.apache.spark.sql.internal.DeprecatedCreateExternalTableSuite", "org.apache.spark.sql.internal.DeprecatedCreateExternalTableSuite", "org.apache.spark.sql.internal.DeprecatedCreateExternalTableSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.QueryExecutionSuite", "org.apache.spark.sql.execution.QueryExecutionSuite", "org.apache.spark.sql.execution.QueryExecutionSuite", "org.apache.spark.sql.execution.QueryExecutionSuite", "org.apache.spark.sql.execution.QueryExecutionSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.ApproxCountDistinctForIntervalsQuerySuite", "org.apache.spark.sql.ApproxCountDistinctForIntervalsQuerySuite", "org.apache.spark.sql.ApproxCountDistinctForIntervalsQuerySuite", "org.apache.spark.sql.ApproxCountDistinctForIntervalsQuerySuite", "org.apache.spark.sql.ApproxCountDistinctForIntervalsQuerySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DeprecatedDatasetAggregatorSuite", "org.apache.spark.sql.DeprecatedDatasetAggregatorSuite", "org.apache.spark.sql.DeprecatedDatasetAggregatorSuite", "org.apache.spark.sql.DeprecatedDatasetAggregatorSuite", "org.apache.spark.sql.DeprecatedDatasetAggregatorSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.LocalTempViewTestSuite", "org.apache.spark.sql.execution.LocalTempViewTestSuite", "org.apache.spark.sql.execution.LocalTempViewTestSuite", "org.apache.spark.sql.execution.LocalTempViewTestSuite", "org.apache.spark.sql.execution.LocalTempViewTestSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.python.BatchEvalPythonExecSuite", "org.apache.spark.sql.execution.python.BatchEvalPythonExecSuite", "org.apache.spark.sql.execution.python.BatchEvalPythonExecSuite", "org.apache.spark.sql.execution.python.BatchEvalPythonExecSuite", "org.apache.spark.sql.execution.python.BatchEvalPythonExecSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV1PartitionDiscoverySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetPartitionDiscoverySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetPartitionDiscoverySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetPartitionDiscoverySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetPartitionDiscoverySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQuerySuite", "org.apache.spark.sql.TPCDSQuerySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.PathFilterStrategySuite", "org.apache.spark.sql.execution.datasources.PathFilterStrategySuite", "org.apache.spark.sql.execution.datasources.PathFilterStrategySuite", "org.apache.spark.sql.execution.datasources.PathFilterStrategySuite", "org.apache.spark.sql.execution.datasources.PathFilterStrategySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.ui.StreamingQueryPageSuite", "org.apache.spark.sql.streaming.ui.StreamingQueryPageSuite", "org.apache.spark.sql.streaming.ui.StreamingQueryPageSuite", "org.apache.spark.sql.streaming.ui.StreamingQueryPageSuite", "org.apache.spark.sql.streaming.ui.StreamingQueryPageSuite", "org.apache.spark.sql.streaming.ui.StreamingQueryPageSuite", "org.apache.spark.sql.streaming.ui.StreamingQueryPageSuite", "org.apache.spark.sql.execution.command.v2.MsckRepairTableSuite", "org.apache.spark.sql.execution.command.v2.MsckRepairTableSuite", "org.apache.spark.sql.execution.command.v2.MsckRepairTableSuite", "org.apache.spark.sql.execution.command.v2.MsckRepairTableSuite", "org.apache.spark.sql.execution.command.v2.MsckRepairTableSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.internal.SharedStateSuite", "org.apache.spark.sql.internal.SharedStateSuite", "org.apache.spark.sql.internal.SharedStateSuite", "org.apache.spark.sql.internal.SharedStateSuite", "org.apache.spark.sql.internal.SharedStateSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.metric.SQLMetricsSuite", "org.apache.spark.sql.execution.metric.SQLMetricsSuite", "org.apache.spark.sql.execution.metric.SQLMetricsSuite", "org.apache.spark.sql.execution.metric.SQLMetricsSuite", "org.apache.spark.sql.execution.metric.SQLMetricsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.MemorySinkSuite", "org.apache.spark.sql.execution.streaming.MemorySinkSuite", "org.apache.spark.sql.execution.streaming.MemorySinkSuite", "org.apache.spark.sql.execution.streaming.MemorySinkSuite", "org.apache.spark.sql.execution.streaming.MemorySinkSuite", "org.apache.spark.sql.execution.streaming.MemorySinkSuite", "org.apache.spark.sql.execution.streaming.MemorySinkSuite", "org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommandSuite", "org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommandSuite", "org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommandSuite", "org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommandSuite", "org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommandSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.v2.AlterTableRenamePartitionSuite", "org.apache.spark.sql.execution.command.v2.AlterTableRenamePartitionSuite", "org.apache.spark.sql.execution.command.v2.AlterTableRenamePartitionSuite", "org.apache.spark.sql.execution.command.v2.AlterTableRenamePartitionSuite", "org.apache.spark.sql.execution.command.v2.AlterTableRenamePartitionSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DataFrameComplexTypeSuite", "org.apache.spark.sql.DataFrameComplexTypeSuite", "org.apache.spark.sql.DataFrameComplexTypeSuite", "org.apache.spark.sql.DataFrameComplexTypeSuite", "org.apache.spark.sql.DataFrameComplexTypeSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.TPCHQuerySuite", "org.apache.spark.sql.TPCHQuerySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.ScalaReflectionRelationSuite", "org.apache.spark.sql.ScalaReflectionRelationSuite", "org.apache.spark.sql.ScalaReflectionRelationSuite", "org.apache.spark.sql.ScalaReflectionRelationSuite", "org.apache.spark.sql.ScalaReflectionRelationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetEncodingSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetEncodingSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetEncodingSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetEncodingSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetEncodingSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.v2.ShowNamespacesSuite", "org.apache.spark.sql.execution.command.v2.ShowNamespacesSuite", "org.apache.spark.sql.execution.command.v2.ShowNamespacesSuite", "org.apache.spark.sql.execution.command.v2.ShowNamespacesSuite", "org.apache.spark.sql.execution.command.v2.ShowNamespacesSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.expressions.ExpressionInfoSuite", "org.apache.spark.sql.expressions.ExpressionInfoSuite", "org.apache.spark.sql.expressions.ExpressionInfoSuite", "org.apache.spark.sql.expressions.ExpressionInfoSuite", "org.apache.spark.sql.expressions.ExpressionInfoSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.columnar.InMemoryColumnarQuerySuite", "org.apache.spark.sql.execution.columnar.InMemoryColumnarQuerySuite", "org.apache.spark.sql.execution.columnar.InMemoryColumnarQuerySuite", "org.apache.spark.sql.execution.columnar.InMemoryColumnarQuerySuite", "org.apache.spark.sql.execution.columnar.InMemoryColumnarQuerySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.v1.TruncateTableSuite", "org.apache.spark.sql.execution.command.v1.TruncateTableSuite", "org.apache.spark.sql.execution.command.v1.TruncateTableSuite", "org.apache.spark.sql.execution.command.v1.TruncateTableSuite", "org.apache.spark.sql.execution.command.v1.TruncateTableSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.debug.DebuggingSuite", "org.apache.spark.sql.execution.debug.DebuggingSuite", "org.apache.spark.sql.execution.debug.DebuggingSuite", "org.apache.spark.sql.execution.debug.DebuggingSuite", "org.apache.spark.sql.execution.debug.DebuggingSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.RemoveRedundantSortsSuite", "org.apache.spark.sql.execution.RemoveRedundantSortsSuite", "org.apache.spark.sql.execution.RemoveRedundantSortsSuite", "org.apache.spark.sql.execution.RemoveRedundantSortsSuite", "org.apache.spark.sql.execution.RemoveRedundantSortsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DataFrameWriterV2Suite", "org.apache.spark.sql.DataFrameWriterV2Suite", "org.apache.spark.sql.DataFrameWriterV2Suite", "org.apache.spark.sql.DataFrameWriterV2Suite", "org.apache.spark.sql.DataFrameWriterV2Suite", "org.apache.spark.sql.DataFrameWriterV2Suite", "org.apache.spark.sql.DataFrameWriterV2Suite", "org.apache.spark.sql.execution.streaming.FileSystemBasedCheckpointFileManagerSuite", "org.apache.spark.sql.execution.streaming.FileSystemBasedCheckpointFileManagerSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.connector.V1WriteFallbackSuite", "org.apache.spark.sql.connector.V1WriteFallbackSuite", "org.apache.spark.sql.connector.V1WriteFallbackSuite", "org.apache.spark.sql.connector.V1WriteFallbackSuite", "org.apache.spark.sql.connector.V1WriteFallbackSuite", "org.apache.spark.sql.connector.V1WriteFallbackSuite", "org.apache.spark.sql.connector.V1WriteFallbackSuite", "org.apache.spark.sql.execution.SQLJsonProtocolSuite", "org.apache.spark.sql.execution.SQLJsonProtocolSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.FileSourceSQLInsertTestSuite", "org.apache.spark.sql.FileSourceSQLInsertTestSuite", "org.apache.spark.sql.FileSourceSQLInsertTestSuite", "org.apache.spark.sql.FileSourceSQLInsertTestSuite", "org.apache.spark.sql.FileSourceSQLInsertTestSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.orc.OrcV1QuerySuite", "org.apache.spark.sql.execution.datasources.orc.OrcQuerySuite", "org.apache.spark.sql.execution.datasources.orc.OrcQuerySuite", "org.apache.spark.sql.execution.datasources.orc.OrcQuerySuite", "org.apache.spark.sql.execution.datasources.orc.OrcQuerySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.aggregate.SortBasedAggregationStoreSuite", "org.apache.spark.sql.execution.aggregate.SortBasedAggregationStoreSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.sources.RateStreamProviderSuite", "org.apache.spark.sql.execution.streaming.sources.RateStreamProviderSuite", "org.apache.spark.sql.execution.streaming.sources.RateStreamProviderSuite", "org.apache.spark.sql.execution.streaming.sources.RateStreamProviderSuite", "org.apache.spark.sql.execution.streaming.sources.RateStreamProviderSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.util.DataFrameCallbackSuite", "org.apache.spark.sql.util.DataFrameCallbackSuite", "org.apache.spark.sql.util.DataFrameCallbackSuite", "org.apache.spark.sql.util.DataFrameCallbackSuite", "org.apache.spark.sql.util.DataFrameCallbackSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.v2.FileTableSuite", "org.apache.spark.sql.execution.datasources.v2.FileTableSuite", "org.apache.spark.sql.execution.datasources.v2.FileTableSuite", "org.apache.spark.sql.execution.datasources.v2.FileTableSuite", "org.apache.spark.sql.execution.datasources.v2.FileTableSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.JsonReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.FlatMapGroupsWithStateSuite", "org.apache.spark.sql.streaming.FlatMapGroupsWithStateSuite", "org.apache.spark.sql.streaming.FlatMapGroupsWithStateSuite", "org.apache.spark.sql.streaming.FlatMapGroupsWithStateSuite", "org.apache.spark.sql.streaming.FlatMapGroupsWithStateSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArraySuite", "org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArraySuite", "org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArraySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.ConfigBehaviorSuite", "org.apache.spark.sql.ConfigBehaviorSuite", "org.apache.spark.sql.ConfigBehaviorSuite", "org.apache.spark.sql.ConfigBehaviorSuite", "org.apache.spark.sql.ConfigBehaviorSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DatasetSuite", "org.apache.spark.sql.DatasetSuite", "org.apache.spark.sql.DatasetSuite", "org.apache.spark.sql.DatasetSuite", "org.apache.spark.sql.DatasetSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DataFrameTungstenSuite", "org.apache.spark.sql.DataFrameTungstenSuite", "org.apache.spark.sql.DataFrameTungstenSuite", "org.apache.spark.sql.DataFrameTungstenSuite", "org.apache.spark.sql.DataFrameTungstenSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreSuite", "org.apache.spark.sql.execution.streaming.state.StateStoreSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreSuite", "org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreSuite", "org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreSuite", "org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMapSuite", "org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMapSuite", "org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMapSuite", "org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMapSuite", "org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMapSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.connector.FileDataSourceV2FallBackSuite", "org.apache.spark.sql.connector.FileDataSourceV2FallBackSuite", "org.apache.spark.sql.connector.FileDataSourceV2FallBackSuite", "org.apache.spark.sql.connector.FileDataSourceV2FallBackSuite", "org.apache.spark.sql.connector.FileDataSourceV2FallBackSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite", "org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite", "org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite", "org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite", "org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerSuite", "org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerSuite", "org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerSuite", "org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerSuite", "org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.test.GenericWordSpecSuite", "org.apache.spark.sql.test.GenericWordSpecSuite", "org.apache.spark.sql.test.GenericWordSpecSuite", "org.apache.spark.sql.test.GenericWordSpecSuite", "org.apache.spark.sql.test.GenericWordSpecSuite", "org.apache.spark.sql.ProductAggSuite", "org.apache.spark.sql.ProductAggSuite", "org.apache.spark.sql.ProductAggSuite", "org.apache.spark.sql.ProductAggSuite", "org.apache.spark.sql.ProductAggSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.bucketing.CoalesceBucketsInJoinSuite", "org.apache.spark.sql.execution.bucketing.CoalesceBucketsInJoinSuite", "org.apache.spark.sql.execution.bucketing.CoalesceBucketsInJoinSuite", "org.apache.spark.sql.execution.bucketing.CoalesceBucketsInJoinSuite", "org.apache.spark.sql.execution.bucketing.CoalesceBucketsInJoinSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilitySuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilitySuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilitySuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilitySuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilitySuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilitySuite", "org.apache.spark.sql.PlanStabilitySuite", "org.apache.spark.sql.PlanStabilitySuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilitySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.QueryTestSuite", "org.apache.spark.sql.QueryTestSuite", "org.apache.spark.sql.QueryTestSuite", "org.apache.spark.sql.QueryTestSuite", "org.apache.spark.sql.QueryTestSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.FileSourceCharVarcharTestSuite", "org.apache.spark.sql.FileSourceCharVarcharTestSuite", "org.apache.spark.sql.FileSourceCharVarcharTestSuite", "org.apache.spark.sql.FileSourceCharVarcharTestSuite", "org.apache.spark.sql.FileSourceCharVarcharTestSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.MiscFunctionsSuite", "org.apache.spark.sql.MiscFunctionsSuite", "org.apache.spark.sql.MiscFunctionsSuite", "org.apache.spark.sql.MiscFunctionsSuite", "org.apache.spark.sql.MiscFunctionsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.FileFormatWriterSuite", "org.apache.spark.sql.execution.datasources.FileFormatWriterSuite", "org.apache.spark.sql.execution.datasources.FileFormatWriterSuite", "org.apache.spark.sql.execution.datasources.FileFormatWriterSuite", "org.apache.spark.sql.execution.datasources.FileFormatWriterSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.orc.OrcSourceSuite", "org.apache.spark.sql.execution.datasources.orc.OrcSourceSuite", "org.apache.spark.sql.execution.datasources.orc.OrcSourceSuite", "org.apache.spark.sql.execution.datasources.orc.OrcSourceSuite", "org.apache.spark.sql.execution.datasources.orc.OrcSourceSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.StreamingLeftSemiJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.streaming.EventTimeWatermarkSuite", "org.apache.spark.sql.streaming.EventTimeWatermarkSuite", "org.apache.spark.sql.streaming.EventTimeWatermarkSuite", "org.apache.spark.sql.streaming.EventTimeWatermarkSuite", "org.apache.spark.sql.streaming.EventTimeWatermarkSuite", "org.apache.spark.sql.streaming.EventTimeWatermarkSuite", "org.apache.spark.sql.streaming.EventTimeWatermarkSuite", "org.apache.spark.sql.execution.ui.SparkPlanInfoSuite", "org.apache.spark.sql.execution.ui.SparkPlanInfoSuite", "org.apache.spark.sql.execution.ui.SparkPlanInfoSuite", "org.apache.spark.sql.execution.ui.SparkPlanInfoSuite", "org.apache.spark.sql.execution.ui.SparkPlanInfoSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV2PartitionDiscoverySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetPartitionDiscoverySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetPartitionDiscoverySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetPartitionDiscoverySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetPartitionDiscoverySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.StreamMetadataSuite", "org.apache.spark.sql.execution.streaming.StreamMetadataSuite", "org.apache.spark.sql.execution.streaming.StreamMetadataSuite", "org.apache.spark.sql.execution.streaming.StreamMetadataSuite", "org.apache.spark.sql.execution.streaming.StreamMetadataSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetThriftCompatibilitySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetThriftCompatibilitySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetThriftCompatibilitySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetThriftCompatibilitySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetThriftCompatibilitySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.sources.ConsoleWriteSupportSuite", "org.apache.spark.sql.execution.streaming.sources.ConsoleWriteSupportSuite", "org.apache.spark.sql.execution.streaming.sources.ConsoleWriteSupportSuite", "org.apache.spark.sql.execution.streaming.sources.ConsoleWriteSupportSuite", "org.apache.spark.sql.execution.streaming.sources.ConsoleWriteSupportSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.SparkScriptTransformationSuite", "org.apache.spark.sql.execution.SparkScriptTransformationSuite", "org.apache.spark.sql.execution.SparkScriptTransformationSuite", "org.apache.spark.sql.execution.SparkScriptTransformationSuite", "org.apache.spark.sql.execution.SparkScriptTransformationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.connector.TableCapabilityCheckSuite", "org.apache.spark.sql.connector.TableCapabilityCheckSuite", "org.apache.spark.sql.connector.TableCapabilityCheckSuite", "org.apache.spark.sql.connector.TableCapabilityCheckSuite", "org.apache.spark.sql.connector.TableCapabilityCheckSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.orc.OrcV2SchemaPruningSuite", "org.apache.spark.sql.execution.datasources.SchemaPruningSuite", "org.apache.spark.sql.execution.datasources.SchemaPruningSuite", "org.apache.spark.sql.execution.datasources.SchemaPruningSuite", "org.apache.spark.sql.execution.datasources.SchemaPruningSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.AlterTableRenamePartitionParserSuite", "org.apache.spark.sql.execution.command.AlterTableRenamePartitionParserSuite", "org.apache.spark.sql.execution.command.AlterTableRenamePartitionParserSuite", "org.apache.spark.sql.execution.command.AlterTableRenamePartitionParserSuite", "org.apache.spark.sql.execution.command.AlterTableRenamePartitionParserSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.sources.PartitionedWriteSuite", "org.apache.spark.sql.sources.PartitionedWriteSuite", "org.apache.spark.sql.sources.PartitionedWriteSuite", "org.apache.spark.sql.sources.PartitionedWriteSuite", "org.apache.spark.sql.sources.PartitionedWriteSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.orc.OrcEncryptionSuite", "org.apache.spark.sql.execution.datasources.orc.OrcEncryptionSuite", "org.apache.spark.sql.execution.datasources.orc.OrcEncryptionSuite", "org.apache.spark.sql.execution.datasources.orc.OrcEncryptionSuite", "org.apache.spark.sql.execution.datasources.orc.OrcEncryptionSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.internal.ExecutorSideSQLConfSuite", "org.apache.spark.sql.internal.ExecutorSideSQLConfSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.internal.SQLConfGetterSuite", "org.apache.spark.sql.internal.SQLConfGetterSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV2QuerySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.json.JsonParsingOptionsSuite", "org.apache.spark.sql.execution.datasources.json.JsonParsingOptionsSuite", "org.apache.spark.sql.execution.datasources.json.JsonParsingOptionsSuite", "org.apache.spark.sql.execution.datasources.json.JsonParsingOptionsSuite", "org.apache.spark.sql.execution.datasources.json.JsonParsingOptionsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.JsonFunctionsSuite", "org.apache.spark.sql.JsonFunctionsSuite", "org.apache.spark.sql.JsonFunctionsSuite", "org.apache.spark.sql.JsonFunctionsSuite", "org.apache.spark.sql.JsonFunctionsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.v2.AlterTableRenameSuite", "org.apache.spark.sql.execution.command.v2.AlterTableRenameSuite", "org.apache.spark.sql.execution.command.v2.AlterTableRenameSuite", "org.apache.spark.sql.execution.command.v2.AlterTableRenameSuite", "org.apache.spark.sql.execution.command.v2.AlterTableRenameSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.test.DataStreamReaderWriterSuite", "org.apache.spark.sql.streaming.test.DataStreamReaderWriterSuite", "org.apache.spark.sql.streaming.test.DataStreamReaderWriterSuite", "org.apache.spark.sql.streaming.test.DataStreamReaderWriterSuite", "org.apache.spark.sql.streaming.test.DataStreamReaderWriterSuite", "org.apache.spark.sql.streaming.test.DataStreamReaderWriterSuite", "org.apache.spark.sql.streaming.test.DataStreamReaderWriterSuite", "org.apache.spark.sql.streaming.StreamingSymmetricHashJoinHelperSuite", "org.apache.spark.sql.streaming.StreamingSymmetricHashJoinHelperSuite", "org.apache.spark.sql.streaming.StreamingSymmetricHashJoinHelperSuite", "org.apache.spark.sql.streaming.StreamingSymmetricHashJoinHelperSuite", "org.apache.spark.sql.streaming.StreamingSymmetricHashJoinHelperSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormatV1Suite", "org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormatSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormatSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormatSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormatSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.PlanStabilitySuite", "org.apache.spark.sql.PlanStabilitySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.DataSourceV2ScanExecRedactionSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.orc.OrcV1PartitionDiscoverySuite", "org.apache.spark.sql.execution.datasources.orc.OrcV1PartitionDiscoverySuite", "org.apache.spark.sql.execution.datasources.orc.OrcV1PartitionDiscoverySuite", "org.apache.spark.sql.execution.datasources.orc.OrcV1PartitionDiscoverySuite", "org.apache.spark.sql.execution.datasources.orc.OrcV1PartitionDiscoverySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.test.TestSparkSessionSuite", "org.apache.spark.sql.test.TestSparkSessionSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.connector.DataSourceV2DataFrameSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetInteroperabilitySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetInteroperabilitySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetInteroperabilitySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetInteroperabilitySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetInteroperabilitySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.QueryPlanningTrackerEndToEndSuite", "org.apache.spark.sql.execution.QueryPlanningTrackerEndToEndSuite", "org.apache.spark.sql.execution.QueryPlanningTrackerEndToEndSuite", "org.apache.spark.sql.execution.QueryPlanningTrackerEndToEndSuite", "org.apache.spark.sql.execution.QueryPlanningTrackerEndToEndSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetProtobufCompatibilitySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetProtobufCompatibilitySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetProtobufCompatibilitySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetProtobufCompatibilitySuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetProtobufCompatibilitySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.exchange.EnsureRequirementsSuite", "org.apache.spark.sql.execution.exchange.EnsureRequirementsSuite", "org.apache.spark.sql.execution.exchange.EnsureRequirementsSuite", "org.apache.spark.sql.execution.exchange.EnsureRequirementsSuite", "org.apache.spark.sql.execution.exchange.EnsureRequirementsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.SimpleSQLViewSuite", "org.apache.spark.sql.execution.SimpleSQLViewSuite", "org.apache.spark.sql.execution.SimpleSQLViewSuite", "org.apache.spark.sql.execution.SimpleSQLViewSuite", "org.apache.spark.sql.execution.SimpleSQLViewSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.v1.DropTableSuite", "org.apache.spark.sql.execution.command.v1.DropTableSuite", "org.apache.spark.sql.execution.command.v1.DropTableSuite", "org.apache.spark.sql.execution.command.v1.DropTableSuite", "org.apache.spark.sql.execution.command.v1.DropTableSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.joins.ExistenceJoinSuite", "org.apache.spark.sql.execution.joins.ExistenceJoinSuite", "org.apache.spark.sql.execution.joins.ExistenceJoinSuite", "org.apache.spark.sql.execution.joins.ExistenceJoinSuite", "org.apache.spark.sql.execution.joins.ExistenceJoinSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.SameResultSuite", "org.apache.spark.sql.execution.SameResultSuite", "org.apache.spark.sql.execution.SameResultSuite", "org.apache.spark.sql.execution.SameResultSuite", "org.apache.spark.sql.execution.SameResultSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.StreamingFullOuterJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.streaming.StreamingJoinSuite", "org.apache.spark.sql.DataFrameStatSuite", "org.apache.spark.sql.DataFrameStatSuite", "org.apache.spark.sql.DataFrameStatSuite", "org.apache.spark.sql.DataFrameStatSuite", "org.apache.spark.sql.DataFrameStatSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.XPathFunctionsSuite", "org.apache.spark.sql.XPathFunctionsSuite", "org.apache.spark.sql.XPathFunctionsSuite", "org.apache.spark.sql.XPathFunctionsSuite", "org.apache.spark.sql.XPathFunctionsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.FileStreamSourceStressTestSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DataFrameWindowFramesSuite", "org.apache.spark.sql.DataFrameWindowFramesSuite", "org.apache.spark.sql.DataFrameWindowFramesSuite", "org.apache.spark.sql.DataFrameWindowFramesSuite", "org.apache.spark.sql.DataFrameWindowFramesSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.status.api.v1.sql.SqlResourceWithActualMetricsSuite", "org.apache.spark.status.api.v1.sql.SqlResourceWithActualMetricsSuite", "org.apache.spark.status.api.v1.sql.SqlResourceWithActualMetricsSuite", "org.apache.spark.status.api.v1.sql.SqlResourceWithActualMetricsSuite", "org.apache.spark.status.api.v1.sql.SqlResourceWithActualMetricsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.state.StateStoreRDDSuite", "org.apache.spark.sql.execution.streaming.state.StateStoreRDDSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.state.StateStoreRDDSuite", "org.apache.spark.sql.execution.streaming.state.StateStoreRDDSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.state.StateStoreRDDSuite", "org.apache.spark.sql.execution.streaming.state.StateStoreRDDSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite", "org.apache.spark.sql.sources.DisableUnnecessaryBucketedScanWithoutHiveSupportSuite", "org.apache.spark.sql.sources.DisableUnnecessaryBucketedScanWithoutHiveSupportSuite", "org.apache.spark.sql.sources.DisableUnnecessaryBucketedScanWithoutHiveSupportSuite", "org.apache.spark.sql.sources.DisableUnnecessaryBucketedScanWithoutHiveSupportSuite", "org.apache.spark.sql.sources.DisableUnnecessaryBucketedScanWithoutHiveSupportSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.PathFilterSuite", "org.apache.spark.sql.execution.datasources.PathFilterSuite", "org.apache.spark.sql.execution.datasources.PathFilterSuite", "org.apache.spark.sql.execution.datasources.PathFilterSuite", "org.apache.spark.sql.execution.datasources.PathFilterSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRenamePartitionSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRenamePartitionSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRenamePartitionSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRenamePartitionSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRenamePartitionSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.state.StreamingSessionWindowStateManagerSuite", "org.apache.spark.sql.execution.streaming.state.StreamingSessionWindowStateManagerSuite", "org.apache.spark.sql.execution.streaming.state.StreamingSessionWindowStateManagerSuite", "org.apache.spark.sql.execution.streaming.state.StreamingSessionWindowStateManagerSuite", "org.apache.spark.sql.execution.streaming.state.StreamingSessionWindowStateManagerSuite", "org.apache.spark.sql.execution.streaming.state.StreamingSessionWindowStateManagerSuite", "org.apache.spark.sql.execution.streaming.state.StreamingSessionWindowStateManagerSuite", "org.apache.spark.sql.execution.datasources.FileSourceStrategySuite", "org.apache.spark.sql.execution.datasources.FileSourceStrategySuite", "org.apache.spark.sql.execution.datasources.FileSourceStrategySuite", "org.apache.spark.sql.execution.datasources.FileSourceStrategySuite", "org.apache.spark.sql.execution.datasources.FileSourceStrategySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.SessionStateSuite", "org.apache.spark.sql.SessionStateSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.connector.LocalScanSuite", "org.apache.spark.sql.connector.LocalScanSuite", "org.apache.spark.sql.connector.LocalScanSuite", "org.apache.spark.sql.connector.LocalScanSuite", "org.apache.spark.sql.connector.LocalScanSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetCompressionCodecPrecedenceSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetCompressionCodecPrecedenceSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetCompressionCodecPrecedenceSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetCompressionCodecPrecedenceSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetCompressionCodecPrecedenceSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.UnsafeRowSerializerSuite", "org.apache.spark.sql.execution.UnsafeRowSerializerSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.v2.ShowTablesSuite", "org.apache.spark.sql.execution.command.v2.ShowTablesSuite", "org.apache.spark.sql.execution.command.v2.ShowTablesSuite", "org.apache.spark.sql.execution.command.v2.ShowTablesSuite", "org.apache.spark.sql.execution.command.v2.ShowTablesSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.BroadcastExchangeSuite", "org.apache.spark.sql.execution.BroadcastExchangeSuite", "org.apache.spark.sql.execution.BroadcastExchangeSuite", "org.apache.spark.sql.execution.BroadcastExchangeSuite", "org.apache.spark.sql.execution.BroadcastExchangeSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.OptimizeMetadataOnlyQuerySuite", "org.apache.spark.sql.execution.OptimizeMetadataOnlyQuerySuite", "org.apache.spark.sql.execution.OptimizeMetadataOnlyQuerySuite", "org.apache.spark.sql.execution.OptimizeMetadataOnlyQuerySuite", "org.apache.spark.sql.execution.OptimizeMetadataOnlyQuerySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.MergedOrcReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.StreamingQuerySuite", "org.apache.spark.sql.streaming.StreamingQuerySuite", "org.apache.spark.sql.streaming.StreamingQuerySuite", "org.apache.spark.sql.streaming.StreamingQuerySuite", "org.apache.spark.sql.streaming.StreamingQuerySuite", "org.apache.spark.sql.streaming.StreamingQuerySuite", "org.apache.spark.sql.streaming.StreamingQuerySuite", "org.apache.spark.sql.execution.datasources.jdbc.connection.DisallowedConnectionProviderSuite", "org.apache.spark.sql.execution.datasources.jdbc.connection.DisallowedConnectionProviderSuite", "org.apache.spark.sql.execution.datasources.jdbc.connection.DisallowedConnectionProviderSuite", "org.apache.spark.sql.execution.datasources.jdbc.connection.DisallowedConnectionProviderSuite", "org.apache.spark.sql.execution.datasources.jdbc.connection.DisallowedConnectionProviderSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.FileStreamSinkV2Suite", "org.apache.spark.sql.streaming.FileStreamSinkSuite", "org.apache.spark.sql.streaming.FileStreamSinkSuite", "org.apache.spark.sql.streaming.FileStreamSinkSuite", "org.apache.spark.sql.streaming.FileStreamSinkSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.AlterTableDropPartitionParserSuite", "org.apache.spark.sql.execution.command.AlterTableDropPartitionParserSuite", "org.apache.spark.sql.execution.command.AlterTableDropPartitionParserSuite", "org.apache.spark.sql.execution.command.AlterTableDropPartitionParserSuite", "org.apache.spark.sql.execution.command.AlterTableDropPartitionParserSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.execution.datasources.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.execution.datasources.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.execution.datasources.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.execution.datasources.PruneFileSourcePartitionsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetCommitterSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetCommitterSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.DDLParserSuite", "org.apache.spark.sql.execution.command.DDLParserSuite", "org.apache.spark.sql.execution.command.DDLParserSuite", "org.apache.spark.sql.execution.command.DDLParserSuite", "org.apache.spark.sql.execution.command.DDLParserSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.v2.V2SessionCatalogNamespaceSuite", "org.apache.spark.sql.execution.datasources.v2.V2SessionCatalogBaseSuite", "org.apache.spark.sql.execution.datasources.v2.V2SessionCatalogBaseSuite", "org.apache.spark.sql.execution.datasources.v2.V2SessionCatalogBaseSuite", "org.apache.spark.sql.execution.datasources.v2.V2SessionCatalogBaseSuite", "org.apache.spark.sql.execution.datasources.v2.V2SessionCatalogBaseSuite", "org.apache.spark.sql.execution.datasources.v2.V2SessionCatalogBaseSuite", "org.apache.spark.sql.sources.ExternalCommandRunnerSuite", "org.apache.spark.sql.sources.ExternalCommandRunnerSuite", "org.apache.spark.sql.sources.ExternalCommandRunnerSuite", "org.apache.spark.sql.sources.ExternalCommandRunnerSuite", "org.apache.spark.sql.sources.ExternalCommandRunnerSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.v2.DropTableSuite", "org.apache.spark.sql.execution.command.v2.DropTableSuite", "org.apache.spark.sql.execution.command.v2.DropTableSuite", "org.apache.spark.sql.execution.command.v2.DropTableSuite", "org.apache.spark.sql.execution.command.v2.DropTableSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.RowSuite", "org.apache.spark.sql.RowSuite", "org.apache.spark.sql.RowSuite", "org.apache.spark.sql.RowSuite", "org.apache.spark.sql.RowSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.v2.TruncateTableSuite", "org.apache.spark.sql.execution.command.v2.TruncateTableSuite", "org.apache.spark.sql.execution.command.v2.TruncateTableSuite", "org.apache.spark.sql.execution.command.v2.TruncateTableSuite", "org.apache.spark.sql.execution.command.v2.TruncateTableSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.RemoveRedundantProjectsSuite", "org.apache.spark.sql.execution.RemoveRedundantProjectsSuite", "org.apache.spark.sql.execution.RemoveRedundantProjectsSuite", "org.apache.spark.sql.execution.RemoveRedundantProjectsSuite", "org.apache.spark.sql.execution.RemoveRedundantProjectsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.connector.V2CommandsCaseSensitivitySuite", "org.apache.spark.sql.connector.V2CommandsCaseSensitivitySuite", "org.apache.spark.sql.connector.V2CommandsCaseSensitivitySuite", "org.apache.spark.sql.connector.V2CommandsCaseSensitivitySuite", "org.apache.spark.sql.connector.V2CommandsCaseSensitivitySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.sources.ResolvedDataSourceSuite", "org.apache.spark.sql.sources.ResolvedDataSourceSuite", "org.apache.spark.sql.sources.ResolvedDataSourceSuite", "org.apache.spark.sql.sources.ResolvedDataSourceSuite", "org.apache.spark.sql.sources.ResolvedDataSourceSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.sources.ForeachWriterSuite", "org.apache.spark.sql.execution.streaming.sources.ForeachWriterSuite", "org.apache.spark.sql.execution.streaming.sources.ForeachWriterSuite", "org.apache.spark.sql.execution.streaming.sources.ForeachWriterSuite", "org.apache.spark.sql.execution.streaming.sources.ForeachWriterSuite", "org.apache.spark.sql.execution.streaming.sources.ForeachWriterSuite", "org.apache.spark.sql.execution.streaming.sources.ForeachWriterSuite", "org.apache.spark.sql.execution.python.PythonUDFSuite", "org.apache.spark.sql.execution.python.PythonUDFSuite", "org.apache.spark.sql.execution.python.PythonUDFSuite", "org.apache.spark.sql.execution.python.PythonUDFSuite", "org.apache.spark.sql.execution.python.PythonUDFSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreIntegrationSuite", "org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreIntegrationSuite", "org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreIntegrationSuite", "org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreIntegrationSuite", "org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreIntegrationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.ReuseExchangeAndSubquerySuite", "org.apache.spark.sql.execution.ReuseExchangeAndSubquerySuite", "org.apache.spark.sql.execution.ReuseExchangeAndSubquerySuite", "org.apache.spark.sql.execution.ReuseExchangeAndSubquerySuite", "org.apache.spark.sql.execution.ReuseExchangeAndSubquerySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DataFrameFunctionsSuite", "org.apache.spark.sql.DataFrameFunctionsSuite", "org.apache.spark.sql.DataFrameFunctionsSuite", "org.apache.spark.sql.DataFrameFunctionsSuite", "org.apache.spark.sql.DataFrameFunctionsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.orc.OrcV2QuerySuite", "org.apache.spark.sql.execution.datasources.orc.OrcQuerySuite", "org.apache.spark.sql.execution.datasources.orc.OrcQuerySuite", "org.apache.spark.sql.execution.datasources.orc.OrcQuerySuite", "org.apache.spark.sql.execution.datasources.orc.OrcQuerySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilityWithStatsSuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilityWithStatsSuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilityWithStatsSuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilityWithStatsSuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilityWithStatsSuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilityWithStatsSuite", "org.apache.spark.sql.PlanStabilitySuite", "org.apache.spark.sql.PlanStabilitySuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilityWithStatsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormatV2Suite", "org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormatSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormatSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormatSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormatSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.MetadataCacheV1Suite", "org.apache.spark.sql.MetadataCacheSuite", "org.apache.spark.sql.MetadataCacheSuite", "org.apache.spark.sql.MetadataCacheSuite", "org.apache.spark.sql.MetadataCacheSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.continuous.ContinuousMetaSuite", "org.apache.spark.sql.streaming.continuous.ContinuousMetaSuite", "org.apache.spark.sql.streaming.continuous.ContinuousSuite", "org.apache.spark.sql.streaming.continuous.ContinuousSuite", "org.apache.spark.sql.streaming.continuous.ContinuousSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.ColumnExpressionSuite", "org.apache.spark.sql.ColumnExpressionSuite", "org.apache.spark.sql.ColumnExpressionSuite", "org.apache.spark.sql.ColumnExpressionSuite", "org.apache.spark.sql.ColumnExpressionSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DataFrameSessionWindowingSuite", "org.apache.spark.sql.DataFrameSessionWindowingSuite", "org.apache.spark.sql.DataFrameSessionWindowingSuite", "org.apache.spark.sql.DataFrameSessionWindowingSuite", "org.apache.spark.sql.DataFrameSessionWindowingSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.util.ExecutionListenerManagerSuite", "org.apache.spark.sql.util.ExecutionListenerManagerSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DataFrameImplicitsSuite", "org.apache.spark.sql.DataFrameImplicitsSuite", "org.apache.spark.sql.DataFrameImplicitsSuite", "org.apache.spark.sql.DataFrameImplicitsSuite", "org.apache.spark.sql.DataFrameImplicitsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.HDFSMetadataLogSuite", "org.apache.spark.sql.execution.streaming.HDFSMetadataLogSuite", "org.apache.spark.sql.execution.streaming.HDFSMetadataLogSuite", "org.apache.spark.sql.execution.streaming.HDFSMetadataLogSuite", "org.apache.spark.sql.execution.streaming.HDFSMetadataLogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.ExchangeSuite", "org.apache.spark.sql.execution.ExchangeSuite", "org.apache.spark.sql.execution.ExchangeSuite", "org.apache.spark.sql.execution.ExchangeSuite", "org.apache.spark.sql.execution.ExchangeSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DatasetOptimizationSuite", "org.apache.spark.sql.DatasetOptimizationSuite", "org.apache.spark.sql.DatasetOptimizationSuite", "org.apache.spark.sql.DatasetOptimizationSuite", "org.apache.spark.sql.DatasetOptimizationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.continuous.ContinuousSuite", "org.apache.spark.sql.streaming.continuous.ContinuousSuite", "org.apache.spark.sql.streaming.continuous.ContinuousSuite", "org.apache.spark.sql.streaming.continuous.ContinuousSuite", "org.apache.spark.sql.streaming.continuous.ContinuousSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.joins.BroadcastJoinSuite", "org.apache.spark.sql.execution.joins.BroadcastJoinSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.json.JsonV1Suite", "org.apache.spark.sql.execution.datasources.json.JsonSuite", "org.apache.spark.sql.execution.datasources.json.JsonSuite", "org.apache.spark.sql.execution.datasources.json.JsonSuite", "org.apache.spark.sql.execution.datasources.json.JsonSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.v1.AlterTableDropPartitionSuite", "org.apache.spark.sql.execution.command.v1.AlterTableDropPartitionSuite", "org.apache.spark.sql.execution.command.v1.AlterTableDropPartitionSuite", "org.apache.spark.sql.execution.command.v1.AlterTableDropPartitionSuite", "org.apache.spark.sql.execution.command.v1.AlterTableDropPartitionSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.state.FlatMapGroupsWithStateExecHelperSuite", "org.apache.spark.sql.execution.streaming.state.FlatMapGroupsWithStateExecHelperSuite", "org.apache.spark.sql.execution.streaming.state.FlatMapGroupsWithStateExecHelperSuite", "org.apache.spark.sql.execution.streaming.state.FlatMapGroupsWithStateExecHelperSuite", "org.apache.spark.sql.execution.streaming.state.FlatMapGroupsWithStateExecHelperSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.ShufflePartitionsUtilSuite", "org.apache.spark.sql.execution.ShufflePartitionsUtilSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.PlanStabilitySuite", "org.apache.spark.sql.PlanStabilitySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.connector.DataSourceV2Suite", "org.apache.spark.sql.connector.DataSourceV2Suite", "org.apache.spark.sql.connector.DataSourceV2Suite", "org.apache.spark.sql.connector.DataSourceV2Suite", "org.apache.spark.sql.connector.DataSourceV2Suite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.ExplainSuite", "org.apache.spark.sql.ExplainSuite", "org.apache.spark.sql.ExplainSuite", "org.apache.spark.sql.ExplainSuite", "org.apache.spark.sql.ExplainSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.TakeOrderedAndProjectSuite", "org.apache.spark.sql.execution.TakeOrderedAndProjectSuite", "org.apache.spark.sql.execution.TakeOrderedAndProjectSuite", "org.apache.spark.sql.execution.TakeOrderedAndProjectSuite", "org.apache.spark.sql.execution.TakeOrderedAndProjectSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.api.r.SQLUtilsSuite", "org.apache.spark.sql.api.r.SQLUtilsSuite", "org.apache.spark.sql.api.r.SQLUtilsSuite", "org.apache.spark.sql.api.r.SQLUtilsSuite", "org.apache.spark.sql.api.r.SQLUtilsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.PlanStabilitySuite", "org.apache.spark.sql.PlanStabilitySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.sources.DDLSourceLoadSuite", "org.apache.spark.sql.sources.DDLSourceLoadSuite", "org.apache.spark.sql.sources.DDLSourceLoadSuite", "org.apache.spark.sql.sources.DDLSourceLoadSuite", "org.apache.spark.sql.sources.DDLSourceLoadSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.StreamingStateStoreFormatCompatibilitySuite", "org.apache.spark.sql.streaming.StreamingStateStoreFormatCompatibilitySuite", "org.apache.spark.sql.streaming.StreamingStateStoreFormatCompatibilitySuite", "org.apache.spark.sql.streaming.StreamingStateStoreFormatCompatibilitySuite", "org.apache.spark.sql.streaming.StreamingStateStoreFormatCompatibilitySuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.ui.StreamingQueryHistorySuite", "org.apache.spark.sql.streaming.ui.StreamingQueryHistorySuite", "org.apache.spark.sql.streaming.ui.StreamingQueryHistorySuite", "org.apache.spark.sql.streaming.ui.StreamingQueryHistorySuite", "org.apache.spark.sql.streaming.ui.StreamingQueryHistorySuite", "org.apache.spark.sql.streaming.ui.StreamingQueryHistorySuite", "org.apache.spark.sql.streaming.ui.StreamingQueryHistorySuite", "org.apache.spark.sql.execution.datasources.json.JsonV2Suite", "org.apache.spark.sql.execution.datasources.json.JsonSuite", "org.apache.spark.sql.execution.datasources.json.JsonSuite", "org.apache.spark.sql.execution.datasources.json.JsonSuite", "org.apache.spark.sql.execution.datasources.json.JsonSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.SimpleShowCreateTableSuite", "org.apache.spark.sql.SimpleShowCreateTableSuite", "org.apache.spark.sql.SimpleShowCreateTableSuite", "org.apache.spark.sql.SimpleShowCreateTableSuite", "org.apache.spark.sql.SimpleShowCreateTableSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.json.JsonLegacyTimeParserSuite", "org.apache.spark.sql.execution.datasources.json.JsonSuite", "org.apache.spark.sql.execution.datasources.json.JsonSuite", "org.apache.spark.sql.execution.datasources.json.JsonSuite", "org.apache.spark.sql.execution.datasources.json.JsonSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.debug.DebuggingSuite", "org.apache.spark.sql.execution.debug.DebuggingSuite", "org.apache.spark.sql.execution.debug.DebuggingSuite", "org.apache.spark.sql.execution.debug.DebuggingSuite", "org.apache.spark.sql.execution.debug.DebuggingSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.csv.CSVv2Suite", "org.apache.spark.sql.execution.datasources.csv.CSVSuite", "org.apache.spark.sql.execution.datasources.csv.CSVSuite", "org.apache.spark.sql.execution.datasources.csv.CSVSuite", "org.apache.spark.sql.execution.datasources.csv.CSVSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.StreamSuite", "org.apache.spark.sql.streaming.StreamSuite", "org.apache.spark.sql.streaming.StreamSuite", "org.apache.spark.sql.streaming.StreamSuite", "org.apache.spark.sql.streaming.StreamSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DataFrameHintSuite", "org.apache.spark.sql.DataFrameHintSuite", "org.apache.spark.sql.DataFrameHintSuite", "org.apache.spark.sql.DataFrameHintSuite", "org.apache.spark.sql.DataFrameHintSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DataFrameRangeSuite", "org.apache.spark.sql.DataFrameRangeSuite", "org.apache.spark.sql.DataFrameRangeSuite", "org.apache.spark.sql.DataFrameRangeSuite", "org.apache.spark.sql.DataFrameRangeSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DataFrameTimeWindowingSuite", "org.apache.spark.sql.DataFrameTimeWindowingSuite", "org.apache.spark.sql.DataFrameTimeWindowingSuite", "org.apache.spark.sql.DataFrameTimeWindowingSuite", "org.apache.spark.sql.DataFrameTimeWindowingSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.FileSourceCharVarcharDDLTestSuite", "org.apache.spark.sql.execution.command.FileSourceCharVarcharDDLTestSuite", "org.apache.spark.sql.execution.command.FileSourceCharVarcharDDLTestSuite", "org.apache.spark.sql.execution.command.FileSourceCharVarcharDDLTestSuite", "org.apache.spark.sql.execution.command.FileSourceCharVarcharDDLTestSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.CheckpointFileManagerSuite", "org.apache.spark.sql.execution.streaming.CheckpointFileManagerSuite", "org.apache.spark.sql.execution.streaming.CheckpointFileManagerSuite", "org.apache.spark.sql.execution.streaming.CheckpointFileManagerSuite", "org.apache.spark.sql.execution.streaming.CheckpointFileManagerSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.IntervalFunctionsSuite", "org.apache.spark.sql.IntervalFunctionsSuite", "org.apache.spark.sql.IntervalFunctionsSuite", "org.apache.spark.sql.IntervalFunctionsSuite", "org.apache.spark.sql.IntervalFunctionsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.DeprecatedWholeStageCodegenSuite", "org.apache.spark.sql.execution.DeprecatedWholeStageCodegenSuite", "org.apache.spark.sql.execution.DeprecatedWholeStageCodegenSuite", "org.apache.spark.sql.execution.DeprecatedWholeStageCodegenSuite", "org.apache.spark.sql.execution.DeprecatedWholeStageCodegenSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.sources.InsertSuite", "org.apache.spark.sql.sources.InsertSuite", "org.apache.spark.sql.sources.InsertSuite", "org.apache.spark.sql.sources.InsertSuite", "org.apache.spark.sql.sources.InsertSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DataFrameSetOperationsSuite", "org.apache.spark.sql.DataFrameSetOperationsSuite", "org.apache.spark.sql.DataFrameSetOperationsSuite", "org.apache.spark.sql.DataFrameSetOperationsSuite", "org.apache.spark.sql.DataFrameSetOperationsSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.SparkPlannerSuite", "org.apache.spark.sql.execution.SparkPlannerSuite", "org.apache.spark.sql.execution.SparkPlannerSuite", "org.apache.spark.sql.execution.SparkPlannerSuite", "org.apache.spark.sql.execution.SparkPlannerSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.DatasetSerializerRegistratorSuite", "org.apache.spark.sql.DatasetSerializerRegistratorSuite", "org.apache.spark.sql.DatasetSerializerRegistratorSuite", "org.apache.spark.sql.DatasetSerializerRegistratorSuite", "org.apache.spark.sql.DatasetSerializerRegistratorSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.command.ShowNamespacesParserSuite", "org.apache.spark.sql.execution.command.ShowNamespacesParserSuite", "org.apache.spark.sql.execution.command.ShowNamespacesParserSuite", "org.apache.spark.sql.execution.command.ShowNamespacesParserSuite", "org.apache.spark.sql.execution.command.ShowNamespacesParserSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.streaming.StreamingQueryStatusAndProgressSuite", "org.apache.spark.sql.streaming.StreamingQueryStatusAndProgressSuite", "org.apache.spark.sql.streaming.StreamingQueryStatusAndProgressSuite", "org.apache.spark.sql.streaming.StreamingQueryStatusAndProgressSuite", "org.apache.spark.sql.streaming.StreamingQueryStatusAndProgressSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.columnar.PartitionBatchPruningSuite", "org.apache.spark.sql.execution.columnar.PartitionBatchPruningSuite", "org.apache.spark.sql.execution.columnar.PartitionBatchPruningSuite", "org.apache.spark.sql.execution.columnar.PartitionBatchPruningSuite", "org.apache.spark.sql.execution.columnar.PartitionBatchPruningSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.streaming.FileStreamSinkLogSuite", "org.apache.spark.sql.execution.streaming.FileStreamSinkLogSuite", "org.apache.spark.sql.execution.streaming.FileStreamSinkLogSuite", "org.apache.spark.sql.execution.streaming.FileStreamSinkLogSuite", "org.apache.spark.sql.execution.streaming.FileStreamSinkLogSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.VectorizedOrcReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.ReadSchemaSuite", "org.apache.spark.sql.execution.datasources.VectorizedOrcReadSchemaSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.ComplexTypesSuite", "org.apache.spark.sql.ComplexTypesSuite", "org.apache.spark.sql.ComplexTypesSuite", "org.apache.spark.sql.ComplexTypesSuite", "org.apache.spark.sql.ComplexTypesSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.joins.HashedRelationSuite", "org.apache.spark.sql.execution.joins.HashedRelationSuite", "org.apache.spark.sql.execution.joins.HashedRelationSuite", "org.apache.spark.sql.execution.joins.HashedRelationSuite", "org.apache.spark.sql.execution.joins.HashedRelationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.jdbc.JDBCWriteSuite", "org.apache.spark.sql.jdbc.JDBCWriteSuite", "org.apache.spark.sql.jdbc.JDBCWriteSuite", "org.apache.spark.sql.jdbc.JDBCWriteSuite", "org.apache.spark.sql.jdbc.JDBCWriteSuite", "org.apache.spark.sql.jdbc.JDBCWriteSuite", "org.apache.spark.sql.jdbc.JDBCWriteSuite", "org.apache.spark.sql.ExplainSuite", "org.apache.spark.sql.ExplainSuite", "org.apache.spark.sql.ExplainSuite", "org.apache.spark.sql.ExplainSuite", "org.apache.spark.sql.ExplainSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.joins.OuterJoinSuite", "org.apache.spark.sql.execution.joins.OuterJoinSuite", "org.apache.spark.sql.execution.joins.OuterJoinSuite", "org.apache.spark.sql.execution.joins.OuterJoinSuite", "org.apache.spark.sql.execution.joins.OuterJoinSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.PlannerSuite", "org.apache.spark.sql.execution.PlannerSuite", "org.apache.spark.sql.execution.PlannerSuite", "org.apache.spark.sql.execution.PlannerSuite", "org.apache.spark.sql.execution.PlannerSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.execution.datasources.orc.OrcColumnarBatchReaderSuite", "org.apache.spark.sql.execution.datasources.orc.OrcColumnarBatchReaderSuite", "org.apache.spark.sql.execution.datasources.orc.OrcColumnarBatchReaderSuite", "org.apache.spark.sql.execution.datasources.orc.OrcColumnarBatchReaderSuite", "org.apache.spark.sql.execution.datasources.orc.OrcColumnarBatchReaderSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.JavaUDFSuite", "org.apache.spark.sql.JavaUDFSuite", "org.apache.spark.sql.JavaUDFSuite", "org.apache.spark.sql.JavaUDFSuite", "org.apache.spark.sql.JavaUDFSuite", "org.apache.spark.sql.JavaUDFSuite", "org.apache.spark.sql.JavaUDFSuite", "org.apache.spark.sql.JavaUDFSuite", "org.apache.spark.sql.JavaUDFSuite", "org.apache.spark.sql.JavaUDFSuite", "org.apache.spark.sql.JavaUDFSuite", "org.apache.spark.sql.JavaUDFSuite", "org.apache.spark.sql.JavaUDFSuite", "org.apache.spark.sql.JavaUDFSuite", "org.apache.spark.sql.JavaUDFSuite", "org.apache.spark.sql.JavaUDFSuite", "org.apache.spark.sql.JavaDataFrameReaderWriterSuite", "org.apache.spark.sql.JavaDataFrameReaderWriterSuite", "org.apache.spark.sql.JavaDataFrameReaderWriterSuite", "org.apache.spark.sql.JavaDataFrameReaderWriterSuite", "org.apache.spark.sql.JavaDataFrameReaderWriterSuite", "org.apache.spark.sql.JavaDataFrameReaderWriterSuite", "org.apache.spark.sql.JavaDataFrameReaderWriterSuite", "org.apache.spark.sql.JavaDataFrameReaderWriterSuite", "org.apache.spark.sql.JavaDataFrameReaderWriterSuite", "org.apache.spark.sql.JavaDatasetAggregatorSuite", "org.apache.spark.sql.JavaDatasetAggregatorSuite", "org.apache.spark.sql.JavaDatasetAggregatorSuite", "org.apache.spark.sql.JavaDatasetAggregatorSuite", "org.apache.spark.sql.JavaDatasetAggregatorSuite", "org.apache.spark.sql.JavaDatasetAggregatorSuite", "org.apache.spark.sql.JavaDatasetAggregatorSuite", "org.apache.spark.sql.JavaDatasetAggregatorSuite", "org.apache.spark.sql.JavaUDAFSuite", "org.apache.spark.sql.JavaUDAFSuite", "org.apache.spark.sql.JavaDatasetAggregatorSuite", "org.apache.spark.sql.JavaDatasetAggregatorSuite", "org.apache.spark.sql.JavaDatasetAggregatorSuite", "org.apache.spark.sql.JavaDatasetAggregatorSuite", "org.apache.spark.sql.JavaDatasetAggregatorSuite", "org.apache.spark.sql.JavaDatasetAggregatorSuite", "org.apache.spark.sql.JavaDatasetAggregatorSuite", "org.apache.spark.sql.JavaDatasetAggregatorSuite", "org.apache.spark.sql.JavaDatasetAggregatorSuite", "org.apache.spark.sql.JavaDatasetAggregatorSuite", "org.apache.spark.sql.JavaApplySchemaSuite", "org.apache.spark.sql.JavaApplySchemaSuite", "org.apache.spark.sql.JavaApplySchemaSuite", "org.apache.spark.sql.JavaApplySchemaSuite", "org.apache.spark.sql.JavaApplySchemaSuite", "org.apache.spark.sql.JavaApplySchemaSuite", "org.apache.spark.sql.JavaColumnExpressionSuite", "org.apache.spark.sql.JavaColumnExpressionSuite", "org.apache.spark.sql.JavaColumnExpressionSuite", "org.apache.spark.sql.JavaColumnExpressionSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite", "org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite", "org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite", "org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite", "org.apache.spark.sql.JavaSaveLoadSuite", "org.apache.spark.sql.JavaSaveLoadSuite", "org.apache.spark.sql.JavaSaveLoadSuite", "org.apache.spark.sql.JavaSaveLoadSuite", "org.apache.spark.sql.JavaHigherOrderFunctionsSuite", "org.apache.spark.sql.JavaHigherOrderFunctionsSuite", "org.apache.spark.sql.JavaHigherOrderFunctionsSuite", "org.apache.spark.sql.JavaHigherOrderFunctionsSuite", "org.apache.spark.sql.JavaHigherOrderFunctionsSuite", "org.apache.spark.sql.JavaHigherOrderFunctionsSuite", "org.apache.spark.sql.JavaHigherOrderFunctionsSuite", "org.apache.spark.sql.JavaHigherOrderFunctionsSuite", "org.apache.spark.sql.JavaHigherOrderFunctionsSuite", "org.apache.spark.sql.JavaHigherOrderFunctionsSuite", "org.apache.spark.sql.JavaHigherOrderFunctionsSuite", "org.apache.spark.sql.JavaHigherOrderFunctionsSuite", "org.apache.spark.sql.JavaHigherOrderFunctionsSuite", "org.apache.spark.sql.JavaHigherOrderFunctionsSuite", "org.apache.spark.sql.JavaHigherOrderFunctionsSuite", "org.apache.spark.sql.JavaHigherOrderFunctionsSuite", "org.apache.spark.sql.JavaHigherOrderFunctionsSuite", "org.apache.spark.sql.JavaHigherOrderFunctionsSuite", "org.apache.spark.sql.JavaHigherOrderFunctionsSuite", "org.apache.spark.sql.JavaHigherOrderFunctionsSuite", "org.apache.spark.sql.JavaBeanDeserializationSuite", "org.apache.spark.sql.JavaBeanDeserializationSuite", "org.apache.spark.sql.JavaBeanDeserializationSuite", "org.apache.spark.sql.JavaBeanDeserializationSuite", "org.apache.spark.sql.JavaBeanDeserializationSuite", "org.apache.spark.sql.JavaBeanDeserializationSuite", "org.apache.spark.sql.JavaBeanDeserializationSuite", "org.apache.spark.sql.JavaBeanDeserializationSuite", "org.apache.spark.sql.JavaBeanDeserializationSuite", "org.apache.spark.sql.JavaBeanDeserializationSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameWriterV2Suite", "org.apache.spark.sql.JavaDataFrameWriterV2Suite", "org.apache.spark.sql.JavaDataFrameWriterV2Suite", "org.apache.spark.sql.JavaDataFrameWriterV2Suite", "org.apache.spark.sql.JavaDataFrameWriterV2Suite", "org.apache.spark.sql.JavaDataFrameWriterV2Suite", "org.apache.spark.sql.JavaDataFrameWriterV2Suite", "org.apache.spark.sql.JavaDataFrameWriterV2Suite", "org.apache.spark.sql.JavaDataFrameWriterV2Suite", "org.apache.spark.sql.JavaDataFrameWriterV2Suite", "org.apache.spark.sql.JavaDataFrameWriterV2Suite", "org.apache.spark.sql.JavaDataFrameWriterV2Suite", "org.apache.spark.sql.JavaUDFSuite", "org.apache.spark.sql.execution.datasources.BasicWriteJobStatsTrackerMetricSuite", "org.apache.spark.sql.JavaDataFrameReaderWriterSuite", "org.apache.spark.sql.Java8DatasetAggregatorSuite", "org.apache.spark.sql.JavaUDAFSuite", "org.apache.spark.sql.JavaDatasetAggregatorSuite", "org.apache.spark.sql.JavaApplySchemaSuite", "org.apache.spark.sql.JavaColumnExpressionSuite", "org.apache.spark.sql.JavaDatasetSuite", "org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite", "org.apache.spark.sql.JavaSaveLoadSuite", "org.apache.spark.sql.JavaHigherOrderFunctionsSuite", "org.apache.spark.sql.JavaBeanDeserializationSuite", "org.apache.spark.sql.execution.WholeStageCodegenSparkSubmitSuite", "org.apache.spark.sql.JavaDataFrameSuite", "org.apache.spark.sql.JavaDataFrameWriterV2Suite", "org.apache.spark.sql.SQLQuerySuite", "org.apache.spark.sql.streaming.MemorySourceStressSuite", "org.apache.spark.sql.execution.HiveResultSuite", "org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderSuite", "org.apache.spark.sql.execution.command.DSV2CharVarcharDDLTestSuite", "org.apache.spark.sql.execution.SparkPlanSuite", "org.apache.spark.sql.streaming.StreamingAggregationSuite", "org.apache.spark.sql.DateFunctionsSuite", "org.apache.spark.sql.NestedDataSourceV1Suite", "org.apache.spark.sql.execution.streaming.state.StateStoreSuite", "org.apache.spark.sql.execution.streaming.sources.ForeachBatchSinkSuite", "org.apache.spark.sql.jdbc.JDBCV2Suite", "org.apache.spark.sql.streaming.continuous.ContinuousQueryStatusAndProgressSuite", "org.apache.spark.sql.sources.BucketedReadWithoutHiveSupportSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRenameSuite", "org.apache.spark.sql.execution.datasources.v2.V2SessionCatalogTableSuite", "org.apache.spark.sql.execution.datasources.BasicWriteTaskStatsTrackerSuite", "org.apache.spark.sql.execution.arrow.ArrowConvertersSuite", "org.apache.spark.sql.execution.command.v2.AlterTableDropPartitionSuite", "org.apache.spark.sql.DynamicPartitionPruningV1Suite", "org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite", "org.apache.spark.sql.execution.command.v1.ShowPartitionsSuite", "org.apache.spark.sql.ReplaceNullWithFalseInPredicateEndToEndSuite", "org.apache.spark.sql.streaming.continuous.ContinuousEpochBacklogSuite", "org.apache.spark.sql.DeprecatedAPISuite", "org.apache.spark.sql.execution.datasources.MergedParquetReadSchemaSuite", "org.apache.spark.sql.streaming.StreamingQueryListenersConfSuite", "org.apache.spark.sql.execution.datasources.OrcReadSchemaSuite", "org.apache.spark.sql.jdbc.JDBCSuite", "org.apache.spark.sql.execution.datasources.HeaderCSVReadSchemaSuite", "org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCTableCatalogSuite", "org.apache.spark.sql.DataFrameJoinSuite", "org.apache.spark.sql.execution.SQLWindowFunctionSuite", "org.apache.spark.sql.streaming.continuous.EpochCoordinatorSuite", "org.apache.spark.sql.streaming.sources.StreamingDataSourceV2Suite", "org.apache.spark.sql.SingleLevelAggregateHashMapSuite", "org.apache.spark.sql.execution.command.v2.AlterTableRecoverPartitionsSuite", "org.apache.spark.sql.TypedImperativeAggregateSuite", "org.apache.spark.sql.execution.datasources.binaryfile.BinaryFileFormatSuite", "org.apache.spark.sql.execution.datasources.FileIndexSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetRebaseDatetimeV2Suite", "org.apache.spark.sql.DatasetPrimitiveSuite", "org.apache.spark.sql.execution.datasources.text.WholeTextFileV2Suite", "org.apache.spark.sql.execution.datasources.parquet.ParquetColumnIndexSuite", "org.apache.spark.sql.execution.PersistedViewTestSuite", "org.apache.spark.sql.connector.DataSourceV2FunctionSuite", "org.apache.spark.sql.execution.streaming.UpdatingSessionsIteratorSuite", "org.apache.spark.sql.connector.V1ReadFallbackWithCatalogSuite", "org.apache.spark.sql.connector.V1WriteFallbackSessionCatalogSuite", "org.apache.spark.sql.execution.streaming.state.StateSchemaCompatibilityCheckerSuite", "org.apache.spark.sql.internal.CatalogSuite", "org.apache.spark.sql.connector.WriteDistributionAndOrderingSuite", "org.apache.spark.sql.streaming.continuous.ContinuousQueuedDataReaderSuite", "org.apache.spark.sql.GeneratorFunctionSuite", "org.apache.spark.sql.execution.datasources.csv.CSVLegacyTimeParserSuite", "org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorSuite", "org.apache.spark.sql.execution.datasources.DataSourceStrategySuite", "org.apache.spark.sql.streaming.FileStreamSourceSuite", "org.apache.spark.sql.JoinHintSuite", "org.apache.spark.sql.DynamicPartitionPruningV2Suite", "org.apache.spark.sql.streaming.StreamingSessionWindowSuite", "org.apache.spark.sql.DSV2SQLInsertTestSuite", "org.apache.spark.sql.execution.python.ExtractPythonUDFsSuite", "org.apache.spark.sql.DynamicPartitionPruningV1Suite", "org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite", "org.apache.spark.sql.execution.ui.SQLAppStatusListenerSuite", "org.apache.spark.sql.execution.datasources.orc.OrcV1FilterSuite", "org.apache.spark.sql.connector.DataSourceV2SQLSuite", "org.apache.spark.sql.DatasetCacheSuite", "org.apache.spark.sql.JoinSuite", "org.apache.spark.sql.execution.joins.BroadcastJoinSuite", "org.apache.spark.sql.UDFSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRecoverPartitionsSuite", "org.apache.spark.sql.CachedTableSuite", "org.apache.spark.sql.execution.command.ShowTablesParserSuite", "org.apache.spark.sql.sources.PathOptionSuite", "org.apache.spark.sql.execution.command.AlterTableAddPartitionParserSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaInferenceSuite", "org.apache.spark.sql.execution.GlobalTempViewTestSuite", "org.apache.spark.sql.SerializationSuite", "org.apache.spark.sql.DataFrameWindowFunctionsSuite", "org.apache.spark.sql.execution.datasources.text.TextV1Suite", "org.apache.spark.sql.sources.BucketedWriteWithoutHiveSupportSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetAvroCompatibilitySuite", "org.apache.spark.sql.sources.CreateTableAsSelectSuite", "org.apache.spark.sql.execution.ColumnarRulesSuite", "org.apache.spark.sql.SQLContextSuite", "org.apache.spark.sql.UnwrapCastInComparisonEndToEndSuite", "org.apache.spark.sql.SetCommandSuite", "org.apache.spark.sql.StatisticsCollectionSuite", "org.apache.spark.sql.SSBQuerySuite", "org.apache.spark.sql.streaming.StreamingQueryListenerSuite", "org.apache.spark.sql.streaming.FileStreamSinkV1Suite", "org.apache.spark.sql.streaming.StreamingOuterJoinSuite", "org.apache.spark.sql.test.DataFrameReaderWriterSuite", "org.apache.spark.sql.StringFunctionsSuite", "org.apache.spark.sql.NestedDataSourceV2Suite", "org.apache.spark.sql.execution.command.AlterTableRecoverPartitionsParserSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetRebaseDatetimeV1Suite", "org.apache.spark.sql.sources.PrunedScanSuite", "org.apache.spark.sql.execution.command.TruncateTableParserSuite", "org.apache.spark.sql.execution.streaming.CompactibleFileStreamLogSuite", "org.apache.spark.sql.streaming.StreamingQueryManagerSuite", "org.apache.spark.sql.execution.datasources.noop.NoopStreamSuite", "org.apache.spark.sql.CsvFunctionsSuite", "org.apache.spark.sql.connector.DataSourceV2DataFrameSessionCatalogSuite", "org.apache.spark.sql.execution.SortSuite", "org.apache.spark.sql.SparkSessionBuilderSuite", "org.apache.spark.sql.streaming.continuous.ContinuousStressSuite", "org.apache.spark.sql.execution.columnar.RefCountedTestCachedBatchSerializerSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV1QuerySuite", "org.apache.spark.sql.sources.FilteredScanSuite", "org.apache.spark.sql.BasicCharVarcharTestSuite", "org.apache.spark.sql.SubquerySuite", "org.apache.spark.sql.execution.command.ShowPartitionsParserSuite", "org.apache.spark.sql.execution.datasources.InMemoryTableMetricSuite", "org.apache.spark.sql.execution.ui.AllExecutionsPageSuite", "org.apache.spark.sql.sources.TableScanSuite", "org.apache.spark.sql.streaming.StreamingDeduplicationSuite", "org.apache.spark.sql.execution.command.v1.MsckRepairTableSuite", "org.apache.spark.sql.MetadataCacheV2Suite", "org.apache.spark.sql.execution.AggregatingAccumulatorSuite", "org.apache.spark.sql.execution.streaming.state.StateStoreCompatibilitySuite", "org.apache.spark.sql.internal.SQLConfSuite", "org.apache.spark.sql.jdbc.JDBCNestedDataSourceSuite", "org.apache.spark.sql.execution.datasources.DataSourceSuite", "org.apache.spark.sql.execution.datasources.text.TextV2Suite", "org.apache.spark.sql.MathFunctionsSuite", "org.apache.spark.sql.streaming.DeprecatedStreamingAggregationSuite", "org.apache.spark.sql.execution.streaming.sources.TextSocketStreamSuite", "org.apache.spark.sql.CTEHintSuite", "org.apache.spark.sql.execution.datasources.ParquetReadSchemaSuite", "org.apache.spark.sql.execution.joins.InnerJoinSuite", "org.apache.spark.sql.execution.datasources.RowDataSourceStrategySuite", "org.apache.spark.sql.execution.GlobalTempViewSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRecoverPartitionsParallelSuite", "org.apache.spark.sql.execution.ui.SQLAppStatusListenerMemoryLeakSuite", "org.apache.spark.sql.execution.datasources.orc.OrcPartitionDiscoverySuite", "org.apache.spark.sql.execution.RemoveRedundantSortsSuite", "org.apache.spark.sql.execution.datasources.HadoopFileLinesReaderSuite", "org.apache.spark.sql.DynamicPartitionPruningV2Suite", "org.apache.spark.sql.test.GenericFunSpecSuite", "org.apache.spark.sql.connector.SupportsCatalogOptionsSuite", "org.apache.spark.sql.DatasetAggregatorSuite", "org.apache.spark.sql.execution.command.v2.AlterTableAddPartitionSuite", "org.apache.spark.sql.execution.command.v2.ShowPartitionsSuite", "org.apache.spark.sql.sources.DisableUnnecessaryBucketedScanWithoutHiveSupportSuite", "org.apache.spark.sql.execution.command.v1.ShowNamespacesSuite", "org.apache.spark.sql.execution.command.v1.ShowTablesSuite", "org.apache.spark.sql.test.GenericFlatSpecSuite", "org.apache.spark.sql.execution.RemoveRedundantProjectsSuite", "org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManagerSuite", "org.apache.spark.sql.streaming.ui.UISeleniumSuite", "org.apache.spark.sql.execution.WholeStageCodegenSuite", "org.apache.spark.sql.CountMinSketchAggQuerySuite", "org.apache.spark.sql.execution.SQLExecutionSuite", "org.apache.spark.sql.execution.DataSourceScanExecRedactionSuite", "org.apache.spark.sql.streaming.test.DataStreamTableAPISuite", "org.apache.spark.sql.DataFrameAggregateSuite", "org.apache.spark.sql.DataFramePivotSuite", "org.apache.spark.sql.streaming.FileStreamStressSuite", "org.apache.spark.sql.execution.columnar.CachedBatchSerializerSuite", "org.apache.spark.sql.execution.datasources.VectorizedParquetReadSchemaSuite", "org.apache.spark.sql.execution.streaming.state.RocksDBSuite", "org.apache.spark.sql.execution.datasources.text.WholeTextFileV1Suite", "org.apache.spark.sql.execution.command.v1.AlterTableAddPartitionSuite", "org.apache.spark.sql.streaming.ui.StreamingQueryStatusListenerSuite", "org.apache.spark.sql.UserDefinedTypeSuite", "org.apache.spark.sql.execution.command.DropTableParserSuite", "org.apache.spark.sql.DSV2CharVarcharTestSuite", "org.apache.spark.sql.SparkSessionExtensionSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite", "org.apache.spark.sql.DataFrameSelfJoinSuite", "org.apache.spark.sql.streaming.StreamingInnerJoinSuite", "org.apache.spark.sql.connector.V1ReadFallbackWithDataFrameReaderSuite", "org.apache.spark.sql.execution.CoalesceShufflePartitionsSuite", "org.apache.spark.sql.execution.datasources.CSVReadSchemaSuite", "org.apache.spark.sql.ApproximatePercentileQuerySuite", "org.apache.spark.sql.DataFrameSuite", "org.apache.spark.sql.DataFrameNaFunctionsSuite", "org.apache.spark.sql.execution.datasources.orc.OrcFilterSuite", "org.apache.spark.sql.execution.datasources.noop.NoopSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapSuite", "org.apache.spark.sql.sources.SaveLoadSuite", "org.apache.spark.sql.FileScanSuite", "org.apache.spark.sql.execution.streaming.MicroBatchExecutionSuite", "org.apache.spark.sql.execution.datasources.csv.CSVv1Suite", "org.apache.spark.sql.FileBasedDataSourceSuite", "org.apache.spark.sql.ExtraStrategiesSuite", "org.apache.spark.sql.execution.datasources.orc.OrcV1SchemaPruningSuite", "org.apache.spark.sql.connector.DataSourceV2SQLSessionCatalogSuite", "org.apache.spark.sql.execution.streaming.OffsetSeqLogSuite", "org.apache.spark.sql.internal.DeprecatedCreateExternalTableSuite", "org.apache.spark.sql.execution.QueryExecutionSuite", "org.apache.spark.sql.ApproxCountDistinctForIntervalsQuerySuite", "org.apache.spark.sql.DeprecatedDatasetAggregatorSuite", "org.apache.spark.sql.execution.LocalTempViewTestSuite", "org.apache.spark.sql.execution.python.BatchEvalPythonExecSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV1PartitionDiscoverySuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.execution.datasources.PathFilterStrategySuite", "org.apache.spark.sql.streaming.ui.StreamingQueryPageSuite", "org.apache.spark.sql.execution.command.v2.MsckRepairTableSuite", "org.apache.spark.sql.internal.SharedStateSuite", "org.apache.spark.sql.execution.metric.SQLMetricsSuite", "org.apache.spark.sql.execution.streaming.MemorySinkSuite", "org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommandSuite", "org.apache.spark.sql.execution.command.v2.AlterTableRenamePartitionSuite", "org.apache.spark.sql.DataFrameComplexTypeSuite", "org.apache.spark.sql.TPCHQuerySuite", "org.apache.spark.sql.ScalaReflectionRelationSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetEncodingSuite", "org.apache.spark.sql.execution.command.v2.ShowNamespacesSuite", "org.apache.spark.sql.expressions.ExpressionInfoSuite", "org.apache.spark.sql.execution.columnar.InMemoryColumnarQuerySuite", "org.apache.spark.sql.execution.command.v1.TruncateTableSuite", "org.apache.spark.sql.execution.debug.DebuggingSuite", "org.apache.spark.sql.execution.RemoveRedundantSortsSuite", "org.apache.spark.sql.DataFrameWriterV2Suite", "org.apache.spark.sql.execution.streaming.FileSystemBasedCheckpointFileManagerSuite", "org.apache.spark.sql.connector.V1WriteFallbackSuite", "org.apache.spark.sql.execution.SQLJsonProtocolSuite", "org.apache.spark.sql.FileSourceSQLInsertTestSuite", "org.apache.spark.sql.execution.datasources.orc.OrcV1QuerySuite", "org.apache.spark.sql.execution.aggregate.SortBasedAggregationStoreSuite", "org.apache.spark.sql.execution.streaming.sources.RateStreamProviderSuite", "org.apache.spark.sql.util.DataFrameCallbackSuite", "org.apache.spark.sql.execution.datasources.v2.FileTableSuite", "org.apache.spark.sql.execution.datasources.JsonReadSchemaSuite", "org.apache.spark.sql.streaming.FlatMapGroupsWithStateSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite", "org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArraySuite", "org.apache.spark.sql.ConfigBehaviorSuite", "org.apache.spark.sql.DatasetSuite", "org.apache.spark.sql.DataFrameTungstenSuite", "org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreSuite", "org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMapSuite", "org.apache.spark.sql.connector.FileDataSourceV2FallBackSuite", "org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite", "org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerSuite", "org.apache.spark.sql.test.GenericWordSpecSuite", "org.apache.spark.sql.ProductAggSuite", "org.apache.spark.sql.execution.bucketing.CoalesceBucketsInJoinSuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilitySuite", "org.apache.spark.sql.QueryTestSuite", "org.apache.spark.sql.FileSourceCharVarcharTestSuite", "org.apache.spark.sql.MiscFunctionsSuite", "org.apache.spark.sql.execution.datasources.FileFormatWriterSuite", "org.apache.spark.sql.execution.datasources.orc.OrcSourceSuite", "org.apache.spark.sql.streaming.StreamingLeftSemiJoinSuite", "org.apache.spark.sql.streaming.EventTimeWatermarkSuite", "org.apache.spark.sql.execution.ui.SparkPlanInfoSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV2PartitionDiscoverySuite", "org.apache.spark.sql.execution.streaming.StreamMetadataSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetThriftCompatibilitySuite", "org.apache.spark.sql.execution.streaming.sources.ConsoleWriteSupportSuite", "org.apache.spark.sql.execution.SparkScriptTransformationSuite", "org.apache.spark.sql.connector.TableCapabilityCheckSuite", "org.apache.spark.sql.execution.datasources.orc.OrcV2SchemaPruningSuite", "org.apache.spark.sql.execution.command.AlterTableRenamePartitionParserSuite", "org.apache.spark.sql.sources.PartitionedWriteSuite", "org.apache.spark.sql.execution.datasources.orc.OrcEncryptionSuite", "org.apache.spark.sql.internal.ExecutorSideSQLConfSuite", "org.apache.spark.sql.internal.SQLConfGetterSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetV2QuerySuite", "org.apache.spark.sql.execution.datasources.json.JsonParsingOptionsSuite", "org.apache.spark.sql.JsonFunctionsSuite", "org.apache.spark.sql.execution.command.v2.AlterTableRenameSuite", "org.apache.spark.sql.streaming.test.DataStreamReaderWriterSuite", "org.apache.spark.sql.streaming.StreamingSymmetricHashJoinHelperSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormatV1Suite", "org.apache.spark.sql.execution.DataSourceV2ScanExecRedactionSuite", "org.apache.spark.sql.execution.datasources.orc.OrcV1PartitionDiscoverySuite", "org.apache.spark.sql.test.TestSparkSessionSuite", "org.apache.spark.sql.connector.DataSourceV2DataFrameSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetInteroperabilitySuite", "org.apache.spark.sql.execution.QueryPlanningTrackerEndToEndSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetProtobufCompatibilitySuite", "org.apache.spark.sql.execution.exchange.EnsureRequirementsSuite", "org.apache.spark.sql.execution.SimpleSQLViewSuite", "org.apache.spark.sql.execution.command.v1.DropTableSuite", "org.apache.spark.sql.execution.joins.ExistenceJoinSuite", "org.apache.spark.sql.execution.SameResultSuite", "org.apache.spark.sql.streaming.StreamingFullOuterJoinSuite", "org.apache.spark.sql.DataFrameStatSuite", "org.apache.spark.sql.XPathFunctionsSuite", "org.apache.spark.sql.streaming.FileStreamSourceStressTestSuite", "org.apache.spark.sql.DataFrameWindowFramesSuite", "org.apache.spark.status.api.v1.sql.SqlResourceWithActualMetricsSuite", "org.apache.spark.sql.execution.streaming.state.StateStoreRDDSuite", "org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite", "org.apache.spark.sql.sources.DisableUnnecessaryBucketedScanWithoutHiveSupportSuite", "org.apache.spark.sql.execution.datasources.PathFilterSuite", "org.apache.spark.sql.execution.command.v1.AlterTableRenamePartitionSuite", "org.apache.spark.sql.execution.streaming.state.StreamingSessionWindowStateManagerSuite", "org.apache.spark.sql.execution.datasources.FileSourceStrategySuite", "org.apache.spark.sql.SessionStateSuite", "org.apache.spark.sql.connector.LocalScanSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetCompressionCodecPrecedenceSuite", "org.apache.spark.sql.execution.UnsafeRowSerializerSuite", "org.apache.spark.sql.execution.command.v2.ShowTablesSuite", "org.apache.spark.sql.execution.BroadcastExchangeSuite", "org.apache.spark.sql.execution.OptimizeMetadataOnlyQuerySuite", "org.apache.spark.sql.execution.datasources.MergedOrcReadSchemaSuite", "org.apache.spark.sql.streaming.StreamingQuerySuite", "org.apache.spark.sql.execution.datasources.jdbc.connection.DisallowedConnectionProviderSuite", "org.apache.spark.sql.streaming.FileStreamSinkV2Suite", "org.apache.spark.sql.execution.command.AlterTableDropPartitionParserSuite", "org.apache.spark.sql.execution.datasources.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetCommitterSuite", "org.apache.spark.sql.execution.command.DDLParserSuite", "org.apache.spark.sql.execution.datasources.v2.V2SessionCatalogNamespaceSuite", "org.apache.spark.sql.sources.ExternalCommandRunnerSuite", "org.apache.spark.sql.execution.command.v2.DropTableSuite", "org.apache.spark.sql.RowSuite", "org.apache.spark.sql.execution.command.v2.TruncateTableSuite", "org.apache.spark.sql.execution.RemoveRedundantProjectsSuite", "org.apache.spark.sql.connector.V2CommandsCaseSensitivitySuite", "org.apache.spark.sql.sources.ResolvedDataSourceSuite", "org.apache.spark.sql.execution.streaming.sources.ForeachWriterSuite", "org.apache.spark.sql.execution.python.PythonUDFSuite", "org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreIntegrationSuite", "org.apache.spark.sql.execution.ReuseExchangeAndSubquerySuite", "org.apache.spark.sql.DataFrameFunctionsSuite", "org.apache.spark.sql.execution.datasources.orc.OrcV2QuerySuite", "org.apache.spark.sql.TPCDSModifiedPlanStabilityWithStatsSuite", "org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormatV2Suite", "org.apache.spark.sql.MetadataCacheV1Suite", "org.apache.spark.sql.streaming.continuous.ContinuousMetaSuite", "org.apache.spark.sql.ColumnExpressionSuite", "org.apache.spark.sql.DataFrameSessionWindowingSuite", "org.apache.spark.sql.util.ExecutionListenerManagerSuite", "org.apache.spark.sql.DataFrameImplicitsSuite", "org.apache.spark.sql.execution.streaming.HDFSMetadataLogSuite", "org.apache.spark.sql.execution.ExchangeSuite", "org.apache.spark.sql.DatasetOptimizationSuite", "org.apache.spark.sql.streaming.continuous.ContinuousSuite", "org.apache.spark.sql.execution.joins.BroadcastJoinSuite", "org.apache.spark.sql.execution.datasources.json.JsonV1Suite", "org.apache.spark.sql.execution.command.v1.AlterTableDropPartitionSuite", "org.apache.spark.sql.execution.streaming.state.FlatMapGroupsWithStateExecHelperSuite", "org.apache.spark.sql.execution.ShufflePartitionsUtilSuite", "org.apache.spark.sql.connector.DataSourceV2Suite", "org.apache.spark.sql.ExplainSuite", "org.apache.spark.sql.execution.TakeOrderedAndProjectSuite", "org.apache.spark.sql.api.r.SQLUtilsSuite", "org.apache.spark.sql.sources.DDLSourceLoadSuite", "org.apache.spark.sql.streaming.StreamingStateStoreFormatCompatibilitySuite", "org.apache.spark.sql.streaming.ui.StreamingQueryHistorySuite", "org.apache.spark.sql.execution.datasources.json.JsonV2Suite", "org.apache.spark.sql.SimpleShowCreateTableSuite", "org.apache.spark.sql.execution.datasources.json.JsonLegacyTimeParserSuite", "org.apache.spark.sql.execution.debug.DebuggingSuite", "org.apache.spark.sql.execution.datasources.csv.CSVv2Suite", "org.apache.spark.sql.streaming.StreamSuite", "org.apache.spark.sql.DataFrameHintSuite", "org.apache.spark.sql.DataFrameRangeSuite", "org.apache.spark.sql.DataFrameTimeWindowingSuite", "org.apache.spark.sql.execution.command.FileSourceCharVarcharDDLTestSuite", "org.apache.spark.sql.execution.streaming.CheckpointFileManagerSuite", "org.apache.spark.sql.IntervalFunctionsSuite", "org.apache.spark.sql.execution.DeprecatedWholeStageCodegenSuite", "org.apache.spark.sql.sources.InsertSuite", "org.apache.spark.sql.DataFrameSetOperationsSuite", "org.apache.spark.sql.execution.SparkPlannerSuite", "org.apache.spark.sql.DatasetSerializerRegistratorSuite", "org.apache.spark.sql.execution.command.ShowNamespacesParserSuite", "org.apache.spark.sql.streaming.StreamingQueryStatusAndProgressSuite", "org.apache.spark.sql.execution.columnar.PartitionBatchPruningSuite", "org.apache.spark.sql.execution.streaming.FileStreamSinkLogSuite", "org.apache.spark.sql.execution.datasources.VectorizedOrcReadSchemaSuite", "org.apache.spark.sql.ComplexTypesSuite", "org.apache.spark.sql.execution.joins.HashedRelationSuite", "org.apache.spark.sql.jdbc.JDBCWriteSuite", "org.apache.spark.sql.ExplainSuite", "org.apache.spark.sql.execution.joins.OuterJoinSuite", "org.apache.spark.sql.execution.PlannerSuite", "org.apache.spark.sql.execution.datasources.orc.OrcColumnarBatchReaderSuite", "org.apache.spark.sql.jdbc.v2.OracleIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.jdbc.v2.MsSqlServerIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.jdbc.DB2IntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.jdbc.v2.DB2IntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.jdbc.MariaDBKrbIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerKrbJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerKrbJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerKrbJDBCIntegrationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.jdbc.v2.PostgresNamespaceSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.jdbc.MsSqlServerIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.jdbc.OracleIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.jdbc.v2.MySQLIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.jdbc.PostgresKrbIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerKrbJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerKrbJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerKrbJDBCIntegrationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.jdbc.PostgresIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.jdbc.DB2KrbIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerKrbJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerKrbJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerKrbJDBCIntegrationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.jdbc.v2.PostgresIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.jdbc.MySQLIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerIntegrationFunSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite", "org.apache.spark.SparkFunSuite", "org.apache.spark.sql.jdbc.v2.OracleIntegrationSuite", "org.apache.spark.sql.jdbc.v2.MsSqlServerIntegrationSuite", "org.apache.spark.sql.jdbc.DB2IntegrationSuite", "org.apache.spark.sql.jdbc.v2.DB2IntegrationSuite", "org.apache.spark.sql.jdbc.MariaDBKrbIntegrationSuite", "org.apache.spark.sql.jdbc.v2.PostgresNamespaceSuite", "org.apache.spark.sql.jdbc.MsSqlServerIntegrationSuite", "org.apache.spark.sql.jdbc.OracleIntegrationSuite", "org.apache.spark.sql.jdbc.v2.MySQLIntegrationSuite", "org.apache.spark.sql.jdbc.PostgresKrbIntegrationSuite", "org.apache.spark.sql.jdbc.PostgresIntegrationSuite", "org.apache.spark.sql.jdbc.DB2KrbIntegrationSuite", "org.apache.spark.sql.jdbc.v2.PostgresIntegrationSuite", "org.apache.spark.sql.jdbc.MySQLIntegrationSuite"], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [3, 6, 23]}, {"file": {"name": "dev/run-tests.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 1, 6]}, {"file": {"name": "pom.xml", "additions": "10", "deletions": "1", "changes": "11"}, "updated": [4, 10, 35]}, {"file": {"name": "resource-managers/yarn/pom.xml", "additions": "32", "deletions": "0", "changes": "32"}, "updated": [0, 1, 1]}]}
{"author": "SaurabhChawla100", "sha": "a09e37faec1be04a20785846f9a2b53c8fdfd663", "commit_date": "2021/08/09 05:49:33", "commit_message": "update the comment", "title": "[SPARK-36452][SQL]: Add the support in Spark for having group by map datatype column for the scenario that works in Hive", "body": "### What changes were proposed in this pull request?\r\nAdd the support in Spark for having group by map datatype column for the scenario that works in Hive.\r\nIn hive this scenario works fine \r\n\r\n```\r\ndescribe extended complex2;\r\nOK\r\nid                  string \r\nc1                  map<int, string>   \r\nDetailed Table Information Table(tableName:complex2, dbName:default, owner:abc, createTime:1627994412, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:string, comment:null), FieldSchema(name:c1, type:map<int,string>, comment:null)], location:/user/hive/warehouse/complex2, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1\r\n\r\nselect * from complex2;\r\nOK\r\n1 {1:\"u\"}\r\n2 {1:\"u\",2:\"uo\"}\r\n1 {1:\"u\",2:\"uo\"}\r\nTime taken: 0.363 seconds, Fetched: 3 row(s)\r\n\r\nWorking Scenario in Hive -: \r\n\r\nselect id, c1, count(*) from complex2 group by id, c1;\r\nOK\r\n1 {1:\"u\"} 1\r\n1 {1:\"u\",2:\"uo\"} 1\r\n2 {1:\"u\",2:\"uo\"} 1\r\nTime taken: 1.621 seconds, Fetched: 3 row(s)\r\n\r\nFailed Scenario in Hive -: \r\nWhen map type is present in aggregated expression \r\nselect id, max(c1), count(*) from complex2 group by id, c1; \r\nFAILED: UDFArgumentTypeException Cannot support comparison of map<> type or complex type containing map<>.\r\n```\r\nBut in spark where the group by map column failed for this scenario where the map column is used in the select without any aggregation, The one that works in hive.\r\n\r\n```\r\nscala> spark.sql(\"select id,c1, count(*) from complex2 group by id, c1\").show\r\norg.apache.spark.sql.AnalysisException: expression spark_catalog.default.complex2.`c1` cannot be used as a grouping expression because its data type map<int,string> is not an orderable data type.;\r\nAggregate [id#1, c1#2], [id#1, c1#2, count(1) AS count(1)#3L]\r\n+- SubqueryAlias spark_catalog.default.complex2\r\n +- HiveTableRelation [`default`.`complex2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#1, c1#2], Partition Cols: []]\r\nat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.failAnalysis(CheckAnalysis.scala:50)\r\n```\r\n\r\n### Why are the changes needed?\r\nThere is need to add the this scenario where grouping expression can have map type if aggregated expression does not have the that map type reference. This helps in migrating the user from hive to Spark.\r\n\r\nAfter the code change \r\n\r\n```\r\nscala> spark.sql(\"select id,c1, count(*) from complex2 group by id, c1\").show\r\n+---+-----------------+--------+                                                \r\n| id|               c1|count(1)|\r\n+---+-----------------+--------+\r\n|  1|         {1 -> u}|       1|\r\n|  2|{1 -> u, 2 -> uo}|       1|\r\n|  1|{1 -> u, 2 -> uo}|       1|\r\n+---+-----------------+--------+\r\n```\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n\r\n### How was this patch tested?\r\nAdded the unit test and also tested using spark-shell the scenario\r\n", "failed_tests": ["org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 4, 14]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ordering.scala", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeFloatingNumbers.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "15", "deletions": "5", "changes": "20"}, "updated": [1, 1, 6]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala", "additions": "17", "deletions": "8", "changes": "25"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameAggregateSuite.scala", "additions": "27", "deletions": "0", "changes": "27"}, "updated": [0, 0, 5]}]}
{"author": "robert3005", "sha": "e3754487a8fc49622059a47e403945f2850731f2", "commit_date": "2017/06/22 15:53:30", "commit_message": "Merge existing registry with default one or configure default metric registry", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala", "additions": "66", "deletions": "19", "changes": "85"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/metrics/MetricsSystemSuite.scala", "additions": "30", "deletions": "9", "changes": "39"}, "updated": [0, 0, 0]}, {"file": {"name": "streaming/src/test/scala/org/apache/spark/streaming/StreamingContextSuite.scala", "additions": "6", "deletions": "3", "changes": "9"}, "updated": [0, 0, 0]}]}
{"author": "xwu99", "sha": "67034f284803bd10a487b5c67eb4c552ace950c3", "commit_date": "2021/09/01 13:11:10", "commit_message": "Add some utilities for profiles", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala", "additions": "72", "deletions": "12", "changes": "84"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 1, 6]}, {"file": {"name": "core/src/main/scala/org/apache/spark/resource/ResourceProfile.scala", "additions": "13", "deletions": "0", "changes": "13"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/resource/ResourceProfileManager.scala", "additions": "55", "deletions": "0", "changes": "55"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala", "additions": "14", "deletions": "3", "changes": "17"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/test/resources/log4j.properties", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/ExecutorAllocationManagerSuite.scala", "additions": "115", "deletions": "0", "changes": "115"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/scheduler/TaskSchedulerImplSuite.scala", "additions": "83", "deletions": "0", "changes": "83"}, "updated": [0, 0, 0]}]}
{"author": "xkrogen", "sha": "9b58975f88eaad623febea4524b3e7a63dd99272", "commit_date": "2021/09/15 21:57:44", "commit_message": "SPARK-34378 [AVRO] Enhance AvroSerializer validation to allow extra nullable Avro fields", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.avro.AvroSerdeSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/TestUtils.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 1]}, {"file": {"name": "external/avro/src/main/scala/org/apache/spark/sql/avro/AvroSerializer.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 3]}, {"file": {"name": "external/avro/src/main/scala/org/apache/spark/sql/avro/AvroUtils.scala", "additions": "10", "deletions": "3", "changes": "13"}, "updated": [0, 0, 3]}, {"file": {"name": "external/avro/src/test/scala/org/apache/spark/sql/avro/AvroSchemaHelperSuite.scala", "additions": "24", "deletions": "1", "changes": "25"}, "updated": [0, 0, 1]}, {"file": {"name": "external/avro/src/test/scala/org/apache/spark/sql/avro/AvroSerdeSuite.scala", "additions": "26", "deletions": "15", "changes": "41"}, "updated": [0, 0, 1]}, {"file": {"name": "external/avro/src/test/scala/org/apache/spark/sql/avro/AvroSuite.scala", "additions": "26", "deletions": "0", "changes": "26"}, "updated": [0, 0, 3]}]}
{"author": "andygrove", "sha": "095a4d57b7a348d6e1dba38a6db37d8667d0482d", "commit_date": "2021/08/03 20:32:30", "commit_message": "backport aqe proposal", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite", "org.apache.spark.sql.execution.CoalesceShufflePartitionsSuite", "org.apache.spark.sql.ExplainSuite"], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/SparkSessionExtensions.scala", "additions": "21", "deletions": "1", "changes": "22"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/Columnar.scala", "additions": "11", "deletions": "6", "changes": "17"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "35", "deletions": "36", "changes": "71"}, "updated": [1, 3, 9]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala", "additions": "6", "deletions": "1", "changes": "7"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 1, 2]}]}
{"author": "darcy-shen", "sha": "54553fd062798c008254ddebe3987ef983b4e2ad", "commit_date": "2021/04/25 08:13:23", "commit_message": "verify inferred schema for _create_dataframe", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "python/pyspark/sql/session.py", "additions": "36", "deletions": "8", "changes": "44"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/tests/test_dataframe.py", "additions": "41", "deletions": "0", "changes": "41"}, "updated": [0, 0, 0]}, {"file": {"name": "python/pyspark/sql/tests/test_types.py", "additions": "10", "deletions": "1", "changes": "11"}, "updated": [0, 0, 0]}]}
{"author": "grarkydev", "sha": "7ebf7088f6ff608da334468137ad8336f5b0f28f", "commit_date": "2021/04/06 11:44:03", "commit_message": "Support spark application managing with spark app handle on kubernetes\n\nCo-authored-by: hongdd <hongdongdong@cmss.chinamobile.com>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/submit/KubernetesClientApplication.scala", "additions": "40", "deletions": "3", "changes": "43"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/submit/PodStatusWatcher.scala", "additions": "34", "deletions": "5", "changes": "39"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/submit/ClientSuite.scala", "additions": "21", "deletions": "8", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/submit/PodStatusWatcherSuite.scala", "additions": "85", "deletions": "0", "changes": "85"}, "updated": [0, 0, 0]}]}
{"author": "AngersZhuuuu", "sha": "f5ec814e4d399e2dc0cea637ff5b2e99382a3c04", "commit_date": "2021/07/23 07:14:45", "commit_message": "[SPARK-34851][CORE][SQL] Add tag for each configuration", "title": "[WIP][SPARK-34851][CORE][SQL] Add tag for each configuration", "body": "### What changes were proposed in this pull request?\r\nAdd tag for each configuration\r\n\r\n\r\n### Why are the changes needed?\r\nAdd tag for each configuration\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n### How was this patch tested?\r\n\r\n", "failed_tests": ["org.apache.spark.sql.jdbc.OracleIntegrationSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/ConfigBuilder.scala", "additions": "12", "deletions": "0", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/Deploy.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/Kryo.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/Python.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/R.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/Streaming.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/Tests.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/UI.scala", "additions": "27", "deletions": "0", "changes": "27"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/Worker.scala", "additions": "12", "deletions": "0", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "305", "deletions": "6", "changes": "311"}, "updated": [0, 2, 5]}, {"file": {"name": "launcher/src/main/java/org/apache/spark/launcher/SparkLauncher.java", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/config.scala", "additions": "54", "deletions": "1", "changes": "55"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [2, 6, 18]}]}
{"author": "AngersZhuuuu", "sha": "a0fc9b99b77f31457a372a8b6496ce430417641b", "commit_date": "2021/04/27 10:06:54", "commit_message": "[SPARK-35228][SQL] Add expression ToHiveString for keep consistent between hive/spark format in df.show and transform", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala", "additions": "406", "deletions": "358", "changes": "764"}, "updated": [0, 1, 11]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala", "additions": "54", "deletions": "0", "changes": "54"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "12", "deletions": "0", "changes": "12"}, "updated": [1, 3, 13]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala", "additions": "41", "deletions": "0", "changes": "41"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala", "additions": "7", "deletions": "1", "changes": "8"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/BaseScriptTransformationExec.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 2, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/HiveResult.scala", "additions": "20", "deletions": "9", "changes": "29"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/transform.sql.out", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [1, 4, 6]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SQLQueryTestHelper.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/BaseScriptTransformationSuite.scala", "additions": "10", "deletions": "21", "changes": "31"}, "updated": [1, 3, 4]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/HiveResultSuite.scala", "additions": "15", "deletions": "26", "changes": "41"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/benchmark/DateTimeBenchmark.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/expressions/ExpressionInfoSuite.scala", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkSQLDriver.scala", "additions": "5", "deletions": "5", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/HiveComparisonTest.scala", "additions": "5", "deletions": "3", "changes": "8"}, "updated": [0, 0, 0]}]}
{"author": "gengliangwang", "sha": "bc94b027e839859d587271c3bcf78e0f632a5234", "commit_date": "2021/07/16 08:26:38", "commit_message": "support timestamp_ntz in parquet", "title": "[WIP][SPARK-36182][SQL] Support TimestampNTZ type in Parquet file source", "body": "This is still WIP. I am deciding the behaviors of the Parquet reader for both schema inference and user-provided schema.", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala", "additions": "10", "deletions": "4", "changes": "14"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetIOSuite.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaSuite.scala", "additions": "31", "deletions": "0", "changes": "31"}, "updated": [0, 0, 0]}]}
{"author": "dgd-contributor", "sha": "838f41303b58701ced288152bb95f8938ebacc66", "commit_date": "2021/07/28 15:38:09", "commit_message": "[SPARK-36096][CORE] Grouping exception in core/resource", "title": "", "body": "", "failed_tests": ["org.apache.spark.scheduler.ExecutorResourceInfoSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/errors/SparkCoreErrors.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [1, 1, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/resource/ResourceProfile.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/resource/ResourceUtils.scala", "additions": "5", "deletions": "4", "changes": "9"}, "updated": [0, 0, 0]}]}
{"author": "dgd-contributor", "sha": "a89af71e932f932e05a22caca449cd9b4872d865", "commit_date": "2021/07/29 01:46:33", "commit_message": "[SPARK-36100][CORE]: group exception messages in core/status", "title": "", "body": "", "failed_tests": ["org.apache.spark.deploy.history.HistoryServerSuite"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/errors/SparkCoreErrors.scala", "additions": "103", "deletions": "0", "changes": "103"}, "updated": [1, 1, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusStore.scala", "additions": "4", "deletions": "5", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/KVUtils.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/LiveEntity.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/api/v1/ApiRootResource.scala", "additions": "9", "deletions": "8", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/api/v1/OneApplicationResource.scala", "additions": "12", "deletions": "12", "changes": "24"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/api/v1/SimpleDateParam.scala", "additions": "3", "deletions": "9", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/api/v1/StagesResource.scala", "additions": "7", "deletions": "7", "changes": "14"}, "updated": [0, 0, 0]}]}
{"author": "shardulm94", "sha": "f39e7c997562c67b4fa9a374e993744512ad3b84", "commit_date": "2021/07/22 21:07:10", "commit_message": "Fix compile error after revert", "title": "[SPARK-36215][SHUFFLE] Add logging for slow fetches to diagnose external shuffle service issues", "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\nAdd logging to `ShuffleBlockFetcherIterator` to log \"slow\" fetches, where slow is defined by two confs: `spark.reducer.shuffleFetchSlowLogThreshold.time` and `spark.reducer.shuffleFetchSlowLogThreshold.bytesPerSec`\r\n<!--\r\nPlease clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. \r\nIf possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.\r\n  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.\r\n  2. If you fix some SQL features, you can provide some references of other DBMSes.\r\n  3. If there is design documentation, please add the link.\r\n  4. If there is a discussion in the mailing list, please add the link.\r\n-->\r\n\r\n\r\n### Why are the changes needed?\r\nCurrently we can see from the metrics that a task or stage has slow fetches, and the logs indicate *all* of the shuffle servers those tasks were fetching from, but often this is a big set (dozens or even hundreds) and narrowing down which one caused issues can be very difficult. This change makes it easier to understand which fetch is \"slow\".\r\n<!--\r\nPlease clarify why the changes are needed. For instance,\r\n  1. If you propose a new API, clarify the use case for a new API.\r\n  2. If you fix a bug, you can clarify why it is a bug.\r\n-->\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n<!--\r\nNote that it means *any* user-facing change including all aspects such as the documentation fix.\r\nIf yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.\r\nIf possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.\r\nIf no, write 'No'.\r\n-->\r\nAdds two configs `spark.reducer.shuffleFetchSlowLogThreshold.time` and `spark.reducer.shuffleFetchSlowLogThreshold.bytesPerSec`\r\n\r\n\r\n### How was this patch tested?\r\n<!--\r\nIf tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.\r\nIf it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.\r\nIf tests were not added, please describe why they were not added and/or why it was difficult to add.\r\n-->\r\nAdded unit test", "failed_tests": ["pyspark.mllib.tests.test_streaming_algorithms"], "files": [{"file": {"name": "core/src/main/scala/org/apache/spark/TestUtils.scala", "additions": "32", "deletions": "1", "changes": "33"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/internal/config/package.scala", "additions": "23", "deletions": "0", "changes": "23"}, "updated": [1, 2, 5]}, {"file": {"name": "core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala", "additions": "43", "deletions": "5", "changes": "48"}, "updated": [0, 1, 4]}, {"file": {"name": "core/src/test/scala/org/apache/spark/storage/ShuffleBlockFetcherIteratorSuite.scala", "additions": "47", "deletions": "3", "changes": "50"}, "updated": [0, 0, 2]}, {"file": {"name": "docs/configuration.md", "additions": "22", "deletions": "0", "changes": "22"}, "updated": [0, 0, 1]}]}
{"author": "sunchao", "sha": "4bf4533b8c7e9c7b2f264ede6a14cad52c33b194", "commit_date": "2021/06/30 20:22:02", "commit_message": "initial commit", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "dev/deps/spark-deps-hadoop-3.2-hive-2.3", "additions": "4", "deletions": "4", "changes": "8"}, "updated": [3, 6, 23]}, {"file": {"name": "dev/run-tests.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [1, 1, 6]}, {"file": {"name": "pom.xml", "additions": "10", "deletions": "1", "changes": "11"}, "updated": [4, 10, 35]}, {"file": {"name": "resource-managers/yarn/pom.xml", "additions": "32", "deletions": "0", "changes": "32"}, "updated": [0, 1, 1]}]}
{"author": "viirya", "sha": "ba4172076f3f8030510632978a0e47d5b720617a", "commit_date": "2020/12/02 06:34:32", "commit_message": "Subexpression elimination for whole-stage codegen in Filter.", "title": "", "body": "", "failed_tests": ["pyspark.pandas.tests.test_indexing", "org.apache.spark.sql.hive.orc.HiveOrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.hive.execution.HiveTableScanSuite", "org.apache.spark.sql.sources.JsonHadoopFsRelationSuite", "org.apache.spark.sql.hive.orc.HiveOrcSourceSuite", "org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite", "org.apache.spark.sql.hive.MetastoreDataSourcesSuite", "org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite", "org.apache.spark.sql.hive.HiveParquetSourceSuite", "org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite", "org.apache.spark.sql.hive.execution.HiveUDFSuite", "org.apache.spark.sql.hive.HiveSchemaInferenceSuite", "org.apache.spark.sql.hive.HiveMetadataCacheSuite", "org.apache.spark.sql.hive.HiveShowCreateTableSuite", "org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite", "org.apache.spark.sql.hive.HiveParquetMetastoreSuite", "org.apache.spark.sql.SQLQueryTestSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "7", "deletions": "0", "changes": "7"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "72", "deletions": "11", "changes": "83"}, "updated": [0, 1, 2]}]}
{"author": "Yikun", "sha": "c6d4f21ca368bbc7ba4236dcd9d09904e7b82e5b", "commit_date": "2021/06/30 15:03:00", "commit_message": "Path level discover in python/run-test.py", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": ".github/workflows/build_and_test.yml", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 1, 26]}, {"file": {"name": "dev/run-tests.py", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [1, 1, 6]}, {"file": {"name": "dev/sparktestsupport/modules.py", "additions": "39", "deletions": "129", "changes": "168"}, "updated": [4, 6, 16]}, {"file": {"name": "python/pyspark/pandas/tests/indexes/test_base.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 7]}, {"file": {"name": "python/pyspark/pandas/tests/indexes/test_datetime.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 10]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 5]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 3]}, {"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 4]}, {"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames_groupby.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 4]}, {"file": {"name": "python/pyspark/pandas/tests/test_series.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [3, 3, 7]}, {"file": {"name": "python/pyspark/pandas/tests/test_stats.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 5]}, {"file": {"name": "python/pyspark/testing/utils.py", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 1]}, {"file": {"name": "python/run-tests.py", "additions": "119", "deletions": "2", "changes": "121"}, "updated": [0, 0, 1]}]}
{"author": "Yikun", "sha": "e46233af0bbd8f125a0b31f045628e239d0c8382", "commit_date": "2021/06/30 15:03:00", "commit_message": "Path level discover in python/run-test.py", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": ".github/workflows/build_and_test.yml", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 1, 26]}, {"file": {"name": "dev/run-tests.py", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [1, 1, 6]}, {"file": {"name": "dev/sparktestsupport/modules.py", "additions": "39", "deletions": "129", "changes": "168"}, "updated": [4, 6, 16]}, {"file": {"name": "python/pyspark/pandas/tests/indexes/test_base.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 7]}, {"file": {"name": "python/pyspark/pandas/tests/indexes/test_datetime.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 10]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 5]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 3]}, {"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 4]}, {"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames_groupby.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 4]}, {"file": {"name": "python/pyspark/pandas/tests/test_series.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [3, 3, 7]}, {"file": {"name": "python/pyspark/pandas/tests/test_stats.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 5]}, {"file": {"name": "python/pyspark/testing/utils.py", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 1]}, {"file": {"name": "python/run-tests.py", "additions": "119", "deletions": "2", "changes": "121"}, "updated": [0, 0, 1]}]}
{"author": "Yikun", "sha": "0915bf6bb053a7fe53c5eb7ba52a81fc26957c8b", "commit_date": "2021/06/30 15:03:00", "commit_message": "Path level discover in python/run-test.py", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": ".github/workflows/build_and_test.yml", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 1, 26]}, {"file": {"name": "dev/run-tests.py", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [1, 1, 6]}, {"file": {"name": "dev/sparktestsupport/modules.py", "additions": "40", "deletions": "130", "changes": "170"}, "updated": [4, 6, 16]}, {"file": {"name": "python/pyspark/pandas/tests/indexes/test_base.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 7]}, {"file": {"name": "python/pyspark/pandas/tests/indexes/test_datetime.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 10]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 5]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 3]}, {"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 4]}, {"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames_groupby.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 4]}, {"file": {"name": "python/pyspark/pandas/tests/test_series.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [3, 3, 7]}, {"file": {"name": "python/pyspark/pandas/tests/test_stats.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 5]}, {"file": {"name": "python/pyspark/testing/utils.py", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 1]}, {"file": {"name": "python/run-tests.py", "additions": "118", "deletions": "2", "changes": "120"}, "updated": [0, 0, 1]}]}
{"author": "Yikun", "sha": "05ff042611c27270e9b059e4fd0483c7194672c1", "commit_date": "2021/06/29 08:56:13", "commit_message": "[SPARK-35721][PYTHON] Path level discover for python unittests\n\n### What changes were proposed in this pull request?\nAdd path level discover for python unittests.\n\n### Why are the changes needed?\nNow we need to specify the python test cases by manually when we add a new testcase. Sometime, we forgot to add the testcase to module list, the testcase would not be executed.\n\nSuch as:\n- pyspark-core pyspark.tests.test_pin_thread\n\nThus we need some auto-discover way to find all testcase rather than specified every case by manually.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nAdd below code in end of `dev/sparktestsupport/modules.py`\n```python\nfor m in sorted(all_modules):\n    for g in sorted(m.python_test_goals):\n        print(m.name, g)\n```\nCompare the result before and after:\nhttps://www.diffchecker.com/iO3FvhKL\n\nCloses #32867 from Yikun/SPARK_DISCOVER_TEST.\n\nAuthored-by: Yikun Jiang <yikunkero@gmail.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": ".github/workflows/build_and_test.yml", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 25]}, {"file": {"name": "dev/run-tests.py", "additions": "9", "deletions": "1", "changes": "10"}, "updated": [1, 1, 6]}, {"file": {"name": "dev/sparktestsupport/modules.py", "additions": "86", "deletions": "140", "changes": "226"}, "updated": [3, 5, 15]}, {"file": {"name": "python/pyspark/pandas/tests/indexes/test_base.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 1, 6]}, {"file": {"name": "python/pyspark/pandas/tests/indexes/test_datetime.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 1, 5]}, {"file": {"name": "python/pyspark/pandas/tests/test_dataframe.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 1, 9]}, {"file": {"name": "python/pyspark/pandas/tests/test_groupby.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 1, 4]}, {"file": {"name": "python/pyspark/pandas/tests/test_indexing.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 1, 2]}, {"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 1, 3]}, {"file": {"name": "python/pyspark/pandas/tests/test_ops_on_diff_frames_groupby.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 1, 3]}, {"file": {"name": "python/pyspark/pandas/tests/test_series.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 2, 6]}, {"file": {"name": "python/pyspark/pandas/tests/test_stats.py", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 1, 4]}, {"file": {"name": "python/pyspark/testing/utils.py", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 1]}]}
{"author": "Yikun", "sha": "8e0065acce17a938482bb102afb6d99ef8d65a8a", "commit_date": "2021/06/29 11:15:11", "commit_message": "Add missing test modules check", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "dev/sparktestsupport/modules.py", "additions": "17", "deletions": "1", "changes": "18"}, "updated": [3, 5, 15]}]}
{"author": "Yikun", "sha": "33a9eaf0fd7f01f56a1f795a283a782053d4ac63", "commit_date": "2021/06/29 05:27:00", "commit_message": "discover-assert", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "dev/sparktestsupport/modules.py", "additions": "16", "deletions": "1", "changes": "17"}, "updated": [1, 3, 13]}]}
{"author": "ulysses-you", "sha": "5beb51810dfec69964e570d90d0634e5a8e0499d", "commit_date": "2021/07/28 03:14:13", "commit_message": "Do not fail application in kubernetes if name is too long", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPi.scala", "additions": "5", "deletions": "1", "changes": "6"}, "updated": [0, 0, 0]}, {"file": {"name": "examples/src/main/scala/org/apache/spark/examples/SparkPiWithoutAppName.scala", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Config.scala", "additions": "4", "deletions": "14", "changes": "18"}, "updated": [0, 3, 4]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala", "additions": "19", "deletions": "7", "changes": "26"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesUtils.scala", "additions": "11", "deletions": "1", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala", "additions": "10", "deletions": "6", "changes": "16"}, "updated": [0, 2, 2]}, {"file": {"name": "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterManager.scala", "additions": "10", "deletions": "2", "changes": "12"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/KubernetesConfSuite.scala", "additions": "9", "deletions": "0", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 2, 3]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/BasicTestsSuite.scala", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}]}
{"author": "ulysses-you", "sha": "a846ecd5221bc4b21416c9c52552cdaa0e683d0d", "commit_date": "2021/04/07 10:02:05", "commit_message": "Support coalesce partition through union", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "78", "deletions": "52", "changes": "130"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/CoalesceShufflePartitionsSuite.scala", "additions": "1", "deletions": "3", "changes": "4"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "86", "deletions": "1", "changes": "87"}, "updated": [0, 1, 5]}]}
{"author": "venkata91", "sha": "ac1659e156eca5899e1eff765698c9986eec5d4c", "commit_date": "2020/06/10 21:02:55", "commit_message": "Basic speculation metrics summary for a stage", "title": "", "body": "", "failed_tests": ["pyspark.mllib.tests.test_streaming_algorithms"], "files": [{"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/stagepage.js", "additions": "29", "deletions": "0", "changes": "29"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/stagespage-template.html", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/resources/org/apache/spark/ui/static/webui.css", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusListener.scala", "additions": "19", "deletions": "0", "changes": "19"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/AppStatusStore.scala", "additions": "11", "deletions": "0", "changes": "11"}, "updated": [1, 1, 3]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/LiveEntity.scala", "additions": "26", "deletions": "0", "changes": "26"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/api/v1/api.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/status/storeTypes.scala", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/application_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/completed_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/limit_app_list_json_expectation.json", "additions": "15", "deletions": "15", "changes": "30"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/minDate_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/minEndDate_app_list_json_expectation.json", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [0, 0, 1]}, {"file": {"name": "core/src/test/resources/HistoryServerExpectations/stage_with_speculation_summary_expectation.json", "additions": "507", "deletions": "0", "changes": "507"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/resources/spark-events/application_1628109047826_1317105", "additions": "52", "deletions": "0", "changes": "52"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/deploy/history/HistoryServerSuite.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [0, 1, 4]}, {"file": {"name": "core/src/test/scala/org/apache/spark/status/AppStatusListenerSuite.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/status/AppStatusStoreSuite.scala", "additions": "56", "deletions": "1", "changes": "57"}, "updated": [0, 0, 0]}, {"file": {"name": "core/src/test/scala/org/apache/spark/ui/StagePageSuite.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "dev/.rat-excludes", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 2]}]}
{"author": "dominikgehl", "sha": "5fc4307084a3cfe321d7e6fdb2e576bce8dab427", "commit_date": "2021/07/22 14:04:36", "commit_message": "expose localtimestamp in pyspark.sql.functions", "title": "[SPARK-36259] Expose localtimestamp in pyspark.sql.functions", "body": "\r\n\r\n### What changes were proposed in this pull request?\r\nExposing localtimestamp in pyspark.sql.functions\r\n\r\n\r\n### Why are the changes needed?\r\nWas previously only available in scala\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nnew localtimestamp in pyspark.sql.functions\r\n\r\n\r\n### How was this patch tested?\r\ntest added inline\r\n", "failed_tests": [], "files": [{"file": {"name": "python/docs/source/reference/pyspark.sql.rst", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [2, 3, 4]}, {"file": {"name": "python/pyspark/sql/functions.py", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 4, 7]}, {"file": {"name": "python/pyspark/sql/functions.pyi", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 1, 1]}, {"file": {"name": "python/pyspark/sql/tests/test_types.py", "additions": "2", "deletions": "2", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "python/pyspark/sql/types.py", "additions": "22", "deletions": "3", "changes": "25"}, "updated": [0, 0, 1]}]}
{"author": "wangyum", "sha": "65df34649227d1f04065cc76a49778770b433536", "commit_date": "2021/07/21 14:51:57", "commit_message": "Deduplicate the right side of left semi/anti join", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.jdbc.OracleIntegrationSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/DeduplicateRightSideOfLeftSemiAntiJoin.scala", "additions": "56", "deletions": "0", "changes": "56"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [2, 2, 13]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/Statistics.scala", "additions": "4", "deletions": "1", "changes": "5"}, "updated": [1, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/DeduplicateRightSideOfLeftSemiAntiJoinSuite.scala", "additions": "98", "deletions": "0", "changes": "98"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AQEOptimizer.scala", "additions": "3", "deletions": "1", "changes": "4"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/QueryStageExec.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q10.sf100/explain.txt", "additions": "119", "deletions": "103", "changes": "222"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-modified/q10.sf100/simplified.txt", "additions": "63", "deletions": "63", "changes": "126"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q10.sf100/explain.txt", "additions": "220", "deletions": "204", "changes": "424"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q10.sf100/simplified.txt", "additions": "48", "deletions": "48", "changes": "96"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a.sf100/explain.txt", "additions": "614", "deletions": "646", "changes": "1260"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a.sf100/simplified.txt", "additions": "143", "deletions": "185", "changes": "328"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b.sf100/explain.txt", "additions": "559", "deletions": "576", "changes": "1135"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b.sf100/simplified.txt", "additions": "151", "deletions": "184", "changes": "335"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q35.sf100/explain.txt", "additions": "230", "deletions": "214", "changes": "444"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q35.sf100/simplified.txt", "additions": "51", "deletions": "51", "changes": "102"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q69.sf100/explain.txt", "additions": "221", "deletions": "183", "changes": "404"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q69.sf100/simplified.txt", "additions": "56", "deletions": "60", "changes": "116"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q95.sf100/explain.txt", "additions": "134", "deletions": "92", "changes": "226"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q95.sf100/simplified.txt", "additions": "42", "deletions": "36", "changes": "78"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q10a.sf100/explain.txt", "additions": "199", "deletions": "183", "changes": "382"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q10a.sf100/simplified.txt", "additions": "42", "deletions": "42", "changes": "84"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14.sf100/explain.txt", "additions": "559", "deletions": "576", "changes": "1135"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14.sf100/simplified.txt", "additions": "151", "deletions": "184", "changes": "335"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a.sf100/explain.txt", "additions": "742", "deletions": "774", "changes": "1516"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a.sf100/simplified.txt", "additions": "158", "deletions": "200", "changes": "358"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35.sf100/explain.txt", "additions": "230", "deletions": "214", "changes": "444"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35.sf100/simplified.txt", "additions": "51", "deletions": "51", "changes": "102"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35a.sf100/explain.txt", "additions": "219", "deletions": "203", "changes": "422"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q35a.sf100/simplified.txt", "additions": "49", "deletions": "49", "changes": "98"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [1, 5, 17]}]}
{"author": "wangyum", "sha": "eb71b8ae8de1fb737eea170e920c24127bcc2b95", "commit_date": "2021/07/17 15:56:31", "commit_message": "Remove the aggregation from left semi/anti join if the same aggregation has already been done on left side", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RemoveRedundantAggregates.scala", "additions": "6", "deletions": "2", "changes": "8"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/DistinctAttributesVisitor.scala", "additions": "100", "deletions": "0", "changes": "100"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/LogicalPlan.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/LogicalPlanDistinctAttributes.scala", "additions": "33", "deletions": "0", "changes": "33"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/RemoveRedundantAggregatesSuite.scala", "additions": "110", "deletions": "11", "changes": "121"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/plans/logical/DistinctAttributesVisitorSuite.scala", "additions": "112", "deletions": "0", "changes": "112"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a.sf100/explain.txt", "additions": "460", "deletions": "481", "changes": "941"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a.sf100/simplified.txt", "additions": "113", "deletions": "118", "changes": "231"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a/explain.txt", "additions": "229", "deletions": "245", "changes": "474"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a/simplified.txt", "additions": "62", "deletions": "64", "changes": "126"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b.sf100/explain.txt", "additions": "393", "deletions": "414", "changes": "807"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b.sf100/simplified.txt", "additions": "106", "deletions": "111", "changes": "217"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b/explain.txt", "additions": "213", "deletions": "229", "changes": "442"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b/simplified.txt", "additions": "62", "deletions": "64", "changes": "126"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q38.sf100/explain.txt", "additions": "143", "deletions": "190", "changes": "333"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q38.sf100/simplified.txt", "additions": "112", "deletions": "125", "changes": "237"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q38/explain.txt", "additions": "79", "deletions": "106", "changes": "185"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q38/simplified.txt", "additions": "60", "deletions": "63", "changes": "123"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q87.sf100/explain.txt", "additions": "143", "deletions": "190", "changes": "333"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q87.sf100/simplified.txt", "additions": "112", "deletions": "125", "changes": "237"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q87/explain.txt", "additions": "79", "deletions": "106", "changes": "185"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q87/simplified.txt", "additions": "60", "deletions": "63", "changes": "123"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14.sf100/explain.txt", "additions": "393", "deletions": "414", "changes": "807"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14.sf100/simplified.txt", "additions": "106", "deletions": "111", "changes": "217"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14/explain.txt", "additions": "213", "deletions": "229", "changes": "442"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14/simplified.txt", "additions": "62", "deletions": "64", "changes": "126"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a.sf100/explain.txt", "additions": "602", "deletions": "623", "changes": "1225"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a.sf100/simplified.txt", "additions": "128", "deletions": "133", "changes": "261"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a/explain.txt", "additions": "279", "deletions": "295", "changes": "574"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a/simplified.txt", "additions": "62", "deletions": "64", "changes": "126"}, "updated": [0, 0, 3]}]}
{"author": "wangyum", "sha": "6339da03b69ac2d0dad37b6aee5d356a84dd92e2", "commit_date": "2021/07/15 08:26:12", "commit_message": "Eliminate join base uniqueness", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.optimizer.AggregateOptimizeSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala", "additions": "21", "deletions": "2", "changes": "23"}, "updated": [0, 1, 7]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/AggregateOptimizeSuite.scala", "additions": "38", "deletions": "9", "changes": "47"}, "updated": [0, 0, 2]}]}
{"author": "wangyum", "sha": "824ba80a0a71bb8c5079c45ec5de6bc9ae198699", "commit_date": "2021/07/10 12:43:25", "commit_message": "First commit", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.JoinSuite", "org.apache.spark.sql.streaming.StreamingOuterJoinSuite", "org.apache.spark.sql.execution.LogicalPlanTagInSparkPlanSuite", "org.apache.spark.sql.streaming.StreamingLeftSemiJoinSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/BroadcastJoinOuterJoinStreamSide.scala", "additions": "48", "deletions": "0", "changes": "48"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 1, 7]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 2, 9]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/BroadcastJoinOuterJoinStreamSideSuite.scala", "additions": "108", "deletions": "0", "changes": "108"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "4", "deletions": "2", "changes": "6"}, "updated": [0, 1, 6]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a.sf100/explain.txt", "additions": "634", "deletions": "533", "changes": "1167"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14a.sf100/simplified.txt", "additions": "97", "deletions": "70", "changes": "167"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b.sf100/explain.txt", "additions": "569", "deletions": "468", "changes": "1037"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q14b.sf100/simplified.txt", "additions": "90", "deletions": "63", "changes": "153"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23a/explain.txt", "additions": "275", "deletions": "245", "changes": "520"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23a/simplified.txt", "additions": "75", "deletions": "67", "changes": "142"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23b/explain.txt", "additions": "364", "deletions": "319", "changes": "683"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23b/simplified.txt", "additions": "93", "deletions": "81", "changes": "174"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q40/explain.txt", "additions": "123", "deletions": "108", "changes": "231"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q40/simplified.txt", "additions": "17", "deletions": "13", "changes": "30"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q72/explain.txt", "additions": "65", "deletions": "50", "changes": "115"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q72/simplified.txt", "additions": "13", "deletions": "9", "changes": "22"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q80/explain.txt", "additions": "462", "deletions": "417", "changes": "879"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q80/simplified.txt", "additions": "65", "deletions": "53", "changes": "118"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14.sf100/explain.txt", "additions": "569", "deletions": "468", "changes": "1037"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14.sf100/simplified.txt", "additions": "90", "deletions": "63", "changes": "153"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a.sf100/explain.txt", "additions": "778", "deletions": "677", "changes": "1455"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q14a.sf100/simplified.txt", "additions": "114", "deletions": "87", "changes": "201"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q72/explain.txt", "additions": "65", "deletions": "50", "changes": "115"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q72/simplified.txt", "additions": "13", "deletions": "9", "changes": "22"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q75/explain.txt", "additions": "614", "deletions": "455", "changes": "1069"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q75/simplified.txt", "additions": "108", "deletions": "66", "changes": "174"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q78/explain.txt", "additions": "320", "deletions": "275", "changes": "595"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q78/simplified.txt", "additions": "51", "deletions": "39", "changes": "90"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q80a/explain.txt", "additions": "532", "deletions": "487", "changes": "1019"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v2_7/q80a/simplified.txt", "additions": "73", "deletions": "61", "changes": "134"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala", "additions": "21", "deletions": "1", "changes": "22"}, "updated": [0, 3, 8]}]}
{"author": "wangyum", "sha": "f2d4d3ae0dd405f4bf62813bd849678f459776d6", "commit_date": "2020/03/26 04:56:44", "commit_message": "Repartition by dynamic partition columns before insert table", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala", "additions": "5", "deletions": "3", "changes": "8"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "13", "deletions": "0", "changes": "13"}, "updated": [1, 8, 47]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala", "additions": "101", "deletions": "1", "changes": "102"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/sources/RepartitionWritingDataSourceSuite.scala", "additions": "230", "deletions": "0", "changes": "230"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveSessionStateBuilder.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 4]}]}
{"author": "beliefer", "sha": "86ea00d27e9979d6057a39ac7c0711131a6b6c26", "commit_date": "2021/07/27 07:37:12", "commit_message": "Refactor first set of 20 query parsing errors to use error classes", "title": "", "body": "", "failed_tests": ["org.apache.spark.SparkThrowableSuite", "org.apache.spark.sql.execution.datasources.PruneFileSourcePartitionsSuite"], "files": [{"file": {"name": "core/src/main/resources/error/error-classes.json", "additions": "52", "deletions": "0", "changes": "52"}, "updated": [0, 2, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala", "additions": "0", "deletions": "2", "changes": "2"}, "updated": [0, 2, 22]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala", "additions": "22", "deletions": "29", "changes": "51"}, "updated": [0, 2, 7]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisTest.scala", "additions": "2", "deletions": "1", "changes": "3"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala", "additions": "8", "deletions": "3", "changes": "11"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/ErrorParserSuite.scala", "additions": "26", "deletions": "9", "changes": "35"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala", "additions": "15", "deletions": "8", "changes": "23"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/SparkScriptTransformationSuite.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/ShowPartitionsParserSuite.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/command/TruncateTableParserSuite.scala", "additions": "4", "deletions": "3", "changes": "7"}, "updated": [0, 0, 0]}]}
{"author": "beliefer", "sha": "b4f298e9a97ad4d3afd1b099cba1887efbcf734d", "commit_date": "2021/03/16 03:25:10", "commit_message": "Support the utils for transform number format", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/NumberUtils.scala", "additions": "190", "deletions": "0", "changes": "190"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/NumberUtilsSuite.scala", "additions": "273", "deletions": "0", "changes": "273"}, "updated": [0, 0, 0]}]}
{"author": "SaurabhChawla100", "sha": "4fab369b87791935e2c8739e69766ddc4a0c31d9", "commit_date": "2021/07/06 15:17:24", "commit_message": "refactor the code", "title": "[SPARK-36027][SQL] Add the code change to pushdown filter in case of typedFilter", "body": "### What changes were proposed in this pull request?\r\nIn case of Filter having child as TypedFilter, Pushdown of Filters does not take place\r\n\r\nscala> def testUdfFunction(r: String): Boolean = {\r\n | r.equals(\"hello\")\r\n | }\r\ntestUdfFunction: (r: String)Boolean\r\n\r\nval df= spark.read.parquet(\"/testDir/testParquetSize/Parquetgzip/\")\r\ndf: org.apache.spark.sql.DataFrame = [_1: string, _2: string ... 1 more field]\r\n\r\nBefore Fix \r\n```\r\ndf.filter(x => testUdfFunction(x.getAs(\"_1\"))).filter(\"_2<='id103855'\").queryExecution.executedPlan\r\n \r\nFilter (isnotnull(_2#1) AND (_2#1 <= id103855))\r\n+- *(1) Filter $line20.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$Lambda$3184/1455948476@5ce4af92.apply\r\n +- *(1) ColumnarToRow\r\n +- FileScan parquet [_1#0,_2#1,_3#2] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/testDir/testParquetSize/Parquetgzip], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<_1:string,_2:string,_3:string>\r\n \r\ndf.filter(x => testUdfFunction(x.getAs(\"_1\"))).filter(\"_2<='id103855'\").queryExecution.optimizedPlan\r\n\r\nFilter (isnotnull(_2#1) AND (_2#1 <= id103855))\r\n+- TypedFilter $line22.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$Lambda$3191/320569017@37a2806c, interface org.apache.spark.sql.Row, [StructField(_1,StringType,true), StructField(_2,StringType,true), StructField(_3,StringType,true)], createexternalrow(_1#0.toString, _2#1.toString, _3#2.toString, StructField(_1,StringType,true), StructField(_2,StringType,true), StructField(_3,StringType,true))\r\n +- Relation[_1#0,_2#1,_3#2] parquet\r\n\r\n```\r\n\r\nAfter fix\r\n```\r\ndf.filter(x => testUdfFunction(x.getAs(\"_1\"))).filter(\"_2<='id103855'\").queryExecution.executedPlan\r\n\r\n*(1) Filter $line16.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$Lambda$2966/426572525@4155e85d.apply\r\n+- *(1) Filter (isnotnull(_2#1) AND (_2#1 <= id103855))\r\n   +- *(1) ColumnarToRow\r\n      +- FileScan parquet [_1#0,_2#1,_3#2] Batched: true, DataFilters: [isnotnull(_2#1), (_2#1 <= id103855)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/testDir/testParquetSize/Parquetgzip], PartitionFilters: [], PushedFilters: [IsNotNull(_2), LessThanOrEqual(_2,id103855)], ReadSchema: struct<_1:string,_2:string,_3:string>\r\n\r\ndf.filter(x => testUdfFunction(x.getAs(\"_1\"))).filter(\"_2<='id103855'\").queryExecution.optimizedPlan\r\n\r\nTypedFilter $line18.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$Lambda$3445/1423649465@62065a8c, interface org.apache.spark.sql.Row, [StructField(_1,StringType,true), StructField(_2,StringType,true), StructField(_3,StringType,true)], createexternalrow(_1#0.toString, _2#1.toString, _3#2.toString, StructField(_1,StringType,true), StructField(_2,StringType,true), StructField(_3,StringType,true))\r\n+- Filter (isnotnull(_2#1) AND (_2#1 <= id103855))\r\n   +- Relation [_1#0,_2#1,_3#2] parquet\r\n```\r\n\r\n### Why are the changes needed?\r\nTill now when Filter is having child as TypedFilter, the filter is not pushed down, There is a need to add this code change for pushing down the filter.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo\r\n\r\n\r\n### How was this patch tested?\r\nAdded the unit test and tested on spark-shell", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "6", "deletions": "0", "changes": "6"}, "updated": [1, 5, 13]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/FilterPushdownSuite.scala", "additions": "25", "deletions": "0", "changes": "25"}, "updated": [0, 0, 0]}]}
{"author": "peter-toth", "sha": "7786c0ccb8d7b716955de0a1e59acc822f0cb00c", "commit_date": "2021/07/09 11:57:23", "commit_message": "[SPARK-36073][SQL] SubExpr elimination should include common child exprs of conditional expressions", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala", "additions": "79", "deletions": "39", "changes": "118"}, "updated": [0, 1, 5]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/SubexpressionEliminationSuite.scala", "additions": "28", "deletions": "0", "changes": "28"}, "updated": [0, 3, 7]}]}
{"author": "peter-toth", "sha": "d86d2c48a3e6244fc091ae09cb9377ade98f66b0", "commit_date": "2021/04/21 17:40:19", "commit_message": "[SPARK-34079][SQL] Merging non-correlated scalar subqueries to multi-column scalar subqueries for better reuse", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueries.scala", "additions": "416", "deletions": "0", "changes": "416"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala", "additions": "32", "deletions": "1", "changes": "33"}, "updated": [2, 8, 16]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreePatterns.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [1, 3, 3]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueriesSuite.scala", "additions": "282", "deletions": "0", "changes": "282"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap.java", "additions": "2", "deletions": "7", "changes": "9"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "2", "deletions": "0", "changes": "2"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala", "additions": "8", "deletions": "3", "changes": "11"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/PlanAdaptiveSubqueries.scala", "additions": "18", "deletions": "6", "changes": "24"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/ReuseAdaptiveSubquery.scala", "additions": "5", "deletions": "3", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala", "additions": "3", "deletions": "2", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala", "additions": "2", "deletions": "8", "changes": "10"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala", "additions": "0", "deletions": "9", "changes": "9"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala", "additions": "21", "deletions": "0", "changes": "21"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/reuse/ReuseExchangeAndSubquery.scala", "additions": "10", "deletions": "3", "changes": "13"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala", "additions": "18", "deletions": "2", "changes": "20"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9.sf100/explain.txt", "additions": "183", "deletions": "598", "changes": "781"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9.sf100/simplified.txt", "additions": "65", "deletions": "170", "changes": "235"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9/explain.txt", "additions": "183", "deletions": "598", "changes": "781"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q9/simplified.txt", "additions": "65", "deletions": "170", "changes": "235"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala", "additions": "72", "deletions": "0", "changes": "72"}, "updated": [1, 3, 10]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala", "additions": "37", "deletions": "0", "changes": "37"}, "updated": [1, 2, 4]}]}
{"author": "Peng-Lei", "sha": "2076413f2caea4ce49116e1e93fcee87d8df9121", "commit_date": "2021/06/29 11:23:43", "commit_message": "Add support YearMonthIntervalType for width_bucket", "title": "[SPARK-35926][SQL] Add support YearMonthIntervalType for width_bucket", "body": "### What changes were proposed in this pull request?\r\n1. The function `width_bucket` is introduced from [SPARK-21117](https://issues.apache.org/jira/browse/SPARK-21117)\r\n2. The YearMonthIntervalType is just store the value with Int.\r\n3. Modify the `inputType` to allow the width_bucket to support the YearMonthIntervalType\r\n\r\n### Why are the changes needed?\r\n[35926](https://issues.apache.org/jira/browse/SPARK-35926)\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes. The user can use `width_bucket` with Period type.\r\n\r\n\r\n### How was this patch tested?\r\nAdd ut test\r\n", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/mathExpressions.scala", "additions": "30", "deletions": "7", "changes": "37"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/MathExpressionsSuite.scala", "additions": "16", "deletions": "0", "changes": "16"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/MathFunctionsSuite.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 1]}]}
{"author": "Peng-Lei", "sha": "67d3622b9f836b39da812259ad5d284d5e62c233", "commit_date": "2021/07/14 05:59:17", "commit_message": "Keep consistent with the namespace naming rule", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite", "org.apache.spark.sql.hive.execution.SQLQuerySuite", "org.apache.spark.sql.hive.MultiDatabaseSuite", "org.apache.spark.sql.SQLQuerySuite", "org.apache.spark.sql.execution.command.InMemoryCatalogedDDLSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/CatalogManager.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [2, 7, 16]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 1, 2]}]}
{"author": "Peng-Lei", "sha": "759dd0a179e3a38e6bc8c3b692ac82df0c7ef8bb", "commit_date": "2021/07/01 13:42:53", "commit_message": "Add the show catalogs feature", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.catalyst.SQLKeywordSuite"], "files": [{"file": {"name": "docs/sql-ref-ansi-compliance.md", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 0, 5]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveCatalogs.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [1, 3, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala", "additions": "8", "deletions": "0", "changes": "8"}, "updated": [1, 5, 15]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/statements.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [1, 3, 4]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/v2Commands.scala", "additions": "14", "deletions": "0", "changes": "14"}, "updated": [1, 3, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/CatalogManager.scala", "additions": "4", "deletions": "0", "changes": "4"}, "updated": [0, 1, 1]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala", "additions": "15", "deletions": "0", "changes": "15"}, "updated": [1, 3, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala", "additions": "3", "deletions": "0", "changes": "3"}, "updated": [1, 2, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ShowCatalogsExec.scala", "additions": "47", "deletions": "0", "changes": "47"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala", "additions": "21", "deletions": "0", "changes": "21"}, "updated": [1, 1, 1]}]}
{"author": "maropu", "sha": "3be38824fec054d1556c7e347def0b54413e563d", "commit_date": "2021/03/09 14:23:38", "commit_message": "MapType supports comparable/orderable semantics", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeMapData.java", "additions": "18", "deletions": "0", "changes": "18"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala", "additions": "0", "deletions": "16", "changes": "16"}, "updated": [0, 2, 7]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala", "additions": "27", "deletions": "0", "changes": "27"}, "updated": [1, 2, 3]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ordering.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeFloatingNumbers.scala", "additions": "22", "deletions": "4", "changes": "26"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeMaps.scala", "additions": "192", "deletions": "0", "changes": "192"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala", "additions": "8", "deletions": "2", "changes": "10"}, "updated": [1, 7, 9]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/ArrayBasedMapData.scala", "additions": "17", "deletions": "0", "changes": "17"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/MapData.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/TypeUtils.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/types/ArrayType.scala", "additions": "1", "deletions": "0", "changes": "1"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/types/MapType.scala", "additions": "28", "deletions": "0", "changes": "28"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala", "additions": "15", "deletions": "25", "changes": "40"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/ExpressionTypeCheckingSuite.scala", "additions": "2", "deletions": "17", "changes": "19"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/PredicateSuite.scala", "additions": "0", "deletions": "8", "changes": "8"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeFloatingPointNumbersSuite.scala", "additions": "63", "deletions": "1", "changes": "64"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NormalizeMapsSuite.scala", "additions": "108", "deletions": "0", "changes": "108"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/ComplexDataSuite.scala", "additions": "22", "deletions": "16", "changes": "38"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala", "additions": "7", "deletions": "32", "changes": "39"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala", "additions": "33", "deletions": "0", "changes": "33"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/comparable-map-explain.sql", "additions": "34", "deletions": "0", "changes": "34"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/inputs/comparable-map.sql", "additions": "182", "deletions": "0", "changes": "182"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/comparable-map-explain.sql.out", "additions": "130", "deletions": "0", "changes": "130"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/comparable-map.sql.out", "additions": "986", "deletions": "0", "changes": "986"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/pivot.sql.out", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/resources/sql-tests/results/udf/udf-pivot.sql.out", "additions": "6", "deletions": "6", "changes": "12"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameAggregateSuite.scala", "additions": "8", "deletions": "10", "changes": "18"}, "updated": [0, 0, 2]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameFunctionsSuite.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 0, 4]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSetOperationsSuite.scala", "additions": "0", "deletions": "19", "changes": "19"}, "updated": [0, 2, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala", "additions": "5", "deletions": "0", "changes": "5"}, "updated": [0, 0, 7]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/sources/BucketedWriteSuite.scala", "additions": "0", "deletions": "5", "changes": "5"}, "updated": [0, 0, 0]}]}
{"author": "zhengruifeng", "sha": "a9888ba9458d012ab71644bc0808e8123955a824", "commit_date": "2021/07/12 06:18:19", "commit_message": "init\n\ninit\n\ninit", "title": "[SPARK-36087][SQL][WIP] An Impl of skew key detection and data inflation optimization", "body": "### What changes were proposed in this pull request?\r\n1, introduce `ShuffleExecAccumulator` in `ShuffleExchangeExec` to support arbitrary statistics;\r\n\r\n2, impl a key sampling `ShuffleExecAccumulator` to detect skew keys and show debug info on SparkUI;\r\n\r\n3, in `OptimizeSkewedJoin`, estimate the joined size of each partition based on the sampled keys, and split a partition if it is not split yet and its estimated joined size is too larger.\r\n\r\n\r\n### Why are the changes needed?\r\n1, make it easy to add a new statistics which can be used in AQE rules;\r\n2, showing skew info on sparkUI is usefully;\r\n3, spliting partitions based on joined size can resolve data inflation;\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes, new features are added\r\n\r\n\r\n### How was this patch tested?\r\nadded testsuites\r\n", "failed_tests": ["org.apache.spark.sql.TPCDSModifiedPlanStabilityWithStatsSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "36", "deletions": "0", "changes": "36"}, "updated": [0, 8, 16]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/CoalesceShufflePartitions.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 1, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeDataInflation.scala", "additions": "125", "deletions": "0", "changes": "125"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeLocalShuffleReader.scala", "additions": "3", "deletions": "3", "changes": "6"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/OptimizeSkewedJoin.scala", "additions": "58", "deletions": "15", "changes": "73"}, "updated": [0, 2, 4]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala", "additions": "7", "deletions": "2", "changes": "9"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala", "additions": "79", "deletions": "1", "changes": "80"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExecAccumulator.scala", "additions": "552", "deletions": "0", "changes": "552"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/ShuffleExecAccumulatorSuite.scala", "additions": "155", "deletions": "0", "changes": "155"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala", "additions": "137", "deletions": "1", "changes": "138"}, "updated": [0, 8, 16]}]}
{"author": "jerqi", "sha": "8e2f4fb0b48cb96cc7b6a774269022f2bd2a3882", "commit_date": "2021/08/05 08:33:30", "commit_message": "[SPARK-36223][SQL][TEST] Cover 3 kinds of join in the TPCDSQueryTestSuite", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/TPCDSQueryTestSuite.scala", "additions": "40", "deletions": "4", "changes": "44"}, "updated": [0, 0, 0]}]}
{"author": "Swinky", "sha": "1e8eb5165a7abb1df04f3dc1408e47c9648fd4db", "commit_date": "2021/06/27 18:35:04", "commit_message": "use existing exprId for subquery", "title": "", "body": "", "failed_tests": [], "files": [{"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/dynamicpruning/PlanDynamicPruningFilters.scala", "additions": "1", "deletions": "1", "changes": "2"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/DynamicPartitionPruningSuite.scala", "additions": "24", "deletions": "1", "changes": "25"}, "updated": [0, 1, 4]}]}
{"author": "shipra-a", "sha": "a7e97ddf3c46641d9c8c298d09c4c4e4ac6024ae", "commit_date": "2021/06/09 01:35:41", "commit_message": "https://github.com/apache/spark/pull/28804/commits\n\nCo-authored-by: Karuppayya Rajendran <karuppayya1990@gmail.com>", "title": "", "body": "", "failed_tests": ["org.apache.spark.sql.TPCDSQueryTestSuite"], "files": [{"file": {"name": "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala", "additions": "42", "deletions": "0", "changes": "42"}, "updated": [0, 3, 14]}, {"file": {"name": "sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap.java", "additions": "13", "deletions": "0", "changes": "13"}, "updated": [0, 0, 0]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala", "additions": "206", "deletions": "49", "changes": "255"}, "updated": [0, 1, 2]}, {"file": {"name": "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashMapGenerator.scala", "additions": "10", "deletions": "0", "changes": "10"}, "updated": [0, 0, 1]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/WholeStageCodegenSuite.scala", "additions": "37", "deletions": "0", "changes": "37"}, "updated": [0, 0, 3]}, {"file": {"name": "sql/core/src/test/scala/org/apache/spark/sql/execution/metric/SQLMetricsSuite.scala", "additions": "45", "deletions": "41", "changes": "86"}, "updated": [0, 1, 3]}, {"file": {"name": "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/AggregationQuerySuite.scala", "additions": "34", "deletions": "27", "changes": "61"}, "updated": [0, 0, 0]}]}
